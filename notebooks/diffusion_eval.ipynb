{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_38' (/home/abhinav/Documents/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /home/abhinav/Documents/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n",
      "PyTorch version 2.4.1+cu121\n",
      "Device count 2\n",
      "/home/abhinav/Documents/isaacgym/python/isaacgym/_bindings/src/gymtorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/abhinav/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/abhinav/.cache/torch_extensions/py38_cu121/gymtorch/build.ninja...\n",
      "Building extension module gymtorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module gymtorch...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from isaac_victor_envs.utils import get_assets_dir\n",
    "from isaac_victor_envs.tasks.allegro import AllegroScrewdriverTurningEnv\n",
    "# from isaac_victor_envs.tasks.allegro_ros import RosAllegroValveTurningEnv\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scipy\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import yaml\n",
    "import pathlib\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "import pytorch_volumetric as pv\n",
    "import pytorch_kinematics as pk\n",
    "import pytorch_kinematics.transforms as tf\n",
    "from torch.func import vmap, jacrev, hessian, jacfwd\n",
    "# import pytorch3d.transforms as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ccai.utils.allegro_utils import *\n",
    "# from allegro_valve_roll import AllegroValveTurning, AllegroContactProblem, PositionControlConstrainedSVGDMPC, \\\n",
    "#    add_trajectories, add_trajectories_hardware\n",
    "\n",
    "from ccai.allegro_contact import AllegroManipulationProblem, PositionControlConstrainedSVGDMPC, add_trajectories, \\\n",
    "    add_trajectories_hardware\n",
    "from ccai.allegro_screwdriver_problem_diffusion import AllegroScrewdriverDiff\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# from ccai.mpc.ipopt import IpoptMPC\n",
    "# from ccai.problem import IpoptProblem\n",
    "from ccai.models.trajectory_samplers import TrajectorySampler\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to PVD\n",
      "Physics Engine: PhysX\n",
      "Physics Device: cpu\n",
      "GPU Pipeline: disabled\n",
      "Using VHACD cache directory '/home/abhinav/.isaacgym/vhacd'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/allegro/base_ns.stl'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/allegro/link_1.0.stl'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/ft_c.stl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinav/Documents/github/pytorch_volumetric/src/pytorch_volumetric/sdf.py:1307: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cache = torch.load(dbpath)\n"
     ]
    }
   ],
   "source": [
    "obj_dof = 3\n",
    "# config = yaml.safe_load(pathlib.Path(f'../examples/config/{sys.argv[1]}.yaml').read_text())\n",
    "config = yaml.safe_load(pathlib.Path(f'../examples/config/allegro_screwdriver_csvto_OOD_ID_test.yaml').read_text())\n",
    "config['visualize'] = False\n",
    "\n",
    "if config['mode'] == 'hardware':\n",
    "    env = RosAllegroValveTurningEnv(1, control_mode='joint_impedance',\n",
    "                                    use_cartesian_controller=False,\n",
    "                                    viewer=True,\n",
    "                                    steps_per_action=60,\n",
    "                                    friction_coefficient=1.0,\n",
    "                                    device=config['sim_device'],\n",
    "                                    valve=config['object_type'],\n",
    "                                    video_save_path=img_save_dir,\n",
    "                                    joint_stiffness=config['kp'],\n",
    "                                    fingers=config['fingers'],\n",
    "                                    )\n",
    "else:\n",
    "    if not config['visualize']:\n",
    "        img_save_dir = None\n",
    "\n",
    "    env = AllegroScrewdriverTurningEnv(1, control_mode='joint_impedance',\n",
    "                                        use_cartesian_controller=False,\n",
    "                                        viewer=config['visualize'],\n",
    "                                        steps_per_action=60,\n",
    "                                        friction_coefficient=config['friction_coefficient'] * 1.05,\n",
    "                                        # friction_coefficient=1.0,  # DEBUG ONLY, set the friction very high\n",
    "                                        device=config['sim_device'],\n",
    "                                        video_save_path=img_save_dir,\n",
    "                                        joint_stiffness=config['kp'],\n",
    "                                        fingers=config['fingers'],\n",
    "                                        )\n",
    "\n",
    "sim, gym, viewer = env.get_sim()\n",
    "\n",
    "state = env.get_state()\n",
    "# try:\n",
    "#     while True:\n",
    "#         start = env.get_state()['q'][:, :-1]\n",
    "#         env.step(start)\n",
    "#         print('waiting for you to finish camera adjustment, ctrl-c when done')\n",
    "#         time.sleep(0.1)\n",
    "# except KeyboardInterrupt:\n",
    "#     pass\n",
    "\n",
    "sim_env = None\n",
    "ros_copy_node = None\n",
    "if config['mode'] == 'hardware':\n",
    "    sim_env = env\n",
    "    from hardware.hardware_env import HardwareEnv\n",
    "\n",
    "    env = HardwareEnv(sim_env.default_dof_pos[:, :16], finger_list=['index', 'thumb'], kp=config['kp'])\n",
    "    env.world_trans = sim_env.world_trans\n",
    "    env.joint_stiffness = sim_env.joint_stiffness\n",
    "    env.device = sim_env.device\n",
    "    env.valve_pose = sim_env.valve_pose\n",
    "elif config['mode'] == 'hardware_copy':\n",
    "    from hardware.hardware_env import RosNode\n",
    "\n",
    "    ros_copy_node = RosNode()\n",
    "\n",
    "results = {}\n",
    "\n",
    "# set up the kinematic chain\n",
    "asset = f'{get_assets_dir()}/xela_models/allegro_hand_right.urdf'\n",
    "ee_names = {\n",
    "    'index': 'allegro_hand_hitosashi_finger_finger_0_aftc_base_link',\n",
    "    'middle': 'allegro_hand_naka_finger_finger_1_aftc_base_link',\n",
    "    'ring': 'allegro_hand_kusuri_finger_finger_2_aftc_base_link',\n",
    "    'thumb': 'allegro_hand_oya_finger_3_aftc_base_link',\n",
    "}\n",
    "config['ee_names'] = ee_names\n",
    "config['obj_dof'] = 3\n",
    "\n",
    "screwdriver_asset = f'{get_assets_dir()}/screwdriver/screwdriver.urdf'\n",
    "\n",
    "chain = pk.build_chain_from_urdf(open(asset).read())\n",
    "screwdriver_chain = pk.build_chain_from_urdf(open(screwdriver_asset).read())\n",
    "frame_indices = [chain.frame_to_idx[ee_names[finger]] for finger in config['fingers']]  # combined chain\n",
    "frame_indices = torch.tensor(frame_indices)\n",
    "state2ee_pos = partial(state2ee_pos, fingers=config['fingers'], chain=chain, frame_indices=frame_indices,\n",
    "                        world_trans=env.world_trans)\n",
    "\n",
    "forward_kinematics = partial(chain.forward_kinematics,\n",
    "                                frame_indices=frame_indices)  # full_to= _partial_state = partial(full_to_partial_state, fingers=config['fingers'])\n",
    "partial_to_full_state = partial(partial_to_full_state, fingers=config['fingers'])\n",
    "\n",
    "controller = 'csvgd'\n",
    "goal = - 0.5 * torch.tensor([0, 0, np.pi])\n",
    "# set up params\n",
    "params = config.copy()\n",
    "params.pop('controllers')\n",
    "params.update(config['controllers'][controller])\n",
    "params['controller'] = controller\n",
    "params['valve_goal'] = goal.to(device=params['device'])\n",
    "params['chain'] = chain.to(device=params['device'])\n",
    "object_location = torch.tensor([0, 0, 1.205]).to(\n",
    "    params['device'])  # TODO: confirm if this is the correct location\n",
    "params['object_location'] = object_location\n",
    "\n",
    "num_fingers = len(params['fingers'])\n",
    "state = env.get_state()\n",
    "start = state['q'].reshape(4 * num_fingers + 4).to(device=params['device'])\n",
    "if 'csvgd' in params['controller']:\n",
    "    # index finger is used for stability\n",
    "    if 'index' in params['fingers']:\n",
    "        fingers = params['fingers']\n",
    "    else:\n",
    "        fingers = ['index'] + params['fingers']\n",
    "\n",
    "pregrasp_problem = AllegroScrewdriverDiff(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    regrasp_fingers=fingers,\n",
    "    contact_fingers=[],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    ")\n",
    "# finger gate index\n",
    "index_regrasp_problem = AllegroScrewdriverDiff(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    regrasp_fingers=['index'],\n",
    "    contact_fingers=['middle', 'thumb'],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    "    default_dof_pos=env.default_dof_pos[:, :16]\n",
    ")\n",
    "thumb_and_middle_regrasp_problem = AllegroScrewdriverDiff(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    contact_fingers=['index'],\n",
    "    regrasp_fingers=['middle', 'thumb'],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    "    default_dof_pos=env.default_dof_pos[:, :16]\n",
    ")\n",
    "turn_problem = AllegroScrewdriverDiff(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    contact_fingers=['index', 'middle', 'thumb'],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    "    default_dof_pos=env.default_dof_pos[:, :16]\n",
    ")\n",
    "contact_mode_dict = {0: 'pregrasp', 2: 'index', 1: 'thumb_middle', 3: 'turn'}\n",
    "t = params['T']\n",
    "# with open(data_path / f'constraint_violations.p', 'wb') as f:\n",
    "#     pickle.dump(constraint_violations_all, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_info(c_state, to_eval, info_list, pregrasp_problem, index_regrasp_problem, thumb_and_middle_regrasp_problem, turn_problem, plans_or_inits, proj_diff=True):\n",
    "    if not torch.is_tensor(to_eval):\n",
    "        to_eval = torch.tensor(to_eval).to(device=params['device'])\n",
    "    else:\n",
    "        to_eval = to_eval.to(device=params['device'])\n",
    "    if c_state == 'pregrasp':\n",
    "        # Call _con_eq and _con_ineq to get constraint violation\n",
    "        prob = pregrasp_problem\n",
    "        inds = torch.arange(27)\n",
    "\n",
    "    elif c_state == 'index':\n",
    "        prob = index_regrasp_problem\n",
    "        # Need to rearrange (index, middle, thumb)\n",
    "        if plans_or_inits == 'plans':\n",
    "            inds = torch.cat((torch.arange(27), torch.arange(30, 36)))\n",
    "        else:\n",
    "            inds = torch.arange(33)\n",
    "\n",
    "    elif c_state == 'thumb_middle':\n",
    "        prob = thumb_and_middle_regrasp_problem\n",
    "        inds = torch.arange(30)\n",
    "\n",
    "    elif c_state == 'turn':\n",
    "        prob = turn_problem\n",
    "        inds = torch.arange(36)\n",
    "\n",
    "    prob._preprocess(to_eval, projected_diffusion=proj_diff)\n",
    "    g, _, _ = prob._con_eq(to_eval[..., inds], compute_grads=False, compute_hess=False, verbose=True, projected_diffusion=proj_diff)\n",
    "    h, _, _ = prob._con_ineq(to_eval[..., inds], compute_grads=False, compute_hess=False, verbose=True, projected_diffusion=proj_diff)\n",
    "    if g is not None:\n",
    "        g = g.cpu()\n",
    "    if h is not None:\n",
    "        h = h.cpu()\n",
    "\n",
    "    info = {'g': g, 'h': h, 'c_state': c_state}\n",
    "    info_list.append(info)\n",
    "    return info_list\n",
    "\n",
    "def gen_constraint_data(plans_or_inits, constraint_violations, path, traj_data=[None]*10):\n",
    "    for trial in tqdm(range(1, 11)):\n",
    "        info_list = []\n",
    "        try:\n",
    "            if traj_data[trial-1] is None:\n",
    "                try:\n",
    "                    with open(path + f'/trial_{trial}/traj_data.p', 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            data = traj_data[trial-1]\n",
    "        if plans_or_inits == 'plans':\n",
    "            data[t]['plans'] = torch.tensor(data[t]['plans']).to(device=params['device'])\n",
    "            loop_range = data[t]['plans'].shape[0]\n",
    "        elif plans_or_inits == 'optimizer_paths':\n",
    "            loop_range = len(data[t]['optimizer_paths'])\n",
    "        elif plans_or_inits == 'planned_inits':\n",
    "            loop_range = len(data[t]['planned_inits'])\n",
    "        else:\n",
    "            loop_range = len(data[t]['inits'])\n",
    "        for i in (range(loop_range)):\n",
    "            c_state = contact_mode_dict[data[t]['contact_state'][i].sum().item()]\n",
    "            to_eval = data[t][plans_or_inits][i]\n",
    "            # if plans_or_inits == 'optimizer_paths':\n",
    "            #     to_eval = to_eval[0].flatten(0, 1)\n",
    "            # info_list = calc_info(c_state, to_eval, info_list, pregrasp_problem, index_regrasp_problem, thumb_and_middle_regrasp_problem, turn_problem, plans_or_inits)\n",
    "            if plans_or_inits in {'plans', 'inits', 'traj'}:\n",
    "                info_list = calc_info(c_state, to_eval, info_list, pregrasp_problem, index_regrasp_problem, thumb_and_middle_regrasp_problem, turn_problem, plans_or_inits, proj_diff=False)\n",
    "            else:\n",
    "                to_eval = to_eval[0]\n",
    "                all_infos = []\n",
    "                for csvto_tstep in (range(data[t]['optimizer_paths'][i][0].shape[0])):\n",
    "                    all_infos = calc_info(c_state, to_eval[csvto_tstep], all_infos, pregrasp_problem, index_regrasp_problem, thumb_and_middle_regrasp_problem, turn_problem, plans_or_inits, proj_diff=False)\n",
    "                info_list.append(all_infos)\n",
    "        constraint_violations.append(info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj(name):\n",
    "    path = f'/home/abhinav/Documents/ccai/data/experiments/{name}/csvgd'\n",
    "\n",
    "    all_data = []\n",
    "    all_x = []\n",
    "    # all_d2goal = []\n",
    "    all_traj_data = []\n",
    "    for trial_num in range(1, 454):\n",
    "        # print(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "        # if 'rand' in name or 'proj' in name or 'diff' in name:\n",
    "        try:\n",
    "            with open(path + f'/trial_{trial_num}/trajectory.pkl', 'rb') as data:\n",
    "                d = pickle.load(data)\n",
    "                traj = np.concatenate((d[:-1]), axis=0)\n",
    "                end = d[-1].reshape(1, -1)\n",
    "                end = np.concatenate((end, np.zeros((1, 21))), axis=1)\n",
    "                traj = np.concatenate((traj, end), axis=0)\n",
    "            # else:\n",
    "            #     d = np.load(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "            #     end_state = d['x'] \n",
    "            with open(path + f'/trial_{trial_num}/traj_data.p', 'rb') as f:\n",
    "                traj_data = pickle.load(f)\n",
    "                for key in traj_data.keys():\n",
    "                    if torch.is_tensor(traj_data[key]):\n",
    "                        traj_data[key] = traj_data[key].cpu().numpy()\n",
    "                # all_d2goal.append(d2goal)\n",
    "                # if 'rand' not in name and 'proj' not in name and 'diff' not in name::\n",
    "                #     traj = traj_data[t]['plans'][:, 0]\n",
    "\n",
    "                    # end = np.concatenate((end_state, np.zeros((1, 21))), axis=1)\n",
    "                    # traj = np.concatenate((traj, end), axis=0)\n",
    "                traj_data[t]['traj'] = np.expand_dims(traj, axis=1)\n",
    "                all_traj_data.append(traj_data)  \n",
    "                all_data.append(traj_data)\n",
    "        except:\n",
    "            from collections import defaultdict\n",
    "            traj_data = defaultdict(list)\n",
    "            all_traj_data.append(traj_data)  \n",
    "            all_data.append(traj_data)\n",
    "            \n",
    "    constraint_violations_all = {\n",
    "        # 'optimizer_paths': [],\n",
    "        'traj': [],\n",
    "        # 'inits': [],\n",
    "        # 'plans': [],\n",
    "\n",
    "    }\n",
    "    # for plans_or_inits in constraint_violations_all.keys():\n",
    "    #     if plans_or_inits == 'traj':\n",
    "    #         gen_constraint_data(plans_or_inits, constraint_violations_all[plans_or_inits], path, traj_data=all_traj_data)\n",
    "    #     else:\n",
    "    #         gen_constraint_data(plans_or_inits, constraint_violations_all[plans_or_inits], path)\n",
    "\n",
    "    # Take the list of dicts and turn it into a dict of lists\n",
    "    all_data = {k: [d[k] for d in all_data] for k in all_data[0]}\n",
    "    # all_data = {}\n",
    "    # for k in all_data[0]:\n",
    "    #     print(k)\n",
    "    #     # all_data[k] = [d[k] for d in all_data]\n",
    "    all_data['violation'] = constraint_violations_all\n",
    "\n",
    "    final_likelihood = []\n",
    "    one_step_recovery_success = []\n",
    "    eventual_recovery_success = []\n",
    "    final_likelihood = [l for l in all_data['final_likelihoods']]\n",
    "    all_delta_l_recovery = []\n",
    "    num_recovery_attempts_to_recover = []\n",
    "    final_likelihood_flat = []\n",
    "    dropped = all_data['dropped']\n",
    "    dropped_recovery = all_data['dropped_recovery']\n",
    "\n",
    "\n",
    "    one_step_recovery_success, eventual_recovery_success, num_recovery_attempts_to_recover, all_delta_l_recovery, dropped_recovery = calculate_recovery_metrics(\n",
    "        final_likelihood, \n",
    "        dropped_info=dropped_recovery\n",
    "    )\n",
    "\n",
    "    # Store the recovery blocks for this trial\n",
    "    if 'recovery_blocks' not in all_data:\n",
    "        all_data['recovery_blocks'] = []\n",
    "\n",
    "    # Add dropped_recovery to all_data\n",
    "    all_data['dropped_recovery'] = dropped_recovery\n",
    "\n",
    "    all_data['drop_pct'] = np.mean(dropped_recovery)\n",
    "\n",
    "    # Calculate overall recovery success rates\n",
    "    recovery_block_count: int = sum(len(blocks) for blocks in all_data['recovery_blocks'])\n",
    "    successful_recovery_count: int = sum(\n",
    "        sum(1 for block in blocks if block['success']) \n",
    "        for blocks in all_data['recovery_blocks']\n",
    "    )\n",
    "\n",
    "    if recovery_block_count > 0:\n",
    "        all_data['overall_recovery_success_rate'] = successful_recovery_count / recovery_block_count\n",
    "    else:\n",
    "        all_data['overall_recovery_success_rate'] = float('nan')\n",
    "\n",
    "    # Calculate average duration of successful and unsuccessful recovery blocks\n",
    "    successful_durations: List[int] = [\n",
    "        block['duration'] \n",
    "        for blocks in all_data['recovery_blocks'] \n",
    "        for block in blocks if block['success']\n",
    "    ]\n",
    "    unsuccessful_durations: List[int] = [\n",
    "        block['duration'] \n",
    "        for blocks in all_data['recovery_blocks'] \n",
    "        for block in blocks if not block['success']\n",
    "    ]\n",
    "\n",
    "    all_data['avg_successful_recovery_duration'] = (\n",
    "        np.mean(successful_durations) if successful_durations else float('nan')\n",
    "    )\n",
    "    all_data['avg_unsuccessful_recovery_duration'] = (\n",
    "        np.mean(unsuccessful_durations) if unsuccessful_durations else float('nan')\n",
    "    )\n",
    "\n",
    "    all_data['delta_l_recovery'] = (all_delta_l_recovery)\n",
    "    all_data['one_step_recovery_success'] = (one_step_recovery_success)\n",
    "    all_data['eventual_recovery_success'] = (eventual_recovery_success)\n",
    "    all_data['num_recovery_attempts_to_recover'] = (num_recovery_attempts_to_recover)\n",
    "    all_data['final_likelihood'] = (final_likelihood_flat)\n",
    "    all_data['dropped'] = (dropped)\n",
    "    all_data['dropped_recovery'] = (dropped_recovery)\n",
    "    # all_data['dropped_or_succeeded'] = (dropped_or_succeeded)\n",
    "\n",
    "    # Compute and store recovery statistics\n",
    "    recovery_stats = compute_recovery_statistics(\n",
    "        all_data,\n",
    "        one_step_recovery_success,\n",
    "        eventual_recovery_success,\n",
    "        num_recovery_attempts_to_recover\n",
    "    )\n",
    "\n",
    "    all_data.update(recovery_stats)\n",
    "\n",
    "    return all_x, all_data#, all_d2goal\n",
    "\n",
    "def calculate_recovery_metrics(\n",
    "    final_likelihood: List[List[float]], \n",
    "    dropped_info: List[bool] = None,\n",
    "    threshold: float = -175.0\n",
    ") -> Tuple[List[int], List[int], List[int], List[float], List[bool]]:\n",
    "    \"\"\"\n",
    "    Calculate recovery metrics from likelihood time series data.\n",
    "    \n",
    "    Args:\n",
    "        final_likelihood: Nested list of likelihood values per trial and step\n",
    "        dropped_info: Optional list of booleans indicating if the object was dropped\n",
    "        threshold: Threshold below which state is considered in need of recovery\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - one_step_recovery_success: Binary indicators of one-step recovery success\n",
    "            - eventual_recovery_success: Binary indicators of eventual recovery success\n",
    "            - num_recovery_attempts_to_recover: Number of attempts needed for recovery\n",
    "            - recovery_improvement: Improvement in likelihood during recovery\n",
    "            - dropped_recovery: Indicators if recovery ended due to dropping\n",
    "    \"\"\"\n",
    "    one_step_recovery_success = []\n",
    "    eventual_recovery_success = []\n",
    "    num_recovery_attempts_to_recover = []\n",
    "    recovery_improvement = []\n",
    "    dropped_recovery = []\n",
    "\n",
    "    \n",
    "    for trial_idx, trial in enumerate(final_likelihood):\n",
    "        trial = [l for l in trial if None not in l and len(l) > 0]\n",
    "\n",
    "        np_trial = np.array(trial)\n",
    "        recovery_blocks = []\n",
    "        is_in_recovery_block = False\n",
    "        current_block = {}\n",
    "        \n",
    "        # Check if dropped_info is available for this trial\n",
    "        trial_dropped = dropped_info[trial_idx] if dropped_info and trial_idx < len(dropped_info) else False\n",
    "        \n",
    "        for step_idx in range(np_trial.shape[0]):\n",
    "            likelihood = np_trial[step_idx]\n",
    "            \n",
    "            # Start of a recovery block\n",
    "            if not is_in_recovery_block and likelihood < threshold:\n",
    "                is_in_recovery_block = True\n",
    "                current_block = {\n",
    "                    'start_idx': step_idx,\n",
    "                    'start_likelihood': likelihood,\n",
    "                    'attempts': 1  # This is the first attempt\n",
    "                }\n",
    "            \n",
    "            # Continue an existing recovery block\n",
    "            elif is_in_recovery_block and likelihood < threshold:\n",
    "                current_block['attempts'] += 1\n",
    "            \n",
    "            # End of a recovery block - successful\n",
    "            elif is_in_recovery_block and likelihood >= threshold:\n",
    "                is_in_recovery_block = False\n",
    "                current_block['end_idx'] = step_idx\n",
    "                current_block['end_likelihood'] = likelihood\n",
    "                current_block['success'] = True\n",
    "                current_block['duration'] = step_idx - current_block['start_idx']\n",
    "                current_block['dropped'] = False\n",
    "                recovery_blocks.append(current_block)\n",
    "                \n",
    "                # Calculate one-step recovery success (duration = 1)\n",
    "                one_step_success = int(current_block['duration'] == 1)\n",
    "                one_step_recovery_success.append(one_step_success)\n",
    "                \n",
    "                # Track other metrics\n",
    "                num_recovery_attempts_to_recover.append(current_block['attempts'])\n",
    "                eventual_recovery_success.append(1)\n",
    "                recovery_improvement.append(\n",
    "                    current_block['end_likelihood'] - current_block['start_likelihood']\n",
    "                )\n",
    "                dropped_recovery.append(False)  # Successful recovery wasn't due to dropping\n",
    "        \n",
    "        # Handle case where trial ended during a recovery block (unsuccessful)\n",
    "        if is_in_recovery_block:\n",
    "            current_block['end_idx'] = np_trial.shape[0] - 1\n",
    "            current_block['end_likelihood'] = np_trial[-1]\n",
    "            current_block['success'] = False\n",
    "            current_block['duration'] = np_trial.shape[0] - 1 - current_block['start_idx']\n",
    "            current_block['dropped'] = trial_dropped\n",
    "            recovery_blocks.append(current_block)\n",
    "            \n",
    "            # Not a one-step recovery\n",
    "            one_step_recovery_success.append(0)\n",
    "            eventual_recovery_success.append(0)\n",
    "            num_recovery_attempts_to_recover.append(current_block['attempts'])\n",
    "            recovery_improvement.append(\n",
    "                current_block['end_likelihood'] - current_block['start_likelihood']\n",
    "            )\n",
    "            dropped_recovery.append(trial_dropped)  # Was this unsuccessful recovery due to dropping?\n",
    "    \n",
    "    return one_step_recovery_success, eventual_recovery_success, num_recovery_attempts_to_recover, recovery_improvement, dropped_recovery\n",
    "\n",
    "def compute_recovery_statistics(\n",
    "    all_data: Dict[str, Any],\n",
    "    one_step_recovery_success: List[int],\n",
    "    eventual_recovery_success: List[int],\n",
    "    num_recovery_attempts: List[int]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute summary statistics from recovery metrics.\n",
    "    \n",
    "    Args:\n",
    "        all_data: Dictionary containing all trial data\n",
    "        one_step_recovery_success: Binary indicators of one-step recovery success\n",
    "        eventual_recovery_success: Binary indicators of eventual recovery success\n",
    "        num_recovery_attempts: Number of attempts needed for successful recovery\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of computed statistics\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # One-step recovery success rate\n",
    "    if one_step_recovery_success:\n",
    "        stats['one_step_recovery_rate'] = np.mean(one_step_recovery_success)\n",
    "        stats['one_step_recovery_count'] = np.sum(one_step_recovery_success)\n",
    "    else:\n",
    "        stats['one_step_recovery_rate'] = float('nan')\n",
    "        stats['one_step_recovery_count'] = 0\n",
    "    \n",
    "    # Eventual recovery metrics\n",
    "    if eventual_recovery_success:\n",
    "        stats['eventual_recovery_rate'] = np.mean(eventual_recovery_success)\n",
    "    else:\n",
    "        stats['eventual_recovery_rate'] = float('nan')\n",
    "    \n",
    "    # Average attempts for successful recoveries\n",
    "    successful_attempts = [\n",
    "        attempt for success, attempt in zip(eventual_recovery_success, num_recovery_attempts)\n",
    "        if success == 1\n",
    "    ]\n",
    "    if successful_attempts:\n",
    "        stats['avg_attempts_for_successful_recovery'] = np.mean(successful_attempts)\n",
    "    else:\n",
    "        stats['avg_attempts_for_successful_recovery'] = float('nan')\n",
    "    \n",
    "    # Calculate drop rate statistics\n",
    "    # print(all_data.keys())\n",
    "    # if 'dropped_recovery' in all_data and all_data['dropped_recovery']:\n",
    "    #     # Count total number of recovery blocks\n",
    "    #     total_recovery_blocks = len(all_data['recovery_blocks']) if isinstance(all_data['recovery_blocks'], list) else 0\n",
    "        \n",
    "    #     # Count number of dropped recovery blocks\n",
    "    #     dropped_recovery_count = sum(1 for drop_status in all_data['dropped_recovery'] if drop_status)\n",
    "        \n",
    "    #     # Calculate drop percentage per recovery block\n",
    "    #     if total_recovery_blocks > 0:\n",
    "    #         stats['drop_pct'] = (dropped_recovery_count / total_recovery_blocks) * 100\n",
    "    #     else:\n",
    "    #         stats['drop_pct'] = float('nan')\n",
    "        \n",
    "    #     # Calculate total number of individual recovery attempts\n",
    "    #     total_recovery_attempts = sum(num_recovery_attempts)\n",
    "        \n",
    "    #     # Calculate drop percentage per individual recovery attempt\n",
    "    #     if total_recovery_attempts > 0:\n",
    "    #         stats['drop_pct_individual'] = (dropped_recovery_count / total_recovery_attempts) * 100\n",
    "    #     else:\n",
    "    #         stats['drop_pct_individual'] = float('nan')\n",
    "    # else:\n",
    "    #     stats['drop_pct'] = float('nan')\n",
    "    #     stats['drop_pct_individual'] = float('nan')\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dofs = {\n",
    "    'roll': -3,\n",
    "    'pitch': -2,\n",
    "    'yaw': -1\n",
    "}\n",
    "conf = scipy.stats.norm.ppf(.95) \n",
    "\n",
    "def gen_plot_screwdriver_angle(data, keys, plans_or_inits, dof_to_plot=['yaw'], stat='mean', label_dict=None):\n",
    "    # data[key][t][trial_ind][plans_or_inits]\n",
    "    fig, ax = plt.subplots(1, len(dof_to_plot), figsize=(len(dof_to_plot)*25/3, 6))\n",
    "    # fig, ax = plt.subplots(1, len(dof_to_plot), figsize=(4, 2.3))\n",
    "    if len(dof_to_plot) == 1:\n",
    "        ax = [ax]\n",
    "    means = dict()\n",
    "    for key in keys:\n",
    "        traj_all = []\n",
    "        to_continue = False\n",
    "        for trial_ind in range(1, 26):\n",
    "        # for trial_ind in []:\n",
    "            try:\n",
    "                if plans_or_inits == 'traj':\n",
    "                    traj = data[key][t][trial_ind][plans_or_inits].reshape(-1, 36)[:, :15]\n",
    "                    traj_all.append(traj)\n",
    "                else:\n",
    "                    if len(data[key])-1 <= trial_ind:\n",
    "                        to_continue = True\n",
    "                        continue\n",
    "                    # traj = torch.stack([i[..., :15] for i in data[key][trial_ind+1][plans_or_inits]], axis=1).flatten(1, 2)\n",
    "                    # traj = traj.cpu()\n",
    "                    # for i in range(traj.shape[0]):\n",
    "                    #     traj_all.append(traj[i])\n",
    "                    traj_all.append(data[key][trial_ind+1][plans_or_inits].cpu().numpy()[..., :15])\n",
    "            except:\n",
    "                continue\n",
    "        # Pad to length 112 with the last value\n",
    "        if to_continue:\n",
    "            continue\n",
    "        lengths = [len(x) for x in traj_all]\n",
    "        num_contact_modes = [x/t for x in lengths]\n",
    "        max_len = max(lengths)\n",
    "        for i in range(len(traj_all)):\n",
    "            traj_all[i] = np.concatenate((traj_all[i], np.tile(traj_all[i][-1], (max_len - len(traj_all[i]), 1))), axis=0)\n",
    "        traj_all = np.stack(traj_all, axis=0)\n",
    "        # Subtract the initial value\n",
    "        start_ind = 0\n",
    "        num_to_plot = max_len\n",
    "        traj_all = traj_all - traj_all[:, start_ind:start_ind+1, :]\n",
    "        # traj_all = traj_all[traj_all[:, start_ind + num_to_plot-1, -1] < 0]\n",
    "        print(max_len, traj_all.shape)\n",
    "        mean = traj_all[:, start_ind + num_to_plot-1, -1].mean()\n",
    "        means[key] = mean\n",
    "        print(key, np.mean(num_contact_modes), traj_all[:, start_ind + num_to_plot-1, -1].mean(), traj_all[:, start_ind + num_to_plot-1, -1])\n",
    "        print()\n",
    "        if stat == 'mean':\n",
    "            traj_all_mean = traj_all.mean(axis=0)[:, [dofs[d] for d in dof_to_plot]]\n",
    "            traj_all_std = traj_all.std(axis=0)[:, [dofs[d] for d in dof_to_plot]]\n",
    "        elif stat == 'median':\n",
    "            traj_all_median = np.median(traj_all, axis=0)[:, [dofs[d] for d in dof_to_plot]]\n",
    "        for i, dof in enumerate(dof_to_plot):\n",
    "            if label_dict is not None:\n",
    "                label = label_dict[key]\n",
    "            else:\n",
    "                label = key\n",
    "            if stat == 'mean':\n",
    "                # print(traj_all[:, :12, -1])\n",
    "                # ax[i].plot(traj_all_mean[:, i], label=label, color = '#1f77b4' if 'A*' in key else '#2ca02c')\n",
    "                ax[i].plot(traj_all_mean[:, i], label=label)\n",
    "                # calculate 95% confidence interval\n",
    "                bound_offset = traj_all_std[:, i] * conf / np.sqrt(traj_all.shape[0])\n",
    "                # ax[i].fill_between(np.arange(traj_all_mean.shape[0]), traj_all_mean[:, i] - bound_offset, traj_all_mean[:, i] + bound_offset, alpha=0.5, color = '#1f77b4' if 'A*' in key else '#2ca02c')\n",
    "                ax[i].fill_between(np.arange(traj_all_mean.shape[0]), traj_all_mean[:, i] - bound_offset, traj_all_mean[:, i] + bound_offset, alpha=0.5)\n",
    "            elif stat == 'median':\n",
    "                ax[i].plot(traj_all_median[:, i], label=label)\n",
    "            elif stat == 'all':\n",
    "                for traj_ind in range(traj_all.shape[0]):\n",
    "                    # print(traj_all.shape)\n",
    "                    ax[i].plot(traj_all[traj_ind, start_ind:start_ind + num_to_plot, dofs[dof]], label=traj_ind)\n",
    "    for i, dof in enumerate(dof_to_plot):\n",
    "        ax[i].legend()\n",
    "        ax[i].set_xlabel('Timestep')\n",
    "        ax[i].set_ylabel(f'Screwdriver {dof} Angle'.title() + ' (rad)')\n",
    "        # Grid lines\n",
    "        ax[i].grid(True)\n",
    "        #y axis numbers on right side as well\n",
    "        ax[i].yaxis.tick_right()\n",
    "        # Title dof\n",
    "        ax[i].set_title(f'Real Screwdriver {dof} Angle 95% Confidence Interval'.title())# + f' (n={traj_all.shape[0]})')\n",
    "    plt.show()\n",
    "    return means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_likelihood_analysis(data_exec, key_groups):\n",
    "    # Loop through data_exec keys, and plot a boxplot of delta_l_recovery for each on the same pllot\n",
    "    def get_min_num_data(data_exec, key_groups):\n",
    "        min_num_data = float('inf')\n",
    "        for key_group in key_groups:\n",
    "            num_data = 0\n",
    "            for key in key_groups[key_group]:\n",
    "                num_data += len(data_exec[key]['delta_l_recovery'])\n",
    "            min_num_data = min(min_num_data, num_data)\n",
    "        return min_num_data\n",
    "    # min_num_data = get_min_num_data(data_exec, key_groups)\n",
    "    min_num_data = 10000\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "    for key_group in key_groups:\n",
    "        all_delta_recoveries = []\n",
    "        all_one_step_recovery_success = []\n",
    "        all_eventual_recovery_success = []\n",
    "        all_projection_times = []\n",
    "        all_num_recovery_attempts_to_recover = []\n",
    "        all_dropped = []\n",
    "        # all_dropped_individual = []\n",
    "        # all_dropped_or_succeeded = []\n",
    "        # all_final_likelihood = []\n",
    "        for key in key_groups[key_group]:\n",
    "            all_delta_recoveries.extend(data_exec[key]['delta_l_recovery'])\n",
    "            all_one_step_recovery_success.extend(data_exec[key]['one_step_recovery_success'])\n",
    "            all_eventual_recovery_success.extend(data_exec[key]['eventual_recovery_success'])\n",
    "            for l in data_exec[key]['project_times']:\n",
    "                all_projection_times.extend(l)\n",
    "            all_num_recovery_attempts_to_recover.extend(data_exec[key]['num_recovery_attempts_to_recover'])\n",
    "            all_dropped.append(data_exec[key]['drop_pct'])\n",
    "            # all_dropped_individual.append(data_exec[key]['drop_pct_individual'])\n",
    "            # all_dropped_or_succeeded.extend(data_exec[key]['dropped_or_succeeded'])\n",
    "            # all_final_likelihood.extend(data_exec[key]['final_likelihood'])\n",
    "        all_delta_recoveries = np.array(all_delta_recoveries)[:min_num_data]\n",
    "        all_one_step_recovery_success = np.array(all_one_step_recovery_success)[:min_num_data]\n",
    "        all_eventual_recovery_success = np.array(all_eventual_recovery_success)[:min_num_data]\n",
    "        all_projection_times = np.array(all_projection_times)[:min_num_data]\n",
    "        all_num_recovery_attempts_to_recover = np.array(all_num_recovery_attempts_to_recover)[:min_num_data]\n",
    "        # all_final_likelihood = np.array(all_final_likelihood)[:min_num_data]\n",
    "\n",
    "        num_points = np.sum(all_delta_recoveries > 0)\n",
    "        total_points = len(all_delta_recoveries)\n",
    "        percentage = num_points / total_points * 100\n",
    "        print('Total points:', total_points)\n",
    "        # print(f'{key_group}: {num_points} points greater than 0, {percentage:.2f}% of total points')\n",
    "        pct_success = np.sum(all_one_step_recovery_success) / total_points * 100\n",
    "        print(f'{key_group}: {pct_success:.2f}% of one-step recoveries were successful')\n",
    "        pct_eventual_success = np.sum(all_eventual_recovery_success) / total_points * 100\n",
    "        print(f'{key_group}: {pct_eventual_success:.2f}% of recoveries were eventually successful')\n",
    "        avg_num_recovery_attempts = np.mean(all_num_recovery_attempts_to_recover)\n",
    "        # print(f'{key_group}: Average number of recovery attempts to recover: {avg_num_recovery_attempts:.2f}')\n",
    "        # 95% confidence interval for the number of recovery attempts\n",
    "        ci_num_recovery_attempts = scipy.stats.norm.interval(0.95, loc=avg_num_recovery_attempts, scale=np.std(all_num_recovery_attempts_to_recover)/np.sqrt(len(all_num_recovery_attempts_to_recover)))\n",
    "        # Print as plus or minus\n",
    "        print(f'{key_group}: 95% CI for number of recovery attempts: {avg_num_recovery_attempts:.2f} +/- {ci_num_recovery_attempts[1] - avg_num_recovery_attempts:.2f}')\n",
    "\n",
    "        drop_pct = np.mean(all_dropped) * 100\n",
    "        print(f'{key_group}: Drop %: {drop_pct:.2f}%')\n",
    "        drop_pct_individual = np.mean(all_dropped_individual) * 100\n",
    "        print(f'{key_group}: Drop % per individual recovery attempt: {drop_pct_individual:.2f}%')\n",
    "        # drop_or_succeed_pct = np.mean(all_dropped_or_succeeded) * 100\n",
    "        # print(f'{key_group}: Drop or succeed %: {drop_or_succeed_pct:.2f}%')\n",
    "        # avg_projection_time = np.mean(all_projection_times)\n",
    "        # print(f'{key_group}: Average projection time: {avg_projection_time:.2f} seconds')\n",
    "        print()\n",
    "        ax.boxplot(all_delta_recoveries, positions=[list(key_groups.keys()).index(key_group)], widths=0.5, showfliers=False)\n",
    "        # ax2.boxplot(all_final_likelihood, positions=[list(key_groups.keys()).index(key_group)], widths=0.5, showfliers=False)\n",
    "    ax.set_xticklabels(key_groups.keys())\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.set_ylabel('Recovery Likelihood Change')\n",
    "    ax.set_title('Recovery Likelihood Change (If Not Dropped)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #Now do histograms of the delta_l_recovery for each key. For each key, print the number of points greater than 0 and the percentage that is of the total\n",
    "    # fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # # fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "    # for key_group in key_groups:\n",
    "    #     all_delta_recoveries = []\n",
    "    #     # all_final_likelihood = []\n",
    "    #     for key in key_groups[key_group]:\n",
    "    #         all_delta_recoveries.extend(data_exec[key]['delta_l_recovery'])\n",
    "    #         # all_final_likelihood.extend(data_exec[key]['final_likelihood'])\n",
    "    #     all_delta_recoveries = np.array(all_delta_recoveries)[:min_num_data]\n",
    "    #     # all_final_likelihood = np.array(all_final_likelihood)[:min_num_data]\n",
    "    #     ax.hist(all_delta_recoveries, bins=50, alpha=0.5, label=key_group, density=True)\n",
    "    #     # ax2.hist(all_final_likelihood, bins=50, alpha=0.5, label=key_group, density=True)\n",
    "    # ax.set_xlabel('Recovery Likelihood Change')\n",
    "    # ax.set_ylabel('Density')\n",
    "    # ax.set_title('Histogram of Recovery Likelihood Change ')\n",
    "    # # Red dashed line at x =0\n",
    "    # ax.axvline(x=0, color='red', linestyle='--')\n",
    "    # # ax2.axvline(x=-175, color='red', linestyle='--')\n",
    "    # ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'drop_pct_individual'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     data_exec[key] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_exec[key], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_data}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 18\u001b[0m \u001b[43mdelta_likelihood_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_exec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m{\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# 'allegro_screwdriver_recovery_data_pi_6_fixed_cpc': ['allegro_screwdriver_recovery_data_pi_6_fixed_cpc'],\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallegro_screwdriver_recovery_model_fixed_cpc_eval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallegro_screwdriver_recovery_model_fixed_cpc_eval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallegro_screwdriver_recovery_model_fixed_cpc_eval_max_likelihood_mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallegro_screwdriver_recovery_model_fixed_cpc_eval_max_likelihood_mode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m t \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m stat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mdelta_likelihood_analysis\u001b[0;34m(data_exec, key_groups)\u001b[0m\n\u001b[1;32m     31\u001b[0m     all_num_recovery_attempts_to_recover\u001b[38;5;241m.\u001b[39mextend(data_exec[key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_recovery_attempts_to_recover\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     32\u001b[0m     all_dropped\u001b[38;5;241m.\u001b[39mappend(data_exec[key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop_pct\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     all_dropped_individual\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdata_exec\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdrop_pct_individual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# all_dropped_or_succeeded.extend(data_exec[key]['dropped_or_succeeded'])\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# all_final_likelihood.extend(data_exec[key]['final_likelihood'])\u001b[39;00m\n\u001b[1;32m     36\u001b[0m all_delta_recoveries \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_delta_recoveries)[:min_num_data]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'drop_pct_individual'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "# while True:\n",
    "# Clear cell output\n",
    "clear_output(wait=True)\n",
    "data_exec = {}\n",
    "t = params['T']\n",
    "for key, name in [\n",
    "                # ('allegro_screwdriver_recovery_data_pi_6_fixed_cpc', 'allegro_screwdriver_recovery_data_pi_6_fixed_cpc'),\n",
    "                ('allegro_screwdriver_recovery_model_fixed_cpc_eval', 'allegro_screwdriver_recovery_model_fixed_cpc_eval'),\n",
    "                ('allegro_screwdriver_recovery_model_fixed_cpc_eval_max_likelihood_mode', 'allegro_screwdriver_recovery_model_fixed_cpc_eval_max_likelihood_mode'),\n",
    "                ]:\n",
    "    data_exec[key] = {}\n",
    "    all_x, all_data = get_traj(name)\n",
    "\n",
    "    data_exec[key] = {**data_exec[key], **all_data}\n",
    "print()\n",
    "\n",
    "delta_likelihood_analysis(data_exec, \n",
    "                        {    \n",
    "                            # 'allegro_screwdriver_recovery_data_pi_6_fixed_cpc': ['allegro_screwdriver_recovery_data_pi_6_fixed_cpc'],\n",
    "                            'allegro_screwdriver_recovery_model_fixed_cpc_eval': ['allegro_screwdriver_recovery_model_fixed_cpc_eval'],\n",
    "                            'allegro_screwdriver_recovery_model_fixed_cpc_eval_max_likelihood_mode': ['allegro_screwdriver_recovery_model_fixed_cpc_eval_max_likelihood_mode'],\n",
    "                        })\n",
    "t = params['T']\n",
    "stat = 'mean'\n",
    "dofs_to_plot = ['roll', 'pitch', 'yaw']\n",
    "\n",
    "keys_exec = [\n",
    "            'allegro_screwdriver_recovery_data_pi_6_fixed_cpc'\n",
    "              ]\n",
    "# means = gen_plot_screwdriver_angle(data_exec, keys_exec, 'traj', dof_to_plot=dofs_to_plot, stat=stat, label_dict=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
