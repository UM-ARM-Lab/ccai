{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_38' (/home/abhinav/Downloads/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /home/abhinav/Downloads/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n",
      "PyTorch version 2.4.1+cu121\n",
      "Device count 1\n",
      "/home/abhinav/Downloads/isaacgym/python/isaacgym/_bindings/src/gymtorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/abhinav/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/abhinav/.cache/torch_extensions/py38_cu121/gymtorch/build.ninja...\n",
      "Building extension module gymtorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module gymtorch...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from isaac_victor_envs.utils import get_assets_dir\n",
    "from isaac_victor_envs.tasks.allegro import AllegroScrewdriverTurningEnv\n",
    "# from isaac_victor_envs.tasks.allegro_ros import RosAllegroValveTurningEnv\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scipy\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import yaml\n",
    "import pathlib\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "import pytorch_volumetric as pv\n",
    "import pytorch_kinematics as pk\n",
    "import pytorch_kinematics.transforms as tf\n",
    "from torch.func import vmap, jacrev, hessian, jacfwd\n",
    "# import pytorch3d.transforms as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ccai.utils.allegro_utils import *\n",
    "# from allegro_valve_roll import AllegroValveTurning, AllegroContactProblem, PositionControlConstrainedSVGDMPC, \\\n",
    "#    add_trajectories, add_trajectories_hardware\n",
    "\n",
    "from ccai.allegro_contact import AllegroManipulationProblem, PositionControlConstrainedSVGDMPC, add_trajectories, \\\n",
    "    add_trajectories_hardware\n",
    "from ccai.allegro_screwdriver_problem_diffusion import AllegroScrewdriverDiff\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# from ccai.mpc.ipopt import IpoptMPC\n",
    "# from ccai.problem import IpoptProblem\n",
    "from ccai.models.trajectory_samplers import TrajectorySampler\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to PVD\n",
      "Physics Engine: PhysX\n",
      "Physics Device: cpu\n",
      "GPU Pipeline: disabled\n",
      "Using VHACD cache directory '/home/abhinav/.isaacgym/vhacd'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/allegro/base_ns.stl'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/allegro/link_1.0.stl'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/ft_c.stl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinav/Documents/github/pytorch_volumetric/src/pytorch_volumetric/sdf.py:1138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cache = torch.load(dbpath)\n"
     ]
    }
   ],
   "source": [
    "class AllegroScrewdriver(AllegroManipulationProblem):\n",
    "    def __init__(self,\n",
    "                 start,\n",
    "                 goal,\n",
    "                 T,\n",
    "                 chain,\n",
    "                 object_location,\n",
    "                 object_type,\n",
    "                 world_trans,\n",
    "                 object_asset_pos,\n",
    "                 regrasp_fingers=[],\n",
    "                 contact_fingers=['index', 'middle', 'ring', 'thumb'],\n",
    "                 friction_coefficient=0.95,\n",
    "                 obj_dof=1,\n",
    "                 obj_ori_rep='euler',\n",
    "                 obj_joint_dim=0,\n",
    "                 optimize_force=False,\n",
    "                 turn=False,\n",
    "                 obj_gravity=False,\n",
    "                 min_force_dict=None,\n",
    "                 device='cuda:0',\n",
    "                 full_dof_goal=False, **kwargs):\n",
    "        self.obj_mass = 0.1\n",
    "        self.obj_dof_type = None\n",
    "        if obj_dof == 3:\n",
    "            object_link_name = 'screwdriver_body'\n",
    "        elif obj_dof == 1:\n",
    "            object_link_name = 'valve'\n",
    "        elif obj_dof == 6:\n",
    "            object_link_name = 'card'\n",
    "        self.obj_link_name = object_link_name\n",
    "        super(AllegroScrewdriver, self).__init__(start=start, goal=goal, T=T, chain=chain,\n",
    "                                                 object_location=object_location,\n",
    "                                                 object_type=object_type, world_trans=world_trans,\n",
    "                                                 object_asset_pos=object_asset_pos,\n",
    "                                                 regrasp_fingers=regrasp_fingers,\n",
    "                                                 contact_fingers=contact_fingers,\n",
    "                                                 friction_coefficient=friction_coefficient,\n",
    "                                                 obj_dof=obj_dof,\n",
    "                                                 obj_ori_rep=obj_ori_rep, obj_joint_dim=1,\n",
    "                                                 optimize_force=optimize_force, device=device,\n",
    "                                                 turn=turn, obj_gravity=obj_gravity,\n",
    "                                                 min_force_dict=min_force_dict, \n",
    "                                                 full_dof_goal=full_dof_goal, **kwargs)\n",
    "        self.friction_coefficient = friction_coefficient\n",
    "\n",
    "    def _cost(self, xu, start, goal):\n",
    "        # TODO: check if the addtional term of the smoothness cost and running goal cost is necessary\n",
    "        state = xu[:, :self.dx]  # state dim = 9\n",
    "        state = torch.cat((start.reshape(1, self.dx), state), dim=0)  # combine the first time step into it\n",
    "\n",
    "        smoothness_cost = torch.sum((state[1:, -self.obj_dof:] - state[:-1, -self.obj_dof:]) ** 2)\n",
    "        upright_cost = 0\n",
    "        # if not self.full_dof_goal:\n",
    "        upright_cost = 500 * torch.sum(\n",
    "            (state[:, -self.obj_dof:-1] + goal[-self.obj_dof:-1]) ** 2)  # the screwdriver should only rotate in z direction\n",
    "        return smoothness_cost + upright_cost + super()._cost(xu, start, goal)\n",
    "\n",
    "\n",
    "obj_dof = 3\n",
    "# config = yaml.safe_load(pathlib.Path(f'../examples/config/{sys.argv[1]}.yaml').read_text())\n",
    "config = yaml.safe_load(pathlib.Path(f'../examples/config/allegro_screwdriver_csvto_only.yaml').read_text())\n",
    "config['visualize'] = False\n",
    "\n",
    "if config['mode'] == 'hardware':\n",
    "    env = RosAllegroValveTurningEnv(1, control_mode='joint_impedance',\n",
    "                                    use_cartesian_controller=False,\n",
    "                                    viewer=True,\n",
    "                                    steps_per_action=60,\n",
    "                                    friction_coefficient=1.0,\n",
    "                                    device=config['sim_device'],\n",
    "                                    valve=config['object_type'],\n",
    "                                    video_save_path=img_save_dir,\n",
    "                                    joint_stiffness=config['kp'],\n",
    "                                    fingers=config['fingers'],\n",
    "                                    )\n",
    "else:\n",
    "    if not config['visualize']:\n",
    "        img_save_dir = None\n",
    "\n",
    "    env = AllegroScrewdriverTurningEnv(1, control_mode='joint_impedance',\n",
    "                                        use_cartesian_controller=False,\n",
    "                                        viewer=config['visualize'],\n",
    "                                        steps_per_action=60,\n",
    "                                        friction_coefficient=config['friction_coefficient'] * 1.05,\n",
    "                                        # friction_coefficient=1.0,  # DEBUG ONLY, set the friction very high\n",
    "                                        device=config['sim_device'],\n",
    "                                        video_save_path=img_save_dir,\n",
    "                                        joint_stiffness=config['kp'],\n",
    "                                        fingers=config['fingers'],\n",
    "                                        )\n",
    "\n",
    "sim, gym, viewer = env.get_sim()\n",
    "\n",
    "state = env.get_state()\n",
    "# try:\n",
    "#     while True:\n",
    "#         start = env.get_state()['q'][:, :-1]\n",
    "#         env.step(start)\n",
    "#         print('waiting for you to finish camera adjustment, ctrl-c when done')\n",
    "#         time.sleep(0.1)\n",
    "# except KeyboardInterrupt:\n",
    "#     pass\n",
    "\n",
    "sim_env = None\n",
    "ros_copy_node = None\n",
    "if config['mode'] == 'hardware':\n",
    "    sim_env = env\n",
    "    from hardware.hardware_env import HardwareEnv\n",
    "\n",
    "    env = HardwareEnv(sim_env.default_dof_pos[:, :16], finger_list=['index', 'thumb'], kp=config['kp'])\n",
    "    env.world_trans = sim_env.world_trans\n",
    "    env.joint_stiffness = sim_env.joint_stiffness\n",
    "    env.device = sim_env.device\n",
    "    env.valve_pose = sim_env.valve_pose\n",
    "elif config['mode'] == 'hardware_copy':\n",
    "    from hardware.hardware_env import RosNode\n",
    "\n",
    "    ros_copy_node = RosNode()\n",
    "\n",
    "results = {}\n",
    "\n",
    "# set up the kinematic chain\n",
    "asset = f'{get_assets_dir()}/xela_models/allegro_hand_right.urdf'\n",
    "ee_names = {\n",
    "    'index': 'allegro_hand_hitosashi_finger_finger_0_aftc_base_link',\n",
    "    'middle': 'allegro_hand_naka_finger_finger_1_aftc_base_link',\n",
    "    'ring': 'allegro_hand_kusuri_finger_finger_2_aftc_base_link',\n",
    "    'thumb': 'allegro_hand_oya_finger_3_aftc_base_link',\n",
    "}\n",
    "config['ee_names'] = ee_names\n",
    "config['obj_dof'] = 3\n",
    "\n",
    "screwdriver_asset = f'{get_assets_dir()}/screwdriver/screwdriver.urdf'\n",
    "\n",
    "chain = pk.build_chain_from_urdf(open(asset).read())\n",
    "screwdriver_chain = pk.build_chain_from_urdf(open(screwdriver_asset).read())\n",
    "frame_indices = [chain.frame_to_idx[ee_names[finger]] for finger in config['fingers']]  # combined chain\n",
    "frame_indices = torch.tensor(frame_indices)\n",
    "state2ee_pos = partial(state2ee_pos, fingers=config['fingers'], chain=chain, frame_indices=frame_indices,\n",
    "                        world_trans=env.world_trans)\n",
    "\n",
    "forward_kinematics = partial(chain.forward_kinematics,\n",
    "                                frame_indices=frame_indices)  # full_to= _partial_state = partial(full_to_partial_state, fingers=config['fingers'])\n",
    "partial_to_full_state = partial(partial_to_full_state, fingers=config['fingers'])\n",
    "\n",
    "controller = 'csvgd'\n",
    "goal = - 0.5 * torch.tensor([0, 0, np.pi])\n",
    "# set up params\n",
    "params = config.copy()\n",
    "params.pop('controllers')\n",
    "params.update(config['controllers'][controller])\n",
    "params['controller'] = controller\n",
    "params['valve_goal'] = goal.to(device=params['device'])\n",
    "params['chain'] = chain.to(device=params['device'])\n",
    "object_location = torch.tensor([0, 0, 1.205]).to(\n",
    "    params['device'])  # TODO: confirm if this is the correct location\n",
    "params['object_location'] = object_location\n",
    "\n",
    "num_fingers = len(params['fingers'])\n",
    "state = env.get_state()\n",
    "start = state['q'].reshape(4 * num_fingers + 4).to(device=params['device'])\n",
    "if 'csvgd' in params['controller']:\n",
    "    # index finger is used for stability\n",
    "    if 'index' in params['fingers']:\n",
    "        fingers = params['fingers']\n",
    "    else:\n",
    "        fingers = ['index'] + params['fingers']\n",
    "\n",
    "pregrasp_problem = AllegroScrewdriver(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    regrasp_fingers=fingers,\n",
    "    contact_fingers=[],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    ")\n",
    "# finger gate index\n",
    "index_regrasp_problem = AllegroScrewdriver(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    regrasp_fingers=['index'],\n",
    "    contact_fingers=['middle', 'thumb'],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    "    default_dof_pos=env.default_dof_pos[:, :16]\n",
    ")\n",
    "thumb_and_middle_regrasp_problem = AllegroScrewdriver(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    contact_fingers=['index'],\n",
    "    regrasp_fingers=['middle', 'thumb'],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    "    default_dof_pos=env.default_dof_pos[:, :16]\n",
    ")\n",
    "turn_problem = AllegroScrewdriver(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    contact_fingers=['index', 'middle', 'thumb'],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    "    default_dof_pos=env.default_dof_pos[:, :16]\n",
    ")\n",
    "contact_mode_dict = {0: 'pregrasp', 2: 'index', 1: 'thumb_middle', 3: 'turn'}\n",
    "t = params['T']\n",
    "# with open(data_path / f'constraint_violations.p', 'wb') as f:\n",
    "#     pickle.dump(constraint_violations_all, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contact_points(name):\n",
    "    path_base = f'/home/abhinav/Documents/ccai/data/training_data/{name}/train_data'\n",
    "\n",
    "    all_data = []\n",
    "    all_x = []\n",
    "    # all_d2goal = []\n",
    "    all_traj_data = []\n",
    "    for collection_idx in range(4):\n",
    "        path = path_base + f'/{name}_data_{collection_idx}/csvgd'\n",
    "        for trial_num in range(1, 61):\n",
    "            # print(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "            # if 'rand' in name or 'proj' in name or 'diff' in name:\n",
    "            # try:\n",
    "                # with open(path + f'/trial_{trial_num}/trajectory.pkl', 'rb') as data:\n",
    "                #     d = pickle.load(data)\n",
    "                #     traj = np.stack((d[:-1]), axis=0)\n",
    "                    # end = d[-1].reshape(1, -1)\n",
    "                    # end = np.concatenate((end, np.zeros((1, 21))), axis=1)\n",
    "                    # traj = np.concatenate((traj, end), axis=0)\n",
    "                # else:\n",
    "                #     d = np.load(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "                #     end_state = d['x']\n",
    "\n",
    "            data = np.load(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "            last_state = data['x']\n",
    "            last_state = last_state.reshape(1, -1)\n",
    "            # Concatenate zeros to the end of the last state\n",
    "            last_last_state = np.concatenate((last_state, np.zeros((1, 21))), axis=1)\n",
    "            traj = []\n",
    "\n",
    "            with open(path + f'/trial_{trial_num}/traj_data.p', 'rb') as f:\n",
    "                traj_data = pickle.load(f)\n",
    "                for key in traj_data.keys():\n",
    "                    if torch.is_tensor(traj_data[key]):\n",
    "                        traj_data[key] = traj_data[key].cpu().numpy()\n",
    "                # all_d2goal.append(d2goal)\n",
    "                # if 'rand' not in name and 'proj' not in name and 'diff' not in name::\n",
    "                #     traj = traj_data[t]['plans'][:, 0]\n",
    "\n",
    "                    # end = np.concatenate((end_state, np.zeros((1, 21))), axis=1)\n",
    "                    # traj = np.concatenate((traj, end), axis=0)\n",
    "                for t in range(12, 1 - 1, -1):\n",
    "                    traj.append(traj_data[t]['starts'][:, 0, :])\n",
    "\n",
    "                traj = np.stack(traj, axis=1)\n",
    "                last_state = traj[1:, 0]\n",
    "                last_state = np.concatenate((last_state, last_last_state), axis=0)\n",
    "                last_state = np.expand_dims(last_state, axis=1)\n",
    "                traj = np.concatenate((traj, last_state), axis=1)\n",
    "                    \n",
    "                traj_data[12]['traj'] = np.expand_dims(traj, axis=1)\n",
    "                all_traj_data.append(traj_data)  \n",
    "                all_data.append(traj_data)\n",
    "\n",
    "            # except:\n",
    "            #     continue\n",
    "            \n",
    "    constraint_violations_all = {\n",
    "        # 'optimizer_paths': [],\n",
    "        'traj': [],\n",
    "        # 'inits': [],\n",
    "        # 'plans': [],\n",
    "\n",
    "    }\n",
    "    # for plans_or_inits in constraint_violations_all.keys():\n",
    "    #     if plans_or_inits == 'traj':\n",
    "    #         gen_constraint_data(plans_or_inits, constraint_violations_all[plans_or_inits], path, traj_data=all_traj_data)\n",
    "    #     else:\n",
    "    #         gen_constraint_data(plans_or_inits, constraint_violations_all[plans_or_inits], path)\n",
    "\n",
    "    # Take the list of dicts and turn it into a dict of lists\n",
    "    all_data = {k: [d[k] for d in all_data] for k in all_data[0]}\n",
    "    all_data['violation'] = constraint_violations_all\n",
    "\n",
    "    return all_x, all_data#, all_d2goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact Point test\n"
     ]
    }
   ],
   "source": [
    "data_exec = {}\n",
    "t = params['T']\n",
    "\n",
    "for key, name in [\n",
    "                ('Contact Point test', 'allegro_high_force_high_eps_pi_6'),\n",
    "                ]:\n",
    "    \n",
    "    print(key)\n",
    "    data_exec[key] = {}\n",
    "    all_x, all_data = get_contact_points(name)\n",
    "\n",
    "\n",
    "    data_exec[key] = {**data_exec[key], **all_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_robot_to_obj(coords, tf_robot_to_world, tf_world_to_obj):\n",
    "    coords_world = tf_robot_to_world.transform_points(coords)\n",
    "    coords_obj = tf_world_to_obj.transform_points(coords_world)\n",
    "    return coords_obj\n",
    "\n",
    "def _convert_obj_to_robot(coords, tf_robot_to_world, tf_world_to_obj):\n",
    "\n",
    "    coords_world = tf_world_to_obj.inverse().transform_points(coords)\n",
    "    coords_robot = tf_robot_to_world.inverse().transform_points(coords_world)\n",
    "\n",
    "    return coords_robot\n",
    "\n",
    "def convert_contact_data_to_obj(contact_points, contact_normals, tf_robot_to_world, tf_world_to_obj):\n",
    "    contact_points_obj = _convert_robot_to_obj(contact_points, tf_robot_to_world, tf_world_to_obj)\n",
    "    contact_normals_obj = _convert_robot_to_obj(contact_normals, tf_robot_to_world, tf_world_to_obj)\n",
    "    return contact_points_obj, contact_normals_obj\n",
    "\n",
    "def convert_contact_data_to_robot(contact_points_obj, contact_normals_obj, tf_robot_to_world, tf_world_to_obj):\n",
    "    orig_shape = contact_points_obj.shape\n",
    "    contact_points_obj = contact_points_obj.reshape(-1, 3)\n",
    "    contact_normals_obj = contact_normals_obj.reshape(-1, 3)\n",
    "    contact_points_robot = _convert_obj_to_robot(contact_points_obj, tf_robot_to_world, tf_world_to_obj)\n",
    "    contact_normals_robot = _convert_obj_to_robot(contact_normals_obj, tf_robot_to_world, tf_world_to_obj)\n",
    "\n",
    "    contact_points_robot = contact_points_robot.reshape(orig_shape)\n",
    "    contact_normals_robot = contact_normals_robot.reshape(orig_shape)\n",
    "    return contact_points_robot, contact_normals_robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = turn_problem.contact_scenes\n",
    "object_location = object_location.reshape(1, 3).to(device=params['device'])\n",
    "tf_robot_to_world = env.world_trans.to(device=params['device'])#.inverse().to(device=params['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ed0538eec14fddb4e34e03bbebd49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 13, 3]) torch.Size([32, 7, 3])\n",
      "torch.Size([32, 7, 13, 3]) torch.Size([32, 7, 3])\n",
      "torch.Size([32, 7, 13, 3]) torch.Size([32, 7, 3])\n",
      "torch.Size([32, 7, 13, 3]) torch.Size([32, 7, 3])\n",
      "torch.Size([32, 7, 13, 3]) torch.Size([32, 7, 3])\n",
      "torch.Size([32, 7, 13, 3]) torch.Size([32, 7, 3])\n",
      "torch.Size([32, 7, 13, 3]) torch.Size([32, 7, 3])\n",
      "torch.Size([16, 7, 13, 3]) torch.Size([16, 7, 3])\n"
     ]
    }
   ],
   "source": [
    "num_trials = len(data_exec['Contact Point test'][12])\n",
    "all_trajs = []\n",
    "for trial_num in range(num_trials):\n",
    "    all_trajs.append(torch.from_numpy(data_exec['Contact Point test'][12][trial_num]['traj'].squeeze()).float())\n",
    "\n",
    "def process_batch(all_trajs):\n",
    "    all_trajs = torch.stack(all_trajs, dim=0).to(device=params['device'])\n",
    "    # (num_trials, num_modes, num_timesteps)\n",
    "    N, C, T, _ = all_trajs.shape\n",
    "    robot_q = all_trajs[..., :12]\n",
    "\n",
    "    screwdriver_ori = all_trajs[..., -obj_dof:]\n",
    "    screwdirver_ori_mat = tf.euler_angles_to_matrix(screwdriver_ori, convention='XYZ').reshape(-1, 3, 3)\n",
    "    screwdriver_ori_quat = tf.matrix_to_quaternion(screwdirver_ori_mat).reshape(-1, 4)\n",
    "    tf_obj_to_world = tf.Transform3d(rot=screwdriver_ori_quat, pos=object_location, device=params['device'])\n",
    "    tf_world_to_obj = tf_obj_to_world.inverse()\n",
    "\n",
    "    q_b = robot_q.reshape(-1, 4 * 3)\n",
    "    theta_b = screwdriver_ori.reshape(-1, obj_dof)\n",
    "    theta_obj_joint = torch.zeros((theta_b.shape[0], 1),\n",
    "                                    device=theta_b.device)  # add an additional dimension for the cap of the screw driver\n",
    "    # the cap does not matter for the task, but needs to be included in the state for the model\n",
    "    theta_b = torch.cat((theta_b, theta_obj_joint), dim=1)\n",
    "    full_q = partial_to_full_state(q_b)\n",
    "\n",
    "    ret_scene = cs.scene_collision_check(full_q.float(), theta_b.float(),\n",
    "                                        compute_gradient=True,\n",
    "                                        compute_hessian=False)\n",
    "\n",
    "    contact_points_obj, contact_normals_obj = convert_contact_data_to_obj(ret_scene['closest_pt_world'], ret_scene['contact_normal'], tf_robot_to_world, tf_world_to_obj)\n",
    "    sdf = ret_scene['sdf'].reshape(N, C, T, -1)\n",
    "    contact_points_obj = contact_points_obj.reshape(N, C, T, -1, 3)\n",
    "    contact_normals_obj = contact_normals_obj.reshape(N, C, T, -1, 3)\n",
    "    tfs = tf_world_to_obj.get_matrix().reshape(N, C, T, 4, 4).unsqueeze(-3).repeat(1, 1, 1, 3, 1, 1)\n",
    "    sdf[sdf < 0] = torch.inf\n",
    "    min_ind = torch.argmin(sdf, dim=-2)\n",
    "\n",
    "    min_dist = torch.gather(sdf, 2, min_ind.unsqueeze(-1)).squeeze(-1)\n",
    "    print(sdf.shape, min_dist.shape)\n",
    "    min_contact_points_obj = torch.gather(contact_points_obj, 2, min_ind.unsqueeze(-2).unsqueeze(-1).expand(-1, -1, -1, -1, 3)).squeeze(-3)\n",
    "    min_contact_normals_obj = torch.gather(contact_normals_obj, 2, min_ind.unsqueeze(-2).unsqueeze(-1).expand(-1, -1, -1, -1, 3)).squeeze(-3)\n",
    "    min_tfs = torch.gather(tfs, 2, min_ind.unsqueeze(-2).unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, -1, 4, 4)).squeeze(2)#[:, :, 0]\n",
    "\n",
    "    return min_dist.cpu(), min_contact_points_obj.cpu(), min_contact_normals_obj.cpu(), min_tfs.cpu()\n",
    "\n",
    "# Loop over all_traj with batch_size 1024\n",
    "min_dist = []\n",
    "min_contact_points_obj = []\n",
    "min_contact_normals_obj = []\n",
    "min_tfs = []\n",
    "batch_size = 32\n",
    "for i in tqdm(range(0, len(all_trajs), batch_size)):\n",
    "    min_dist_batch, min_contact_points_obj_batch, min_contact_normals_obj_batch, min_tfs_batch = process_batch(all_trajs[i:i+batch_size])\n",
    "    min_contact_points_obj.append(min_contact_points_obj_batch)\n",
    "    min_contact_normals_obj.append(min_contact_normals_obj_batch)\n",
    "    min_tfs.append(min_tfs_batch)\n",
    "    min_dist.append(min_dist_batch)\n",
    "\n",
    "min_dist = torch.cat(min_dist, dim=0)\n",
    "min_contact_points_obj = torch.cat(min_contact_points_obj, dim=0)\n",
    "min_contact_normals_obj = torch.cat(min_contact_normals_obj, dim=0)\n",
    "min_tfs = torch.cat(min_tfs, dim=0)\n",
    "\n",
    "\n",
    "# Group the contact points and normals by contact mode\n",
    "contact_data_obj_by_mode = defaultdict(list)\n",
    "all_trajs = torch.stack(all_trajs, dim=0)\n",
    "robot_q = all_trajs[..., :12]\n",
    "N, C, T, _ = all_trajs.shape\n",
    "for trial_num in range(N):\n",
    "    contact_states = data_exec['Contact Point test'][12][trial_num]['contact_state']\n",
    "    for contact_state_idx in range(C):\n",
    "        contact_state_idx_tuple = tuple(contact_states[contact_state_idx].tolist())\n",
    "        contact_data_obj_by_mode[contact_state_idx_tuple].append(\n",
    "            (min_contact_points_obj[trial_num, contact_state_idx], \n",
    "             min_contact_normals_obj[trial_num, contact_state_idx],\n",
    "             min_tfs[trial_num, contact_state_idx], min_dist[trial_num, contact_state_idx])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg: /home/abhinav/miniconda3/envs/ccai/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)\n",
      "ffmpeg: /home/abhinav/miniconda3/envs/ccai/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)\n",
      "ffmpeg: /home/abhinav/miniconda3/envs/ccai/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)\n",
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from 'images_avg/img/im_%4d.png':\n",
      "  Duration: 00:00:00.04, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgba(pc), 1920x1163 [SAR 39:39 DAR 1920:1163], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> png (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[Parsed_palettegen_0 @ 0x5a5834c6d9c0] 234(+1) colors generated out of 234 colors; ratio=1.000000\n",
      "Output #0, image2, to '/home/abhinav/palette.png':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: png, rgba(pc, gbr/unknown/unknown, progressive), 16x16 [SAR 1:1 DAR 1:1], q=2-31, 200 kb/s, 25 fps, 25 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 png\n",
      "frame=    1 fps=0.0 q=-0.0 Lsize=N/A time=00:00:00.04 bitrate=N/A speed=1.46x    \n",
      "video:1kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n",
      "ffmpeg: /home/abhinav/miniconda3/envs/ccai/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)\n",
      "ffmpeg: /home/abhinav/miniconda3/envs/ccai/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)\n",
      "ffmpeg: /home/abhinav/miniconda3/envs/ccai/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)\n",
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from 'images_avg/img/im_%4d.png':\n",
      "  Duration: 00:00:00.50, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgba(pc), 1920x1163 [SAR 39:39 DAR 1920:1163], 2 fps, 2 tbr, 2 tbn, 2 tbc\n",
      "Input #1, png_pipe, from '/home/abhinav/palette.png':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "  Stream #1:0: Video: png, rgba(pc), 16x16 [SAR 1:1 DAR 1:1], 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "images_avg/gif/trajectory.gif: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "mode = (0.0, 1.0, 1.0)\n",
    "\n",
    "pts_th_m = torch.stack([i[0] for i in contact_data_obj_by_mode[mode]], dim=1)#.reshape(-1, 3)\n",
    "orig_shape = pts_th_m.shape\n",
    "\n",
    "pts_th_m = pts_th_m.reshape(-1, 3)\n",
    "normals_th_m = torch.stack([i[1] for i in contact_data_obj_by_mode[mode]], dim=1).reshape(-1, 3)\n",
    "tfs_th_m = torch.stack([i[2] for i in contact_data_obj_by_mode[mode]], dim=1).reshape(-1, 4, 4)#[0].unsqueeze(0).repeat(pts_th_m.shape[0], 1, 1)\n",
    "dist_th_m = torch.stack([i[3] for i in contact_data_obj_by_mode[mode]], dim=1)\n",
    "\n",
    "# tfs_th_m = torch.tensor([\n",
    "#     [1., 0, 0, 0],\n",
    "#     [0, 1., 0, 0],\n",
    "#     [0, 0, 1., -1.205],\n",
    "#     [0, 0, 0, 1]\n",
    "# ]).unsqueeze(0).repeat(normals_th_m.shape[0], 1, 1).to(device=params['device'])\n",
    "tfs_th_m = tf.Transform3d(matrix=tfs_th_m)\n",
    "tf_robot_to_world = tf_robot_to_world.cpu()\n",
    "\n",
    "contact_points_obj = pts_th_m\n",
    "contact_normals_obj = normals_th_m\n",
    "contact_points_obj = contact_points_obj.reshape(-1, 3)\n",
    "contact_normals_obj = contact_normals_obj.reshape(-1, 3)\n",
    "contact_points_robot = _convert_obj_to_robot(contact_points_obj.unsqueeze(1), tf_robot_to_world, tfs_th_m)\n",
    "contact_normals_robot = _convert_obj_to_robot(contact_normals_obj.unsqueeze(1), tf_robot_to_world, tfs_th_m)\n",
    "pts_th_m_rob = contact_points_robot.reshape(orig_shape).squeeze()\n",
    "normals_th_m_rob = contact_normals_robot.reshape(orig_shape).squeeze()\n",
    "\n",
    "\n",
    "# pts_th_m_rob, normals_th_m_rob = convert_contact_data_to_robot(pts_th_m, normals_th_m, tf_robot_to_world, tf_world_to_obj)\n",
    "colors = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "])\n",
    "colors = colors.unsqueeze(1).expand(-1, pts_th_m_rob.shape[1], -1)\n",
    "\n",
    "pcd_points = torch.cat((pts_th_m_rob, colors), dim=-1).cpu().numpy().reshape(-1, 6)\n",
    "normals = normals_th_m_rob.cpu().numpy().reshape(-1, 3)\n",
    "\n",
    "# dist_mask = dist_th_m == dist_th_m.min(dim=1)[0].unsqueeze(-1)\n",
    "dist_mask = dist_th_m < 1000\n",
    "dist_mask = dist_mask.flatten()\n",
    "pcd_points = pcd_points[dist_mask]\n",
    "normals = normals[dist_mask]\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(pcd_points[:, :3])\n",
    "pcd.colors = o3d.utility.Vector3dVector(pcd_points[:, 3:])\n",
    "pcd.normals = o3d.utility.Vector3dVector(normals)\n",
    "theta_b = torch.zeros((*robot_q.shape[:-1], obj_dof+1))\n",
    "traj_for_viz = torch.cat((robot_q, theta_b), dim=-1)[0:1, 0, 0]\n",
    "\n",
    "visualize_trajectory(traj_for_viz, cs, 'images_avg', config['fingers'], obj_dof+1, pcd=pcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "a = torch.tensor([\n",
    "        [\n",
    "        [1, 2, 3],\n",
    "        [4, 5., 6],\n",
    "        # [7, 8., 9]\n",
    "    ],\n",
    "    [\n",
    "        [10, 11, 12],\n",
    "        [13, 14, 15],\n",
    "        # [16, 17, 18]\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1]) torch.Size([5, 2]) torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "def h_poly(t):\n",
    "    tt = t[None, :]**torch.arange(4, device=t.device)[:, None]\n",
    "    A = torch.tensor([\n",
    "        [1, 0, -3, 2],\n",
    "        [0, 1, -2, 1],\n",
    "        [0, 0, 3, -2],\n",
    "        [0, 0, -1, 1]\n",
    "    ], dtype=t.dtype, device=t.device)\n",
    "    return A @ tt\n",
    "\n",
    "def interp(x, y, xs):\n",
    "    x_ = x.reshape(-1, 1)\n",
    "    m = (y[1:] - y[:-1]) / (x_[1:] - x_[:-1])\n",
    "    m = torch.cat([m[[0]], (m[1:] + m[:-1]) / 2, m[[-1]]])\n",
    "\n",
    "    idxs = torch.searchsorted(x[1:].flatten(), xs)\n",
    "    dx = (x[idxs + 1] - x[idxs])\n",
    "    hh = h_poly((xs - x[idxs]) / dx).unsqueeze(-1)\n",
    "    ret = hh[0] * y[idxs]\n",
    "    dx = dx.unsqueeze(-1)\n",
    "\n",
    "    ret += hh[1] * m[idxs] * dx\n",
    "    ret += hh[2] * y[idxs + 1]\n",
    "    ret += hh[3] * m[idxs + 1] * dx\n",
    "    return ret\n",
    "\n",
    "xs = torch.tensor([0.1, 0.5, 1.5, 2.5, 2.75], requires_grad=True)\n",
    "x = torch.tensor([0, 1, 2., 3])\n",
    "y = torch.tensor([[1., 1], [2.25, 2.25], [3, 3], [4, 4.]])\n",
    "# y = torch.tensor([1, 2.25, 3, 4.])\n",
    "interpolated = interp(x, y, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated.retain_grad()\n",
    "interpolated.backward(torch.ones_like(interpolated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2925, 1.3125, 0.6562, 1.0312, 1.0391])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9027753365380764\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[0.9010, 1.9010, 2.9010],\n",
      "        [3.6287, 4.6287, 5.6287]])\n"
     ]
    }
   ],
   "source": [
    "r = random.random()\n",
    "print(r)\n",
    "a_interp = F.interpolate(a.unsqueeze(0), scale_factor=(1/r, 1), align_corners=False, mode='bicubic').squeeze()\n",
    "print(a_interp.shape)\n",
    "print(a_interp[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
