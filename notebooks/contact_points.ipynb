{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_38' (/home/abhinav/Downloads/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /home/abhinav/Downloads/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n",
      "PyTorch version 2.4.0+cu121\n",
      "Device count 2\n",
      "/home/abhinav/Downloads/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/src/gymtorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/abhinav/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/abhinav/.cache/torch_extensions/py38_cu121/gymtorch/build.ninja...\n",
      "Building extension module gymtorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module gymtorch...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from isaac_victor_envs.utils import get_assets_dir\n",
    "from isaac_victor_envs.tasks.allegro import AllegroScrewdriverTurningEnv\n",
    "# from isaac_victor_envs.tasks.allegro_ros import RosAllegroValveTurningEnv\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scipy\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import yaml\n",
    "import pathlib\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "import pytorch_volumetric as pv\n",
    "import pytorch_kinematics as pk\n",
    "import pytorch_kinematics.transforms as tf\n",
    "from torch.func import vmap, jacrev, hessian, jacfwd\n",
    "# import pytorch3d.transforms as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ccai.utils.allegro_utils import *\n",
    "# from allegro_valve_roll import AllegroValveTurning, AllegroContactProblem, PositionControlConstrainedSVGDMPC, \\\n",
    "#    add_trajectories, add_trajectories_hardware\n",
    "\n",
    "from ccai.allegro_contact import AllegroManipulationProblem, PositionControlConstrainedSVGDMPC, add_trajectories, \\\n",
    "    add_trajectories_hardware\n",
    "from ccai.allegro_screwdriver_problem_diffusion import AllegroScrewdriverDiff\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# from ccai.mpc.ipopt import IpoptMPC\n",
    "# from ccai.problem import IpoptProblem\n",
    "from ccai.models.trajectory_samplers import TrajectorySampler\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not connected to PVD\n",
      "Physics Engine: PhysX\n",
      "Physics Device: cpu\n",
      "GPU Pipeline: disabled\n",
      "Using VHACD cache directory '/home/abhinav/.isaacgym/vhacd'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/allegro/base_ns.stl'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/allegro/link_1.0.stl'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/ft_c.stl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinav/Documents/github/pytorch_volumetric/src/pytorch_volumetric/sdf.py:1138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cache = torch.load(dbpath)\n"
     ]
    }
   ],
   "source": [
    "class AllegroScrewdriver(AllegroManipulationProblem):\n",
    "    def __init__(self,\n",
    "                 start,\n",
    "                 goal,\n",
    "                 T,\n",
    "                 chain,\n",
    "                 object_location,\n",
    "                 object_type,\n",
    "                 world_trans,\n",
    "                 object_asset_pos,\n",
    "                 regrasp_fingers=[],\n",
    "                 contact_fingers=['index', 'middle', 'ring', 'thumb'],\n",
    "                 friction_coefficient=0.95,\n",
    "                 obj_dof=1,\n",
    "                 obj_ori_rep='euler',\n",
    "                 obj_joint_dim=0,\n",
    "                 optimize_force=False,\n",
    "                 turn=False,\n",
    "                 obj_gravity=False,\n",
    "                 min_force_dict=None,\n",
    "                 device='cuda:0',\n",
    "                 full_dof_goal=False, **kwargs):\n",
    "        self.obj_mass = 0.1\n",
    "        self.obj_dof_type = None\n",
    "        if obj_dof == 3:\n",
    "            object_link_name = 'screwdriver_body'\n",
    "        elif obj_dof == 1:\n",
    "            object_link_name = 'valve'\n",
    "        elif obj_dof == 6:\n",
    "            object_link_name = 'card'\n",
    "        self.obj_link_name = object_link_name\n",
    "        super(AllegroScrewdriver, self).__init__(start=start, goal=goal, T=T, chain=chain,\n",
    "                                                 object_location=object_location,\n",
    "                                                 object_type=object_type, world_trans=world_trans,\n",
    "                                                 object_asset_pos=object_asset_pos,\n",
    "                                                 regrasp_fingers=regrasp_fingers,\n",
    "                                                 contact_fingers=contact_fingers,\n",
    "                                                 friction_coefficient=friction_coefficient,\n",
    "                                                 obj_dof=obj_dof,\n",
    "                                                 obj_ori_rep=obj_ori_rep, obj_joint_dim=1,\n",
    "                                                 optimize_force=optimize_force, device=device,\n",
    "                                                 turn=turn, obj_gravity=obj_gravity,\n",
    "                                                 min_force_dict=min_force_dict, \n",
    "                                                 full_dof_goal=full_dof_goal, **kwargs)\n",
    "        self.friction_coefficient = friction_coefficient\n",
    "\n",
    "    def _cost(self, xu, start, goal):\n",
    "        # TODO: check if the addtional term of the smoothness cost and running goal cost is necessary\n",
    "        state = xu[:, :self.dx]  # state dim = 9\n",
    "        state = torch.cat((start.reshape(1, self.dx), state), dim=0)  # combine the first time step into it\n",
    "\n",
    "        smoothness_cost = torch.sum((state[1:, -self.obj_dof:] - state[:-1, -self.obj_dof:]) ** 2)\n",
    "        upright_cost = 0\n",
    "        # if not self.full_dof_goal:\n",
    "        upright_cost = 500 * torch.sum(\n",
    "            (state[:, -self.obj_dof:-1] + goal[-self.obj_dof:-1]) ** 2)  # the screwdriver should only rotate in z direction\n",
    "        return smoothness_cost + upright_cost + super()._cost(xu, start, goal)\n",
    "\n",
    "\n",
    "obj_dof = 3\n",
    "# config = yaml.safe_load(pathlib.Path(f'../examples/config/{sys.argv[1]}.yaml').read_text())\n",
    "config = yaml.safe_load(pathlib.Path(f'../examples/config/allegro_screwdriver_csvto_only.yaml').read_text())\n",
    "config['visualize'] = False\n",
    "\n",
    "if config['mode'] == 'hardware':\n",
    "    env = RosAllegroValveTurningEnv(1, control_mode='joint_impedance',\n",
    "                                    use_cartesian_controller=False,\n",
    "                                    viewer=True,\n",
    "                                    steps_per_action=60,\n",
    "                                    friction_coefficient=1.0,\n",
    "                                    device=config['sim_device'],\n",
    "                                    valve=config['object_type'],\n",
    "                                    video_save_path=img_save_dir,\n",
    "                                    joint_stiffness=config['kp'],\n",
    "                                    fingers=config['fingers'],\n",
    "                                    )\n",
    "else:\n",
    "    if not config['visualize']:\n",
    "        img_save_dir = None\n",
    "\n",
    "    env = AllegroScrewdriverTurningEnv(1, control_mode='joint_impedance',\n",
    "                                        use_cartesian_controller=False,\n",
    "                                        viewer=config['visualize'],\n",
    "                                        steps_per_action=60,\n",
    "                                        friction_coefficient=config['friction_coefficient'] * 1.05,\n",
    "                                        # friction_coefficient=1.0,  # DEBUG ONLY, set the friction very high\n",
    "                                        device=config['sim_device'],\n",
    "                                        video_save_path=img_save_dir,\n",
    "                                        joint_stiffness=config['kp'],\n",
    "                                        fingers=config['fingers'],\n",
    "                                        )\n",
    "\n",
    "sim, gym, viewer = env.get_sim()\n",
    "\n",
    "state = env.get_state()\n",
    "# try:\n",
    "#     while True:\n",
    "#         start = env.get_state()['q'][:, :-1]\n",
    "#         env.step(start)\n",
    "#         print('waiting for you to finish camera adjustment, ctrl-c when done')\n",
    "#         time.sleep(0.1)\n",
    "# except KeyboardInterrupt:\n",
    "#     pass\n",
    "\n",
    "sim_env = None\n",
    "ros_copy_node = None\n",
    "if config['mode'] == 'hardware':\n",
    "    sim_env = env\n",
    "    from hardware.hardware_env import HardwareEnv\n",
    "\n",
    "    env = HardwareEnv(sim_env.default_dof_pos[:, :16], finger_list=['index', 'thumb'], kp=config['kp'])\n",
    "    env.world_trans = sim_env.world_trans\n",
    "    env.joint_stiffness = sim_env.joint_stiffness\n",
    "    env.device = sim_env.device\n",
    "    env.valve_pose = sim_env.valve_pose\n",
    "elif config['mode'] == 'hardware_copy':\n",
    "    from hardware.hardware_env import RosNode\n",
    "\n",
    "    ros_copy_node = RosNode()\n",
    "\n",
    "results = {}\n",
    "\n",
    "# set up the kinematic chain\n",
    "asset = f'{get_assets_dir()}/xela_models/allegro_hand_right.urdf'\n",
    "ee_names = {\n",
    "    'index': 'allegro_hand_hitosashi_finger_finger_0_aftc_base_link',\n",
    "    'middle': 'allegro_hand_naka_finger_finger_1_aftc_base_link',\n",
    "    'ring': 'allegro_hand_kusuri_finger_finger_2_aftc_base_link',\n",
    "    'thumb': 'allegro_hand_oya_finger_3_aftc_base_link',\n",
    "}\n",
    "config['ee_names'] = ee_names\n",
    "config['obj_dof'] = 3\n",
    "\n",
    "screwdriver_asset = f'{get_assets_dir()}/screwdriver/screwdriver.urdf'\n",
    "\n",
    "chain = pk.build_chain_from_urdf(open(asset).read())\n",
    "screwdriver_chain = pk.build_chain_from_urdf(open(screwdriver_asset).read())\n",
    "frame_indices = [chain.frame_to_idx[ee_names[finger]] for finger in config['fingers']]  # combined chain\n",
    "frame_indices = torch.tensor(frame_indices)\n",
    "state2ee_pos = partial(state2ee_pos, fingers=config['fingers'], chain=chain, frame_indices=frame_indices,\n",
    "                        world_trans=env.world_trans)\n",
    "\n",
    "forward_kinematics = partial(chain.forward_kinematics,\n",
    "                                frame_indices=frame_indices)  # full_to= _partial_state = partial(full_to_partial_state, fingers=config['fingers'])\n",
    "partial_to_full_state = partial(partial_to_full_state, fingers=config['fingers'])\n",
    "\n",
    "controller = 'csvgd'\n",
    "goal = - 0.5 * torch.tensor([0, 0, np.pi])\n",
    "# set up params\n",
    "params = config.copy()\n",
    "params.pop('controllers')\n",
    "params.update(config['controllers'][controller])\n",
    "params['controller'] = controller\n",
    "params['valve_goal'] = goal.to(device=params['device'])\n",
    "params['chain'] = chain.to(device=params['device'])\n",
    "object_location = torch.tensor([0, 0, 1.205]).to(\n",
    "    params['device'])  # TODO: confirm if this is the correct location\n",
    "params['object_location'] = object_location\n",
    "\n",
    "num_fingers = len(params['fingers'])\n",
    "state = env.get_state()\n",
    "start = state['q'].reshape(4 * num_fingers + 4).to(device=params['device'])\n",
    "if 'csvgd' in params['controller']:\n",
    "    # index finger is used for stability\n",
    "    if 'index' in params['fingers']:\n",
    "        fingers = params['fingers']\n",
    "    else:\n",
    "        fingers = ['index'] + params['fingers']\n",
    "\n",
    "pregrasp_problem = AllegroScrewdriver(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    regrasp_fingers=fingers,\n",
    "    contact_fingers=[],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    ")\n",
    "# finger gate index\n",
    "index_regrasp_problem = AllegroScrewdriver(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    regrasp_fingers=['index'],\n",
    "    contact_fingers=['middle', 'thumb'],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    "    default_dof_pos=env.default_dof_pos[:, :16]\n",
    ")\n",
    "thumb_and_middle_regrasp_problem = AllegroScrewdriver(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    contact_fingers=['index'],\n",
    "    regrasp_fingers=['middle', 'thumb'],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    "    default_dof_pos=env.default_dof_pos[:, :16]\n",
    ")\n",
    "turn_problem = AllegroScrewdriver(\n",
    "    start=start[:4 * num_fingers + obj_dof],\n",
    "    goal=params['valve_goal'] * 0,\n",
    "    T=params['T'],\n",
    "    chain=params['chain'],\n",
    "    device=params['device'],\n",
    "    object_asset_pos=env.table_pose,\n",
    "    object_location=params['object_location'],\n",
    "    object_type=params['object_type'],\n",
    "    world_trans=env.world_trans,\n",
    "    contact_fingers=['index', 'middle', 'thumb'],\n",
    "    obj_dof=obj_dof,\n",
    "    obj_joint_dim=1,\n",
    "    optimize_force=params['optimize_force'],\n",
    "    default_dof_pos=env.default_dof_pos[:, :16]\n",
    ")\n",
    "contact_mode_dict = {0: 'pregrasp', 2: 'index', 1: 'thumb_middle', 3: 'turn'}\n",
    "t = params['T']\n",
    "# with open(data_path / f'constraint_violations.p', 'wb') as f:\n",
    "#     pickle.dump(constraint_violations_all, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contact_points(name):\n",
    "    path_base = f'/home/abhinav/Documents/ccai/data/experiments/{name}/train_data'\n",
    "\n",
    "    all_data = []\n",
    "    all_x = []\n",
    "    # all_d2goal = []\n",
    "    all_traj_data = []\n",
    "    for collection_idx in range(4):\n",
    "        path = path_base + f'/{name}_data_{collection_idx}/csvgd'\n",
    "        for trial_num in range(1, 61):\n",
    "            # print(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "            # if 'rand' in name or 'proj' in name or 'diff' in name:\n",
    "            # try:\n",
    "                # with open(path + f'/trial_{trial_num}/trajectory.pkl', 'rb') as data:\n",
    "                #     d = pickle.load(data)\n",
    "                #     traj = np.stack((d[:-1]), axis=0)\n",
    "                    # end = d[-1].reshape(1, -1)\n",
    "                    # end = np.concatenate((end, np.zeros((1, 21))), axis=1)\n",
    "                    # traj = np.concatenate((traj, end), axis=0)\n",
    "                # else:\n",
    "                #     d = np.load(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "                #     end_state = d['x']\n",
    "\n",
    "            data = np.load(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "            last_state = data['x']\n",
    "            last_state = last_state.reshape(1, -1)\n",
    "            # Concatenate zeros to the end of the last state\n",
    "            last_last_state = np.concatenate((last_state, np.zeros((1, 21))), axis=1)\n",
    "            traj = []\n",
    "\n",
    "            with open(path + f'/trial_{trial_num}/traj_data.p', 'rb') as f:\n",
    "                traj_data = pickle.load(f)\n",
    "                for key in traj_data.keys():\n",
    "                    if torch.is_tensor(traj_data[key]):\n",
    "                        traj_data[key] = traj_data[key].cpu().numpy()\n",
    "                # all_d2goal.append(d2goal)\n",
    "                # if 'rand' not in name and 'proj' not in name and 'diff' not in name::\n",
    "                #     traj = traj_data[t]['plans'][:, 0]\n",
    "\n",
    "                    # end = np.concatenate((end_state, np.zeros((1, 21))), axis=1)\n",
    "                    # traj = np.concatenate((traj, end), axis=0)\n",
    "                for t in range(12, 1 - 1, -1):\n",
    "                    traj.append(traj_data[t]['starts'][:, 0, :])\n",
    "\n",
    "                traj = np.stack(traj, axis=1)\n",
    "                last_state = traj[1:, 0]\n",
    "                last_state = np.concatenate((last_state, last_last_state), axis=0)\n",
    "                last_state = np.expand_dims(last_state, axis=1)\n",
    "                traj = np.concatenate((traj, last_state), axis=1)\n",
    "                    \n",
    "                traj_data[12]['traj'] = np.expand_dims(traj, axis=1)\n",
    "                all_traj_data.append(traj_data)  \n",
    "                all_data.append(traj_data)\n",
    "\n",
    "            # except:\n",
    "            #     continue\n",
    "            \n",
    "    constraint_violations_all = {\n",
    "        # 'optimizer_paths': [],\n",
    "        'traj': [],\n",
    "        # 'inits': [],\n",
    "        # 'plans': [],\n",
    "\n",
    "    }\n",
    "    # for plans_or_inits in constraint_violations_all.keys():\n",
    "    #     if plans_or_inits == 'traj':\n",
    "    #         gen_constraint_data(plans_or_inits, constraint_violations_all[plans_or_inits], path, traj_data=all_traj_data)\n",
    "    #     else:\n",
    "    #         gen_constraint_data(plans_or_inits, constraint_violations_all[plans_or_inits], path)\n",
    "\n",
    "    # Take the list of dicts and turn it into a dict of lists\n",
    "    all_data = {k: [d[k] for d in all_data] for k in all_data[0]}\n",
    "    all_data['violation'] = constraint_violations_all\n",
    "\n",
    "    return all_x, all_data#, all_d2goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact Point test\n"
     ]
    }
   ],
   "source": [
    "data_exec = {}\n",
    "t = params['T']\n",
    "\n",
    "for key, name in [\n",
    "                ('Contact Point test', 'allegro_high_force_high_eps_pi_6'),\n",
    "                ]:\n",
    "    \n",
    "    print(key)\n",
    "    data_exec[key] = {}\n",
    "    all_x, all_data = get_contact_points(name)\n",
    "\n",
    "\n",
    "    data_exec[key] = {**data_exec[key], **all_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_robot_to_obj(coords, tf_robot_to_world, tf_world_to_obj):\n",
    "    coords_world = tf_robot_to_world.transform_points(coords)\n",
    "    coords_obj = tf_world_to_obj.transform_points(coords_world)\n",
    "    return coords_obj\n",
    "\n",
    "def _convert_obj_to_robot(coords, tf_robot_to_world, tf_world_to_obj):\n",
    "\n",
    "    coords_world = tf_world_to_obj.inverse().transform_points(coords)\n",
    "    coords_robot = tf_robot_to_world.inverse().transform_points(coords_world)\n",
    "\n",
    "    return coords_robot\n",
    "\n",
    "def convert_contact_data_to_obj(contact_points, contact_normals, tf_robot_to_world, tf_world_to_obj):\n",
    "    contact_points_obj = _convert_robot_to_obj(contact_points, tf_robot_to_world, tf_world_to_obj)\n",
    "    contact_normals_obj = _convert_robot_to_obj(contact_normals, tf_robot_to_world, tf_world_to_obj)\n",
    "    return contact_points_obj, contact_normals_obj\n",
    "\n",
    "def convert_contact_data_to_robot(contact_points_obj, contact_normals_obj, tf_robot_to_world, tf_world_to_obj):\n",
    "    orig_shape = contact_points_obj.shape\n",
    "    contact_points_obj = contact_points_obj.reshape(-1, 3)\n",
    "    contact_normals_obj = contact_normals_obj.reshape(-1, 3)\n",
    "    contact_points_robot = _convert_obj_to_robot(contact_points_obj, tf_robot_to_world, tf_world_to_obj)\n",
    "    contact_normals_robot = _convert_obj_to_robot(contact_normals_obj, tf_robot_to_world, tf_world_to_obj)\n",
    "\n",
    "    contact_points_robot = contact_points_robot.reshape(orig_shape)\n",
    "    contact_normals_robot = contact_normals_robot.reshape(orig_shape)\n",
    "    return contact_points_robot, contact_normals_robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d6dcc4f1ab4d35965891abb806988e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([91, 16])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 1 has a total capacity of 23.65 GiB of which 10.31 MiB is free. Process 627143 has 2.42 GiB memory in use. Process 654772 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 20.80 GiB memory in use. Of the allocated memory 20.32 GiB is allocated by PyTorch, and 34.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_trajs), batch_size)):\n\u001b[0;32m---> 58\u001b[0m     min_contact_points_obj_batch, min_contact_normals_obj_batch, min_tfs_batch \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_trajs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     min_contact_points_obj\u001b[38;5;241m.\u001b[39mappend(min_contact_points_obj_batch)\n\u001b[1;32m     60\u001b[0m     min_contact_normals_obj\u001b[38;5;241m.\u001b[39mappend(min_contact_normals_obj_batch)\n",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(all_trajs)\u001b[0m\n\u001b[1;32m     30\u001b[0m full_q \u001b[38;5;241m=\u001b[39m partial_to_full_state(q_b)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(full_q\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 34\u001b[0m ret_scene \u001b[38;5;241m=\u001b[39m \u001b[43mcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscene_collision_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcompute_gradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcompute_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m contact_points_obj, contact_normals_obj \u001b[38;5;241m=\u001b[39m convert_contact_data_to_obj(ret_scene[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosest_pt_world\u001b[39m\u001b[38;5;124m'\u001b[39m], ret_scene[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontact_normal\u001b[39m\u001b[38;5;124m'\u001b[39m], tf_robot_to_world, tf_world_to_obj)\n\u001b[1;32m     39\u001b[0m sdf \u001b[38;5;241m=\u001b[39m ret_scene[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msdf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(N, C, T, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/github/pytorch_volumetric/src/pytorch_volumetric/robot_scene.py:682\u001b[0m, in \u001b[0;36mRobotScene.scene_collision_check\u001b[0;34m(self, q, env_q, compute_gradient, compute_hessian)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m env_q \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMust provide environment configuration if scene is articulated SDF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collision_check_against_robot_sdf_per_link\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscene_sdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mcompute_hessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collision_check(q, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscene_sdf, compute_gradient, compute_hessian)\n",
      "File \u001b[0;32m~/Documents/github/pytorch_volumetric/src/pytorch_volumetric/robot_scene.py:499\u001b[0m, in \u001b[0;36mRobotScene._collision_check_against_robot_sdf_per_link\u001b[0;34m(self, q, env_q, sdf, compute_gradient, compute_hessian)\u001b[0m\n\u001b[1;32m    496\u001b[0m pts_world \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_to_world(q)\n\u001b[1;32m    497\u001b[0m pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_world_to_scene(pts_world)\n\u001b[0;32m--> 499\u001b[0m sdf_result \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_extra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m sdf_vals \u001b[38;5;241m=\u001b[39m sdf_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msdf_val\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    501\u001b[0m sdf_grads \u001b[38;5;241m=\u001b[39m sdf_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msdf_grad\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/github/pytorch_volumetric/src/pytorch_volumetric/model_to_sdf.py:158\u001b[0m, in \u001b[0;36mRobotSDF.__call__\u001b[0;34m(self, points_in_object_frame, return_extra_info)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, points_in_object_frame, return_extra_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    Query for SDF value and SDF gradients for points in the robot's frame\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    :param points_in_object_frame: [B x] N x 3 optionally arbitrarily batched points in the robot frame; B can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    number of batch dimensions.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints_in_object_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_extra_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/pytorch_volumetric/src/pytorch_volumetric/sdf.py:761\u001b[0m, in \u001b[0;36mComposedSDF.__call__\u001b[0;34m(self, points_in_object_frame, return_extra_info)\u001b[0m\n\u001b[1;32m    759\u001b[0m     pts \u001b[38;5;241m=\u001b[39m points_in_object_frame\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtsf_batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(S, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtsf_batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    760\u001b[0m     N \u001b[38;5;241m=\u001b[39m pts\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 761\u001b[0m     pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj_frame_to_link_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m     pts \u001b[38;5;241m=\u001b[39m pts\u001b[38;5;241m.\u001b[39mreshape(S, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtsf_batch, N, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m# flatten it for the transform\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/pytorch_kinematics/src/pytorch_kinematics/transforms/transform3d.py:384\u001b[0m, in \u001b[0;36mTransform3d.transform_points\u001b[0;34m(self, points, eps)\u001b[0m\n\u001b[1;32m    382\u001b[0m N, P, _3 \u001b[38;5;241m=\u001b[39m points_batch\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    383\u001b[0m ones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(N, P, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mpoints\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mpoints\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 384\u001b[0m points_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpoints_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mones\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m composed_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_matrix()\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    387\u001b[0m points_out \u001b[38;5;241m=\u001b[39m _broadcast_bmm(points_batch, composed_matrix)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 1 has a total capacity of 23.65 GiB of which 10.31 MiB is free. Process 627143 has 2.42 GiB memory in use. Process 654772 has 384.00 MiB memory in use. Including non-PyTorch memory, this process has 20.80 GiB memory in use. Of the allocated memory 20.32 GiB is allocated by PyTorch, and 34.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "cs = turn_problem.contact_scenes\n",
    "object_location = object_location.reshape(1, 3).to(device=params['device'])\n",
    "tf_robot_to_world = env.world_trans.to(device=params['device'])#.inverse().to(device=params['device'])\n",
    "\n",
    "\n",
    "\n",
    "num_trials = len(data_exec['Contact Point test'][12])\n",
    "all_trajs = []\n",
    "for trial_num in range(num_trials):\n",
    "    all_trajs.append(torch.from_numpy(data_exec['Contact Point test'][12][trial_num]['traj'].squeeze()).to(device=params['device']))\n",
    "\n",
    "def process_batch(all_trajs):\n",
    "    all_trajs = torch.stack(all_trajs, dim=0)\n",
    "    # (num_trials, num_modes, num_timesteps)\n",
    "    N, C, T, _ = all_trajs.shape\n",
    "    robot_q = all_trajs[..., :12]\n",
    "\n",
    "    screwdriver_ori = all_trajs[..., -obj_dof:]\n",
    "    screwdirver_ori_mat = tf.euler_angles_to_matrix(screwdriver_ori, convention='XYZ').reshape(-1, 3, 3)\n",
    "    screwdriver_ori_quat = tf.matrix_to_quaternion(screwdirver_ori_mat).reshape(-1, 4)\n",
    "    tf_obj_to_world = tf.Transform3d(rot=screwdriver_ori_quat, pos=object_location, device=params['device'])\n",
    "    tf_world_to_obj = tf_obj_to_world.inverse()\n",
    "\n",
    "    q_b = robot_q.reshape(-1, 4 * 3)\n",
    "    theta_b = screwdriver_ori.reshape(-1, obj_dof)\n",
    "    theta_obj_joint = torch.zeros((theta_b.shape[0], 1),\n",
    "                                    device=theta_b.device)  # add an additional dimension for the cap of the screw driver\n",
    "    # the cap does not matter for the task, but needs to be included in the state for the model\n",
    "    theta_b = torch.cat((theta_b, theta_obj_joint), dim=1)\n",
    "    full_q = partial_to_full_state(q_b)\n",
    "\n",
    "    print(full_q.shape)\n",
    "\n",
    "    ret_scene = cs.scene_collision_check(full_q.float(), theta_b.float(),\n",
    "                                        compute_gradient=True,\n",
    "                                        compute_hessian=False)\n",
    "\n",
    "    contact_points_obj, contact_normals_obj = convert_contact_data_to_obj(ret_scene['closest_pt_world'], ret_scene['contact_normal'], tf_robot_to_world, tf_world_to_obj)\n",
    "    sdf = ret_scene['sdf'].reshape(N, C, T, -1)\n",
    "    contact_points_obj = contact_points_obj.reshape(N, C, T, -1, 3)\n",
    "    contact_normals_obj = contact_normals_obj.reshape(N, C, T, -1, 3)\n",
    "    tfs = tf_world_to_obj.get_matrix().reshape(N, C, T, 4, 4).unsqueeze(-3).repeat(1, 1, 1, 3, 1, 1)\n",
    "    min_ind = torch.argmin(sdf.abs(), dim=-2)\n",
    "\n",
    "    min_dist = torch.gather(sdf, 2, min_ind.unsqueeze(-1)).squeeze(-1)\n",
    "    min_contact_points_obj = torch.gather(contact_points_obj, 2, min_ind.unsqueeze(-2).unsqueeze(-1).expand(-1, -1, -1, -1, 3)).squeeze(-3)\n",
    "    min_contact_normals_obj = torch.gather(contact_normals_obj, 2, min_ind.unsqueeze(-2).unsqueeze(-1).expand(-1, -1, -1, -1, 3)).squeeze(-3)\n",
    "    min_tfs = torch.gather(tfs, 2, min_ind.unsqueeze(-2).unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, -1, 4, 4)).squeeze(2)#[:, :, 0]\n",
    "\n",
    "    return min_contact_points_obj, min_contact_normals_obj, min_tfs\n",
    "\n",
    "# Loop over all_traj with batch_size 1024\n",
    "min_contact_points_obj = []\n",
    "min_contact_normals_obj = []\n",
    "min_tfs = []\n",
    "batch_size = 1\n",
    "for i in tqdm(range(0, len(all_trajs), batch_size)):\n",
    "    min_contact_points_obj_batch, min_contact_normals_obj_batch, min_tfs_batch = process_batch(all_trajs[i:i+batch_size])\n",
    "    min_contact_points_obj.append(min_contact_points_obj_batch)\n",
    "    min_contact_normals_obj.append(min_contact_normals_obj_batch)\n",
    "    min_tfs.append(min_tfs_batch)\n",
    "\n",
    "min_contact_points_obj = torch.cat(min_contact_points_obj, dim=0)\n",
    "min_contact_normals_obj = torch.cat(min_contact_normals_obj, dim=0)\n",
    "min_tfs = torch.cat(min_tfs, dim=0)\n",
    "\n",
    "\n",
    "# Group the contact points and normals by contact mode\n",
    "contact_data_obj_by_mode = defaultdict(list)\n",
    "for trial_num in range(N):\n",
    "    contact_states = data_exec['Contact Point test'][12][trial_num]['contact_state']\n",
    "    for contact_state_idx in range(C):\n",
    "        contact_state_idx_tuple = tuple(contact_states[contact_state_idx].tolist())\n",
    "        contact_data_obj_by_mode[contact_state_idx_tuple].append(\n",
    "            (min_contact_points_obj[trial_num, contact_state_idx], \n",
    "             min_contact_normals_obj[trial_num, contact_state_idx],\n",
    "             min_tfs[trial_num, contact_state_idx])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contact_data_obj_by_mode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m mode \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m pts_th_m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcontact_data_obj_by_mode\u001b[49m[mode]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#.reshape(-1, 3)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m orig_shape \u001b[38;5;241m=\u001b[39m pts_th_m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      5\u001b[0m pts_th_m \u001b[38;5;241m=\u001b[39m pts_th_m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'contact_data_obj_by_mode' is not defined"
     ]
    }
   ],
   "source": [
    "mode = (1.0, 1.0, 1.0)\n",
    "\n",
    "pts_th_m = torch.stack([i[0] for i in contact_data_obj_by_mode[mode]], dim=1)#.reshape(-1, 3)\n",
    "orig_shape = pts_th_m.shape\n",
    "pts_th_m = pts_th_m.reshape(-1, 3)\n",
    "normals_th_m = torch.stack([i[1] for i in contact_data_obj_by_mode[mode]], dim=1).reshape(-1, 3)\n",
    "tfs_th_m = torch.stack([i[2] for i in contact_data_obj_by_mode[mode]], dim=1).reshape(-1, 4, 4)#[0].unsqueeze(0).repeat(pts_th_m.shape[0], 1, 1)\n",
    "\n",
    "\n",
    "# tfs_th_m = torch.tensor([\n",
    "#     [1., 0, 0, 0],\n",
    "#     [0, 1., 0, 0],\n",
    "#     [0, 0, 1., -1.205],\n",
    "#     [0, 0, 0, 1]\n",
    "# ]).unsqueeze(0).repeat(normals_th_m.shape[0], 1, 1).to(device=params['device'])\n",
    "tfs_th_m = tf.Transform3d(matrix=tfs_th_m, device=params['device'])\n",
    "\n",
    "contact_points_obj = pts_th_m\n",
    "contact_normals_obj = normals_th_m\n",
    "contact_points_obj = contact_points_obj.reshape(-1, 3)\n",
    "contact_normals_obj = contact_normals_obj.reshape(-1, 3)\n",
    "contact_points_robot = _convert_obj_to_robot(contact_points_obj.unsqueeze(1), tf_robot_to_world, tfs_th_m)\n",
    "contact_normals_robot = _convert_obj_to_robot(contact_normals_obj.unsqueeze(1), tf_robot_to_world, tfs_th_m)\n",
    "pts_th_m_rob = contact_points_robot.reshape(orig_shape).squeeze()\n",
    "normals_th_m_rob = contact_normals_robot.reshape(orig_shape).squeeze()\n",
    "\n",
    "\n",
    "# pts_th_m_rob, normals_th_m_rob = convert_contact_data_to_robot(pts_th_m, normals_th_m, tf_robot_to_world, tf_world_to_obj)\n",
    "colors = torch.tensor([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "], device=params['device'])\n",
    "colors = colors.unsqueeze(1).expand(-1, pts_th_m_rob.shape[1], -1)\n",
    "\n",
    "pcd_points = torch.cat((pts_th_m_rob, colors), dim=-1).cpu().numpy().reshape(-1, 6)\n",
    "normals = normals_th_m_rob.cpu().numpy().reshape(-1, 3)\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(pcd_points[:, :3])\n",
    "pcd.colors = o3d.utility.Vector3dVector(pcd_points[:, 3:])\n",
    "pcd.normals = o3d.utility.Vector3dVector(normals)\n",
    "\n",
    "traj_for_viz = torch.cat((robot_q, theta_b.reshape(*robot_q.shape[:-1], -1)), dim=-1)[0:1, 0, 0]\n",
    "\n",
    "visualize_trajectory(traj_for_viz, cs, 'images', config['fingers'], obj_dof+1, pcd=pcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
