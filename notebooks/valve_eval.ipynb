{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing module 'gym_38' (/home/abhinav/Documents/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)\n",
      "Setting GYM_USD_PLUG_INFO_PATH to /home/abhinav/Documents/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json\n",
      "PyTorch version 2.4.1+cu121\n",
      "Device count 2\n",
      "/home/abhinav/Documents/isaacgym/python/isaacgym/_bindings/src/gymtorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/abhinav/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/abhinav/.cache/torch_extensions/py38_cu121/gymtorch/build.ninja...\n",
      "Building extension module gymtorch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module gymtorch...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from isaac_victor_envs.utils import get_assets_dir\n",
    "from isaac_victor_envs.tasks.allegro import AllegroValveTurningEnv\n",
    "# from isaac_victor_envs.tasks.allegro_ros import RosAllegroValveTurningEnv\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scipy\n",
    "\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "import pytorch_volumetric as pv\n",
    "import pytorch_kinematics as pk\n",
    "import pytorch_kinematics.transforms as tf\n",
    "from torch.func import vmap, jacrev, hessian, jacfwd\n",
    "# import pytorch3d.transforms as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ccai.utils.allegro_utils import *\n",
    "# from allegro_valve_roll import AllegroValveTurning, AllegroContactProblem, PositionControlConstrainedSVGDMPC, \\\n",
    "#    add_trajectories, add_trajectories_hardware\n",
    "\n",
    "from ccai.allegro_contact import AllegroManipulationProblem, PositionControlConstrainedSVGDMPC, add_trajectories, \\\n",
    "    add_trajectories_hardware\n",
    "from ccai.allegro_screwdriver_problem_diffusion import AllegroScrewdriverDiff\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# from ccai.mpc.ipopt import IpoptMPC\n",
    "# from ccai.problem import IpoptProblem\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robot pose [ 0.02  -0.35   0.376] [0, 0, 0.7071068, 0.7071068]\n",
      "Not connected to PVD\n",
      "Physics Engine: PhysX\n",
      "Physics Device: cpu\n",
      "GPU Pipeline: disabled\n",
      "Using VHACD cache directory '/home/abhinav/.isaacgym/vhacd'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/allegro/base_ns.stl'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/allegro/link_1.0.stl'\n",
      "Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/ft_c.stl'\n"
     ]
    }
   ],
   "source": [
    "obj_dof = 3\n",
    "# config = yaml.safe_load(pathlib.Path(f'../examples/config/{sys.argv[1]}.yaml').read_text())\n",
    "config = yaml.safe_load(pathlib.Path(f'../examples/config/screwdriver/allegro_screwdriver_csvto_OOD_ID_test.yaml').read_text())\n",
    "config['visualize'] = False\n",
    "\n",
    "if config['mode'] == 'hardware':\n",
    "    env = RosAllegroValveTurningEnv(1, control_mode='joint_impedance',\n",
    "                                    use_cartesian_controller=False,\n",
    "                                    viewer=True,\n",
    "                                    steps_per_action=60,\n",
    "                                    friction_coefficient=1.0,\n",
    "                                    device=config['sim_device'],\n",
    "                                    valve=config['object_type'],\n",
    "                                    video_save_path=img_save_dir,\n",
    "                                    joint_stiffness=config['kp'],\n",
    "                                    fingers=config['fingers'],\n",
    "                                    )\n",
    "else:\n",
    "    if not config['visualize']:\n",
    "        img_save_dir = None\n",
    "\n",
    "    env = AllegroValveTurningEnv(1, control_mode='joint_impedance',\n",
    "                                        use_cartesian_controller=False,\n",
    "                                        viewer=config['visualize'],\n",
    "                                        steps_per_action=60,\n",
    "                                        friction_coefficient=config['friction_coefficient'] * 1.05,\n",
    "                                        # friction_coefficient=1.0,  # DEBUG ONLY, set the friction very high\n",
    "                                        device=config['sim_device'],\n",
    "                                        video_save_path=img_save_dir,\n",
    "                                        joint_stiffness=config['kp'],\n",
    "                                        fingers=config['fingers'],\n",
    "                                        )\n",
    "\n",
    "sim, gym, viewer = env.get_sim()\n",
    "\n",
    "state = env.get_state()\n",
    "# try:\n",
    "#     while True:\n",
    "#         start = env.get_state()['q'][:, :-1]\n",
    "#         env.step(start)\n",
    "#         print('waiting for you to finish camera adjustment, ctrl-c when done')\n",
    "#         time.sleep(0.1)\n",
    "# except KeyboardInterrupt:\n",
    "#     pass\n",
    "\n",
    "sim_env = None\n",
    "ros_copy_node = None\n",
    "if config['mode'] == 'hardware':\n",
    "    sim_env = env\n",
    "    from hardware.hardware_env import HardwareEnv\n",
    "\n",
    "    env = HardwareEnv(sim_env.default_dof_pos[:, :16], finger_list=['index', 'thumb'], kp=config['kp'])\n",
    "    env.world_trans = sim_env.world_trans\n",
    "    env.joint_stiffness = sim_env.joint_stiffness\n",
    "    env.device = sim_env.device\n",
    "    env.valve_pose = sim_env.valve_pose\n",
    "elif config['mode'] == 'hardware_copy':\n",
    "    from hardware.hardware_env import RosNode\n",
    "\n",
    "    ros_copy_node = RosNode()\n",
    "\n",
    "results = {}\n",
    "\n",
    "# set up the kinematic chain\n",
    "asset = f'{get_assets_dir()}/xela_models/allegro_hand_right.urdf'\n",
    "ee_names = {\n",
    "    'index': 'allegro_hand_hitosashi_finger_finger_0_aftc_base_link',\n",
    "    'middle': 'allegro_hand_naka_finger_finger_1_aftc_base_link',\n",
    "    'ring': 'allegro_hand_kusuri_finger_finger_2_aftc_base_link',\n",
    "    'thumb': 'allegro_hand_oya_finger_3_aftc_base_link',\n",
    "}\n",
    "config['ee_names'] = ee_names\n",
    "config['obj_dof'] = 3\n",
    "\n",
    "screwdriver_asset = f'{get_assets_dir()}/valve/valve_cross.urdf'\n",
    "\n",
    "chain = pk.build_chain_from_urdf(open(asset).read())\n",
    "screwdriver_chain = pk.build_chain_from_urdf(open(screwdriver_asset).read())\n",
    "frame_indices = [chain.frame_to_idx[ee_names[finger]] for finger in config['fingers']]  # combined chain\n",
    "frame_indices = torch.tensor(frame_indices)\n",
    "state2ee_pos = partial(state2ee_pos, fingers=config['fingers'], chain=chain, frame_indices=frame_indices,\n",
    "                        world_trans=env.world_trans)\n",
    "\n",
    "forward_kinematics = partial(chain.forward_kinematics,\n",
    "                                frame_indices=frame_indices)  # full_to= _partial_state = partial(full_to_partial_state, fingers=config['fingers'])\n",
    "partial_to_full_state = partial(partial_to_full_state, fingers=config['fingers'])\n",
    "\n",
    "controller = 'csvgd'\n",
    "goal = - 0.5 * torch.tensor([0, 0, np.pi])\n",
    "# set up params\n",
    "params = config.copy()\n",
    "params.pop('controllers')\n",
    "params.update(config['controllers'][controller])\n",
    "params['controller'] = controller\n",
    "params['valve_goal'] = goal.to(device=params['device'])\n",
    "params['chain'] = chain.to(device=params['device'])\n",
    "object_location = torch.tensor([0, 0, 1.205]).to(\n",
    "    params['device'])  # TODO: confirm if this is the correct location\n",
    "params['object_location'] = object_location\n",
    "\n",
    "num_fingers = len(params['fingers'])\n",
    "state = env.get_state()\n",
    "start = state['q'].reshape(4 * num_fingers + 1).to(device=params['device'])\n",
    "if 'csvgd' in params['controller']:\n",
    "    # index finger is used for stability\n",
    "    if 'index' in params['fingers']:\n",
    "        fingers = params['fingers']\n",
    "    else:\n",
    "        fingers = ['index'] + params['fingers']\n",
    "\n",
    "t = 3\n",
    "# with open(data_path / f'constraint_violations.p', 'wb') as f:\n",
    "#     pickle.dump(constraint_violations_all, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_recovery_metrics(\n",
    "    final_likelihood: List[List[float]], \n",
    "    pre_action_likelihoods: List[List[float]],\n",
    "    dropped_info: List[bool] = None,\n",
    "    threshold: float = -70.0,\n",
    "    safe_rl: bool = False\n",
    ") -> Tuple[List[int], List[int], List[int], List[float], List[bool]]:\n",
    "    \"\"\"\n",
    "    Calculate recovery metrics from likelihood time series data.\n",
    "    \n",
    "    Args:\n",
    "        final_likelihood: Nested list of likelihood values per trial and step\n",
    "        dropped_info: Optional list of booleans indicating if the object was dropped\n",
    "        threshold: Threshold below which state is considered in need of recovery\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - one_step_recovery_success: Binary indicators of one-step recovery success\n",
    "            - eventual_recovery_success: Binary indicators of eventual recovery success\n",
    "            - num_recovery_attempts_to_recover: Number of attempts needed for recovery\n",
    "            - recovery_improvement: Improvement in likelihood during recovery\n",
    "            - dropped_recovery: Indicators if recovery ended due to dropping\n",
    "    \"\"\"\n",
    "    one_step_recovery_success = []\n",
    "    recovery_improvement = []\n",
    "    dropped_recovery = []\n",
    "    \n",
    "    eventual_recovery_success = []\n",
    "    recovery_improvement_block = []\n",
    "    dropped_recovery_block = []\n",
    "    num_recovery_attempts_to_recover = []\n",
    "\n",
    "    \n",
    "    for trial_idx, trial in enumerate(final_likelihood):\n",
    "        # print(trial_idx+1)\n",
    "        # print(trial)\n",
    "        # print( [trial_idx])\n",
    "        # trial = sum(trial, [])\n",
    "        # print(trial)\n",
    "        trial = [l for l in trial if None not in l and len(l) > 0]\n",
    "        # print(trial)\n",
    "        # trial.insert(0, [pre_action_likelihoods[trial_idx][0][-1]])\n",
    "        for k in range(len(trial)):\n",
    "            if len(trial[k]) == 0 and len(pre_action_likelihoods[trial_idx][k]) > 0:\n",
    "                trial[k].append(pre_action_likelihoods[trial_idx][k][-1])\n",
    "            # elif len(trial[k]) == 0:\n",
    "            #     trial[k].append(-200) # Placeholder\n",
    "        # print(trial)\n",
    "        # print()\n",
    "        if safe_rl:\n",
    "            # Delete any negative values from the trial\n",
    "            trial = [l for l in trial if l[0] >= 0]\n",
    "            trial = [[-i.item() for i in l] for l in trial]\n",
    "        np_trial = np.array(trial)\n",
    "        # print(np_trial.flatten())\n",
    "        # print()\n",
    "        recovery_blocks = []\n",
    "        is_in_recovery_block = False\n",
    "        current_block = {}\n",
    "        \n",
    "        # Check if dropped_info is available for this trial\n",
    "        trial_dropped = dropped_info[trial_idx] if dropped_info and trial_idx < len(dropped_info) else False\n",
    "        \n",
    "        for step_idx in range(np_trial.shape[0]):\n",
    "            likelihood = np_trial[step_idx]\n",
    "            # print(likelihood)\n",
    "            \n",
    "            # Start of a recovery block\n",
    "            if not is_in_recovery_block and likelihood < threshold:\n",
    "                is_in_recovery_block = True\n",
    "                current_block = {\n",
    "                    'start_idx': step_idx,\n",
    "                    'start_likelihood': likelihood,\n",
    "                    'attempts': 1  # This is the first attempt\n",
    "                }\n",
    "            \n",
    "            # Continue an existing recovery block\n",
    "            elif is_in_recovery_block and ( likelihood < threshold):\n",
    "                current_block['attempts'] += 1\n",
    "                # if likelihood is None:\n",
    "                #     likelihood = -160\n",
    "                recovery_improvement.append(\n",
    "                    likelihood - current_block['start_likelihood']\n",
    "                )\n",
    "                dropped_recovery.append(False)  # Successful recovery wasn't due to dropping\n",
    "                one_step_recovery_success.append(0)\n",
    "            \n",
    "            # End of a recovery block - successful\n",
    "            elif is_in_recovery_block and likelihood >= threshold:\n",
    "                is_in_recovery_block = False\n",
    "                current_block['end_idx'] = step_idx\n",
    "                current_block['end_likelihood'] = likelihood\n",
    "                current_block['success'] = True\n",
    "                current_block['duration'] = step_idx - current_block['start_idx']\n",
    "                current_block['dropped'] = False\n",
    "                recovery_blocks.append(current_block)\n",
    "                \n",
    "                # Calculate one-step recovery success (duration = 1)\n",
    "                one_step_success = int(current_block['duration'] == 1)\n",
    "                one_step_recovery_success.append(one_step_success)\n",
    "                \n",
    "                # Track other metrics\n",
    "                num_recovery_attempts_to_recover.append(current_block['attempts'])\n",
    "                eventual_recovery_success.append(1)\n",
    "                recovery_improvement_block.append(\n",
    "                    current_block['end_likelihood'] - current_block['start_likelihood']\n",
    "                )\n",
    "                recovery_improvement.append(\n",
    "                    current_block['end_likelihood'] - current_block['start_likelihood']\n",
    "                )\n",
    "                dropped_recovery_block.append(False)  # Successful recovery wasn't due to dropping\n",
    "                dropped_recovery.append(False)  # Successful recovery wasn't due to dropping\n",
    "            # else:\n",
    "            #     print('a', step_idx)\n",
    "        \n",
    "        # Handle case where trial ended during a recovery block (unsuccessful)\n",
    "        if is_in_recovery_block:\n",
    "            current_block['end_idx'] = np_trial.shape[0] - 1\n",
    "            current_block['end_likelihood'] = np_trial[-1]\n",
    "            # if current_block['end_likelihood'] is None:\n",
    "            #     current_block['end_likelihood'] = -160\n",
    "            current_block['success'] = False\n",
    "            current_block['duration'] = np_trial.shape[0] - 1 - current_block['start_idx']\n",
    "            current_block['dropped'] = trial_dropped\n",
    "            recovery_blocks.append(current_block)\n",
    "            \n",
    "            # Not a one-step recovery\n",
    "            one_step_recovery_success.append(0)\n",
    "            eventual_recovery_success.append(0)\n",
    "            num_recovery_attempts_to_recover.append(current_block['attempts'])\n",
    "            recovery_improvement_block.append(\n",
    "                current_block['end_likelihood'] - current_block['start_likelihood']\n",
    "            )\n",
    "            recovery_improvement.append(\n",
    "                current_block['end_likelihood'] - current_block['start_likelihood']\n",
    "            )\n",
    "            dropped_recovery_block.append(trial_dropped)  # Was this unsuccessful recovery due to dropping?\n",
    "            dropped_recovery.append(trial_dropped)  # Successful recovery wasn't due to dropping\n",
    "        # print(one_step_recovery_success)\n",
    "        # print(eventual_recovery_success)\n",
    "        # print()\n",
    "    return one_step_recovery_success, eventual_recovery_success, num_recovery_attempts_to_recover, recovery_improvement, dropped_recovery, recovery_improvement_block, dropped_recovery_block\n",
    "\n",
    "def compute_recovery_statistics(\n",
    "    all_data: Dict[str, Any],\n",
    "    one_step_recovery_success: List[int],\n",
    "    eventual_recovery_success: List[int],\n",
    "    num_recovery_attempts: List[int]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute summary statistics from recovery metrics.\n",
    "    \n",
    "    Args:\n",
    "        all_data: Dictionary containing all trial data\n",
    "        one_step_recovery_success: Binary indicators of one-step recovery success\n",
    "        eventual_recovery_success: Binary indicators of eventual recovery success\n",
    "        num_recovery_attempts: Number of attempts needed for successful recovery\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of computed statistics\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # One-step recovery success rate\n",
    "    if one_step_recovery_success:\n",
    "        stats['one_step_recovery_rate'] = np.mean(one_step_recovery_success)\n",
    "        stats['one_step_recovery_count'] = np.sum(one_step_recovery_success)\n",
    "    else:\n",
    "        stats['one_step_recovery_rate'] = float('nan')\n",
    "        stats['one_step_recovery_count'] = 0\n",
    "    \n",
    "    # Eventual recovery metrics\n",
    "    if eventual_recovery_success:\n",
    "        stats['eventual_recovery_rate'] = np.mean(eventual_recovery_success)\n",
    "    else:\n",
    "        stats['eventual_recovery_rate'] = float('nan')\n",
    "    \n",
    "    # Average attempts for successful recoveries\n",
    "    successful_attempts = [\n",
    "        attempt for success, attempt in zip(eventual_recovery_success, num_recovery_attempts)\n",
    "        if success == 1\n",
    "    ]\n",
    "    if successful_attempts:\n",
    "        stats['avg_attempts_for_successful_recovery'] = np.mean(successful_attempts)\n",
    "    else:\n",
    "        stats['avg_attempts_for_successful_recovery'] = float('nan')\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj(name):\n",
    "    path = f'/home/abhinav/Documents/ccai/data/experiments/{name}/csvgd'\n",
    "    # print(name)\n",
    "    all_data = []\n",
    "    all_x = []\n",
    "    # all_d2goal = []\n",
    "    all_traj_data = []\n",
    "    for trial_num in range(1, 51):\n",
    "        # print(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "        # if 'rand' in name or 'proj' in name or 'diff' in name:\n",
    "        try:\n",
    "            with open(path + f'/trial_{trial_num}/trajectory.pkl', 'rb') as data:\n",
    "                d = pickle.load(data)\n",
    "                traj = np.concatenate((d[:-1]), axis=0)\n",
    "                end = d[-1].reshape(1, -1)\n",
    "                end = np.concatenate((end, np.zeros((1, 21))), axis=1)\n",
    "                traj = np.concatenate((traj, end), axis=0)\n",
    "            # else:\n",
    "            #     d = np.load(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "            #     end_state = d['x'] \n",
    "            with open(path + f'/trial_{trial_num}/traj_data.p', 'rb') as f:\n",
    "                traj_data = pickle.load(f)\n",
    "                for key in traj_data.keys():\n",
    "                    if torch.is_tensor(traj_data[key]):\n",
    "                        traj_data[key] = traj_data[key].cpu().numpy()\n",
    "                # all_d2goal.append(d2goal)\n",
    "                # if 'rand' not in name and 'proj' not in name and 'diff' not in name::\n",
    "                #     traj = traj_data[t]['plans'][:, 0]\n",
    "\n",
    "                    # end = np.concatenate((end_state, np.zeros((1, 21))), axis=1)\n",
    "                    # traj = np.concatenate((traj, end), axis=0)\n",
    "                traj_data[t]['traj'] = np.expand_dims(traj, axis=1)\n",
    "                all_traj_data.append(traj_data)  \n",
    "                all_data.append(traj_data)\n",
    "        except:\n",
    "            pass\n",
    "            # from collections import defaultdict\n",
    "            # traj_data = defaultdict(list)\n",
    "            # all_traj_data.append(traj_data)  \n",
    "            # all_data.append(traj_data)\n",
    "            \n",
    "    constraint_violations_all = {\n",
    "        # 'optimizer_paths': [],\n",
    "        'traj': [],\n",
    "        # 'inits': [],\n",
    "        # 'plans': [],\n",
    "\n",
    "    }\n",
    "    # for plans_or_inits in constraint_violations_all.keys():\n",
    "    #     if plans_or_inits == 'traj':\n",
    "    #         gen_constraint_data(plans_or_inits, constraint_violations_all[plans_or_inits], path, traj_data=all_traj_data)\n",
    "    #     else:\n",
    "    #         gen_constraint_data(plans_or_inits, constraint_violations_all[plans_or_inits], path)\n",
    "    \n",
    "    # Take the list of dicts and turn it into a dict of lists\n",
    "    all_data = {k: [d[k] for d in all_data] for k in all_data[0]}\n",
    "    # all_data = {}\n",
    "    # for k in all_data[0]:\n",
    "    #     print(k)\n",
    "    #     # all_data[k] = [d[k] for d in all_data]\n",
    "    all_data['violation'] = constraint_violations_all\n",
    "\n",
    "    final_likelihood = []\n",
    "    one_step_recovery_success = []\n",
    "    eventual_recovery_success = []\n",
    "    final_likelihood = [l for l in all_data['final_likelihoods']]\n",
    "    \n",
    "    \n",
    "    all_delta_l_recovery = []\n",
    "    num_recovery_attempts_to_recover = []\n",
    "    final_likelihood_flat = []\n",
    "    dropped = all_data['dropped']\n",
    "    dropped_recovery = all_data['dropped_recovery']\n",
    "\n",
    "    threshold = -.1 if 'safe_rl' in name else -70.0\n",
    "    one_step_recovery_success, eventual_recovery_success, num_recovery_attempts_to_recover, all_delta_l_recovery, dropped_recovery, recovery_improvement_block, dropped_recovery_block = calculate_recovery_metrics(\n",
    "        final_likelihood, \n",
    "        all_data['pre_action_likelihoods'],\n",
    "        dropped_info=dropped_recovery,\n",
    "        threshold=threshold,\n",
    "        safe_rl='safe_rl' in name\n",
    "    )\n",
    "\n",
    "    # Store the recovery blocks for this trial\n",
    "    if 'recovery_blocks' not in all_data:\n",
    "        all_data['recovery_blocks'] = []\n",
    "\n",
    "    # Add dropped_recovery to all_data\n",
    "    all_data['dropped_recovery'] = dropped_recovery\n",
    "\n",
    "    all_data['drop_pct'] = np.mean(dropped_recovery)\n",
    "    \n",
    "    all_data['dropped_recovery_block'] = dropped_recovery_block\n",
    "\n",
    "    all_data['drop_pct_block'] = np.mean(dropped_recovery_block)\n",
    "\n",
    "    # Calculate overall recovery success rates\n",
    "    recovery_block_count: int = sum(len(blocks) for blocks in all_data['recovery_blocks'])\n",
    "    successful_recovery_count: int = sum(\n",
    "        sum(1 for block in blocks if block['success']) \n",
    "        for blocks in all_data['recovery_blocks']\n",
    "    )\n",
    "\n",
    "    if recovery_block_count > 0:\n",
    "        all_data['overall_recovery_success_rate'] = successful_recovery_count / recovery_block_count\n",
    "    else:\n",
    "        all_data['overall_recovery_success_rate'] = float('nan')\n",
    "\n",
    "    # Calculate average duration of successful and unsuccessful recovery blocks\n",
    "    successful_durations: List[int] = [\n",
    "        block['duration'] \n",
    "        for blocks in all_data['recovery_blocks'] \n",
    "        for block in blocks if block['success']\n",
    "    ]\n",
    "    unsuccessful_durations: List[int] = [\n",
    "        block['duration'] \n",
    "        for blocks in all_data['recovery_blocks'] \n",
    "        for block in blocks if not block['success']\n",
    "    ]\n",
    "\n",
    "    all_data['avg_successful_recovery_duration'] = (\n",
    "        np.mean(successful_durations) if successful_durations else float('nan')\n",
    "    )\n",
    "    all_data['avg_unsuccessful_recovery_duration'] = (\n",
    "        np.mean(unsuccessful_durations) if unsuccessful_durations else float('nan')\n",
    "    )\n",
    "\n",
    "    all_data['delta_l_recovery'] = (all_delta_l_recovery)\n",
    "    all_data['delta_l_recovery_block'] = (recovery_improvement_block)\n",
    "    all_data['one_step_recovery_success'] = (one_step_recovery_success)\n",
    "    all_data['eventual_recovery_success'] = (eventual_recovery_success)\n",
    "    all_data['num_recovery_attempts_to_recover'] = (num_recovery_attempts_to_recover)\n",
    "    all_data['final_likelihood'] = (final_likelihood_flat)\n",
    "    all_data['dropped'] = (dropped)\n",
    "    all_data['dropped_recovery'] = (dropped_recovery)\n",
    "    # all_data['dropped_or_succeeded'] = (dropped_or_succeeded)\n",
    "\n",
    "    # Compute and store recovery statistics\n",
    "    recovery_stats = compute_recovery_statistics(\n",
    "        all_data,\n",
    "        one_step_recovery_success,\n",
    "        eventual_recovery_success,\n",
    "        num_recovery_attempts_to_recover\n",
    "    )\n",
    "\n",
    "    all_data.update(recovery_stats)\n",
    "\n",
    "    return all_x, all_data#, all_d2goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dofs = {\n",
    "    'roll': -3,\n",
    "    'pitch': -2,\n",
    "    'yaw': -1\n",
    "}\n",
    "conf = scipy.stats.norm.ppf(.95) \n",
    "cutoff_radians = np.deg2rad(2)\n",
    "\n",
    "goal_yaw = -np.pi / 3 + cutoff_radians\n",
    "\n",
    "cutoff_degrees = 5\n",
    "cutoff_radians = np.deg2rad(cutoff_degrees)\n",
    "\n",
    "def gen_plot_screwdriver_angle(data, keys, plans_or_inits, dof_to_plot=['yaw'], stat='mean', label_dict=None):\n",
    "    # data[key][t][trial_ind][plans_or_inits]\n",
    "    fig, ax = plt.subplots(1, len(dof_to_plot), figsize=(len(dof_to_plot)*25/3, 6))\n",
    "    # fig, ax = plt.subplots(1, len(dof_to_plot), figsize=(4, 2.3))\n",
    "    if len(dof_to_plot) == 1:\n",
    "        ax = [ax]\n",
    "    means = dict()\n",
    "    for key in keys:\n",
    "        traj_all = []\n",
    "        to_continue = False\n",
    "        for trial_ind in range(50):\n",
    "        # for trial_ind in []:\n",
    "            try:\n",
    "                if plans_or_inits == 'traj':\n",
    "                    traj = data[key][t][trial_ind][plans_or_inits].reshape(-1, 34)[:, :13]\n",
    "                    traj_all.append(traj)\n",
    "                else:\n",
    "                    if len(data[key])-1 <= trial_ind:\n",
    "                        to_continue = True\n",
    "                        continue\n",
    "                    # traj = torch.stack([i[..., :15] for i in data[key][trial_ind+1][plans_or_inits]], axis=1).flatten(1, 2)\n",
    "                    # traj = traj.cpu()\n",
    "                    # for i in range(traj.shape[0]):\n",
    "                    #     traj_all.append(traj[i])\n",
    "                    traj_all.append(data[key][trial_ind+1][plans_or_inits].cpu().numpy()[..., :13])\n",
    "            except:\n",
    "                continue\n",
    "        # Pad to length 112 with the last value\n",
    "        if to_continue:\n",
    "            continue\n",
    "        lengths = [len(x) for x in traj_all]\n",
    "        num_contact_modes = [x/t for x in lengths]\n",
    "        num_transitions = sum([l-1 for l in lengths])\n",
    "        max_len = max(lengths)\n",
    "        for i in range(len(traj_all)):\n",
    "            traj_all[i] = np.concatenate((traj_all[i], np.tile(traj_all[i][-1], (max_len - len(traj_all[i]), 1))), axis=0)\n",
    "        traj_all = np.stack(traj_all, axis=0)\n",
    "        # Subtract the initial value\n",
    "        traj_all[:, :, -1] -= traj_all[:, 0, -1:]\n",
    "        print(traj_all[:, -1, -1])\n",
    "        start_ind = 0\n",
    "        num_to_plot = max_len\n",
    "        # traj_all = np.abs(traj_all - goal_yaw)\n",
    "        success = (traj_all[:, -1, -1] < goal_yaw + cutoff_radians)\n",
    "        success_pct = success.mean()\n",
    "        len_array = np.array(lengths)\n",
    "        success_lengths = len_array[success]\n",
    "        # traj_all = traj_all[traj_all[:, start_ind + num_to_plot-1, -1] < 0]\n",
    "\n",
    "        mean = np.clip(traj_all[:, -1, -1] - goal_yaw, 0, 1000).mean()\n",
    "        means[key] = mean\n",
    "        mean_episode_length_success = success_lengths.mean()\n",
    "        mean_episode_length = len_array.mean()\n",
    "        print(f'{key}: Num traj: {traj_all.shape[0]}, Num transitions: {num_transitions}')\n",
    "        print(f'Success Rate: {success_pct:.2f}')\n",
    "        print(f'Avg. Execution steps (for success): {mean_episode_length_success:.1f}')\n",
    "        print(f'Avg. Execution steps: {mean_episode_length:.1f}')\n",
    "        print(f'Average final goal distance: {mean:.2f}')\n",
    "        print()\n",
    "        # print(traj_all[..., 14])\n",
    "        if stat == 'mean':\n",
    "            traj_all_mean = traj_all.mean(axis=0)[:, [dofs[d] for d in dof_to_plot]]\n",
    "            traj_all_std = traj_all.std(axis=0)[:, [dofs[d] for d in dof_to_plot]]\n",
    "        elif stat == 'median':\n",
    "            traj_all_median = np.median(traj_all, axis=0)[:, [dofs[d] for d in dof_to_plot]]\n",
    "        for i, dof in enumerate(dof_to_plot):\n",
    "            if label_dict is not None:\n",
    "                label = label_dict[key]\n",
    "            else:\n",
    "                label = key\n",
    "            if stat == 'mean':\n",
    "                # print(traj_all[:, :12, -1])\n",
    "                # ax[i].plot(traj_all_mean[:, i], label=label, color = '#1f77b4' if 'A*' in key else '#2ca02c')\n",
    "                ax[i].plot(traj_all_mean[:, i], label=label)\n",
    "                # calculate 95% confidence interval\n",
    "                bound_offset = traj_all_std[:, i] * conf / np.sqrt(traj_all.shape[0])\n",
    "                print(bound_offset[-1])\n",
    "                # ax[i].fill_between(np.arange(traj_all_mean.shape[0]), traj_all_mean[:, i] - bound_offset, traj_all_mean[:, i] + bound_offset, alpha=0.5, color = '#1f77b4' if 'A*' in key else '#2ca02c')\n",
    "                ax[i].fill_between(np.arange(traj_all_mean.shape[0]), traj_all_mean[:, i] - bound_offset, traj_all_mean[:, i] + bound_offset, alpha=0.5)\n",
    "            elif stat == 'median':\n",
    "                ax[i].plot(traj_all_median[:, i], label=label)\n",
    "            elif stat == 'all':\n",
    "                for traj_ind in range(traj_all.shape[0]):\n",
    "                    # print(traj_all.shape)\n",
    "                    ax[i].plot(traj_all[traj_ind, start_ind:start_ind + num_to_plot, dofs[dof]])\n",
    "    for i, dof in enumerate(dof_to_plot):\n",
    "        ax[i].legend()\n",
    "        ax[i].set_xlabel('Timestep')\n",
    "        ax[i].set_ylabel(f'Screwdriver {dof} Angle'.title() + ' (rad)')\n",
    "        # Grid lines\n",
    "        ax[i].grid(True)\n",
    "        #y axis numbers on right side as well\n",
    "        ax[i].yaxis.tick_right()\n",
    "        # Title dof\n",
    "        ax[i].set_title(f'Real Screwdriver {dof} Angle 95% Confidence Interval'.title())# + f' (n={traj_all.shape[0]})')\n",
    "    plt.show()\n",
    "    return means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_likelihood_analysis(data_exec, key_groups):\n",
    "    # Loop through data_exec keys, and plot a boxplot of delta_l_recovery for each on the same pllot\n",
    "    def get_min_num_data(data_exec, key_groups):\n",
    "        min_num_data = float('inf')\n",
    "        for key_group in key_groups:\n",
    "            num_data = 0\n",
    "            for key in key_groups[key_group]:\n",
    "                num_data += len(data_exec[key]['delta_l_recovery'])\n",
    "            min_num_data = min(min_num_data, num_data)\n",
    "        return min_num_data\n",
    "    # min_num_data = get_min_num_data(data_exec, key_groups)\n",
    "    min_num_data = 10000\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "    for key_group in key_groups:\n",
    "        all_delta_recoveries = []\n",
    "        all_delta_recoveries_block = []\n",
    "        all_one_step_recovery_success = []\n",
    "        all_eventual_recovery_success = []\n",
    "        all_projection_times = []\n",
    "        all_num_recovery_attempts_to_recover = []\n",
    "        all_dropped = []\n",
    "        all_dropped_block = []\n",
    "        all_contact_plan_times = []\n",
    "        # all_dropped_individual = []\n",
    "        # all_dropped_or_succeeded = []\n",
    "        # all_final_likelihood = []\n",
    "        for key in key_groups[key_group]:\n",
    "            all_delta_recoveries.extend(data_exec[key]['delta_l_recovery'])\n",
    "            all_delta_recoveries_block.extend(data_exec[key]['delta_l_recovery_block'])\n",
    "            all_one_step_recovery_success.extend(data_exec[key]['one_step_recovery_success'])\n",
    "            all_eventual_recovery_success.extend(data_exec[key]['eventual_recovery_success'])\n",
    "            for l in data_exec[key]['project_times']:\n",
    "                all_projection_times.extend(l)\n",
    "            all_num_recovery_attempts_to_recover.extend(data_exec[key]['num_recovery_attempts_to_recover'])\n",
    "            all_dropped.append(data_exec[key]['drop_pct'])\n",
    "            all_dropped_block.append(data_exec[key]['drop_pct_block'])\n",
    "            contact_plan_time = sum(data_exec[key]['contact_plan_times'], start=[])\n",
    "            all_contact_plan_times.extend(contact_plan_time)\n",
    "            # all_dropped_individual.append(data_exec[key]['drop_pct_individual'])\n",
    "            # all_dropped_or_succeeded.extend(data_exec[key]['dropped_or_succeeded'])\n",
    "            # all_final_likelihood.extend(data_exec[key]['final_likelihood'])\n",
    "        all_delta_recoveries = np.array(all_delta_recoveries)[:min_num_data]\n",
    "        all_delta_recoveries_block = np.array(all_delta_recoveries_block)[:min_num_data]\n",
    "        all_one_step_recovery_success = np.array(all_one_step_recovery_success)[:min_num_data]\n",
    "        all_eventual_recovery_success = np.array(all_eventual_recovery_success)[:min_num_data]\n",
    "        all_projection_times = np.array(all_projection_times)[:min_num_data]\n",
    "        all_num_recovery_attempts_to_recover = np.array(all_num_recovery_attempts_to_recover)[:min_num_data] * 3\n",
    "        # all_final_likelihood = np.array(all_final_likelihood)[:min_num_data]\n",
    "\n",
    "        num_points = np.sum(all_delta_recoveries > 0)\n",
    "        total_points = len(all_delta_recoveries)\n",
    "        percentage = num_points / total_points * 100\n",
    "        print('Total points:', total_points)\n",
    "        print('Total points block:', len(all_delta_recoveries_block))\n",
    "        # print(f'{key_group}: {num_points} points greater than 0, {percentage:.2f}% of total points')\n",
    "        pct_success = np.mean(all_one_step_recovery_success) * 100\n",
    "        print(f'{key_group}: {pct_success:.2f}% of one-step recoveries were successful')\n",
    "        pct_eventual_success = np.mean(all_eventual_recovery_success) * 100\n",
    "        print(f'{key_group}: {pct_eventual_success:.2f}% of recoveries were eventually successful')\n",
    "        avg_num_recovery_attempts = np.mean(all_num_recovery_attempts_to_recover) * 3\n",
    "        # print(f'{key_group}: Average number of recovery attempts to recover: {avg_num_recovery_attempts:.2f}')\n",
    "        # 95% confidence interval for the number of recovery attempts\n",
    "        ci_num_recovery_attempts = scipy.stats.norm.interval(0.95, loc=avg_num_recovery_attempts, scale=np.std(all_num_recovery_attempts_to_recover)/np.sqrt(len(all_num_recovery_attempts_to_recover)))\n",
    "        # Print as plus or minus\n",
    "        print(f'{key_group}: 95% CI for number of recovery steps: {avg_num_recovery_attempts:.2f} +/- {ci_num_recovery_attempts[1] - avg_num_recovery_attempts:.2f}')\n",
    "\n",
    "        # drop_pct = np.mean(all_dropped) * 100\n",
    "        # print(f'{key_group}: Drop %: {drop_pct:.2f}%')\n",
    "        # drop_pct_block = np.mean(all_dropped_block) * 100\n",
    "        # print(f'{key_group}: Drop % per recovery block: {drop_pct_block:.2f}%')\n",
    "        \n",
    "        # drop_pct_individual = np.mean(all_dropped_individual) * 100\n",
    "        # print(f'{key_group}: Drop % per individual recovery attempt: {drop_pct_individual:.2f}%')\n",
    "        # drop_or_succeed_pct = np.mean(all_dropped_or_succeeded) * 100\n",
    "        # print(f'{key_group}: Drop or succeed %: {drop_or_succeed_pct:.2f}%')\n",
    "        avg_projection_time = np.mean(all_projection_times)\n",
    "        print(f'{key_group}: Average projection time: {avg_projection_time:.2f} seconds')\n",
    "        mean_contact_plan_time = np.mean(all_contact_plan_times)\n",
    "        print(f'{key_group}: Mean contact plan time: {mean_contact_plan_time:.2f} seconds')\n",
    "        ax.boxplot(all_delta_recoveries_block, positions=[list(key_groups.keys()).index(key_group)], widths=0.5, showfliers=False)\n",
    "        print()\n",
    "        # ax2.boxplot(all_final_likelihood, positions=[list(key_groups.keys()).index(key_group)], widths=0.5, showfliers=False)\n",
    "    ax.set_xticklabels(key_groups.keys())\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.set_ylabel('Recovery Likelihood Change')\n",
    "    ax.set_title('Recovery Likelihood Change (If Not Dropped)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #Now do histograms of the delta_l_recovery for each key. For each key, print the number of points greater than 0 and the percentage that is of the total\n",
    "    # fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # # fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "    # for key_group in key_groups:\n",
    "    #     all_delta_recoveries = []\n",
    "    #     # all_final_likelihood = []\n",
    "    #     for key in key_groups[key_group]:\n",
    "    #         all_delta_recoveries.extend(data_exec[key]['delta_l_recovery'])\n",
    "    #         # all_final_likelihood.extend(data_exec[key]['final_likelihood'])\n",
    "    #     all_delta_recoveries = np.array(all_delta_recoveries)[:min_num_data]\n",
    "    #     # all_final_likelihood = np.array(all_final_likelihood)[:min_num_data]\n",
    "    #     ax.hist(all_delta_recoveries, bins=50, alpha=0.5, label=key_group, density=True)\n",
    "    #     # ax2.hist(all_final_likelihood, bins=50, alpha=0.5, label=key_group, density=True)\n",
    "    # ax.set_xlabel('Recovery Likelihood Change')\n",
    "    # ax.set_ylabel('Density')\n",
    "    # ax.set_title('Histogram of Recovery Likelihood Change ')\n",
    "    # # Red dashed line at x =0\n",
    "    # ax.axvline(x=0, color='red', linestyle='--')\n",
    "    # # ax2.axvline(x=-175, color='red', linestyle='--')\n",
    "    # ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "# while True:\n",
    "# Clear cell output\n",
    "clear_output(wait=True)\n",
    "data_exec = {}\n",
    "t = 3\n",
    "for key, name in [\n",
    "                # ('Offline Recovery', 'allegro_valve_recovery_data_generation_std_.2_thresh_85_jitter_std_.05_warmup_100_save_perturbation'),\n",
    "                # ('No Recovery', 'allegro_valve_safe_rl_data_gen_std_.2_save_perturbation'),\n",
    "                # ('Offline Recovery .1', 'allegro_valve_recovery_data_generation_std_.1_thresh_90'),\n",
    "                # ('No Recovery .1', 'allegro_valve_safe_rl_data_gen_std_.1_save_perturbation'),\n",
    "                # ('Offline Recovery .15', 'allegro_valve_recovery_data_generation_std_.15_thresh_95'),\n",
    "                # ('No Recovery .15', 'allegro_valve_safe_rl_data_gen_std_.15_save_perturbation'),\n",
    "                # ('Task Data Gen 0 fric', 'allegro_valve_task_data_friction_0.15_pi_5x2_.0875_valve_random_obj_rob_start_0_fric'),\n",
    "                # ('Task Data Gen .125 fric .75 min force', 'allegro_valve_task_data_friction_0.15_pi_5x2_.0875_valve_random_obj_rob_start_.125_fric_pregrasp_T_3_min_force_.75'),\n",
    "                # ('Task Data Gen .1 valve .125 fric .75 min force', 'allegro_valve_task_data_friction_0.15_pi_5x2_.1_valve_random_obj_rob_start_.125_fric_pregrasp_T_3_min_force_.75')\n",
    "                # ('Task Data Gen .125 fric 1 min force', 'allegro_valve_task_data_friction_0.15_pi_5x2_.1_valve_random_obj_rob_start_.125_fric_pregrasp_T_3_min_force_1')\n",
    "                # ('Task Data Gen .1 valve .125 fric 1 min force', 'allegro_valve_task_data_friction_0.125_pi_5x2_.1_valve_random_obj_rob_start_.125_fric_pregrasp_T_3_min_force_1'),\n",
    "                # ('Task Data Gen low friction', 'allegro_valve_task_data_valve_friction_.025_friction_.2'),\n",
    "                # ('Task Data Gen lower friction', 'allegro_valve_task_data_valve_friction_.025_friction_.05'),\n",
    "                # ('Task Data Gen low friction action perturb', 'allegro_valve_task_data_valve_friction_.025_friction_.2_action_perturb'),\n",
    "                # ('Task Data Gen lower friction action perturb', 'allegro_valve_task_data_valve_friction_.025_friction_.05_action_perturb'),\n",
    "                # # ('Task Data Gen .1 valve .125 fric 1 min force perturb .05', 'allegro_valve_task_data_friction_0.125_pi_5x2_.1_valve_random_obj_rob_start_.125_fric_pregrasp_T_3_min_force_1_.05_perturb'),\n",
    "                # ('Task Data Gen .1 valve .125 fric 1 min force perturb .1', 'allegro_valve_task_data_friction_0.125_pi_5x2_.1_valve_random_obj_rob_start_.125_fric_pregrasp_T_3_min_force_1_.1_perturb'),\n",
    "                # ('Offline Recovery', 'allegro_valve_recovery_data_generation_std_.0_thresh_70_cutoff_96_N_4'),\n",
    "                \n",
    "                # ('Online Recovery', 'allegro_valve_recovery_model'),\n",
    "                # ('Online Recovery MLP Ablation', 'allegro_valve_recovery_model_mlp_ablation'),\n",
    "                # ('No Recovery', 'allegro_valve_safe_rl_data_gen_std_.03_save_perturbation'),\n",
    "                # ('MLP Ablation', 'allegro_valve_recovery_model_mlp_ablation_eval_sigmoid_fix_normalized')\n",
    "                ('Recovery RL', 'allegro_valve_safe_rl_mppi_recovery_new'),\n",
    "                # ('Likelihood MPPI', 'allegro_valve_likelihood_mppi_recovery')\n",
    "                \n",
    "                ]:\n",
    "    data_exec[key] = {}\n",
    "    all_x, all_data = get_traj(name)\n",
    "\n",
    "    data_exec[key] = {**data_exec[key], **all_data}\n",
    "print()\n",
    "\n",
    "# delta_likelihood_analysis(data_exec, \n",
    "#                         {    \n",
    "#                             # 'Offline Recovery': ['Offline Recovery'],\n",
    "#                             # 'Online Recovery': ['Online Recovery'],\n",
    "#                             # 'Online Recovery MLP Ablation': ['Online Recovery MLP Ablation'],\n",
    "#                             'MLP Ablation': ['MLP Ablation']\n",
    "#                         })\n",
    "# t = 3\n",
    "# stat = 'mean'\n",
    "# dofs_to_plot = ['yaw']\n",
    "\n",
    "# keys_exec = [\n",
    "#             # 'Task Data Gen 0 fric',\n",
    "#             # 'Task Data Gen low friction',\n",
    "#             # 'Task Data Gen low friction action perturb',\n",
    "#             # 'Task Data Gen lower friction',\n",
    "#             # 'Task Data Gen lower friction action perturb'\n",
    "#             # 'Task Data Gen .125 fric .75 min force',\n",
    "#             # 'No Recovery',\n",
    "#             # 'Online Recovery',\n",
    "#             # 'Online Recovery MLP Ablation',\n",
    "#             # 'Offline Recovery',\n",
    "#             'MLP Ablation'\n",
    "#             ]\n",
    "# means = gen_plot_screwdriver_angle(data_exec, keys_exec, 'traj', dof_to_plot=dofs_to_plot, stat=stat, label_dict=None)\n",
    "# del data_exec\n",
    "# keys_exec = [\n",
    "#             # 'Task Data Gen 0 fric',\n",
    "#             'Task Data Gen .1 valve .125 fric 1 min force perturb .05'\n",
    "#             # 'No Recovery',\n",
    "#             # 'Offline Recovery .1',\n",
    "#             # 'No Recovery .1',\n",
    "#             # 'Offline Recovery .15',\n",
    "#             # 'No Recovery .15'\n",
    "#             ]\n",
    "# means = gen_plot_screwdriver_angle(data_exec, keys_exec, 'traj', dof_to_plot=dofs_to_plot, stat=stat, label_dict=None)\n",
    "\n",
    "# keys_exec = [\n",
    "#             # 'Task Data Gen 0 fric',\n",
    "#             'Task Data Gen .1 valve .125 fric 1 min force perturb .1'\n",
    "#             # 'No Recovery',\n",
    "#             # 'Offline Recovery .1',\n",
    "#             # 'No Recovery .1',\n",
    "#             # 'Offline Recovery .15',\n",
    "#             # 'No Recovery .15'\n",
    "#             ]\n",
    "# means = gen_plot_screwdriver_angle(data_exec, keys_exec, 'traj', dof_to_plot=dofs_to_plot, stat=stat, label_dict=None)\n",
    "# keys_exec = [\n",
    "#             # 'Task Data Gen 0 fric',\n",
    "#             'Task Data Gen .1 valve .125 fric 1 min force',\n",
    "#             # 'No Recovery',\n",
    "#             # 'Offline Recovery .1',\n",
    "#             # 'No Recovery .1',\n",
    "#             # 'Offline Recovery .15',\n",
    "#             # 'No Recovery .15'\n",
    "#             ]\n",
    "# means = gen_plot_screwdriver_angle(data_exec, keys_exec, 'traj', dof_to_plot=dofs_to_plot, stat=stat, label_dict=None)\n",
    "\n",
    "# % increase in mean compared to ablation \n",
    "# for key in keys_exec:\n",
    "#     if key == 'contact_mode_diffusion_ablation':\n",
    "#         continue\n",
    "#     print(key, (means[key] - means['contact_mode_diffusion_ablation']) / means['contact_mode_diffusion_ablation'] * 100)\n",
    "#     # time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.927342867365725"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovery_times = []\n",
    "for t in data_exec['Recovery RL']['csvto_times']:\n",
    "    for l in t:\n",
    "        if len(l) == 1:\n",
    "            recovery_times.append(l)\n",
    "np.mean(recovery_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
