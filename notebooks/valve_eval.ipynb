{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isaac_victor_envs.utils import get_assets_dir\n",
    "from isaac_victor_envs.tasks.allegro import AllegroValveTurningEnv\n",
    "# from isaac_victor_envs.tasks.allegro_ros import RosAllegroValveTurningEnv\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import scipy\n",
    "\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "import pytorch_volumetric as pv\n",
    "import pytorch_kinematics as pk\n",
    "import pytorch_kinematics.transforms as tf\n",
    "from torch.func import vmap, jacrev, hessian, jacfwd\n",
    "# import pytorch3d.transforms as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ccai.utils.allegro_utils import *\n",
    "# from allegro_valve_roll import AllegroValveTurning, AllegroContactProblem, PositionControlConstrainedSVGDMPC, \\\n",
    "#    add_trajectories, add_trajectories_hardware\n",
    "\n",
    "from ccai.allegro_contact import AllegroManipulationProblem, PositionControlConstrainedSVGDMPC, add_trajectories, \\\n",
    "    add_trajectories_hardware\n",
    "from ccai.allegro_screwdriver_problem_diffusion import AllegroScrewdriverDiff\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# from ccai.mpc.ipopt import IpoptMPC\n",
    "# from ccai.problem import IpoptProblem\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yaml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m obj_dof \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# config = yaml.safe_load(pathlib.Path(f'../examples/config/{sys.argv[1]}.yaml').read_text())\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241m.\u001b[39msafe_load(pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../examples/config/screwdriver/allegro_screwdriver_csvto_OOD_ID_test.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread_text())\n\u001b[1;32m      4\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisualize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhardware\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yaml' is not defined"
     ]
    }
   ],
   "source": [
    "obj_dof = 3\n",
    "# config = yaml.safe_load(pathlib.Path(f'../examples/config/{sys.argv[1]}.yaml').read_text())\n",
    "config = yaml.safe_load(pathlib.Path(f'../examples/config/screwdriver/allegro_screwdriver_csvto_OOD_ID_test.yaml').read_text())\n",
    "config['visualize'] = False\n",
    "\n",
    "if config['mode'] == 'hardware':\n",
    "    env = RosAllegroValveTurningEnv(1, control_mode='joint_impedance',\n",
    "                                    use_cartesian_controller=False,\n",
    "                                    viewer=True,\n",
    "                                    steps_per_action=60,\n",
    "                                    friction_coefficient=1.0,\n",
    "                                    device=config['sim_device'],\n",
    "                                    valve=config['object_type'],\n",
    "                                    video_save_path=img_save_dir,\n",
    "                                    joint_stiffness=config['kp'],\n",
    "                                    fingers=config['fingers'],\n",
    "                                    )\n",
    "else:\n",
    "    if not config['visualize']:\n",
    "        img_save_dir = None\n",
    "\n",
    "    env = AllegroValveTurningEnv(1, control_mode='joint_impedance',\n",
    "                                        use_cartesian_controller=False,\n",
    "                                        viewer=config['visualize'],\n",
    "                                        steps_per_action=60,\n",
    "                                        friction_coefficient=config['friction_coefficient'] * 1.05,\n",
    "                                        # friction_coefficient=1.0,  # DEBUG ONLY, set the friction very high\n",
    "                                        device=config['sim_device'],\n",
    "                                        video_save_path=img_save_dir,\n",
    "                                        joint_stiffness=config['kp'],\n",
    "                                        fingers=config['fingers'],\n",
    "                                        )\n",
    "\n",
    "sim, gym, viewer = env.get_sim()\n",
    "\n",
    "state = env.get_state()\n",
    "# try:\n",
    "#     while True:\n",
    "#         start = env.get_state()['q'][:, :-1]\n",
    "#         env.step(start)\n",
    "#         print('waiting for you to finish camera adjustment, ctrl-c when done')\n",
    "#         time.sleep(0.1)\n",
    "# except KeyboardInterrupt:\n",
    "#     pass\n",
    "\n",
    "sim_env = None\n",
    "ros_copy_node = None\n",
    "if config['mode'] == 'hardware':\n",
    "    sim_env = env\n",
    "    from hardware.hardware_env import HardwareEnv\n",
    "\n",
    "    env = HardwareEnv(sim_env.default_dof_pos[:, :16], finger_list=['index', 'thumb'], kp=config['kp'])\n",
    "    env.world_trans = sim_env.world_trans\n",
    "    env.joint_stiffness = sim_env.joint_stiffness\n",
    "    env.device = sim_env.device\n",
    "    env.valve_pose = sim_env.valve_pose\n",
    "elif config['mode'] == 'hardware_copy':\n",
    "    from hardware.hardware_env import RosNode\n",
    "\n",
    "    ros_copy_node = RosNode()\n",
    "\n",
    "results = {}\n",
    "\n",
    "# set up the kinematic chain\n",
    "asset = f'{get_assets_dir()}/xela_models/allegro_hand_right.urdf'\n",
    "ee_names = {\n",
    "    'index': 'allegro_hand_hitosashi_finger_finger_0_aftc_base_link',\n",
    "    'middle': 'allegro_hand_naka_finger_finger_1_aftc_base_link',\n",
    "    'ring': 'allegro_hand_kusuri_finger_finger_2_aftc_base_link',\n",
    "    'thumb': 'allegro_hand_oya_finger_3_aftc_base_link',\n",
    "}\n",
    "config['ee_names'] = ee_names\n",
    "config['obj_dof'] = 3\n",
    "\n",
    "screwdriver_asset = f'{get_assets_dir()}/valve/valve_cross.urdf'\n",
    "\n",
    "chain = pk.build_chain_from_urdf(open(asset).read())\n",
    "screwdriver_chain = pk.build_chain_from_urdf(open(screwdriver_asset).read())\n",
    "frame_indices = [chain.frame_to_idx[ee_names[finger]] for finger in config['fingers']]  # combined chain\n",
    "frame_indices = torch.tensor(frame_indices)\n",
    "state2ee_pos = partial(state2ee_pos, fingers=config['fingers'], chain=chain, frame_indices=frame_indices,\n",
    "                        world_trans=env.world_trans)\n",
    "\n",
    "forward_kinematics = partial(chain.forward_kinematics,\n",
    "                                frame_indices=frame_indices)  # full_to= _partial_state = partial(full_to_partial_state, fingers=config['fingers'])\n",
    "partial_to_full_state = partial(partial_to_full_state, fingers=config['fingers'])\n",
    "\n",
    "controller = 'csvgd'\n",
    "goal = - 0.5 * torch.tensor([0, 0, np.pi])\n",
    "# set up params\n",
    "params = config.copy()\n",
    "params.pop('controllers')\n",
    "params.update(config['controllers'][controller])\n",
    "params['controller'] = controller\n",
    "params['valve_goal'] = goal.to(device=params['device'])\n",
    "params['chain'] = chain.to(device=params['device'])\n",
    "object_location = torch.tensor([0, 0, 1.205]).to(\n",
    "    params['device'])  # TODO: confirm if this is the correct location\n",
    "params['object_location'] = object_location\n",
    "\n",
    "num_fingers = len(params['fingers'])\n",
    "state = env.get_state()\n",
    "start = state['q'].reshape(4 * num_fingers + 1).to(device=params['device'])\n",
    "if 'csvgd' in params['controller']:\n",
    "    # index finger is used for stability\n",
    "    if 'index' in params['fingers']:\n",
    "        fingers = params['fingers']\n",
    "    else:\n",
    "        fingers = ['index'] + params['fingers']\n",
    "\n",
    "t = 3\n",
    "# with open(data_path / f'constraint_violations.p', 'wb') as f:\n",
    "#     pickle.dump(constraint_violations_all, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recovery_metrics(\n",
    "    final_likelihood: List[List[float]], \n",
    "    pre_action_likelihoods: List[List[float]],\n",
    "    dropped_info: List[bool] = None,\n",
    "    threshold: float = -160.0\n",
    ") -> Tuple[List[int], List[int], List[int], List[float], List[bool]]:\n",
    "    \"\"\"\n",
    "    Calculate recovery metrics from likelihood time series data.\n",
    "    \n",
    "    Args:\n",
    "        final_likelihood: Nested list of likelihood values per trial and step\n",
    "        dropped_info: Optional list of booleans indicating if the object was dropped\n",
    "        threshold: Threshold below which state is considered in need of recovery\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - one_step_recovery_success: Binary indicators of one-step recovery success\n",
    "            - eventual_recovery_success: Binary indicators of eventual recovery success\n",
    "            - num_recovery_attempts_to_recover: Number of attempts needed for recovery\n",
    "            - recovery_improvement: Improvement in likelihood during recovery\n",
    "            - dropped_recovery: Indicators if recovery ended due to dropping\n",
    "    \"\"\"\n",
    "    one_step_recovery_success = []\n",
    "    recovery_improvement = []\n",
    "    dropped_recovery = []\n",
    "    \n",
    "    eventual_recovery_success = []\n",
    "    recovery_improvement_block = []\n",
    "    dropped_recovery_block = []\n",
    "    num_recovery_attempts_to_recover = []\n",
    "\n",
    "    \n",
    "    for trial_idx, trial in enumerate(final_likelihood):\n",
    "        # print(trial_idx+1)\n",
    "        # print(trial)\n",
    "        # print( [trial_idx])\n",
    "        # trial = sum(trial, [])\n",
    "        trial = [l for l in trial if None not in l and len(l) > 0]\n",
    "        # trial.insert(0, [pre_action_likelihoods[trial_idx][0][-1]])\n",
    "        for k in range(len(trial)):\n",
    "            if len(trial[k]) == 0 and len(pre_action_likelihoods[trial_idx][k]) > 0:\n",
    "                trial[k].append(pre_action_likelihoods[trial_idx][k][-1])\n",
    "            # elif len(trial[k]) == 0:\n",
    "            #     trial[k].append(-200) # Placeholder\n",
    "        # print(trial)\n",
    "        # print()\n",
    "\n",
    "        np_trial = np.array(trial)\n",
    "        recovery_blocks = []\n",
    "        is_in_recovery_block = False\n",
    "        current_block = {}\n",
    "        \n",
    "        # Check if dropped_info is available for this trial\n",
    "        trial_dropped = dropped_info[trial_idx] if dropped_info and trial_idx < len(dropped_info) else False\n",
    "        \n",
    "        for step_idx in range(np_trial.shape[0]):\n",
    "            likelihood = np_trial[step_idx]\n",
    "            # print(likelihood)\n",
    "            \n",
    "            # Start of a recovery block\n",
    "            if not is_in_recovery_block and likelihood < threshold:\n",
    "                is_in_recovery_block = True\n",
    "                current_block = {\n",
    "                    'start_idx': step_idx,\n",
    "                    'start_likelihood': likelihood,\n",
    "                    'attempts': 1  # This is the first attempt\n",
    "                }\n",
    "            \n",
    "            # Continue an existing recovery block\n",
    "            elif is_in_recovery_block and ( likelihood < threshold):\n",
    "                current_block['attempts'] += 1\n",
    "                # if likelihood is None:\n",
    "                #     likelihood = -160\n",
    "                recovery_improvement.append(\n",
    "                    likelihood - current_block['start_likelihood']\n",
    "                )\n",
    "                dropped_recovery.append(False)  # Successful recovery wasn't due to dropping\n",
    "                one_step_recovery_success.append(0)\n",
    "            \n",
    "            # End of a recovery block - successful\n",
    "            elif is_in_recovery_block and likelihood >= threshold:\n",
    "                is_in_recovery_block = False\n",
    "                current_block['end_idx'] = step_idx\n",
    "                current_block['end_likelihood'] = likelihood\n",
    "                current_block['success'] = True\n",
    "                current_block['duration'] = step_idx - current_block['start_idx']\n",
    "                current_block['dropped'] = False\n",
    "                recovery_blocks.append(current_block)\n",
    "                \n",
    "                # Calculate one-step recovery success (duration = 1)\n",
    "                one_step_success = int(current_block['duration'] == 1)\n",
    "                one_step_recovery_success.append(one_step_success)\n",
    "                \n",
    "                # Track other metrics\n",
    "                num_recovery_attempts_to_recover.append(current_block['attempts'])\n",
    "                eventual_recovery_success.append(1)\n",
    "                recovery_improvement_block.append(\n",
    "                    current_block['end_likelihood'] - current_block['start_likelihood']\n",
    "                )\n",
    "                recovery_improvement.append(\n",
    "                    current_block['end_likelihood'] - current_block['start_likelihood']\n",
    "                )\n",
    "                dropped_recovery_block.append(False)  # Successful recovery wasn't due to dropping\n",
    "                dropped_recovery.append(False)  # Successful recovery wasn't due to dropping\n",
    "            # else:\n",
    "            #     print('a', step_idx)\n",
    "        \n",
    "        # Handle case where trial ended during a recovery block (unsuccessful)\n",
    "        if is_in_recovery_block:\n",
    "            current_block['end_idx'] = np_trial.shape[0] - 1\n",
    "            current_block['end_likelihood'] = np_trial[-1]\n",
    "            # if current_block['end_likelihood'] is None:\n",
    "            #     current_block['end_likelihood'] = -160\n",
    "            current_block['success'] = False\n",
    "            current_block['duration'] = np_trial.shape[0] - 1 - current_block['start_idx']\n",
    "            current_block['dropped'] = trial_dropped\n",
    "            recovery_blocks.append(current_block)\n",
    "            \n",
    "            # Not a one-step recovery\n",
    "            one_step_recovery_success.append(0)\n",
    "            eventual_recovery_success.append(0)\n",
    "            num_recovery_attempts_to_recover.append(current_block['attempts'])\n",
    "            recovery_improvement_block.append(\n",
    "                current_block['end_likelihood'] - current_block['start_likelihood']\n",
    "            )\n",
    "            recovery_improvement.append(\n",
    "                current_block['end_likelihood'] - current_block['start_likelihood']\n",
    "            )\n",
    "            dropped_recovery_block.append(trial_dropped)  # Was this unsuccessful recovery due to dropping?\n",
    "            dropped_recovery.append(trial_dropped)  # Successful recovery wasn't due to dropping\n",
    "        # print(one_step_recovery_success)\n",
    "        # print(eventual_recovery_success)\n",
    "        # print()\n",
    "    return one_step_recovery_success, eventual_recovery_success, num_recovery_attempts_to_recover, recovery_improvement, dropped_recovery, recovery_improvement_block, dropped_recovery_block\n",
    "\n",
    "def compute_recovery_statistics(\n",
    "    all_data: Dict[str, Any],\n",
    "    one_step_recovery_success: List[int],\n",
    "    eventual_recovery_success: List[int],\n",
    "    num_recovery_attempts: List[int]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute summary statistics from recovery metrics.\n",
    "    \n",
    "    Args:\n",
    "        all_data: Dictionary containing all trial data\n",
    "        one_step_recovery_success: Binary indicators of one-step recovery success\n",
    "        eventual_recovery_success: Binary indicators of eventual recovery success\n",
    "        num_recovery_attempts: Number of attempts needed for successful recovery\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of computed statistics\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # One-step recovery success rate\n",
    "    if one_step_recovery_success:\n",
    "        stats['one_step_recovery_rate'] = np.mean(one_step_recovery_success)\n",
    "        stats['one_step_recovery_count'] = np.sum(one_step_recovery_success)\n",
    "    else:\n",
    "        stats['one_step_recovery_rate'] = float('nan')\n",
    "        stats['one_step_recovery_count'] = 0\n",
    "    \n",
    "    # Eventual recovery metrics\n",
    "    if eventual_recovery_success:\n",
    "        stats['eventual_recovery_rate'] = np.mean(eventual_recovery_success)\n",
    "    else:\n",
    "        stats['eventual_recovery_rate'] = float('nan')\n",
    "    \n",
    "    # Average attempts for successful recoveries\n",
    "    successful_attempts = [\n",
    "        attempt for success, attempt in zip(eventual_recovery_success, num_recovery_attempts)\n",
    "        if success == 1\n",
    "    ]\n",
    "    if successful_attempts:\n",
    "        stats['avg_attempts_for_successful_recovery'] = np.mean(successful_attempts)\n",
    "    else:\n",
    "        stats['avg_attempts_for_successful_recovery'] = float('nan')\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj(name):\n",
    "    path = f'/home/abhinavkumar/Documents/ccai/data/experiments/{name}/csvgd'\n",
    "    # print(name)\n",
    "    all_data = []\n",
    "    all_x = []\n",
    "    # all_d2goal = []\n",
    "    all_traj_data = []\n",
    "    for trial_num in range(1, 3000):\n",
    "        # print(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "        # if 'rand' in name or 'proj' in name or 'diff' in name:\n",
    "        with open(path + f'/trial_{trial_num}/trajectory.pkl', 'rb') as data:\n",
    "            d = pickle.load(data)\n",
    "            traj = np.concatenate((d[:-1]), axis=0)\n",
    "            end = d[-1].reshape(1, -1)\n",
    "            end = np.concatenate((end, np.zeros((1, 21))), axis=1)\n",
    "            traj = np.concatenate((traj, end), axis=0)\n",
    "        try:\n",
    "\n",
    "            with open(path + f'/trial_{trial_num}/trajectory.pkl', 'rb') as data:\n",
    "                d = pickle.load(data)\n",
    "                traj = np.concatenate((d[:-1]), axis=0)\n",
    "                end = d[-1].reshape(1, -1)\n",
    "                end = np.concatenate((end, np.zeros((1, 21))), axis=1)\n",
    "                traj = np.concatenate((traj, end), axis=0)\n",
    "            # else:\n",
    "            #     d = np.load(path + f'/trial_{trial_num}/trajectory.npz')\n",
    "            #     end_state = d['x'] \n",
    "            with open(path + f'/trial_{trial_num}/traj_data.p', 'rb') as f:\n",
    "                traj_data = pickle.load(f)\n",
    "                for key in traj_data.keys():\n",
    "                    if torch.is_tensor(traj_data[key]):\n",
    "                        traj_data[key] = traj_data[key].cpu().numpy()\n",
    "                # all_d2goal.append(d2goal)\n",
    "                # if 'rand' not in name and 'proj' not in name and 'diff' not in name::\n",
    "                #     traj = traj_data[t]['plans'][:, 0]\n",
    "\n",
    "                    # end = np.concatenate((end_state, np.zeros((1, 21))), axis=1)\n",
    "                    # traj = np.concatenate((traj, end), axis=0)\n",
    "                traj_data[t]['traj'] = np.expand_dims(traj, axis=1)\n",
    "                all_traj_data.append(traj_data)  \n",
    "                all_data.append(traj_data)\n",
    "        except:\n",
    "            pass\n",
    "            # from collections import defaultdict\n",
    "            # traj_data = defaultdict(list)\n",
    "            # all_traj_data.append(traj_data)  \n",
    "            # all_data.append(traj_data)\n",
    "            \n",
    "    constraint_violations_all = {\n",
    "        # 'optimizer_paths': [],\n",
    "        'traj': [],\n",
    "        # 'inits': [],\n",
    "        # 'plans': [],\n",
    "\n",
    "    }\n",
    "    # for plans_or_inits in constraint_violations_all.keys():\n",
    "    #     if plans_or_inits == 'traj':\n",
    "    #         gen_constraint_data(plans_or_inits, constraint_violations_all[plans_or_inits], path, traj_data=all_traj_data)\n",
    "    #     else:\n",
    "    #         gen_constraint_data(plans_or_inits, constraint_violations_all[plans_or_inits], path)\n",
    "    \n",
    "    # Take the list of dicts and turn it into a dict of lists\n",
    "    all_data = {k: [d[k] for d in all_data] for k in all_data[0]}\n",
    "    # all_data = {}\n",
    "    # for k in all_data[0]:\n",
    "    #     print(k)\n",
    "    #     # all_data[k] = [d[k] for d in all_data]\n",
    "    all_data['violation'] = constraint_violations_all\n",
    "\n",
    "    final_likelihood = []\n",
    "    one_step_recovery_success = []\n",
    "    eventual_recovery_success = []\n",
    "    final_likelihood = [l for l in all_data['final_likelihoods']]\n",
    "    \n",
    "    \n",
    "    all_delta_l_recovery = []\n",
    "    num_recovery_attempts_to_recover = []\n",
    "    final_likelihood_flat = []\n",
    "    dropped = all_data['dropped']\n",
    "    dropped_recovery = all_data['dropped_recovery']\n",
    "\n",
    "\n",
    "    one_step_recovery_success, eventual_recovery_success, num_recovery_attempts_to_recover, all_delta_l_recovery, dropped_recovery, recovery_improvement_block, dropped_recovery_block = calculate_recovery_metrics(\n",
    "        final_likelihood, \n",
    "        all_data['pre_action_likelihoods'],\n",
    "        dropped_info=dropped_recovery\n",
    "    )\n",
    "\n",
    "    # Store the recovery blocks for this trial\n",
    "    if 'recovery_blocks' not in all_data:\n",
    "        all_data['recovery_blocks'] = []\n",
    "\n",
    "    # Add dropped_recovery to all_data\n",
    "    all_data['dropped_recovery'] = dropped_recovery\n",
    "\n",
    "    all_data['drop_pct'] = np.mean(dropped_recovery)\n",
    "    \n",
    "    all_data['dropped_recovery_block'] = dropped_recovery_block\n",
    "\n",
    "    all_data['drop_pct_block'] = np.mean(dropped_recovery_block)\n",
    "\n",
    "    # Calculate overall recovery success rates\n",
    "    recovery_block_count: int = sum(len(blocks) for blocks in all_data['recovery_blocks'])\n",
    "    successful_recovery_count: int = sum(\n",
    "        sum(1 for block in blocks if block['success']) \n",
    "        for blocks in all_data['recovery_blocks']\n",
    "    )\n",
    "\n",
    "    if recovery_block_count > 0:\n",
    "        all_data['overall_recovery_success_rate'] = successful_recovery_count / recovery_block_count\n",
    "    else:\n",
    "        all_data['overall_recovery_success_rate'] = float('nan')\n",
    "\n",
    "    # Calculate average duration of successful and unsuccessful recovery blocks\n",
    "    successful_durations: List[int] = [\n",
    "        block['duration'] \n",
    "        for blocks in all_data['recovery_blocks'] \n",
    "        for block in blocks if block['success']\n",
    "    ]\n",
    "    unsuccessful_durations: List[int] = [\n",
    "        block['duration'] \n",
    "        for blocks in all_data['recovery_blocks'] \n",
    "        for block in blocks if not block['success']\n",
    "    ]\n",
    "\n",
    "    all_data['avg_successful_recovery_duration'] = (\n",
    "        np.mean(successful_durations) if successful_durations else float('nan')\n",
    "    )\n",
    "    all_data['avg_unsuccessful_recovery_duration'] = (\n",
    "        np.mean(unsuccessful_durations) if unsuccessful_durations else float('nan')\n",
    "    )\n",
    "\n",
    "    all_data['delta_l_recovery'] = (all_delta_l_recovery)\n",
    "    all_data['delta_l_recovery_block'] = (recovery_improvement_block)\n",
    "    all_data['one_step_recovery_success'] = (one_step_recovery_success)\n",
    "    all_data['eventual_recovery_success'] = (eventual_recovery_success)\n",
    "    all_data['num_recovery_attempts_to_recover'] = (num_recovery_attempts_to_recover)\n",
    "    all_data['final_likelihood'] = (final_likelihood_flat)\n",
    "    all_data['dropped'] = (dropped)\n",
    "    all_data['dropped_recovery'] = (dropped_recovery)\n",
    "    # all_data['dropped_or_succeeded'] = (dropped_or_succeeded)\n",
    "\n",
    "    # Compute and store recovery statistics\n",
    "    recovery_stats = compute_recovery_statistics(\n",
    "        all_data,\n",
    "        one_step_recovery_success,\n",
    "        eventual_recovery_success,\n",
    "        num_recovery_attempts_to_recover\n",
    "    )\n",
    "\n",
    "    all_data.update(recovery_stats)\n",
    "\n",
    "    return all_x, all_data#, all_d2goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dofs = {\n",
    "    'roll': -3,\n",
    "    'pitch': -2,\n",
    "    'yaw': -1\n",
    "}\n",
    "conf = scipy.stats.norm.ppf(.95) \n",
    "goal_yaw = -np.pi / 3\n",
    "\n",
    "cutoff_degrees = 5\n",
    "cutoff_radians = np.deg2rad(cutoff_degrees)\n",
    "\n",
    "def gen_plot_screwdriver_angle(data, keys, plans_or_inits, dof_to_plot=['yaw'], stat='mean', label_dict=None):\n",
    "    # data[key][t][trial_ind][plans_or_inits]\n",
    "    fig, ax = plt.subplots(1, len(dof_to_plot), figsize=(len(dof_to_plot)*25/3, 6))\n",
    "    # fig, ax = plt.subplots(1, len(dof_to_plot), figsize=(4, 2.3))\n",
    "    if len(dof_to_plot) == 1:\n",
    "        ax = [ax]\n",
    "    means = dict()\n",
    "    for key in keys:\n",
    "        traj_all = []\n",
    "        to_continue = False\n",
    "        for trial_ind in range(3000):\n",
    "        # for trial_ind in []:\n",
    "            try:\n",
    "                if plans_or_inits == 'traj':\n",
    "                    traj = data[key][t][trial_ind][plans_or_inits].reshape(-1, 34)[:, :13]\n",
    "                    traj_all.append(traj)\n",
    "                else:\n",
    "                    if len(data[key])-1 <= trial_ind:\n",
    "                        to_continue = True\n",
    "                        continue\n",
    "                    # traj = torch.stack([i[..., :15] for i in data[key][trial_ind+1][plans_or_inits]], axis=1).flatten(1, 2)\n",
    "                    # traj = traj.cpu()\n",
    "                    # for i in range(traj.shape[0]):\n",
    "                    #     traj_all.append(traj[i])\n",
    "                    traj_all.append(data[key][trial_ind+1][plans_or_inits].cpu().numpy()[..., :13])\n",
    "            except:\n",
    "                continue\n",
    "        # Pad to length 112 with the last value\n",
    "        if to_continue:\n",
    "            continue\n",
    "        lengths = [len(x) for x in traj_all]\n",
    "        num_contact_modes = [x/t for x in lengths]\n",
    "        \n",
    "        max_len = max(lengths)\n",
    "        for i in range(len(traj_all)):\n",
    "            traj_all[i] = np.concatenate((traj_all[i], np.tile(traj_all[i][-1], (max_len - len(traj_all[i]), 1))), axis=0)\n",
    "        traj_all = np.stack(traj_all, axis=0)\n",
    "        # Subtract the initial value\n",
    "        start_ind = 0\n",
    "        num_to_plot = max_len\n",
    "        traj_all = np.abs(traj_all - goal_yaw)\n",
    "        success = (traj_all[:, -1, -1] < goal_yaw).mean()\n",
    "        # traj_all = traj_all[traj_all[:, start_ind + num_to_plot-1, -1] < 0]\n",
    "        print(max_len, traj_all.shape)\n",
    "        mean = traj_all[:, :, -1].min(1).mean()\n",
    "        means[key] = mean\n",
    "        print(key, np.mean(lengths), mean, success)\n",
    "        print()\n",
    "        # print(traj_all[..., 14])\n",
    "        if stat == 'mean':\n",
    "            traj_all_mean = traj_all.mean(axis=0)[:, [dofs[d] for d in dof_to_plot]]\n",
    "            traj_all_std = traj_all.std(axis=0)[:, [dofs[d] for d in dof_to_plot]]\n",
    "        elif stat == 'median':\n",
    "            traj_all_median = np.median(traj_all, axis=0)[:, [dofs[d] for d in dof_to_plot]]\n",
    "        for i, dof in enumerate(dof_to_plot):\n",
    "            if label_dict is not None:\n",
    "                label = label_dict[key]\n",
    "            else:\n",
    "                label = key\n",
    "            if stat == 'mean':\n",
    "                # print(traj_all[:, :12, -1])\n",
    "                # ax[i].plot(traj_all_mean[:, i], label=label, color = '#1f77b4' if 'A*' in key else '#2ca02c')\n",
    "                ax[i].plot(traj_all_mean[:, i], label=label)\n",
    "                # calculate 95% confidence interval\n",
    "                bound_offset = traj_all_std[:, i] * conf / np.sqrt(traj_all.shape[0])\n",
    "                # ax[i].fill_between(np.arange(traj_all_mean.shape[0]), traj_all_mean[:, i] - bound_offset, traj_all_mean[:, i] + bound_offset, alpha=0.5, color = '#1f77b4' if 'A*' in key else '#2ca02c')\n",
    "                ax[i].fill_between(np.arange(traj_all_mean.shape[0]), traj_all_mean[:, i] - bound_offset, traj_all_mean[:, i] + bound_offset, alpha=0.5)\n",
    "            elif stat == 'median':\n",
    "                ax[i].plot(traj_all_median[:, i], label=label)\n",
    "            elif stat == 'all':\n",
    "                for traj_ind in range(traj_all.shape[0]):\n",
    "                    # print(traj_all.shape)\n",
    "                    ax[i].plot(traj_all[traj_ind, start_ind:start_ind + num_to_plot, dofs[dof]], label=traj_ind)\n",
    "    for i, dof in enumerate(dof_to_plot):\n",
    "        ax[i].legend()\n",
    "        ax[i].set_xlabel('Timestep')\n",
    "        ax[i].set_ylabel(f'Screwdriver {dof} Angle'.title() + ' (rad)')\n",
    "        # Grid lines\n",
    "        ax[i].grid(True)\n",
    "        #y axis numbers on right side as well\n",
    "        ax[i].yaxis.tick_right()\n",
    "        # Title dof\n",
    "        ax[i].set_title(f'Real Screwdriver {dof} Angle 95% Confidence Interval'.title())# + f' (n={traj_all.shape[0]})')\n",
    "    plt.show()\n",
    "    return means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_likelihood_analysis(data_exec, key_groups):\n",
    "    # Loop through data_exec keys, and plot a boxplot of delta_l_recovery for each on the same pllot\n",
    "    def get_min_num_data(data_exec, key_groups):\n",
    "        min_num_data = float('inf')\n",
    "        for key_group in key_groups:\n",
    "            num_data = 0\n",
    "            for key in key_groups[key_group]:\n",
    "                num_data += len(data_exec[key]['delta_l_recovery'])\n",
    "            min_num_data = min(min_num_data, num_data)\n",
    "        return min_num_data\n",
    "    # min_num_data = get_min_num_data(data_exec, key_groups)\n",
    "    min_num_data = 10000\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "    for key_group in key_groups:\n",
    "        all_delta_recoveries = []\n",
    "        all_delta_recoveries_block = []\n",
    "        all_one_step_recovery_success = []\n",
    "        all_eventual_recovery_success = []\n",
    "        all_projection_times = []\n",
    "        all_num_recovery_attempts_to_recover = []\n",
    "        all_dropped = []\n",
    "        all_dropped_block = []\n",
    "        all_contact_plan_times = []\n",
    "        # all_dropped_individual = []\n",
    "        # all_dropped_or_succeeded = []\n",
    "        # all_final_likelihood = []\n",
    "        for key in key_groups[key_group]:\n",
    "            all_delta_recoveries.extend(data_exec[key]['delta_l_recovery'])\n",
    "            all_delta_recoveries_block.extend(data_exec[key]['delta_l_recovery_block'])\n",
    "            all_one_step_recovery_success.extend(data_exec[key]['one_step_recovery_success'])\n",
    "            all_eventual_recovery_success.extend(data_exec[key]['eventual_recovery_success'])\n",
    "            for l in data_exec[key]['project_times']:\n",
    "                all_projection_times.extend(l)\n",
    "            all_num_recovery_attempts_to_recover.extend(data_exec[key]['num_recovery_attempts_to_recover'])\n",
    "            all_dropped.append(data_exec[key]['drop_pct'])\n",
    "            all_dropped_block.append(data_exec[key]['drop_pct_block'])\n",
    "            contact_plan_time = sum(data_exec[key]['contact_plan_times'], start=[])\n",
    "            all_contact_plan_times.extend(contact_plan_time)\n",
    "            # all_dropped_individual.append(data_exec[key]['drop_pct_individual'])\n",
    "            # all_dropped_or_succeeded.extend(data_exec[key]['dropped_or_succeeded'])\n",
    "            # all_final_likelihood.extend(data_exec[key]['final_likelihood'])\n",
    "        all_delta_recoveries = np.array(all_delta_recoveries)[:min_num_data]\n",
    "        all_delta_recoveries_block = np.array(all_delta_recoveries_block)[:min_num_data]\n",
    "        all_one_step_recovery_success = np.array(all_one_step_recovery_success)[:min_num_data]\n",
    "        all_eventual_recovery_success = np.array(all_eventual_recovery_success)[:min_num_data]\n",
    "        all_projection_times = np.array(all_projection_times)[:min_num_data]\n",
    "        all_num_recovery_attempts_to_recover = np.array(all_num_recovery_attempts_to_recover)[:min_num_data] * 3\n",
    "        # all_final_likelihood = np.array(all_final_likelihood)[:min_num_data]\n",
    "\n",
    "        num_points = np.sum(all_delta_recoveries > 0)\n",
    "        total_points = len(all_delta_recoveries)\n",
    "        percentage = num_points / total_points * 100\n",
    "        print('Total points:', total_points)\n",
    "        print('Total points block:', len(all_delta_recoveries_block))\n",
    "        # print(f'{key_group}: {num_points} points greater than 0, {percentage:.2f}% of total points')\n",
    "        pct_success = np.mean(all_one_step_recovery_success) * 100\n",
    "        print(f'{key_group}: {pct_success:.2f}% of one-step recoveries were successful')\n",
    "        pct_eventual_success = np.mean(all_eventual_recovery_success) * 100\n",
    "        print(f'{key_group}: {pct_eventual_success:.2f}% of recoveries were eventually successful')\n",
    "        avg_num_recovery_attempts = np.mean(all_num_recovery_attempts_to_recover)\n",
    "        # print(f'{key_group}: Average number of recovery attempts to recover: {avg_num_recovery_attempts:.2f}')\n",
    "        # 95% confidence interval for the number of recovery attempts\n",
    "        ci_num_recovery_attempts = scipy.stats.norm.interval(0.95, loc=avg_num_recovery_attempts, scale=np.std(all_num_recovery_attempts_to_recover)/np.sqrt(len(all_num_recovery_attempts_to_recover)))\n",
    "        # Print as plus or minus\n",
    "        print(f'{key_group}: 95% CI for number of recovery attempts: {avg_num_recovery_attempts:.2f} +/- {ci_num_recovery_attempts[1] - avg_num_recovery_attempts:.2f}')\n",
    "\n",
    "        # drop_pct = np.mean(all_dropped) * 100\n",
    "        # print(f'{key_group}: Drop %: {drop_pct:.2f}%')\n",
    "        # drop_pct_block = np.mean(all_dropped_block) * 100\n",
    "        # print(f'{key_group}: Drop % per recovery block: {drop_pct_block:.2f}%')\n",
    "        \n",
    "        # drop_pct_individual = np.mean(all_dropped_individual) * 100\n",
    "        # print(f'{key_group}: Drop % per individual recovery attempt: {drop_pct_individual:.2f}%')\n",
    "        # drop_or_succeed_pct = np.mean(all_dropped_or_succeeded) * 100\n",
    "        # print(f'{key_group}: Drop or succeed %: {drop_or_succeed_pct:.2f}%')\n",
    "        # avg_projection_time = np.mean(all_projection_times)\n",
    "        # print(f'{key_group}: Average projection time: {avg_projection_time:.2f} seconds')\n",
    "        mean_contact_plan_time = np.mean(all_contact_plan_times)\n",
    "        print(f'{key_group}: Mean contact plan time: {mean_contact_plan_time:.2f} seconds')\n",
    "        ax.boxplot(all_delta_recoveries_block, positions=[list(key_groups.keys()).index(key_group)], widths=0.5, showfliers=False)\n",
    "        print()\n",
    "        # ax2.boxplot(all_final_likelihood, positions=[list(key_groups.keys()).index(key_group)], widths=0.5, showfliers=False)\n",
    "    ax.set_xticklabels(key_groups.keys())\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.set_ylabel('Recovery Likelihood Change')\n",
    "    ax.set_title('Recovery Likelihood Change (If Not Dropped)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #Now do histograms of the delta_l_recovery for each key. For each key, print the number of points greater than 0 and the percentage that is of the total\n",
    "    # fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    # # fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "    # for key_group in key_groups:\n",
    "    #     all_delta_recoveries = []\n",
    "    #     # all_final_likelihood = []\n",
    "    #     for key in key_groups[key_group]:\n",
    "    #         all_delta_recoveries.extend(data_exec[key]['delta_l_recovery'])\n",
    "    #         # all_final_likelihood.extend(data_exec[key]['final_likelihood'])\n",
    "    #     all_delta_recoveries = np.array(all_delta_recoveries)[:min_num_data]\n",
    "    #     # all_final_likelihood = np.array(all_final_likelihood)[:min_num_data]\n",
    "    #     ax.hist(all_delta_recoveries, bins=50, alpha=0.5, label=key_group, density=True)\n",
    "    #     # ax2.hist(all_final_likelihood, bins=50, alpha=0.5, label=key_group, density=True)\n",
    "    # ax.set_xlabel('Recovery Likelihood Change')\n",
    "    # ax.set_ylabel('Density')\n",
    "    # ax.set_title('Histogram of Recovery Likelihood Change ')\n",
    "    # # Red dashed line at x =0\n",
    "    # ax.axvline(x=0, color='red', linestyle='--')\n",
    "    # # ax2.axvline(x=-175, color='red', linestyle='--')\n",
    "    # ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/abhinav/Documents/ccai/data/experiments/allegro_valve_safe_rl_data_gen_std_.2/csvgd/trial_1/trajectory.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 42\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m      8\u001b[0m                 \u001b[38;5;66;03m# ('allegro_screwdriver_recovery_data_pi_6_fixed_cpc', 'allegro_screwdriver_recovery_data_pi_6_fixed_cpc'),\u001b[39;00m\n\u001b[1;32m      9\u001b[0m                 \u001b[38;5;66;03m# ('recovery_model_old_no_perturb_during_recovery', 'allegro_screwdriver_recovery_model_fixed_cpc_eval'),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Recovery\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallegro_valve_safe_rl_data_gen_std_.2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m                 ]:\n\u001b[1;32m     41\u001b[0m     data_exec[key] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 42\u001b[0m     all_x, all_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_traj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     data_exec[key] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_exec[key], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_data}\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m, in \u001b[0;36mget_traj\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      7\u001b[0m all_traj_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3000\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(path + f'/trial_{trial_num}/trajectory.npz')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# if 'rand' in name or 'proj' in name or 'diff' in name:\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/trial_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrial_num\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/trajectory.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[1;32m     12\u001b[0m         d \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(data)\n\u001b[1;32m     13\u001b[0m         traj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((d[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/diffusion/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/abhinav/Documents/ccai/data/experiments/allegro_valve_safe_rl_data_gen_std_.2/csvgd/trial_1/trajectory.pkl'"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "# while True:\n",
    "# Clear cell output\n",
    "clear_output(wait=True)\n",
    "data_exec = {}\n",
    "t = 3\n",
    "for key, name in [\n",
    "                # ('allegro_screwdriver_recovery_data_pi_6_fixed_cpc', 'allegro_screwdriver_recovery_data_pi_6_fixed_cpc'),\n",
    "                # ('recovery_model_old_no_perturb_during_recovery', 'allegro_screwdriver_recovery_model_fixed_cpc_eval'),\n",
    "                # ('recovery_model_old_perturb_during_recovery', 'allegro_screwdriver_recovery_model_fixed_cpc_eval_perturb_during_recovery'),\n",
    "                \n",
    "                # ('recovery_model_new_perturb_during_recovery',\n",
    "                #  'allegro_screwdriver_recovery_model_fixed_rand_pct_eval'),\n",
    "                # ('contact_mode_diffusion_ablation',\n",
    "                # 'allegro_screwdriver_recovery_model_mlp_ablation_fixed_rand_pct_eval'),\n",
    "                \n",
    "                \n",
    "                # ('recovery_data_gen_pi_2', \n",
    "                #  'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2'),\n",
    "                # ('recovery_data_gen_pi_2_9000', \n",
    "                #  'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000'),\n",
    "                # ('recovery_data_gen_pi_2_9000_185_thresh', \n",
    "                #  'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000_185_thresh'),\n",
    "                # ('recovery_data_gen_pi_2_9000_160_thresh', \n",
    "                #  'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000_160_thresh'),\n",
    "                # ('recovery_data_gen_pi_2_9000_160_thresh_double_cost', \n",
    "                #  'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000_160_thresh_double_cost'),\n",
    "                # ('recovery_data_gen_pi_2_9000_160_thresh_double_cost_100_warm', \n",
    "                #  'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000_160_thresh_double_cost_100_warm'),\n",
    "                # ('recovery_data_gen_pi_2_9000_160_thresh_double_cost_100_warm_no_diff_init', \n",
    "                #  'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000_160_thresh_double_cost_100_warm_no_diff_init'),\n",
    "                # ('recovery_data_gen_pi_2_9000_160_thresh_no_perturb', \n",
    "                #  'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000_160_thresh_no_perturb'),\n",
    "                # ('recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005', 'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000_160_thresh_no_perturb_cpc_.005'),\n",
    "                # ('recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005_smooth_100', 'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000_160_thresh_no_perturb_cpc_.005_smooth_100'),\n",
    "                # ('recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005_smooth_100_cost_div_10', 'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000_160_thresh_no_perturb_cpc_.005_smooth_100_cost_div_10'),\n",
    "                # ('recovery_data_gen_pi_2_9000_160_thresh_no_perturb_kinematics_x_10',\n",
    "                #  'allegro_screwdriver_recovery_data_pi_6_rand_pct_.25_1.75_N_r_.5_1_pi_2_9000_160_thresh_no_perturb_kinematics_x_10'),\n",
    "                ('No Recovery', 'allegro_valve_safe_rl_data_gen_std_.2')\n",
    "                ]:\n",
    "    data_exec[key] = {}\n",
    "    all_x, all_data = get_traj(name)\n",
    "\n",
    "    data_exec[key] = {**data_exec[key], **all_data}\n",
    "print()\n",
    "\n",
    "# delta_likelihood_analysis(data_exec, \n",
    "#                         {    \n",
    "#                             # 'allegro_screwdriver_recovery_data_pi_6_fixed_cpc': ['allegro_screwdriver_recovery_data_pi_6_fixed_cpc'],\n",
    "#                             # 'recovery_model_old_no_perturb_during_recovery': ['recovery_model_old_no_perturb_during_recovery'],\n",
    "#                             # 'recovery_model_old_perturb_during_recovery': ['recovery_model_old_perturb_during_recovery'],\n",
    "#                             'recovery_model_new_perturb_during_recovery': ['recovery_model_new_perturb_during_recovery'],\n",
    "#                             'contact_mode_diffusion_ablation': ['contact_mode_diffusion_ablation'],\n",
    "#                             # 'recovery_data_gen_pi_2': ['recovery_data_gen_pi_2'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000': ['recovery_data_gen_pi_2_9000'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000_185_thresh': ['recovery_data_gen_pi_2_9000_185_thresh'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000_160_thresh': ['recovery_data_gen_pi_2_9000_160_thresh'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000_160_thresh_double_cost': ['recovery_data_gen_pi_2_9000_160_thresh_double_cost'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000_160_thresh_double_cost_100_warm': ['recovery_data_gen_pi_2_9000_160_thresh_double_cost_100_warm'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000_160_thresh_double_cost_100_warm_no_diff_init': ['recovery_data_gen_pi_2_9000_160_thresh_double_cost_100_warm_no_diff_init'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000_160_thresh_no_perturb': ['recovery_data_gen_pi_2_9000_160_thresh_no_perturb'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005': ['recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005_smooth_100': ['recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005_smooth_100'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005_smooth_100_cost_div_10': ['recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005_smooth_100_cost_div_10'],\n",
    "#                             # 'recovery_data_perturb_during_recovery': ['recovery_data_perturb_during_recovery'],\n",
    "#                             # 'recovery_model_new_perturb_during_recovery': ['recovery_model_new_perturb_during_recovery'],\n",
    "#                             # 'recovery_data_gen_pi_2_9000_160_thresh_no_perturb_kinematics_x_10': ['recovery_data_gen_pi_2_9000_160_thresh_no_perturb_kinematics_x_10'],\n",
    "#                             'recovery_data_gen_pi_2_9000_low_budget': ['recovery_data_gen_pi_2_9000_low_budget']\n",
    "\n",
    "#                         })\n",
    "t = 3\n",
    "stat = 'mean'\n",
    "dofs_to_plot = ['yaw']\n",
    "\n",
    "keys_exec = [\n",
    "            # 'recovery_model_old_no_perturb_during_recovery',\n",
    "            # 'recovery_model_old_perturb_during_recovery',\n",
    "            # 'recovery_model_new_perturb_during_recovery',\n",
    "            # 'contact_mode_diffusion_ablation',\n",
    "            # 'recovery_data_gen_pi_2',\n",
    "            # 'recovery_data_gen_pi_2_9000',\n",
    "            # 'recovery_data_gen_pi_2_9000_185_thresh',\n",
    "            # 'recovery_data_gen_pi_2_9000_160_thresh',\n",
    "            # 'recovery_data_gen_pi_2_9000_160_thresh_double_cost',\n",
    "            # 'recovery_data_gen_pi_2_9000_160_thresh_double_cost_100_warm',\n",
    "            # 'recovery_data_gen_pi_2_9000_160_thresh_double_cost_100_warm_no_diff_init',\n",
    "            # 'recovery_data_gen_pi_2_9000_160_thresh_no_perturb',\n",
    "            # 'recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005',\n",
    "            # 'recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005_smooth_100',\n",
    "            # 'recovery_data_gen_pi_2_9000_160_thresh_no_perturb_cpc_.005_smooth_100_cost_div_10',\n",
    "            # 'recovery_data_gen_pi_2_9000_160_thresh_no_perturb_kinematics_x_10'\n",
    "            'No Recovery'\n",
    "            ]\n",
    "means = gen_plot_screwdriver_angle(data_exec, keys_exec, 'traj', dof_to_plot=dofs_to_plot, stat=stat, label_dict=None)\n",
    "\n",
    "# % increase in mean compared to ablation\n",
    "for key in keys_exec:\n",
    "    if key == 'contact_mode_diffusion_ablation':\n",
    "        continue\n",
    "    print(key, (means[key] - means['contact_mode_diffusion_ablation']) / means['contact_mode_diffusion_ablation'] * 100)\n",
    "    # time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our method results\n",
    "# Total points: 610\n",
    "# Total points block: 359\n",
    "# recovery_model_new_perturb_during_recovery: 41.31% of one-step recoveries were successful\n",
    "# recovery_model_new_perturb_during_recovery: 80.78% of recoveries were eventually successful\n",
    "# recovery_model_new_perturb_during_recovery: 95% CI for number of recovery attempts: 1.70 +/- 0.18\n",
    "# recovery_model_new_perturb_during_recovery: Drop %: 9.84%\n",
    "# recovery_model_new_perturb_during_recovery: Drop % per recovery block: 16.71%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
