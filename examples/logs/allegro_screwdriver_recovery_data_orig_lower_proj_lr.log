Importing module 'gym_38' (/home/abhinav/Documents/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/abhinav/Documents/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 2.4.1+cu121
Device count 2
/home/abhinav/Documents/isaacgym/python/isaacgym/_bindings/src/gymtorch
ninja: no work to do.
No ROS install found, continuing
CCAI_PATH /home/abhinav/Documents/ccai
Not connected to PVD
Physics Engine: PhysX
Physics Device: cpu
GPU Pipeline: disabled
Using VHACD cache directory '/home/abhinav/.isaacgym/vhacd'
Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/allegro/base_ns.stl'
Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/allegro/link_1.0.stl'
Found existing convex decomposition for mesh '/home/abhinav/Documents/github/isaacgym-arm-envs/isaac_victor_envs/assets/xela_models/mesh/ft_c.stl'
[ models/temporal ] Channel dimensions: [(37, 128), (128, 256), (256, 512)]
[ models/temporal ] Channel dimensions: [(37, 128), (128, 256), (256, 512)]

Trial 1
Loaded trajectory sampler
Current yaw: tensor([ 5.5207e-05,  1.3631e-02, -4.8130e-02], device='cuda:0')
Current yaw: tensor([ 5.5207e-05,  1.3631e-02, -4.8130e-02], device='cuda:0')
1 turn
Sampling time 3.95493087900104
tensor([ 1.5538e-01,  6.1090e-01,  5.7053e-01,  6.0998e-01, -1.1557e-01,
         5.3958e-01,  8.8539e-01,  9.3525e-01,  1.2574e+00,  2.2958e-01,
         2.3280e-01,  1.2081e+00,  5.5207e-05,  1.3631e-02, -4.8130e-02,
         4.4650e-01], device='cuda:0')
Original likelihood: -19.054738998413086
Adjusted likelihood: -19.054738998413086
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.120591536979191
Current ori: tensor([ 5.5207e-05,  1.3631e-02, -4.8130e-02], device='cuda:0')
Middle force: tensor([0.5791, 0.5709, 1.1438, 0.5580, 1.1024, 0.6482, 0.5358, 0.5249, 0.5829,
        0.5540, 0.5206, 0.5708], device='cuda:0')
Thumb force: tensor([0.8664, 0.8197, 0.7561, 1.0166, 0.9685, 0.6757, 0.5213, 0.5700, 0.5453,
        0.6043, 0.5627, 0.5500], device='cuda:0')
Index force: tensor([0.5978, 0.6008, 0.5560, 0.5699, 0.7921, 0.5254, 1.0088, 0.5495, 0.5799,
        0.6410, 0.5969, 0.5416], device='cuda:0')
Storing NORMAL transition: reward=0.0014 (scaled=0.0014), steps=1
Reward stats updated: mean 0.0000 -> 0.0014, std: 0.0000
Collected 1 transitions for RL
tensor([ 0.2647,  0.6666,  0.4227,  0.6614, -0.2022,  0.4756,  0.8889,  1.0473,
         1.2578,  0.2486,  0.2143,  1.1629,  0.0055,  0.0214, -0.0499, -0.7408],
       device='cuda:0')
Original likelihood: -33.53627395629883
Adjusted likelihood: -33.53627395629883
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0268)
State is out of distribution
Projection step: 0, Loss: 34.35027313232422
Projection step: 1, Loss: 32.57298278808594
Projection step: 2, Loss: 32.37873840332031
Projection step: 3, Loss: 31.969833374023438
Projection step: 4, Loss: 31.692378997802734
Projection step: 5, Loss: 31.291454315185547
Projection step: 6, Loss: 29.884178161621094
Projection step: 7, Loss: 29.869121551513672
Projection step: 8, Loss: 29.74644660949707
Projection step: 9, Loss: 27.924205780029297
Projection step: 10, Loss: 27.623458862304688
Projection step: 11, Loss: 26.90530776977539
Projection step: 12, Loss: 26.51662826538086
Projection step: 13, Loss: 26.06368637084961
Projection step: 14, Loss: 26.658920288085938
Projection step: 15, Loss: 25.231365203857422
Projection step: 16, Loss: 24.74030303955078
Projection step: 17, Loss: 26.927066802978516
Projection step: 18, Loss: 23.633054733276367
Projection step: 19, Loss: 23.18689727783203
Projection step: 20, Loss: 22.37120819091797
Projection step: 21, Loss: 22.139713287353516
Projection step: 22, Loss: 21.980976104736328
Projection step: 23, Loss: 21.518892288208008
Projection step: 24, Loss: 21.686235427856445
Final likelihood: tensor([-21.0051, -22.5094, -19.1267, -20.5505, -21.4512, -20.9660, -22.6076,
        -21.0179, -23.3866, -22.0055, -19.3216, -18.7160, -22.5890, -18.8029,
        -21.8229, -23.8539])
Final projection likelihood: -21.2333
1 mode projection succeeded
New goal: tensor([ 0.2201,  0.6640,  0.5033,  0.5750, -0.1299,  0.4885,  0.8726,  1.1055,
         1.2746,  0.2776,  0.1976,  1.0590,  0.0042,  0.0167, -2.1220],
       device='cuda:0')
tensor([[0.0068]], device='cuda:0') tensor([[0.0077]], device='cuda:0') tensor([[0.0106]], device='cuda:0')
Original likelihood: -26.163663864135742
Adjusted likelihood: -26.163663864135742
Likelihood residual: 0.0
Original likelihood: -27.27367401123047
Adjusted likelihood: -27.27367401123047
Likelihood residual: 0.0
{'index': 27.27367401123047, 'thumb_middle': 26.163663864135742}
Current yaw: tensor([ 0.0055,  0.0214, -0.0499], device='cuda:0')
2 thumb_middle
tensor([ 0.2647,  0.6666,  0.4227,  0.6614, -0.2022,  0.4756,  0.8889,  1.0473,
         1.2578,  0.2486,  0.2143,  1.1629,  0.0055,  0.0214, -0.0499, -0.7408],
       device='cuda:0')
Solve time for step 1 9.468618298997171
Current ori: tensor([ 0.0055,  0.0214, -0.0499], device='cuda:0')
Index force: tensor([0.5715, 0.5817, 0.5772, 0.5946], device='cuda:0')
tensor([ 0.2247,  0.7310,  0.5084,  0.5711, -0.2109,  0.4645,  0.8504,  1.0776,
         1.2479,  0.2668,  0.1491,  1.0503,  0.0050,  0.0273, -0.0657, -1.0110],
       device='cuda:0')
Solve time for step 2 3.6955081960186362
Current ori: tensor([ 0.0050,  0.0273, -0.0657], device='cuda:0')
Index force: tensor([0.5701, 0.5707, 0.5867], device='cuda:0')
tensor([ 0.2353,  0.7347,  0.5145,  0.5742, -0.2256,  0.4583,  0.8359,  1.0858,
         1.2707,  0.2701,  0.1494,  1.0316,  0.0039,  0.0209, -0.0617, -0.9985],
       device='cuda:0')
Solve time for step 3 3.801622968981974
Current ori: tensor([ 0.0039,  0.0209, -0.0617], device='cuda:0')
Index force: tensor([0.5641, 0.5798], device='cuda:0')
tensor([ 0.2381,  0.7271,  0.5228,  0.5884, -0.2449,  0.4628,  0.8467,  1.1032,
         1.2740,  0.2792,  0.1432,  1.0399,  0.0060,  0.0202, -0.0535, -0.9965],
       device='cuda:0')
Solve time for step 4 3.4990052899811417
Current ori: tensor([ 0.0060,  0.0202, -0.0535], device='cuda:0')
Index force: tensor([0.5627], device='cuda:0')
Storing RECOVERY transition: reward=0.0233 (scaled=0.0233), steps=1
Reward stats updated: mean 0.0014 -> 0.0124, std: 0.0109
Collected 2 transitions for RL
Original likelihood: -32.53196716308594
Adjusted likelihood: -32.53196716308594
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0835)
State is out of distribution
Projection step: 0, Loss: 31.323150634765625
Projection step: 1, Loss: 30.555713653564453
Projection step: 2, Loss: 31.266904830932617
Projection step: 3, Loss: 30.56123924255371
Projection step: 4, Loss: 29.736513137817383
Projection step: 5, Loss: 29.683815002441406
Projection step: 6, Loss: 29.02716827392578
Projection step: 7, Loss: 27.95092010498047
Projection step: 8, Loss: 26.78972625732422
Projection step: 9, Loss: 27.518482208251953
Projection step: 10, Loss: 26.454742431640625
Projection step: 11, Loss: 27.428071975708008
Projection step: 12, Loss: 25.448890686035156
Projection step: 13, Loss: 25.6450252532959
Projection step: 14, Loss: 24.376773834228516
Projection step: 15, Loss: 23.805814743041992
Projection step: 16, Loss: 24.71693229675293
Projection step: 17, Loss: 22.76919937133789
Projection step: 18, Loss: 23.366416931152344
Projection step: 19, Loss: 22.62660026550293
Projection step: 20, Loss: 22.39274787902832
Projection step: 21, Loss: 21.162986755371094
Projection step: 22, Loss: 20.406558990478516
Projection step: 23, Loss: 20.615936279296875
Projection step: 24, Loss: 20.12307357788086
Final likelihood: tensor([-21.0234, -24.2949, -21.5996, -19.6419, -18.4174, -22.5051, -21.9669,
        -18.6656, -16.7201, -20.0097, -16.8914, -17.8395, -17.9516, -17.7544,
        -21.6032, -40.8976])
Final projection likelihood: -21.1114
1 mode projection succeeded
New goal: tensor([ 0.1877,  0.6647,  0.5051,  0.5266, -0.1282,  0.5016,  0.8485,  1.0714,
         1.3447,  0.3011,  0.1992,  1.0698,  0.0130,  0.0204, -2.4927],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[0.0016]], device='cuda:0')
Original likelihood: -28.66383171081543
Adjusted likelihood: -28.66383171081543
Likelihood residual: 0.0
Original likelihood: -21.85137939453125
Adjusted likelihood: -21.85137939453125
Likelihood residual: 0.0
{'index': 21.85137939453125, 'thumb_middle': 28.66383171081543}
Current yaw: tensor([ 0.0106,  0.0331, -0.0739], device='cuda:0')
3 index
tensor([ 0.2183,  0.7046,  0.5306,  0.5875, -0.1817,  0.5026,  0.8742,  1.0995,
         1.3492,  0.2917,  0.2030,  1.0720,  0.0106,  0.0331, -0.0739, -0.9993],
       device='cuda:0')
Solve time for step 1 10.355495980998967
Current ori: tensor([ 0.0106,  0.0331, -0.0739], device='cuda:0')
Middle force: tensor([0.5869, 0.5852, 0.5273, 0.5715], device='cuda:0')
Thumb force: tensor([0.5841, 0.6050, 0.5106, 0.5859], device='cuda:0')
tensor([ 0.2316,  0.6196,  0.4728,  0.5228, -0.1724,  0.5101,  0.8744,  1.0921,
         1.3487,  0.2902,  0.1935,  1.0689,  0.0061,  0.0271, -0.0835, -0.2384],
       device='cuda:0')
Solve time for step 2 4.150756244023796
Current ori: tensor([ 0.0061,  0.0271, -0.0835], device='cuda:0')
Middle force: tensor([0.5430, 0.5612, 0.5769], device='cuda:0')
Thumb force: tensor([0.5211, 0.5535, 0.5648], device='cuda:0')
tensor([ 0.2286,  0.6194,  0.4681,  0.5160, -0.1695,  0.5155,  0.8742,  1.0906,
         1.3497,  0.2896,  0.1836,  1.0755,  0.0036,  0.0243, -0.0898,  0.2886],
       device='cuda:0')
Solve time for step 3 4.016708948998712
Current ori: tensor([ 0.0036,  0.0243, -0.0898], device='cuda:0')
Middle force: tensor([0.6089, 0.5595], device='cuda:0')
Thumb force: tensor([0.5384, 0.5770], device='cuda:0')
tensor([ 2.2959e-01,  6.2023e-01,  4.6840e-01,  5.1413e-01, -1.6417e-01,
         5.2136e-01,  8.7066e-01,  1.0852e+00,  1.3511e+00,  2.8628e-01,
         1.7783e-01,  1.0739e+00,  6.9575e-04,  2.1444e-02, -9.1225e-02,
         6.0514e-01], device='cuda:0')
Solve time for step 4 3.857282640004996
Current ori: tensor([ 0.0007,  0.0214, -0.0912], device='cuda:0')
Middle force: tensor([0.5600], device='cuda:0')
Thumb force: tensor([0.5121], device='cuda:0')
Storing RECOVERY transition: reward=0.0542 (scaled=0.0542), steps=1
Reward stats updated: mean 0.0124 -> 0.0263, std: 0.0216
Collected 3 transitions for RL
Original likelihood: -24.204254150390625
Adjusted likelihood: -24.204254150390625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9992)
Current yaw: tensor([-0.0036,  0.0228, -0.1041], device='cuda:0')
4 turn
Sampling time 3.93817769899033
tensor([ 0.1683,  0.6887,  0.5122,  0.5302, -0.1658,  0.5254,  0.8665,  1.0820,
         1.3542,  0.2892,  0.1746,  1.0702, -0.0036,  0.0228, -0.1041,  0.7180],
       device='cuda:0')
Original likelihood: -26.398178100585938
Adjusted likelihood: -26.398178100585938
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9753)
Solve time for step 1 14.705469244043343
Current ori: tensor([-0.0036,  0.0228, -0.1041], device='cuda:0')
Middle force: tensor([1.3565, 0.5206, 0.4969, 0.4941, 0.6368, 0.5944, 1.0607, 0.7673, 0.8474,
        0.6320, 0.5340, 0.4878], device='cuda:0')
Thumb force: tensor([1.9725, 1.9620, 1.4810, 0.6565, 1.1347, 0.7602, 1.5024, 0.6057, 0.7141,
        0.6801, 0.6422, 0.5012], device='cuda:0')
Index force: tensor([0.5776, 0.8308, 0.9539, 0.6448, 0.5825, 0.5488, 0.5842, 0.5356, 0.5711,
        0.5672, 0.5129, 0.6358], device='cuda:0')
Storing NORMAL transition: reward=0.0688 (scaled=0.0688), steps=1
Reward stats updated: mean 0.0263 -> 0.0369, std: 0.0262
Collected 4 transitions for RL
tensor([ 1.4041e-01,  7.0076e-01,  4.2842e-01,  5.9914e-01, -1.9042e-01,
         4.9451e-01,  8.7726e-01,  1.1178e+00,  1.3651e+00,  3.1106e-01,
         1.9318e-01,  1.0453e+00,  6.8313e-04,  3.9871e-02, -1.7413e-01,
         7.5849e-01], device='cuda:0')
Original likelihood: -32.15003204345703
Adjusted likelihood: -32.15003204345703
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.1204)
State is out of distribution
Projection step: 0, Loss: 33.63661193847656
Projection step: 1, Loss: 31.66183853149414
Projection step: 2, Loss: 32.53919982910156
Projection step: 3, Loss: 34.08896255493164
Projection step: 4, Loss: 31.132734298706055
Projection step: 5, Loss: 34.2769660949707
Projection step: 6, Loss: 29.38072967529297
Projection step: 7, Loss: 29.26432991027832
Projection step: 8, Loss: 28.644775390625
Projection step: 9, Loss: 28.57201385498047
Projection step: 10, Loss: 28.286209106445312
Projection step: 11, Loss: 27.15496826171875
Projection step: 12, Loss: 26.97121810913086
Projection step: 13, Loss: 26.63490104675293
Projection step: 14, Loss: 26.647674560546875
Projection step: 15, Loss: 25.069011688232422
Projection step: 16, Loss: 23.983327865600586
Projection step: 17, Loss: 24.6789608001709
Projection step: 18, Loss: 23.353776931762695
Projection step: 19, Loss: 21.949918746948242
Projection step: 20, Loss: 22.508590698242188
Projection step: 21, Loss: 21.08181381225586
Projection step: 22, Loss: 21.436309814453125
Projection step: 23, Loss: 20.0576171875
Projection step: 24, Loss: 20.08803367614746
Final likelihood: tensor([-21.0497, -25.6079, -18.1486, -21.6839, -22.2551, -17.0903, -16.1353,
        -21.8010, -17.6927, -16.2444, -15.1912, -19.9730, -15.9218, -18.3053,
        -14.2743, -22.5061])
Final projection likelihood: -18.9925
1 mode projection succeeded
New goal: tensor([ 0.1075,  0.6635,  0.4513,  0.5516, -0.1306,  0.4692,  0.8082,  1.0430,
         1.3275,  0.2674,  0.1569,  1.1150, -0.0049,  0.0270, -2.2369],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -29.210018157958984
Adjusted likelihood: -29.210018157958984
Likelihood residual: 0.0
Original likelihood: -21.593017578125
Adjusted likelihood: -21.593017578125
Likelihood residual: 0.0
{'index': 21.593017578125, 'thumb_middle': 29.210018157958984}
Current yaw: tensor([ 0.0007,  0.0399, -0.1741], device='cuda:0')
5 index
tensor([ 1.4041e-01,  7.0076e-01,  4.2842e-01,  5.9914e-01, -1.9042e-01,
         4.9451e-01,  8.7726e-01,  1.1178e+00,  1.3651e+00,  3.1106e-01,
         1.9318e-01,  1.0453e+00,  6.8313e-04,  3.9871e-02, -1.7413e-01,
         7.5849e-01], device='cuda:0')
Solve time for step 1 10.101710183022078
Current ori: tensor([ 0.0007,  0.0399, -0.1741], device='cuda:0')
Middle force: tensor([0.5790, 0.5154, 0.5836, 0.6025], device='cuda:0')
Thumb force: tensor([0.5658, 0.5021, 0.5962, 0.5787], device='cuda:0')
tensor([ 0.1608,  0.6119,  0.4043,  0.5430, -0.1618,  0.5119,  0.8797,  1.1009,
         1.3719,  0.2844,  0.1474,  1.0770, -0.0052,  0.0214, -0.1714, -0.6955],
       device='cuda:0')
Solve time for step 2 4.314379725023173
Current ori: tensor([-0.0052,  0.0214, -0.1714], device='cuda:0')
Middle force: tensor([0.5142, 0.5798, 0.5979], device='cuda:0')
Thumb force: tensor([0.5017, 0.5920, 0.5752], device='cuda:0')
tensor([ 0.1591,  0.6092,  0.4067,  0.5344, -0.1479,  0.5258,  0.8743,  1.0883,
         1.3764,  0.2726,  0.1294,  1.0776, -0.0118,  0.0133, -0.1755, -1.4910],
       device='cuda:0')
Solve time for step 3 4.129376377037261
Current ori: tensor([-0.0118,  0.0133, -0.1755], device='cuda:0')
Middle force: tensor([0.5742, 0.5929], device='cuda:0')
Thumb force: tensor([0.5837, 0.5706], device='cuda:0')
tensor([ 0.1574,  0.6089,  0.4053,  0.5333, -0.1536,  0.5212,  0.8751,  1.0921,
         1.3808,  0.2676,  0.1328,  1.0774, -0.0099,  0.0168, -0.1741, -1.9284],
       device='cuda:0')
Solve time for step 4 3.9585623079910874
Current ori: tensor([-0.0099,  0.0168, -0.1741], device='cuda:0')
Middle force: tensor([0.5865], device='cuda:0')
Thumb force: tensor([0.5635], device='cuda:0')
Storing RECOVERY transition: reward=0.0016 (scaled=0.0016), steps=1
Reward stats updated: mean 0.0369 -> 0.0299, std: 0.0274
Collected 5 transitions for RL
Original likelihood: -19.303207397460938
Adjusted likelihood: -19.303207397460938
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0101,  0.0201, -0.1745], device='cuda:0')
6 turn
Sampling time 3.7109924780088477
tensor([ 0.0984,  0.6767,  0.4502,  0.5514, -0.1582,  0.5192,  0.8743,  1.0914,
         1.3836,  0.2664,  0.1382,  1.0707, -0.0101,  0.0201, -0.1745, -2.0188],
       device='cuda:0')
Original likelihood: -19.78217315673828
Adjusted likelihood: -19.78217315673828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.788214470027015
Current ori: tensor([-0.0101,  0.0201, -0.1745], device='cuda:0')
Middle force: tensor([0.5451, 0.6514, 0.5542, 0.5035, 0.7797, 1.0911, 0.5778, 0.5715, 0.5494,
        0.5668, 0.5961, 0.5991], device='cuda:0')
Thumb force: tensor([0.9284, 0.7149, 1.5974, 2.4823, 0.8662, 1.8205, 0.6117, 0.6471, 0.5553,
        1.6351, 0.5689, 0.5785], device='cuda:0')
Index force: tensor([0.5223, 0.8688, 0.6068, 0.5877, 0.6040, 0.6062, 0.5980, 0.5400, 0.5010,
        0.7659, 0.7119, 0.5824], device='cuda:0')
Storing NORMAL transition: reward=0.0001 (scaled=0.0001), steps=1
Reward stats updated: mean 0.0299 -> 0.0249, std: 0.0274
Collected 6 transitions for RL
tensor([ 1.1483e-01,  7.3037e-01,  4.0421e-01,  4.9379e-01, -1.6453e-01,
         4.8893e-01,  9.4898e-01,  9.8185e-01,  1.4226e+00,  3.8439e-01,
         1.5314e-03,  1.0931e+00, -2.5134e-02,  1.0723e-02, -1.7488e-01,
        -2.0462e+00], device='cuda:0')
Original likelihood: -23.35593032836914
Adjusted likelihood: -23.35593032836914
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9999)
Solve time for step 2 5.372071705991402
Current ori: tensor([-0.0251,  0.0107, -0.1749], device='cuda:0')
Middle force: tensor([1.4239, 0.5282, 0.5290, 0.6201, 1.3340, 0.5343, 0.5241, 0.5494, 0.5198,
        0.5415, 0.5226], device='cuda:0')
Thumb force: tensor([0.8370, 0.5023, 0.8025, 0.7833, 0.9103, 0.7231, 0.6041, 0.5293, 0.8245,
        0.9124, 1.0678], device='cuda:0')
Index force: tensor([0.6742, 0.7705, 0.5743, 0.5076, 0.5257, 0.5042, 0.5504, 0.5497, 0.5222,
        0.5528, 0.5085], device='cuda:0')
Storing NORMAL transition: reward=0.0436 (scaled=0.0436), steps=1
Reward stats updated: mean 0.0249 -> 0.0276, std: 0.0262
Collected 7 transitions for RL
tensor([ 8.2116e-02,  6.6386e-01,  4.8400e-01,  4.7126e-01, -1.3386e-01,
         4.9196e-01,  9.5698e-01,  1.0827e+00,  1.4605e+00,  2.5172e-01,
         6.0876e-02,  9.5370e-01, -5.7490e-03,  6.3516e-04, -2.1783e-01,
        -1.3966e+00], device='cuda:0')
Original likelihood: -21.53247833251953
Adjusted likelihood: -21.53247833251953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.037133628968149
Current ori: tensor([-0.0057,  0.0006, -0.2178], device='cuda:0')
Middle force: tensor([0.5051, 1.4279, 0.6808, 0.5016, 0.5504, 0.5025, 0.5673, 0.5341, 0.5510,
        0.5878], device='cuda:0')
Thumb force: tensor([1.2352, 0.5888, 0.5202, 0.9454, 1.1689, 0.5468, 0.5944, 0.5388, 1.2590,
        0.6015], device='cuda:0')
Index force: tensor([0.6605, 0.7226, 0.5519, 0.6200, 0.6295, 0.6553, 0.5503, 0.6240, 0.6854,
        0.5887], device='cuda:0')
Storing NORMAL transition: reward=-0.1103 (scaled=-0.1103), steps=1
Reward stats updated: mean 0.0276 -> 0.0103, std: 0.0517
Collected 8 transitions for RL
tensor([ 0.2122,  0.6629,  0.5701,  0.4772, -0.0560,  0.6106,  0.9052,  1.0120,
         1.3442,  0.3413,  0.0395,  1.0612, -0.0310, -0.0620, -0.1116, -1.9546],
       device='cuda:0')
Original likelihood: -29.276567459106445
Adjusted likelihood: -29.276567459106445
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.6535)
Solve time for step 4 4.861207425012253
Current ori: tensor([-0.0310, -0.0620, -0.1116], device='cuda:0')
Middle force: tensor([1.3809, 0.6691, 0.5015, 0.5511, 0.5023, 0.5656, 0.5390, 0.5503, 0.5848],
       device='cuda:0')
Thumb force: tensor([0.5873, 0.5203, 0.9259, 1.1397, 0.5438, 0.5872, 0.5312, 1.2322, 0.5981],
       device='cuda:0')
Index force: tensor([0.7153, 0.5525, 0.6176, 0.6316, 0.6527, 0.5511, 0.6178, 0.6828, 0.5862],
       device='cuda:0')
Storing NORMAL transition: reward=0.0017 (scaled=0.0017), steps=1
Reward stats updated: mean 0.0103 -> 0.0094, std: 0.0489
Collected 9 transitions for RL
tensor([ 0.1773,  0.6508,  0.5851,  0.5210, -0.0596,  0.6099,  0.8954,  1.0230,
         1.3440,  0.3332,  0.0543,  1.0462, -0.0298, -0.0589, -0.1129, -1.9815],
       device='cuda:0')
Original likelihood: -22.4589786529541
Adjusted likelihood: -22.4589786529541
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.566846472967882
Current ori: tensor([-0.0298, -0.0589, -0.1129], device='cuda:0')
Middle force: tensor([0.6630, 0.5017, 0.5510, 0.5024, 0.5645, 0.5433, 0.5494, 0.5818],
       device='cuda:0')
Thumb force: tensor([0.5193, 0.9069, 1.1230, 0.5403, 0.5834, 0.5276, 1.2110, 0.5948],
       device='cuda:0')
Index force: tensor([0.5518, 0.6042, 0.6277, 0.6472, 0.5501, 0.6073, 0.6778, 0.5843],
       device='cuda:0')
Storing NORMAL transition: reward=0.0086 (scaled=0.0086), steps=1
Reward stats updated: mean 0.0094 -> 0.0093, std: 0.0463
Collected 10 transitions for RL
tensor([ 0.1575,  0.6312,  0.5841,  0.5403, -0.0770,  0.5934,  0.8969,  1.0368,
         1.3670,  0.3064,  0.0842,  0.9987, -0.0254, -0.0461, -0.1202, -1.9969],
       device='cuda:0')
Original likelihood: -22.58371353149414
Adjusted likelihood: -22.58371353149414
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.371934627997689
Current ori: tensor([-0.0254, -0.0461, -0.1202], device='cuda:0')
Middle force: tensor([0.5016, 0.5500, 0.5022, 0.5632, 0.5431, 0.5474, 0.5792],
       device='cuda:0')
Thumb force: tensor([0.8928, 1.1119, 0.5391, 0.5807, 0.5261, 1.1955, 0.5916],
       device='cuda:0')
Index force: tensor([0.5968, 0.6212, 0.6450, 0.5489, 0.6025, 0.6719, 0.5825],
       device='cuda:0')
Storing NORMAL transition: reward=0.1429 (scaled=0.1429), steps=1
Reward stats updated: mean 0.0093 -> 0.0214, std: 0.0585
Collected 11 transitions for RL
tensor([ 0.1319,  0.4912,  0.6418,  0.7506, -0.1268,  0.5966,  0.7902,  1.1340,
         1.4679,  0.3769,  0.1226,  0.8411,  0.0237, -0.0191, -0.2617, -1.8139],
       device='cuda:0')
Original likelihood: -25.71877670288086
Adjusted likelihood: -25.71877670288086
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9903)
Solve time for step 7 4.206389138998929
Current ori: tensor([ 0.0237, -0.0191, -0.2617], device='cuda:0')
Middle force: tensor([0.5214, 0.5066, 0.5321, 1.0029, 0.5026, 0.5443], device='cuda:0')
Thumb force: tensor([0.6757, 0.5030, 0.5472, 0.5241, 0.5208, 0.5301], device='cuda:0')
Index force: tensor([0.5056, 0.5076, 0.5925, 0.5218, 0.5572, 0.5015], device='cuda:0')
Storing NORMAL transition: reward=-0.0116 (scaled=-0.0116), steps=1
Reward stats updated: mean 0.0214 -> 0.0187, std: 0.0568
Collected 12 transitions for RL
tensor([ 0.0699,  0.4950,  0.6004,  0.7042, -0.1563,  0.6049,  0.7555,  1.1223,
         1.5000,  0.3822,  0.1314,  0.7677,  0.0153,  0.0126, -0.2494, -1.9382],
       device='cuda:0')
Original likelihood: -25.39798355102539
Adjusted likelihood: -25.39798355102539
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9940)
Solve time for step 8 4.180280463013332
Current ori: tensor([ 0.0153,  0.0126, -0.2494], device='cuda:0')
Middle force: tensor([0.5191, 0.5128, 0.5111, 0.5170, 0.5155], device='cuda:0')
Thumb force: tensor([0.5089, 0.7442, 0.6554, 0.6025, 0.5588], device='cuda:0')
Index force: tensor([0.5339, 0.5073, 0.5165, 0.5031, 0.5451], device='cuda:0')
Storing NORMAL transition: reward=0.0080 (scaled=0.0080), steps=1
Reward stats updated: mean 0.0187 -> 0.0179, std: 0.0546
Collected 13 transitions for RL
tensor([ 0.0046,  0.5306,  0.5193,  0.6474, -0.2014,  0.6299,  0.6603,  1.1562,
         1.5000,  0.3303,  0.1121,  0.7189,  0.0025,  0.0484, -0.2597, -2.0330],
       device='cuda:0')
Original likelihood: -38.41844940185547
Adjusted likelihood: -38.41844940185547
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.11199951171875
Projection step: 1, Loss: 35.82975387573242
Projection step: 2, Loss: 33.88158416748047
Projection step: 3, Loss: 33.55200958251953
Projection step: 4, Loss: 32.607444763183594
Projection step: 5, Loss: 33.54597091674805
Projection step: 6, Loss: 31.293909072875977
Projection step: 7, Loss: 31.60674285888672
Projection step: 8, Loss: 31.089521408081055
Projection step: 9, Loss: 30.494489669799805
Projection step: 10, Loss: 30.144794464111328
Projection step: 11, Loss: 29.173730850219727
Projection step: 12, Loss: 27.118804931640625
Projection step: 13, Loss: 28.985923767089844
Projection step: 14, Loss: 28.2344970703125
Projection step: 15, Loss: 27.699195861816406
Projection step: 16, Loss: 27.28863525390625
Projection step: 17, Loss: 25.918344497680664
Projection step: 18, Loss: 25.105899810791016
Projection step: 19, Loss: 25.41998863220215
Projection step: 20, Loss: 23.873065948486328
Projection step: 21, Loss: 26.275131225585938
Projection step: 22, Loss: 25.036529541015625
Projection step: 23, Loss: 23.705904006958008
Projection step: 24, Loss: 23.493091583251953
Final likelihood: tensor([-20.7350, -26.0454, -25.2980, -21.7906, -20.2402, -26.3945, -28.7972,
        -28.2783, -23.0868, -26.2509, -31.2182, -19.7708, -24.8610, -20.9980,
        -24.7148, -19.6455])
Final projection likelihood: -24.2578
1 mode projection succeeded
New goal: tensor([-0.0046,  0.5510,  0.5531,  0.6095, -0.1521,  0.5905,  0.5515,  1.0679,
         1.4817,  0.3010,  0.1196,  0.9209, -0.0040,  0.0372, -1.0077],
       device='cuda:0')
tensor([[0.0102]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0030]], device='cuda:0')
Original likelihood: -28.824756622314453
Adjusted likelihood: -28.824756622314453
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 28.824756622314453}
Current yaw: tensor([ 0.0025,  0.0484, -0.2597], device='cuda:0')
7 thumb_middle
tensor([ 0.0046,  0.5306,  0.5193,  0.6474, -0.2014,  0.6299,  0.6603,  1.1562,
         1.5000,  0.3303,  0.1121,  0.7189,  0.0025,  0.0484, -0.2597, -2.0330],
       device='cuda:0')
Solve time for step 1 8.45428654900752
Current ori: tensor([ 0.0025,  0.0484, -0.2597], device='cuda:0')
Index force: tensor([0.6015, 0.5881, 0.6115, 0.5097], device='cuda:0')
tensor([-8.9263e-04,  5.2088e-01,  5.3825e-01,  6.2613e-01, -2.2749e-01,
         6.1740e-01,  5.6977e-01,  1.0741e+00,  1.4550e+00,  2.9463e-01,
         6.0392e-02,  8.6027e-01,  2.9135e-03,  5.1591e-02, -2.5953e-01,
        -2.0446e+00], device='cuda:0')
Solve time for step 2 3.5942956790095195
Current ori: tensor([ 0.0029,  0.0516, -0.2595], device='cuda:0')
Index force: tensor([0.5805, 0.5687, 0.5673], device='cuda:0')
tensor([ 0.0023,  0.5486,  0.5217,  0.5888, -0.2208,  0.6263,  0.5640,  1.0654,
         1.4473,  0.2872,  0.0475,  0.8815, -0.0069,  0.0497, -0.2595, -2.0544],
       device='cuda:0')
Solve time for step 3 3.4006609359639697
Current ori: tensor([-0.0069,  0.0497, -0.2595], device='cuda:0')
Index force: tensor([0.5567, 0.5555], device='cuda:0')
tensor([ 7.1896e-03,  5.2568e-01,  5.4701e-01,  6.1047e-01, -2.1604e-01,
         6.2902e-01,  5.5476e-01,  1.0604e+00,  1.4481e+00,  2.8762e-01,
         4.3792e-02,  8.8862e-01,  1.0598e-04,  4.7066e-02, -2.5953e-01,
        -2.0385e+00], device='cuda:0')
Solve time for step 4 3.269306918955408
Current ori: tensor([ 1.0598e-04,  4.7066e-02, -2.5953e-01], device='cuda:0')
Index force: tensor([0.5000], device='cuda:0')
Storing RECOVERY transition: reward=0.0105 (scaled=0.0013), steps=8
Reward stats updated: mean 0.0179 -> 0.0167, std: 0.0528
Collected 14 transitions for RL
Original likelihood: -30.507465362548828
Adjusted likelihood: -30.507465362548828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.3909)
Current yaw: tensor([-0.0021,  0.0437, -0.2697], device='cuda:0')
8 turn
Sampling time 3.662744900037069
tensor([ 0.0065,  0.5328,  0.5437,  0.5954, -0.1528,  0.6587,  0.5804,  1.0781,
         1.4999,  0.3042,  0.1000,  0.9202, -0.0021,  0.0437, -0.2697, -1.9911],
       device='cuda:0')
Original likelihood: -30.528911590576172
Adjusted likelihood: -30.528911590576172
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.3864)
State is out of distribution
Projection step: 0, Loss: 30.857315063476562
Projection step: 1, Loss: 30.562755584716797
Projection step: 2, Loss: 29.910160064697266
Projection step: 3, Loss: 28.58526611328125
Projection step: 4, Loss: 29.120861053466797
Projection step: 5, Loss: 25.832500457763672
Projection step: 6, Loss: 27.602521896362305
Projection step: 7, Loss: 25.77029800415039
Projection step: 8, Loss: 25.93090057373047
Projection step: 9, Loss: 24.090394973754883
Projection step: 10, Loss: 24.706241607666016
Projection step: 11, Loss: 23.043807983398438
Projection step: 12, Loss: 25.023670196533203
Projection step: 13, Loss: 24.984785079956055
Projection step: 14, Loss: 22.72924041748047
Projection step: 15, Loss: 22.585649490356445
Projection step: 16, Loss: 22.297618865966797
Projection step: 17, Loss: 22.85445213317871
Projection step: 18, Loss: 23.084543228149414
Projection step: 19, Loss: 21.10637855529785
Projection step: 20, Loss: 22.968881607055664
Projection step: 21, Loss: 21.555355072021484
Projection step: 22, Loss: 21.739910125732422
Projection step: 23, Loss: 20.879730224609375
Projection step: 24, Loss: 21.314105987548828
Final likelihood: tensor([-20.2574, -21.3356, -17.5774, -20.9313, -23.1685, -20.5154, -19.8838,
        -17.3351, -27.4618, -20.4099, -19.8374, -19.1413, -22.2547, -16.2268,
        -17.9941, -24.5981])
Final projection likelihood: -20.5580
1 mode projection succeeded
New goal: tensor([ 0.0079,  0.5665,  0.5542,  0.5939, -0.1091,  0.6203,  0.5007,  1.0485,
         1.4593,  0.2613,  0.1097,  1.0005, -0.0084,  0.0338, -1.0566],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -24.089744567871094
Adjusted likelihood: -24.089744567871094
Likelihood residual: 0.0
Original likelihood: -24.721805572509766
Adjusted likelihood: -24.721805572509766
Likelihood residual: 0.0
{'index': 24.721805572509766, 'thumb_middle': 24.089744567871094}
Current yaw: tensor([-0.0021,  0.0437, -0.2697], device='cuda:0')
9 thumb_middle
tensor([ 0.0065,  0.5328,  0.5437,  0.5954, -0.1528,  0.6587,  0.5804,  1.0781,
         1.4999,  0.3042,  0.1000,  0.9202, -0.0021,  0.0437, -0.2697, -1.9911],
       device='cuda:0')
Solve time for step 1 8.842497100995388
Current ori: tensor([-0.0021,  0.0437, -0.2697], device='cuda:0')
Index force: tensor([0.5859, 0.5707, 0.5657, 0.5032], device='cuda:0')
tensor([ 3.3789e-04,  5.3117e-01,  5.4386e-01,  5.8880e-01, -1.9747e-01,
         6.2590e-01,  5.0174e-01,  1.0381e+00,  1.4160e+00,  2.5591e-01,
         5.2876e-02,  9.7505e-01, -2.2036e-03,  4.7447e-02, -2.6970e-01,
        -2.0003e+00], device='cuda:0')
Solve time for step 2 3.4199282440240495
Current ori: tensor([-0.0022,  0.0474, -0.2697], device='cuda:0')
Index force: tensor([0.5015, 0.5701, 0.5569], device='cuda:0')
tensor([-0.0036,  0.5386,  0.5348,  0.5795, -0.2007,  0.6305,  0.4940,  1.0359,
         1.4264,  0.2503,  0.0413,  0.9714, -0.0046,  0.0496, -0.2697, -2.0093],
       device='cuda:0')
Solve time for step 3 3.297118575021159
Current ori: tensor([-0.0046,  0.0496, -0.2697], device='cuda:0')
Index force: tensor([0.5613, 0.5496], device='cuda:0')
tensor([ 0.0150,  0.5407,  0.5447,  0.5870, -0.1898,  0.6369,  0.4922,  1.0346,
         1.4217,  0.2436,  0.0312,  0.9697, -0.0049,  0.0392, -0.2697, -1.9823],
       device='cuda:0')
Solve time for step 4 3.310343090968672
Current ori: tensor([-0.0049,  0.0392, -0.2697], device='cuda:0')
Index force: tensor([0.5674], device='cuda:0')
Storing RECOVERY transition: reward=0.0055 (scaled=0.0055), steps=0
Reward stats updated: mean 0.0167 -> 0.0159, std: 0.0511
Collected 15 transitions for RL
Original likelihood: -29.3248291015625
Adjusted likelihood: -29.3248291015625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.6437)
State is out of distribution
Projection step: 0, Loss: 30.342300415039062
Projection step: 1, Loss: 27.62331771850586
Projection step: 2, Loss: 27.662687301635742
Projection step: 3, Loss: 27.00577735900879
Projection step: 4, Loss: 26.591110229492188
Projection step: 5, Loss: 27.547351837158203
Projection step: 6, Loss: 25.6506290435791
Projection step: 7, Loss: 25.726951599121094
Projection step: 8, Loss: 26.22888946533203
Projection step: 9, Loss: 25.309635162353516
Projection step: 10, Loss: 26.13529396057129
Projection step: 11, Loss: 24.508140563964844
Projection step: 12, Loss: 23.850643157958984
Projection step: 13, Loss: 23.605640411376953
Projection step: 14, Loss: 24.055686950683594
Projection step: 15, Loss: 22.77969741821289
Projection step: 16, Loss: 22.13300323486328
Projection step: 17, Loss: 21.945938110351562
Projection step: 18, Loss: 22.687522888183594
Projection step: 19, Loss: 22.86726188659668
Projection step: 20, Loss: 22.853313446044922
Projection step: 21, Loss: 21.164546966552734
Projection step: 22, Loss: 20.75710678100586
Projection step: 23, Loss: 21.064207077026367
Projection step: 24, Loss: 20.03343963623047
Final likelihood: tensor([-15.4656, -21.1503, -17.4712, -20.3019, -19.4103, -19.7188, -20.7352,
        -20.0694, -19.8820, -15.4424, -25.5510, -18.1219, -17.3228, -18.1220,
        -21.9262, -33.5937])
Final projection likelihood: -20.2678
1 mode projection succeeded
New goal: tensor([ 0.0099,  0.5517,  0.5299,  0.6333, -0.0828,  0.6405,  0.4614,  1.0490,
         1.4386,  0.2108,  0.1207,  1.0477, -0.0089,  0.0300, -0.6875],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -20.865711212158203
Adjusted likelihood: -20.865711212158203
Likelihood residual: 0.0
Original likelihood: -24.469602584838867
Adjusted likelihood: -24.469602584838867
Likelihood residual: 0.0
{'index': 24.469602584838867, 'thumb_middle': 20.865711212158203}
Current yaw: tensor([-0.0044,  0.0421, -0.2752], device='cuda:0')
10 thumb_middle
tensor([ 0.0091,  0.5402,  0.5390,  0.5896, -0.1269,  0.6763,  0.5203,  1.0525,
         1.4847,  0.2601,  0.0845,  1.0008, -0.0044,  0.0421, -0.2752, -1.9831],
       device='cuda:0')
Solve time for step 1 8.861266521038488
Current ori: tensor([-0.0044,  0.0421, -0.2752], device='cuda:0')
Index force: tensor([0.5697, 0.5435, 0.5801, 0.5755], device='cuda:0')
tensor([ 1.0553e-02,  5.3552e-01,  5.2485e-01,  6.3258e-01, -1.7466e-01,
         6.3569e-01,  4.4684e-01,  1.0416e+00,  1.4055e+00,  2.0626e-01,
         4.7623e-02,  1.0196e+00,  4.8815e-04,  4.1770e-02, -2.7508e-01,
        -1.9716e+00], device='cuda:0')
Solve time for step 2 3.395946307980921
Current ori: tensor([ 0.0005,  0.0418, -0.2751], device='cuda:0')
Index force: tensor([0.5371, 0.5722, 0.5678], device='cuda:0')
tensor([ 2.1173e-02,  5.3782e-01,  5.2946e-01,  6.3549e-01, -1.6980e-01,
         6.5680e-01,  4.5195e-01,  9.9804e-01,  1.4101e+00,  1.9380e-01,
         4.3770e-02,  9.9846e-01, -1.7656e-05,  3.5915e-02, -2.7508e-01,
        -1.9571e+00], device='cuda:0')
Solve time for step 3 3.2493178569711745
Current ori: tensor([-1.7656e-05,  3.5915e-02, -2.7508e-01], device='cuda:0')
Index force: tensor([0.5589, 0.5562], device='cuda:0')
tensor([ 0.0182,  0.5513,  0.5109,  0.6297, -0.1693,  0.6484,  0.4332,  1.0435,
         1.4053,  0.1998,  0.0367,  1.0171, -0.0036,  0.0374, -0.2751, -1.9645],
       device='cuda:0')
Solve time for step 4 3.4071810470195487
Current ori: tensor([-0.0036,  0.0374, -0.2751], device='cuda:0')
Index force: tensor([0.5490], device='cuda:0')
Storing RECOVERY transition: reward=-0.0057 (scaled=-0.0057), steps=0
Reward stats updated: mean 0.0159 -> 0.0146, std: 0.0498
Collected 16 transitions for RL
Original likelihood: -34.13782501220703
Adjusted likelihood: -34.13782501220703
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0120)
State is out of distribution
Projection step: 0, Loss: 33.500511169433594
Projection step: 1, Loss: 32.41722869873047
Projection step: 2, Loss: 31.068309783935547
Projection step: 3, Loss: 28.793136596679688
Projection step: 4, Loss: 28.81538200378418
Projection step: 5, Loss: 29.479286193847656
Projection step: 6, Loss: 29.89415740966797
Projection step: 7, Loss: 28.120941162109375
Projection step: 8, Loss: 29.138011932373047
Projection step: 9, Loss: 28.973169326782227
Projection step: 10, Loss: 28.653972625732422
Projection step: 11, Loss: 29.287586212158203
Projection step: 12, Loss: 30.955917358398438
Projection step: 13, Loss: 29.21467399597168
Projection step: 14, Loss: 28.104814529418945
Projection step: 15, Loss: 27.03900909423828
Projection step: 16, Loss: 28.364105224609375
Projection step: 17, Loss: 26.010494232177734
Projection step: 18, Loss: 27.939300537109375
Projection step: 19, Loss: 27.126384735107422
Projection step: 20, Loss: 26.31673812866211
Projection step: 21, Loss: 27.19934844970703
Projection step: 22, Loss: 26.607952117919922
Projection step: 23, Loss: 24.83605194091797
Projection step: 24, Loss: 25.64959716796875
Final likelihood: tensor([-28.5257, -23.9486, -23.5588, -23.8264, -27.7104, -24.8093, -27.3581,
        -26.6196, -23.0989, -27.7547, -25.7993, -26.8157, -26.0739, -24.2545,
        -25.3464, -26.3134])
Final projection likelihood: -25.7384
1 mode projection succeeded
New goal: tensor([-0.0060,  0.5320,  0.5070,  0.7041, -0.0945,  0.6432,  0.4131,  1.0727,
         1.4446,  0.1564,  0.1209,  1.1135, -0.0016,  0.0431, -0.4167],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -30.3192195892334
Adjusted likelihood: -30.3192195892334
Likelihood residual: 0.0
Original likelihood: -27.270259857177734
Adjusted likelihood: -27.270259857177734
Likelihood residual: 0.0
{'index': 27.270259857177734, 'thumb_middle': 30.3192195892334}
Current yaw: tensor([ 0.0019,  0.0564, -0.2653], device='cuda:0')
11 index
tensor([-1.6850e-02,  5.2820e-01,  5.1875e-01,  6.1868e-01, -1.2491e-01,
         6.7533e-01,  4.7553e-01,  1.0378e+00,  1.4881e+00,  1.9315e-01,
         9.6693e-02,  1.0543e+00,  1.9223e-03,  5.6405e-02, -2.6527e-01,
        -2.0176e+00], device='cuda:0')
Solve time for step 1 9.874572700995486
Current ori: tensor([ 0.0019,  0.0564, -0.2653], device='cuda:0')
Middle force: tensor([0.5645, 0.5000, 0.5045, 0.5073], device='cuda:0')
Thumb force: tensor([0.5834, 0.6182, 0.5696, 0.5069], device='cuda:0')
tensor([ 0.0353,  0.4675,  0.4560,  0.6610, -0.1378,  0.6910,  0.4588,  1.0893,
         1.5000,  0.1818,  0.0788,  1.0660,  0.0056,  0.0548, -0.2745, -2.2327],
       device='cuda:0')
Solve time for step 2 3.952656670997385
Current ori: tensor([ 0.0056,  0.0548, -0.2745], device='cuda:0')
Middle force: tensor([0.5858, 0.5368, 0.5382], device='cuda:0')
Thumb force: tensor([0.5907, 0.5964, 0.5712], device='cuda:0')
tensor([ 0.0387,  0.4694,  0.4496,  0.6707, -0.1387,  0.6957,  0.4481,  1.0938,
         1.5000,  0.1819,  0.0785,  1.0697,  0.0059,  0.0558, -0.2667, -2.2566],
       device='cuda:0')
Solve time for step 3 3.9859340940020047
Current ori: tensor([ 0.0059,  0.0558, -0.2667], device='cuda:0')
Middle force: tensor([0.5028, 0.5055], device='cuda:0')
Thumb force: tensor([0.5568, 0.5053], device='cuda:0')
tensor([ 0.0375,  0.4672,  0.4498,  0.6718, -0.1309,  0.7006,  0.4490,  1.0896,
         1.5000,  0.1775,  0.0708,  1.0699,  0.0034,  0.0506, -0.2739, -2.1428],
       device='cuda:0')
Solve time for step 4 3.875620512990281
Current ori: tensor([ 0.0034,  0.0506, -0.2739], device='cuda:0')
Middle force: tensor([0.5295], device='cuda:0')
Thumb force: tensor([0.5334], device='cuda:0')
Storing RECOVERY transition: reward=0.0105 (scaled=0.0105), steps=0
Reward stats updated: mean 0.0146 -> 0.0143, std: 0.0483
Collected 17 transitions for RL
Original likelihood: -32.0377311706543
Adjusted likelihood: -32.0377311706543
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.1331)
State is out of distribution
Projection step: 0, Loss: 34.36579132080078
Projection step: 1, Loss: 34.33018112182617
Projection step: 2, Loss: 34.36863708496094
Projection step: 3, Loss: 34.16969299316406
Projection step: 4, Loss: 33.61582565307617
Projection step: 5, Loss: 33.19112014770508
Projection step: 6, Loss: 31.596023559570312
Projection step: 7, Loss: 35.567955017089844
Projection step: 8, Loss: 30.60036849975586
Projection step: 9, Loss: 30.119625091552734
Projection step: 10, Loss: 32.454368591308594
Projection step: 11, Loss: 28.648319244384766
Projection step: 12, Loss: 27.31092071533203
Projection step: 13, Loss: 29.298519134521484
Projection step: 14, Loss: 26.295692443847656
Projection step: 15, Loss: 29.7396297454834
Projection step: 16, Loss: 28.211408615112305
Projection step: 17, Loss: 27.69685173034668
Projection step: 18, Loss: 27.470592498779297
Projection step: 19, Loss: 26.020174026489258
Projection step: 20, Loss: 27.489768981933594
Projection step: 21, Loss: 26.481849670410156
Projection step: 22, Loss: 26.394433975219727
Projection step: 23, Loss: 26.53885269165039
Projection step: 24, Loss: 25.150493621826172
Final likelihood: tensor([-23.9630, -23.1295, -28.2358, -24.1328, -27.7743, -24.4351, -24.7265,
        -27.5919, -21.7473, -25.7838, -25.8350, -24.8102, -25.3574, -25.6278,
        -28.7112, -25.7295])
Final projection likelihood: -25.4744
1 mode projection succeeded
New goal: tensor([-6.3595e-03,  5.4854e-01,  5.1466e-01,  6.9613e-01, -9.7415e-02,
         6.7113e-01,  3.9481e-01,  1.0632e+00,  1.4515e+00,  1.6386e-01,
         8.0370e-02,  1.1163e+00,  1.0626e-03,  3.7377e-02, -7.9567e-01],
       device='cuda:0')
tensor([[0.0030]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -28.894893646240234
Adjusted likelihood: -28.894893646240234
Likelihood residual: 0.0
Original likelihood: -27.108177185058594
Adjusted likelihood: -27.108177185058594
Likelihood residual: 0.0
{'index': 27.108177185058594, 'thumb_middle': 28.894893646240234}
Current yaw: tensor([ 0.0035,  0.0497, -0.2808], device='cuda:0')
12 index
tensor([-0.0059,  0.5255,  0.4922,  0.6962, -0.1329,  0.7033,  0.4514,  1.0921,
         1.5000,  0.1886,  0.0644,  1.0703,  0.0035,  0.0497, -0.2808, -2.0847],
       device='cuda:0')
Solve time for step 1 10.25838986999588
Current ori: tensor([ 0.0035,  0.0497, -0.2808], device='cuda:0')
Middle force: tensor([0.5188, 0.5324, 0.5093, 0.5048], device='cuda:0')
Thumb force: tensor([0.5676, 0.5030, 0.6156, 0.6361], device='cuda:0')
tensor([ 4.5696e-02,  4.7369e-01,  4.5112e-01,  6.6569e-01, -1.2190e-01,
         7.1911e-01,  4.3698e-01,  1.0874e+00,  1.5000e+00,  1.9409e-01,
         4.4937e-02,  1.0757e+00, -1.3721e-03,  4.2833e-02, -2.7975e-01,
        -2.3700e+00], device='cuda:0')
Solve time for step 2 4.142524444963783
Current ori: tensor([-0.0014,  0.0428, -0.2798], device='cuda:0')
Middle force: tensor([0.5300, 0.5083, 0.5041], device='cuda:0')
Thumb force: tensor([0.5025, 0.6117, 0.6332], device='cuda:0')
tensor([ 0.0468,  0.4752,  0.4497,  0.6626, -0.1320,  0.7333,  0.4430,  1.0936,
         1.5000,  0.1901,  0.0478,  1.0740,  0.0027,  0.0411, -0.2822, -2.5024],
       device='cuda:0')
Solve time for step 3 3.985049518989399
Current ori: tensor([ 0.0027,  0.0411, -0.2822], device='cuda:0')
Middle force: tensor([0.5074, 0.5032], device='cuda:0')
Thumb force: tensor([0.5995, 0.6253], device='cuda:0')
tensor([ 0.0500,  0.4760,  0.4515,  0.6636, -0.1247,  0.7324,  0.4480,  1.1011,
         1.5000,  0.1856,  0.0358,  1.0873,  0.0041,  0.0354, -0.2895, -2.5512],
       device='cuda:0')
Solve time for step 4 3.7186006270349026
Current ori: tensor([ 0.0041,  0.0354, -0.2895], device='cuda:0')
Middle force: tensor([0.5784], device='cuda:0')
Thumb force: tensor([0.5715], device='cuda:0')
Storing RECOVERY transition: reward=0.0210 (scaled=0.0210), steps=0
Reward stats updated: mean 0.0143 -> 0.0147, std: 0.0469
Collected 18 transitions for RL
Original likelihood: -30.84319305419922
Adjusted likelihood: -30.84319305419922
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.3227)
State is out of distribution
Projection step: 0, Loss: 30.193574905395508
Projection step: 1, Loss: 30.354110717773438
Projection step: 2, Loss: 29.943531036376953
Projection step: 3, Loss: 28.098140716552734
Projection step: 4, Loss: 26.763675689697266
Projection step: 5, Loss: 27.86833953857422
Projection step: 6, Loss: 29.32467269897461
Projection step: 7, Loss: 26.036739349365234
Projection step: 8, Loss: 25.327106475830078
Projection step: 9, Loss: 24.485013961791992
Projection step: 10, Loss: 24.304092407226562
Projection step: 11, Loss: 24.202838897705078
Projection step: 12, Loss: 24.048744201660156
Projection step: 13, Loss: 24.022911071777344
Projection step: 14, Loss: 23.34995460510254
Projection step: 15, Loss: 23.65850830078125
Projection step: 16, Loss: 24.751705169677734
Projection step: 17, Loss: 22.431201934814453
Projection step: 18, Loss: 21.744491577148438
Projection step: 19, Loss: 22.866535186767578
Projection step: 20, Loss: 21.789339065551758
Projection step: 21, Loss: 21.340038299560547
Projection step: 22, Loss: 21.233123779296875
Projection step: 23, Loss: 22.25843048095703
Projection step: 24, Loss: 20.386474609375
Final likelihood: tensor([-19.4333, -19.7177, -20.5821, -21.4509, -20.8281, -21.0349, -18.8980,
        -22.9661, -18.4029, -21.8586, -19.5093, -22.1194, -21.0106, -20.2077,
        -19.8354, -23.4587])
Final projection likelihood: -20.7071
1 mode projection succeeded
New goal: tensor([ 1.2815e-03,  5.5331e-01,  5.2500e-01,  6.8950e-01, -8.2704e-02,
         6.8770e-01,  4.0322e-01,  1.0434e+00,  1.4504e+00,  1.6974e-01,
         8.2614e-02,  1.0761e+00,  2.2306e-03,  2.4432e-02, -9.8204e-01],
       device='cuda:0')
tensor([[0.0031]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0031]], device='cuda:0')
Original likelihood: -21.640777587890625
Adjusted likelihood: -21.640777587890625
Likelihood residual: 0.0
Original likelihood: -26.82693099975586
Adjusted likelihood: -26.82693099975586
Likelihood residual: 0.0
{'index': 26.82693099975586, 'thumb_middle': 21.640777587890625}
Current yaw: tensor([ 0.0021,  0.0341, -0.2901], device='cuda:0')
13 thumb_middle
tensor([ 3.8943e-03,  5.3486e-01,  4.9236e-01,  6.8511e-01, -1.2466e-01,
         7.4862e-01,  4.4085e-01,  1.0840e+00,  1.5000e+00,  1.8441e-01,
         3.5028e-02,  1.0831e+00,  2.0803e-03,  3.4125e-02, -2.9007e-01,
        -2.5587e+00], device='cuda:0')
Solve time for step 1 8.807469305989798
Current ori: tensor([ 0.0021,  0.0341, -0.2901], device='cuda:0')
Index force: tensor([0.5921, 0.5665, 0.5963, 0.5926], device='cuda:0')
tensor([-1.5261e-04,  5.2754e-01,  5.0386e-01,  6.7518e-01, -1.7520e-01,
         6.9091e-01,  3.8723e-01,  1.0317e+00,  1.4275e+00,  1.5665e-01,
         1.7307e-02,  1.0521e+00,  2.7818e-03,  3.6663e-02, -2.9005e-01,
        -2.5703e+00], device='cuda:0')
Solve time for step 2 3.4526558090001345
Current ori: tensor([ 0.0028,  0.0367, -0.2900], device='cuda:0')
Index force: tensor([0.5591, 0.5881, 0.5865], device='cuda:0')
tensor([ 1.1848e-02,  5.3052e-01,  5.1108e-01,  6.7416e-01, -1.6850e-01,
         7.0027e-01,  3.9468e-01,  1.0305e+00,  1.4219e+00,  1.4981e-01,
         6.7926e-03,  1.0519e+00,  1.8237e-03,  2.9991e-02, -2.9005e-01,
        -2.5554e+00], device='cuda:0')
Solve time for step 3 3.4347875459934585
Current ori: tensor([ 0.0018,  0.0300, -0.2900], device='cuda:0')
Index force: tensor([0.5771, 0.5788], device='cuda:0')
tensor([ 2.2979e-02,  5.4700e-01,  5.0046e-01,  6.6890e-01, -1.6437e-01,
         7.0423e-01,  4.0119e-01,  1.0410e+00,  1.4168e+00,  1.5306e-01,
         2.9305e-03,  1.0432e+00, -2.5239e-03,  2.3742e-02, -2.9004e-01,
        -2.5448e+00], device='cuda:0')
Solve time for step 4 3.3115478699910454
Current ori: tensor([-0.0025,  0.0237, -0.2900], device='cuda:0')
Index force: tensor([0.5665], device='cuda:0')
Storing RECOVERY transition: reward=0.0148 (scaled=0.0148), steps=0
Reward stats updated: mean 0.0147 -> 0.0147, std: 0.0457
Collected 19 transitions for RL
Original likelihood: -32.91670227050781
Adjusted likelihood: -32.91670227050781
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0557)
State is out of distribution
Projection step: 0, Loss: 29.158222198486328
Projection step: 1, Loss: 29.00830841064453
Projection step: 2, Loss: 29.975807189941406
Projection step: 3, Loss: 27.920658111572266
Projection step: 4, Loss: 26.70500946044922
Projection step: 5, Loss: 25.34598731994629
Projection step: 6, Loss: 24.768882751464844
Projection step: 7, Loss: 24.832408905029297
Projection step: 8, Loss: 25.99611473083496
Projection step: 9, Loss: 24.75232696533203
Projection step: 10, Loss: 25.641014099121094
Projection step: 11, Loss: 24.489059448242188
Projection step: 12, Loss: 23.002155303955078
Projection step: 13, Loss: 25.11608123779297
Projection step: 14, Loss: 23.848777770996094
Projection step: 15, Loss: 25.11311912536621
Projection step: 16, Loss: 22.812192916870117
Projection step: 17, Loss: 22.277057647705078
Projection step: 18, Loss: 22.244552612304688
Projection step: 19, Loss: 23.028108596801758
Projection step: 20, Loss: 22.743297576904297
Projection step: 21, Loss: 22.355937957763672
Projection step: 22, Loss: 20.69345474243164
Projection step: 23, Loss: 19.861032485961914
Projection step: 24, Loss: 20.12152099609375
Final likelihood: tensor([-14.9142, -21.8778, -15.8039, -19.1177, -19.9611, -19.1835, -19.2044,
        -18.4151, -22.5403, -18.6588, -18.5504, -19.3264, -21.5653, -19.1237,
        -17.6124, -19.5416])
Final projection likelihood: -19.0873
1 mode projection succeeded
New goal: tensor([-0.0076,  0.5312,  0.5121,  0.6841, -0.0772,  0.6779,  0.3856,  1.0617,
         1.4303,  0.1595,  0.1227,  1.0815,  0.0025,  0.0288, -0.6489],
       device='cuda:0')
tensor([[0.0030]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0030]], device='cuda:0')
Original likelihood: -24.496566772460938
Adjusted likelihood: -24.496566772460938
Likelihood residual: 0.0
Original likelihood: -25.07722282409668
Adjusted likelihood: -25.07722282409668
Likelihood residual: 0.0
{'index': 25.07722282409668, 'thumb_middle': 24.496566772460938}
Current yaw: tensor([ 0.0028,  0.0414, -0.2845], device='cuda:0')
14 thumb_middle
tensor([-0.0102,  0.5281,  0.4976,  0.6701, -0.1186,  0.7327,  0.4338,  1.0681,
         1.4940,  0.1714,  0.0628,  1.0802,  0.0028,  0.0414, -0.2845, -2.5891],
       device='cuda:0')
Solve time for step 1 8.707664321002085
Current ori: tensor([ 0.0028,  0.0414, -0.2845], device='cuda:0')
Index force: tensor([0.5782, 0.5922, 0.5766, 0.5820], device='cuda:0')
tensor([-0.0118,  0.5237,  0.4980,  0.6790, -0.1736,  0.6853,  0.3767,  1.0403,
         1.4137,  0.1300,  0.0498,  1.0589,  0.0043,  0.0429, -0.2845, -2.5903],
       device='cuda:0')
Solve time for step 2 3.538162716955412
Current ori: tensor([ 0.0043,  0.0429, -0.2845], device='cuda:0')
Index force: tensor([0.5795, 0.5653, 0.5713], device='cuda:0')
tensor([-8.3880e-03,  5.3742e-01,  4.8783e-01,  6.6636e-01, -1.7231e-01,
         6.8255e-01,  3.7313e-01,  1.0534e+00,  1.3984e+00,  1.4473e-01,
         5.0480e-02,  1.0665e+00, -1.2829e-04,  4.1162e-02, -2.8445e-01,
        -2.5908e+00], device='cuda:0')
Solve time for step 3 3.4215231869602576
Current ori: tensor([-1.2829e-04,  4.1162e-02, -2.8445e-01], device='cuda:0')
Index force: tensor([0.5523, 0.5591], device='cuda:0')
tensor([ 0.0038,  0.5231,  0.5050,  0.6929, -0.1590,  0.6832,  0.3849,  1.0254,
         1.4035,  0.1333,  0.0516,  1.0455,  0.0055,  0.0344, -0.2845, -2.5660],
       device='cuda:0')
Solve time for step 4 3.2991986790439114
Current ori: tensor([ 0.0055,  0.0344, -0.2845], device='cuda:0')
Index force: tensor([0.5436], device='cuda:0')
Storing RECOVERY transition: reward=0.0049 (scaled=0.0049), steps=0
Reward stats updated: mean 0.0147 -> 0.0142, std: 0.0446
Collected 20 transitions for RL
Original likelihood: -30.725830078125
Adjusted likelihood: -30.725830078125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.3460)
Current yaw: tensor([ 0.0129,  0.0477, -0.2752], device='cuda:0')
15 turn
Sampling time 3.615706916025374
tensor([-0.0227,  0.4974,  0.5157,  0.6992, -0.1060,  0.7377,  0.3864,  1.0706,
         1.4670,  0.1733,  0.1067,  1.1031,  0.0129,  0.0477, -0.2752, -2.5935],
       device='cuda:0')
Original likelihood: -30.47315216064453
Adjusted likelihood: -30.47315216064453
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.3981)
State is out of distribution
Projection step: 0, Loss: 30.87803077697754
Projection step: 1, Loss: 30.875022888183594
Projection step: 2, Loss: 28.89600372314453
Projection step: 3, Loss: 27.393085479736328
Projection step: 4, Loss: 26.924055099487305
Projection step: 5, Loss: 26.925750732421875
Projection step: 6, Loss: 25.191722869873047
Projection step: 7, Loss: 25.0714111328125
Projection step: 8, Loss: 24.81464385986328
Projection step: 9, Loss: 23.84480857849121
Projection step: 10, Loss: 24.164871215820312
Projection step: 11, Loss: 24.033092498779297
Projection step: 12, Loss: 23.699337005615234
Projection step: 13, Loss: 22.023488998413086
Projection step: 14, Loss: 22.229156494140625
Projection step: 15, Loss: 22.433135986328125
Projection step: 16, Loss: 21.750526428222656
Projection step: 17, Loss: 21.85628890991211
Projection step: 18, Loss: 21.860448837280273
Projection step: 19, Loss: 20.71158790588379
Projection step: 20, Loss: 20.346263885498047
Projection step: 21, Loss: 20.062414169311523
Projection step: 22, Loss: 19.611461639404297
Projection step: 23, Loss: 18.903892517089844
Final likelihood: tensor([-18.3202, -19.1746, -18.8649, -20.4089, -19.2135, -19.0281, -20.0801,
        -22.4492, -17.1536, -16.9553, -17.3049, -17.8995, -17.2006, -20.2783,
        -20.5564, -17.5740])
Final projection likelihood: -18.9039
1 mode projection succeeded
New goal: tensor([-0.0227,  0.5059,  0.5250,  0.6986, -0.0790,  0.6993,  0.3487,  1.0612,
         1.4066,  0.1777,  0.1411,  1.1172,  0.0162,  0.0339, -0.4838],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0030]], device='cuda:0')
Original likelihood: -26.204862594604492
Adjusted likelihood: -26.204862594604492
Likelihood residual: 0.0
Original likelihood: -24.35687255859375
Adjusted likelihood: -24.35687255859375
Likelihood residual: 0.0
{'index': 24.35687255859375, 'thumb_middle': 26.204862594604492}
Current yaw: tensor([ 0.0129,  0.0477, -0.2752], device='cuda:0')
16 index
tensor([-0.0227,  0.4974,  0.5157,  0.6992, -0.1060,  0.7377,  0.3864,  1.0706,
         1.4670,  0.1733,  0.1067,  1.1031,  0.0129,  0.0477, -0.2752, -2.5935],
       device='cuda:0')
Solve time for step 1 10.40613839001162
Current ori: tensor([ 0.0129,  0.0477, -0.2752], device='cuda:0')
Middle force: tensor([0.5529, 0.5190, 0.5690, 0.5158], device='cuda:0')
Thumb force: tensor([0.5647, 0.5446, 0.5826, 0.5012], device='cuda:0')
tensor([ 0.0236,  0.4455,  0.4719,  0.6728, -0.1190,  0.7422,  0.3879,  1.0927,
         1.4644,  0.1951,  0.1026,  1.1056,  0.0151,  0.0492, -0.2803, -2.5584],
       device='cuda:0')
Solve time for step 2 4.113349458028097
Current ori: tensor([ 0.0151,  0.0492, -0.2803], device='cuda:0')
Middle force: tensor([0.5166, 0.5651, 0.5136], device='cuda:0')
Thumb force: tensor([0.5411, 0.5780, 0.5009], device='cuda:0')
tensor([ 0.0184,  0.4452,  0.4686,  0.6686, -0.1227,  0.7413,  0.3870,  1.0908,
         1.4582,  0.2091,  0.1122,  1.1007,  0.0154,  0.0521, -0.2758, -2.4644],
       device='cuda:0')
Solve time for step 3 3.928108273015823
Current ori: tensor([ 0.0154,  0.0521, -0.2758], device='cuda:0')
Middle force: tensor([0.5327, 0.5142], device='cuda:0')
Thumb force: tensor([0.5542, 0.5627], device='cuda:0')
tensor([ 0.0205,  0.4458,  0.4677,  0.6678, -0.1158,  0.7532,  0.3760,  1.0759,
         1.4773,  0.1725,  0.0968,  1.0950,  0.0106,  0.0490, -0.2859, -2.2938],
       device='cuda:0')
Solve time for step 4 3.9117106730118394
Current ori: tensor([ 0.0106,  0.0490, -0.2859], device='cuda:0')
Middle force: tensor([0.5128], device='cuda:0')
Thumb force: tensor([0.5587], device='cuda:0')
Storing RECOVERY transition: reward=0.0230 (scaled=0.0230), steps=0
Reward stats updated: mean 0.0142 -> 0.0146, std: 0.0436
Collected 21 transitions for RL
Original likelihood: -35.084449768066406
Adjusted likelihood: -35.084449768066406
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0028)
State is out of distribution
Projection step: 0, Loss: 33.93569564819336
Projection step: 1, Loss: 32.52680206298828
Projection step: 2, Loss: 32.773719787597656
Projection step: 3, Loss: 32.580718994140625
Projection step: 4, Loss: 31.208301544189453
Projection step: 5, Loss: 31.023569107055664
Projection step: 6, Loss: 29.370555877685547
Projection step: 7, Loss: 29.81198501586914
Projection step: 8, Loss: 29.102840423583984
Projection step: 9, Loss: 30.911968231201172
Projection step: 10, Loss: 28.37211036682129
Projection step: 11, Loss: 26.723798751831055
Projection step: 12, Loss: 29.15243148803711
Projection step: 13, Loss: 27.125244140625
Projection step: 14, Loss: 27.963741302490234
Projection step: 15, Loss: 27.968059539794922
Projection step: 16, Loss: 26.770713806152344
Projection step: 17, Loss: 27.438526153564453
Projection step: 18, Loss: 26.085609436035156
Projection step: 19, Loss: 26.77370834350586
Projection step: 20, Loss: 26.601783752441406
Projection step: 21, Loss: 24.915756225585938
Projection step: 22, Loss: 25.025249481201172
Projection step: 23, Loss: 24.48862075805664
Projection step: 24, Loss: 25.284393310546875
Final likelihood: tensor([-21.2592, -21.6922, -23.4061, -18.8790, -26.4607, -24.4578, -22.6848,
        -25.8397, -26.0261, -21.3458, -23.0523, -23.6184, -22.5563, -21.9629,
        -24.2392, -27.5785])
Final projection likelihood: -23.4412
1 mode projection succeeded
New goal: tensor([-0.0264,  0.5264,  0.5300,  0.7163, -0.0946,  0.7112,  0.3207,  1.0545,
         1.4164,  0.1821,  0.1046,  1.1219,  0.0145,  0.0393, -0.8053],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -29.790639877319336
Adjusted likelihood: -29.790639877319336
Likelihood residual: 0.0
Original likelihood: -25.675331115722656
Adjusted likelihood: -25.675331115722656
Likelihood residual: 0.0
{'index': 25.675331115722656, 'thumb_middle': 29.790639877319336}
Current yaw: tensor([ 0.0108,  0.0531, -0.2988], device='cuda:0')
17 index
tensor([-0.0254,  0.5023,  0.5104,  0.6919, -0.1238,  0.7525,  0.3767,  1.0741,
         1.4682,  0.2049,  0.1034,  1.0910,  0.0108,  0.0531, -0.2988, -2.2213],
       device='cuda:0')
Solve time for step 1 10.29723552201176
Current ori: tensor([ 0.0108,  0.0531, -0.2988], device='cuda:0')
Middle force: tensor([0.5014, 0.5583, 0.5668, 0.5646], device='cuda:0')
Thumb force: tensor([0.5624, 0.5555, 0.5226, 0.5442], device='cuda:0')
tensor([ 0.0279,  0.4492,  0.4637,  0.6824, -0.1277,  0.7535,  0.3677,  1.0918,
         1.4792,  0.1981,  0.0894,  1.1002,  0.0126,  0.0544, -0.3086, -2.3010],
       device='cuda:0')
Solve time for step 2 4.175267505983356
Current ori: tensor([ 0.0126,  0.0544, -0.3086], device='cuda:0')
Middle force: tensor([0.5539, 0.5724, 0.5990], device='cuda:0')
Thumb force: tensor([0.5408, 0.5177, 0.5573], device='cuda:0')
tensor([ 0.0290,  0.4505,  0.4625,  0.6811, -0.1317,  0.7558,  0.3622,  1.0883,
         1.4847,  0.1976,  0.0842,  1.1017,  0.0109,  0.0574, -0.3097, -2.2212],
       device='cuda:0')
Solve time for step 3 4.1650210820371285
Current ori: tensor([ 0.0109,  0.0574, -0.3097], device='cuda:0')
Middle force: tensor([0.5643, 0.5425], device='cuda:0')
Thumb force: tensor([0.5557, 0.5059], device='cuda:0')
tensor([ 0.0290,  0.4525,  0.4594,  0.6788, -0.1255,  0.7661,  0.3573,  1.0769,
         1.4793,  0.2030,  0.0800,  1.1035,  0.0077,  0.0535, -0.3033, -2.0382],
       device='cuda:0')
Solve time for step 4 4.285308229969814
Current ori: tensor([ 0.0077,  0.0535, -0.3033], device='cuda:0')
Middle force: tensor([0.5000], device='cuda:0')
Thumb force: tensor([0.5450], device='cuda:0')
Storing RECOVERY transition: reward=0.0360 (scaled=0.0360), steps=0
Reward stats updated: mean 0.0146 -> 0.0156, std: 0.0428
Collected 22 transitions for RL
Original likelihood: -34.091888427734375
Adjusted likelihood: -34.091888427734375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0128)
State is out of distribution
Projection step: 0, Loss: 33.73006820678711
Projection step: 1, Loss: 34.87596893310547
Projection step: 2, Loss: 33.918190002441406
Projection step: 3, Loss: 32.692413330078125
Projection step: 4, Loss: 33.22294235229492
Projection step: 5, Loss: 32.880043029785156
Projection step: 6, Loss: 33.12181091308594
Projection step: 7, Loss: 30.908206939697266
Projection step: 8, Loss: 31.552593231201172
Projection step: 9, Loss: 31.654211044311523
Projection step: 10, Loss: 30.561830520629883
Projection step: 11, Loss: 29.058300018310547
Projection step: 12, Loss: 29.958152770996094
Projection step: 13, Loss: 29.784934997558594
Projection step: 14, Loss: 29.55171775817871
Projection step: 15, Loss: 28.746803283691406
Projection step: 16, Loss: 29.118568420410156
Projection step: 17, Loss: 28.844249725341797
Projection step: 18, Loss: 27.79176902770996
Projection step: 19, Loss: 29.19668960571289
Projection step: 20, Loss: 28.045303344726562
Projection step: 21, Loss: 27.954906463623047
Projection step: 22, Loss: 29.385780334472656
Projection step: 23, Loss: 27.515817642211914
Projection step: 24, Loss: 27.808691024780273
Final likelihood: tensor([-26.8778, -27.9014, -29.8234, -32.0207, -24.5820, -27.5152, -21.9316,
        -28.1824, -26.2679, -26.1675, -24.9380, -24.7978, -23.8576, -30.4929,
        -28.2185, -24.3595])
Final projection likelihood: -26.7459
1 mode projection succeeded
New goal: tensor([-0.0270,  0.5383,  0.5298,  0.7211, -0.1091,  0.7229,  0.3036,  1.0542,
         1.4404,  0.1834,  0.0663,  1.1301,  0.0070,  0.0461, -0.9344],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -31.112995147705078
Adjusted likelihood: -31.112995147705078
Likelihood residual: 0.0
Original likelihood: -33.47307586669922
Adjusted likelihood: -33.47307586669922
Likelihood residual: 0.0
{'index': 33.47307586669922, 'thumb_middle': 31.112995147705078}
Current yaw: tensor([ 0.0086,  0.0566, -0.3122], device='cuda:0')
18 thumb_middle
tensor([-0.0171,  0.5075,  0.5044,  0.7039, -0.1314,  0.7640,  0.3582,  1.0787,
         1.4835,  0.2095,  0.0746,  1.1072,  0.0086,  0.0566, -0.3122, -1.9552],
       device='cuda:0')
Solve time for step 1 8.534286685986444
Current ori: tensor([ 0.0086,  0.0566, -0.3122], device='cuda:0')
Index force: tensor([0.5863, 0.5932, 0.5987, 0.5972], device='cuda:0')
tensor([-0.0132,  0.5131,  0.5022,  0.6994, -0.1869,  0.7378,  0.3003,  1.0448,
         1.4215,  0.1716,  0.0164,  1.1071,  0.0068,  0.0547, -0.3121, -1.9559],
       device='cuda:0')
Solve time for step 2 3.449815645988565
Current ori: tensor([ 0.0068,  0.0547, -0.3121], device='cuda:0')
Index force: tensor([0.5001, 0.5596, 0.5924], device='cuda:0')
tensor([-0.0171,  0.5119,  0.4943,  0.7109, -0.1916,  0.7415,  0.2946,  1.0404,
         1.4267,  0.1710,  0.0107,  1.1124,  0.0083,  0.0569, -0.3121, -1.9587],
       device='cuda:0')
Solve time for step 3 3.39474923099624
Current ori: tensor([ 0.0083,  0.0569, -0.3121], device='cuda:0')
Index force: tensor([0.5541, 0.5873], device='cuda:0')
tensor([ 0.0021,  0.5076,  0.5158,  0.7132, -0.1836,  0.7465,  0.2986,  1.0412,
         1.4196,  0.1658,  0.0049,  1.1098,  0.0088,  0.0465, -0.3121, -1.9326],
       device='cuda:0')
Solve time for step 4 3.3066476559615694
Current ori: tensor([ 0.0088,  0.0465, -0.3121], device='cuda:0')
Index force: tensor([0.5747], device='cuda:0')
Storing RECOVERY transition: reward=0.0333 (scaled=0.0333), steps=0
Reward stats updated: mean 0.0156 -> 0.0164, std: 0.0420
Collected 23 transitions for RL
Original likelihood: -33.886985778808594
Adjusted likelihood: -33.886985778808594
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0170)
State is out of distribution
Projection step: 0, Loss: 32.572181701660156
Projection step: 1, Loss: 33.86024475097656
Projection step: 2, Loss: 30.364418029785156
Projection step: 3, Loss: 32.46826171875
Projection step: 4, Loss: 31.093624114990234
Projection step: 5, Loss: 29.25704574584961
Projection step: 6, Loss: 31.547210693359375
Projection step: 7, Loss: 28.732906341552734
Projection step: 8, Loss: 29.511348724365234
Projection step: 9, Loss: 27.941232681274414
Projection step: 10, Loss: 28.946889877319336
Projection step: 11, Loss: 31.573434829711914
Projection step: 12, Loss: 28.852794647216797
Projection step: 13, Loss: 28.455827713012695
Projection step: 14, Loss: 28.097667694091797
Projection step: 15, Loss: 26.59746551513672
Projection step: 16, Loss: 28.71686363220215
Projection step: 17, Loss: 27.288724899291992
Projection step: 18, Loss: 27.078893661499023
Projection step: 19, Loss: 26.421504974365234
Projection step: 20, Loss: 26.992406845092773
Projection step: 21, Loss: 26.685585021972656
Projection step: 22, Loss: 29.054494857788086
Projection step: 23, Loss: 26.065296173095703
Projection step: 24, Loss: 25.095252990722656
Final likelihood: tensor([-26.6130, -27.3687, -22.4007, -26.2492, -26.4196, -22.3002, -29.9049,
        -27.5156, -21.5480, -27.0046, -27.2678, -25.8256, -26.0678, -21.8670,
        -22.7471, -21.7642])
Final projection likelihood: -25.1790
1 mode projection succeeded
New goal: tensor([-0.0267,  0.5299,  0.5331,  0.7619, -0.1020,  0.7356,  0.2941,  1.0354,
         1.4455,  0.1690,  0.0580,  1.1436,  0.0161,  0.0432, -0.9746],
       device='cuda:0')
tensor([[0.0030]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -32.598655700683594
Adjusted likelihood: -32.598655700683594
Likelihood residual: 0.0
Original likelihood: -28.54081153869629
Adjusted likelihood: -28.54081153869629
Likelihood residual: 0.0
{'index': 28.54081153869629, 'thumb_middle': 32.598655700683594}
Current yaw: tensor([ 0.0127,  0.0542, -0.3092], device='cuda:0')
19 index
tensor([-0.0134,  0.4982,  0.5114,  0.7226, -0.1179,  0.7849,  0.3264,  1.0601,
         1.4878,  0.1930,  0.0572,  1.1412,  0.0127,  0.0542, -0.3092, -1.9516],
       device='cuda:0')
Solve time for step 1 10.283370624005329
Current ori: tensor([ 0.0127,  0.0542, -0.3092], device='cuda:0')
Middle force: tensor([0.5219, 0.5262, 0.5336, 0.5278], device='cuda:0')
Thumb force: tensor([0.6049, 0.5001, 0.5304, 0.5136], device='cuda:0')
tensor([ 0.0348,  0.4469,  0.4578,  0.7158, -0.1233,  0.7780,  0.3306,  1.0654,
         1.4900,  0.1941,  0.0644,  1.1333,  0.0145,  0.0576, -0.3197, -2.2435],
       device='cuda:0')
Solve time for step 2 4.157750180980656
Current ori: tensor([ 0.0145,  0.0576, -0.3197], device='cuda:0')
Middle force: tensor([0.5240, 0.5143, 0.5018], device='cuda:0')
Thumb force: tensor([0.5154, 0.5223, 0.5432], device='cuda:0')
tensor([ 0.0396,  0.4475,  0.4582,  0.7175, -0.1090,  0.7934,  0.3239,  1.0487,
         1.4950,  0.1758,  0.0464,  1.1342,  0.0078,  0.0488, -0.3223, -2.2847],
       device='cuda:0')
Solve time for step 3 3.9114891909994185
Current ori: tensor([ 0.0078,  0.0488, -0.3223], device='cuda:0')
Middle force: tensor([0.5711, 0.5016], device='cuda:0')
Thumb force: tensor([0.5745, 0.5891], device='cuda:0')
tensor([ 0.0359,  0.4441,  0.4584,  0.7188, -0.1202,  0.7894,  0.3260,  1.0538,
         1.5000,  0.1729,  0.0544,  1.1281,  0.0101,  0.0544, -0.3258, -2.2009],
       device='cuda:0')
Solve time for step 4 3.7349327810225077
Current ori: tensor([ 0.0101,  0.0544, -0.3258], device='cuda:0')
Middle force: tensor([0.5338], device='cuda:0')
Thumb force: tensor([0.5695], device='cuda:0')
Storing RECOVERY transition: reward=0.0378 (scaled=0.0378), steps=0
Reward stats updated: mean 0.0164 -> 0.0173, std: 0.0413
Collected 24 transitions for RL
Original likelihood: -35.45838928222656
Adjusted likelihood: -35.45838928222656
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0014)
State is out of distribution
Projection step: 0, Loss: 33.96350860595703
Projection step: 1, Loss: 34.41117477416992
Projection step: 2, Loss: 34.92218017578125
Projection step: 3, Loss: 33.2650032043457
Projection step: 4, Loss: 32.03594970703125
Projection step: 5, Loss: 31.845394134521484
Projection step: 6, Loss: 32.05059051513672
Projection step: 7, Loss: 30.975963592529297
Projection step: 8, Loss: 31.20499038696289
Projection step: 9, Loss: 30.447341918945312
Projection step: 10, Loss: 29.7589168548584
Projection step: 11, Loss: 31.036865234375
Projection step: 12, Loss: 29.42237663269043
Projection step: 13, Loss: 29.668195724487305
Projection step: 14, Loss: 31.63420867919922
Projection step: 15, Loss: 29.739551544189453
Projection step: 16, Loss: 29.00641632080078
Projection step: 17, Loss: 28.946060180664062
Projection step: 18, Loss: 29.31333351135254
Projection step: 19, Loss: 29.06132698059082
Projection step: 20, Loss: 28.243288040161133
Projection step: 21, Loss: 29.21478843688965
Projection step: 22, Loss: 28.105554580688477
Projection step: 23, Loss: 27.272571563720703
Projection step: 24, Loss: 27.793514251708984
Final likelihood: tensor([-26.2436, -24.9221, -30.5064, -27.3564, -27.8592, -23.8525, -25.6659,
        -25.6127, -25.7342, -27.6720, -27.2061, -28.8003, -30.0614, -23.5074,
        -26.6775, -26.2576])
Final projection likelihood: -26.7460
1 mode projection succeeded
New goal: tensor([-0.0191,  0.5261,  0.5275,  0.7779, -0.1031,  0.7558,  0.2914,  1.0226,
         1.4548,  0.1647,  0.0514,  1.1488,  0.0044,  0.0439, -0.9683],
       device='cuda:0')
tensor([[0.0030]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -31.77193260192871
Adjusted likelihood: -31.77193260192871
Likelihood residual: 0.0
Original likelihood: -30.80685806274414
Adjusted likelihood: -30.80685806274414
Likelihood residual: 0.0
{'index': 30.80685806274414, 'thumb_middle': 31.77193260192871}
Current yaw: tensor([ 0.0065,  0.0539, -0.3137], device='cuda:0')
20 index
tensor([-0.0083,  0.5019,  0.4995,  0.7439, -0.1245,  0.8106,  0.3176,  1.0313,
         1.5000,  0.1810,  0.0539,  1.1147,  0.0065,  0.0539, -0.3137, -2.1688],
       device='cuda:0')
Solve time for step 1 10.39824305503862
Current ori: tensor([ 0.0065,  0.0539, -0.3137], device='cuda:0')
Middle force: tensor([0.5370, 0.5712, 0.5266, 0.5676], device='cuda:0')
Thumb force: tensor([0.5660, 0.5981, 0.5418, 0.5963], device='cuda:0')
tensor([ 0.0388,  0.4440,  0.4546,  0.7364, -0.1276,  0.8080,  0.3352,  1.0562,
         1.5000,  0.1880,  0.0395,  1.1348,  0.0135,  0.0500, -0.3212, -2.4553],
       device='cuda:0')
Solve time for step 2 4.137791524990462
Current ori: tensor([ 0.0135,  0.0500, -0.3212], device='cuda:0')
Middle force: tensor([0.5683, 0.5252, 0.5637], device='cuda:0')
Thumb force: tensor([0.5906, 0.5404, 0.5930], device='cuda:0')
tensor([ 0.0402,  0.4430,  0.4550,  0.7383, -0.1314,  0.8198,  0.3347,  1.0483,
         1.5000,  0.1874,  0.0391,  1.1310,  0.0122,  0.0489, -0.3242, -2.5506],
       device='cuda:0')
Solve time for step 3 4.0433007450192235
Current ori: tensor([ 0.0122,  0.0489, -0.3242], device='cuda:0')
Middle force: tensor([0.5240, 0.5610], device='cuda:0')
Thumb force: tensor([0.5365, 0.5887], device='cuda:0')
tensor([ 0.0404,  0.4443,  0.4542,  0.7371, -0.1424,  0.8285,  0.3312,  1.0423,
         1.5000,  0.1921,  0.0483,  1.1221,  0.0123,  0.0532, -0.3219, -2.5582],
       device='cuda:0')
Solve time for step 4 3.9015780219924636
Current ori: tensor([ 0.0123,  0.0532, -0.3219], device='cuda:0')
Middle force: tensor([0.5590], device='cuda:0')
Thumb force: tensor([0.5760], device='cuda:0')
Storing RECOVERY transition: reward=0.0317 (scaled=0.0317), steps=0
Reward stats updated: mean 0.0173 -> 0.0179, std: 0.0406
Collected 25 transitions for RL
Original likelihood: -33.78897476196289
Adjusted likelihood: -33.78897476196289
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0193)
State is out of distribution
Projection step: 0, Loss: 35.39952087402344
Projection step: 1, Loss: 32.58100128173828
Projection step: 2, Loss: 32.43640899658203
Projection step: 3, Loss: 35.11476135253906
Projection step: 4, Loss: 32.531700134277344
Projection step: 5, Loss: 31.826656341552734
Projection step: 6, Loss: 30.797439575195312
Projection step: 7, Loss: 31.52480125427246
Projection step: 8, Loss: 30.652891159057617
Projection step: 9, Loss: 30.241230010986328
Projection step: 10, Loss: 29.043529510498047
Projection step: 11, Loss: 29.468229293823242
Projection step: 12, Loss: 32.060997009277344
Projection step: 13, Loss: 28.61571502685547
Projection step: 14, Loss: 28.61945343017578
Projection step: 15, Loss: 28.700397491455078
Projection step: 16, Loss: 29.150789260864258
Projection step: 17, Loss: 29.101030349731445
Projection step: 18, Loss: 27.953598022460938
Projection step: 19, Loss: 27.27479362487793
Projection step: 20, Loss: 27.321826934814453
Projection step: 21, Loss: 29.974218368530273
Projection step: 22, Loss: 26.023874282836914
Projection step: 23, Loss: 27.160903930664062
Projection step: 24, Loss: 25.720909118652344
Final likelihood: tensor([-32.0652, -27.1786, -25.0955, -25.5870, -33.7398, -25.0390, -26.4767,
        -23.5927, -29.2695, -24.8718, -25.8022, -24.4282, -26.5305, -23.2888,
        -27.7587, -27.0327])
Final projection likelihood: -26.7348
1 mode projection succeeded
New goal: tensor([-0.0114,  0.5189,  0.5315,  0.8091, -0.1097,  0.7685,  0.3065,  1.0049,
         1.4623,  0.1716,  0.0434,  1.1564,  0.0140,  0.0396, -1.0040],
       device='cuda:0')
Marked last transition as done (final step)
{}

Trial 2
Loaded trajectory sampler
Current yaw: tensor([ 5.5207e-05,  1.3631e-02, -4.8130e-02], device='cuda:0')
Current yaw: tensor([ 5.5207e-05,  1.3631e-02, -4.8130e-02], device='cuda:0')
1 turn
Sampling time 3.746465915988665
tensor([ 1.4251e-01,  6.1868e-01,  5.5400e-01,  5.9544e-01, -9.9055e-02,
         4.9836e-01,  9.2137e-01,  9.0683e-01,  1.2363e+00,  2.6520e-01,
         2.6317e-01,  1.1654e+00,  5.5207e-05,  1.3631e-02, -4.8130e-02,
         4.4650e-01], device='cuda:0')
Original likelihood: -17.379825592041016
Adjusted likelihood: -17.379825592041016
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.904377508035395
Current ori: tensor([ 5.5207e-05,  1.3631e-02, -4.8130e-02], device='cuda:0')
Middle force: tensor([0.5259, 0.4975, 1.2484, 1.3592, 0.5617, 0.5353, 0.5021, 0.5557, 0.5738,
        0.5557, 0.5211, 0.5596], device='cuda:0')
Thumb force: tensor([0.7227, 0.5880, 1.1492, 1.0499, 0.7093, 0.8869, 0.5465, 1.1442, 1.0249,
        0.6031, 0.5889, 0.6122], device='cuda:0')
Index force: tensor([0.5035, 0.6687, 0.5932, 0.5319, 0.9102, 0.5375, 0.5803, 0.5013, 0.5815,
        0.5588, 0.5628, 0.5444], device='cuda:0')
Storing NORMAL transition: reward=-0.0004 (scaled=-0.0004), steps=1
Reward stats updated: mean 0.0179 -> 0.0172, std: 0.0400
Collected 26 transitions for RL
tensor([ 0.1865,  0.6744,  0.5452,  0.5575, -0.1488,  0.5295,  0.7812,  0.9807,
         1.3855,  0.0237,  0.1053,  1.1105, -0.0138, -0.0160, -0.0479,  0.4937],
       device='cuda:0')
Original likelihood: -31.022075653076172
Adjusted likelihood: -31.022075653076172
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.2885)
State is out of distribution
Projection step: 0, Loss: 30.929588317871094
Projection step: 1, Loss: 30.38495635986328
Projection step: 2, Loss: 29.140182495117188
Projection step: 3, Loss: 26.569313049316406
Projection step: 4, Loss: 27.968347549438477
Projection step: 5, Loss: 27.408828735351562
Projection step: 6, Loss: 25.970096588134766
Projection step: 7, Loss: 27.817583084106445
Projection step: 8, Loss: 26.230724334716797
Projection step: 9, Loss: 26.69788932800293
Projection step: 10, Loss: 25.14191436767578
Projection step: 11, Loss: 24.030393600463867
Projection step: 12, Loss: 23.830509185791016
Projection step: 13, Loss: 24.432151794433594
Projection step: 14, Loss: 23.897987365722656
Projection step: 15, Loss: 23.72614097595215
Projection step: 16, Loss: 23.140525817871094
Projection step: 17, Loss: 23.29510498046875
Projection step: 18, Loss: 23.404346466064453
Projection step: 19, Loss: 22.713693618774414
Projection step: 20, Loss: 22.08677864074707
Projection step: 21, Loss: 22.420028686523438
Projection step: 22, Loss: 21.583709716796875
Projection step: 23, Loss: 22.585683822631836
Projection step: 24, Loss: 21.01895523071289
Final likelihood: tensor([-21.1169, -20.4233, -18.3645, -22.8810, -21.8898, -22.7570, -23.5662,
        -21.1359, -20.9504, -20.9826, -18.8492, -24.2051, -21.7739, -25.0657,
        -24.5494, -19.5198])
Final projection likelihood: -21.7519
1 mode projection succeeded
New goal: tensor([ 0.1482,  0.6602,  0.5066,  0.5615, -0.0872,  0.5664,  0.8323,  0.9873,
         1.4067,  0.0626,  0.0894,  1.0824, -0.0131, -0.0121, -1.9055],
       device='cuda:0')
tensor([[0.0069]], device='cuda:0') tensor([[0.0126]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -23.193984985351562
Adjusted likelihood: -23.193984985351562
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 23.193984985351562}
Current yaw: tensor([-0.0138, -0.0160, -0.0479], device='cuda:0')
2 thumb_middle
tensor([ 0.1865,  0.6744,  0.5452,  0.5575, -0.1488,  0.5295,  0.7812,  0.9807,
         1.3855,  0.0237,  0.1053,  1.1105, -0.0138, -0.0160, -0.0479,  0.4937],
       device='cuda:0')
Solve time for step 1 8.570204515010118
Current ori: tensor([-0.0138, -0.0160, -0.0479], device='cuda:0')
Index force: tensor([0.6052, 0.5992, 0.5742, 0.5031], device='cuda:0')
tensor([ 0.1834,  0.6829,  0.5249,  0.5664, -0.1763,  0.5400,  0.7959,  0.9688,
         1.3812,  0.0392,  0.0316,  1.0617, -0.0150, -0.0143, -0.0479,  0.4891],
       device='cuda:0')
Solve time for step 2 3.5432592119905166
Current ori: tensor([-0.0150, -0.0143, -0.0479], device='cuda:0')
Index force: tensor([0.5003, 0.5756, 0.6899], device='cuda:0')
tensor([ 0.1767,  0.6869,  0.5243,  0.5427, -0.1880,  0.5517,  0.7994,  0.9728,
         1.3895,  0.0458,  0.0189,  1.0541, -0.0178, -0.0113, -0.0479,  0.4764],
       device='cuda:0')
Solve time for step 3 3.5276667189900763
Current ori: tensor([-0.0178, -0.0113, -0.0479], device='cuda:0')
Index force: tensor([0.5689, 0.6785], device='cuda:0')
tensor([ 0.1689,  0.6926,  0.5052,  0.5481, -0.1926,  0.5436,  0.8133,  0.9761,
         1.3897,  0.0521,  0.0162,  1.0567, -0.0188, -0.0070, -0.0479,  0.4675],
       device='cuda:0')
Solve time for step 4 3.2791878770221956
Current ori: tensor([-0.0188, -0.0070, -0.0479], device='cuda:0')
Index force: tensor([0.6506], device='cuda:0')
Storing RECOVERY transition: reward=0.0098 (scaled=0.0098), steps=1
Reward stats updated: mean 0.0172 -> 0.0169, std: 0.0392
Collected 27 transitions for RL
Original likelihood: -23.600194931030273
Adjusted likelihood: -23.600194931030273
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9998)
Current yaw: tensor([-0.0141, -0.0077, -0.0576], device='cuda:0')
3 turn
Sampling time 3.6002784660086036
tensor([ 0.1669,  0.6783,  0.5174,  0.5589, -0.1233,  0.5787,  0.8617,  0.9997,
         1.4614,  0.0721,  0.0669,  1.0748, -0.0141, -0.0077, -0.0576,  0.5416],
       device='cuda:0')
Original likelihood: -22.06146240234375
Adjusted likelihood: -22.06146240234375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.985437676019501
Current ori: tensor([-0.0141, -0.0077, -0.0576], device='cuda:0')
Middle force: tensor([0.6054, 0.6544, 0.6675, 0.5603, 0.5618, 0.5503, 0.7717, 0.5631, 0.5248,
        0.5238, 0.5669, 0.6060], device='cuda:0')
Thumb force: tensor([0.6008, 2.3508, 1.8097, 0.7406, 1.6916, 0.5598, 0.7528, 0.5867, 0.7487,
        0.6210, 0.5069, 0.5782], device='cuda:0')
Index force: tensor([0.6093, 0.7435, 0.5489, 0.5657, 1.2970, 0.6499, 0.6461, 0.5370, 0.5753,
        0.6552, 0.5995, 0.6162], device='cuda:0')
Storing NORMAL transition: reward=-0.0139 (scaled=-0.0139), steps=1
Reward stats updated: mean 0.0169 -> 0.0158, std: 0.0390
Collected 28 transitions for RL
tensor([ 0.0985,  0.5904,  0.5470,  0.5934, -0.1723,  0.5871,  0.7096,  1.0636,
         1.5000,  0.0472,  0.2214,  0.9117,  0.0025,  0.0468, -0.0455,  0.6302],
       device='cuda:0')
Original likelihood: -34.03789520263672
Adjusted likelihood: -34.03789520263672
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0138)
State is out of distribution
Projection step: 0, Loss: 34.900428771972656
Projection step: 1, Loss: 34.23591613769531
Projection step: 2, Loss: 32.9180908203125
Projection step: 3, Loss: 32.114906311035156
Projection step: 4, Loss: 31.50545883178711
Projection step: 5, Loss: 31.451431274414062
Projection step: 6, Loss: 31.456748962402344
Projection step: 7, Loss: 28.865373611450195
Projection step: 8, Loss: 28.339271545410156
Projection step: 9, Loss: 29.032730102539062
Projection step: 10, Loss: 26.67194175720215
Projection step: 11, Loss: 27.087509155273438
Projection step: 12, Loss: 26.349843978881836
Projection step: 13, Loss: 25.82772445678711
Projection step: 14, Loss: 26.64269256591797
Projection step: 15, Loss: 25.304737091064453
Projection step: 16, Loss: 23.978572845458984
Projection step: 17, Loss: 25.05683135986328
Projection step: 18, Loss: 24.13153076171875
Projection step: 19, Loss: 23.77802276611328
Projection step: 20, Loss: 23.010704040527344
Projection step: 21, Loss: 23.14929962158203
Projection step: 22, Loss: 22.776559829711914
Projection step: 23, Loss: 23.602237701416016
Projection step: 24, Loss: 22.441448211669922
Final likelihood: tensor([-21.4358, -20.2593, -20.0199, -21.4748, -24.5538, -27.5860, -22.2514,
        -28.2140, -19.6495, -27.6716, -23.0109, -23.3253, -18.8761, -21.7940,
        -22.5665, -19.7214])
Final projection likelihood: -22.6507
1 mode projection succeeded
New goal: tensor([ 0.0660,  0.5808,  0.5460,  0.5702, -0.1324,  0.5805,  0.7073,  0.8950,
         1.4680,  0.1063,  0.2017,  1.0766, -0.0045,  0.0335, -1.0713],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -29.2681827545166
Adjusted likelihood: -29.2681827545166
Likelihood residual: 0.0
Original likelihood: -18.227500915527344
Adjusted likelihood: -18.227500915527344
Likelihood residual: 0.0
{'index': 18.227500915527344, 'thumb_middle': 29.2681827545166}
Current yaw: tensor([ 0.0025,  0.0468, -0.0455], device='cuda:0')
4 index
tensor([ 0.0985,  0.5904,  0.5470,  0.5934, -0.1723,  0.5871,  0.7096,  1.0636,
         1.5000,  0.0472,  0.2214,  0.9117,  0.0025,  0.0468, -0.0455,  0.6302],
       device='cuda:0')
Solve time for step 1 10.178576915990561
Current ori: tensor([ 0.0025,  0.0468, -0.0455], device='cuda:0')
Middle force: tensor([0.5522, 0.5181, 0.6122, 0.5700], device='cuda:0')
Thumb force: tensor([0.6143, 0.5079, 0.5877, 0.5704], device='cuda:0')
tensor([ 0.1206,  0.5226,  0.4960,  0.5500, -0.1586,  0.5679,  0.7723,  1.0041,
         1.4666,  0.0918,  0.2010,  0.9612,  0.0045,  0.0375, -0.0312,  1.5285],
       device='cuda:0')
Solve time for step 2 4.215207201021258
Current ori: tensor([ 0.0045,  0.0375, -0.0312], device='cuda:0')
Middle force: tensor([0.5162, 0.6069, 0.5651], device='cuda:0')
Thumb force: tensor([0.5065, 0.5842, 0.5668], device='cuda:0')
tensor([ 0.1213,  0.5235,  0.4953,  0.5467, -0.1353,  0.5842,  0.7772,  0.9738,
         1.4671,  0.0740,  0.1748,  0.9709, -0.0027,  0.0228, -0.0359,  1.9303],
       device='cuda:0')
Solve time for step 3 4.036494196974672
Current ori: tensor([-0.0027,  0.0228, -0.0359], device='cuda:0')
Middle force: tensor([0.5506, 0.5766], device='cuda:0')
Thumb force: tensor([0.5122, 0.5675], device='cuda:0')
tensor([ 1.1984e-01,  5.2617e-01,  4.9303e-01,  5.4564e-01, -1.3563e-01,
         5.8949e-01,  7.8027e-01,  9.7137e-01,  1.4581e+00,  8.5635e-02,
         1.7093e-01,  9.8553e-01, -8.4079e-04,  2.0621e-02, -2.6267e-02,
         2.0418e+00], device='cuda:0')
Solve time for step 4 3.7564807570306584
Current ori: tensor([-0.0008,  0.0206, -0.0263], device='cuda:0')
Middle force: tensor([0.5526], device='cuda:0')
Thumb force: tensor([0.5485], device='cuda:0')
Storing RECOVERY transition: reward=-0.0024 (scaled=-0.0024), steps=1
Reward stats updated: mean 0.0158 -> 0.0152, std: 0.0384
Collected 29 transitions for RL
Original likelihood: -18.698314666748047
Adjusted likelihood: -18.698314666748047
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0071,  0.0159, -0.0414], device='cuda:0')
5 turn
Sampling time 3.5924680559546687
tensor([ 0.0713,  0.5905,  0.5379,  0.5664, -0.1305,  0.6029,  0.7767,  0.9596,
         1.4599,  0.0806,  0.1657,  0.9719, -0.0071,  0.0159, -0.0414,  2.0593],
       device='cuda:0')
Original likelihood: -16.571460723876953
Adjusted likelihood: -16.571460723876953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.023721762001514
Current ori: tensor([-0.0071,  0.0159, -0.0414], device='cuda:0')
Middle force: tensor([0.5214, 0.4994, 1.2127, 1.2804, 0.6414, 0.5466, 0.5241, 0.5552, 0.5639,
        0.5900, 0.5208, 0.5225], device='cuda:0')
Thumb force: tensor([0.7880, 0.5375, 1.1137, 0.9857, 0.6184, 0.8237, 0.5357, 1.1582, 0.6818,
        0.5234, 0.5595, 0.5956], device='cuda:0')
Index force: tensor([0.5315, 0.6790, 0.5811, 0.5216, 0.8928, 0.5244, 0.5705, 0.4945, 0.5101,
        0.7016, 0.6609, 0.6999], device='cuda:0')
Storing NORMAL transition: reward=0.0128 (scaled=0.0128), steps=1
Reward stats updated: mean 0.0152 -> 0.0151, std: 0.0378
Collected 30 transitions for RL
tensor([ 0.1064,  0.4852,  0.6304,  0.7372, -0.1103,  0.5671,  0.7243,  0.9716,
         1.5000, -0.1050,  0.1692,  1.0096,  0.0338,  0.0035, -0.0550,  2.1990],
       device='cuda:0')
Original likelihood: -24.995006561279297
Adjusted likelihood: -24.995006561279297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9968)
Solve time for step 2 5.511854230018798
Current ori: tensor([ 0.0338,  0.0035, -0.0550], device='cuda:0')
Middle force: tensor([0.5003, 1.1662, 1.2290, 0.5971, 0.5409, 0.5208, 0.5508, 0.5570, 0.5788,
        0.5148, 0.5163], device='cuda:0')
Thumb force: tensor([0.5346, 1.0743, 0.9612, 0.6292, 0.8040, 0.5327, 1.1228, 0.6719, 0.5210,
        0.5550, 0.5930], device='cuda:0')
Index force: tensor([0.6655, 0.5759, 0.5194, 0.8937, 0.5245, 0.5703, 0.5033, 0.5089, 0.7031,
        0.6751, 0.7102], device='cuda:0')
Storing NORMAL transition: reward=-0.0039 (scaled=-0.0039), steps=1
Reward stats updated: mean 0.0151 -> 0.0145, std: 0.0373
Collected 31 transitions for RL
tensor([ 0.1430,  0.5338,  0.6009,  0.7314, -0.0646,  0.6735,  0.6887,  1.0040,
         1.3553,  0.2418,  0.2331,  0.9339,  0.0391, -0.0210, -0.0519,  1.9587],
       device='cuda:0')
Original likelihood: -28.75116539001465
Adjusted likelihood: -28.75116539001465
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.7522)
Solve time for step 3 5.127056373981759
Current ori: tensor([ 0.0391, -0.0210, -0.0519], device='cuda:0')
Middle force: tensor([1.1282, 1.1939, 0.5883, 0.5367, 0.5179, 0.5460, 0.5513, 0.5713, 0.5123,
        0.5146], device='cuda:0')
Thumb force: tensor([1.0392, 0.9327, 0.6212, 0.7883, 0.5321, 1.0949, 0.6633, 0.5214, 0.5533,
        0.5901], device='cuda:0')
Index force: tensor([0.5692, 0.5167, 0.8743, 0.5238, 0.5663, 0.5028, 0.5078, 0.6885, 0.6692,
        0.6987], device='cuda:0')
Storing NORMAL transition: reward=-0.0576 (scaled=-0.0576), steps=1
Reward stats updated: mean 0.0145 -> 0.0122, std: 0.0388
Collected 32 transitions for RL
tensor([ 0.0629,  0.5563,  0.5520,  0.6132, -0.1964,  0.7604,  0.7168,  0.8831,
         1.4489,  0.1149,  0.2473,  0.8714,  0.0217,  0.0180,  0.0069,  1.7596],
       device='cuda:0')
Original likelihood: -31.02811050415039
Adjusted likelihood: -31.02811050415039
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.2874)
State is out of distribution
Projection step: 0, Loss: 29.18734359741211
Projection step: 1, Loss: 28.592777252197266
Projection step: 2, Loss: 29.05522918701172
Projection step: 3, Loss: 27.69890594482422
Projection step: 4, Loss: 27.925033569335938
Projection step: 5, Loss: 26.041927337646484
Projection step: 6, Loss: 25.86022186279297
Projection step: 7, Loss: 25.256784439086914
Projection step: 8, Loss: 25.94106674194336
Projection step: 9, Loss: 24.679489135742188
Projection step: 10, Loss: 24.381254196166992
Projection step: 11, Loss: 23.954837799072266
Projection step: 12, Loss: 23.81148910522461
Projection step: 13, Loss: 23.548763275146484
Projection step: 14, Loss: 23.08489418029785
Projection step: 15, Loss: 21.975194931030273
Projection step: 16, Loss: 21.98199462890625
Projection step: 17, Loss: 21.73370361328125
Projection step: 18, Loss: 21.48221206665039
Projection step: 19, Loss: 21.030284881591797
Projection step: 20, Loss: 20.786638259887695
Projection step: 21, Loss: 20.520845413208008
Projection step: 22, Loss: 19.258567810058594
Projection step: 23, Loss: 19.13699722290039
Projection step: 24, Loss: 19.530073165893555
Final likelihood: tensor([-20.6284, -20.8005, -21.7974, -15.2941, -20.9853, -19.8033, -21.1123,
        -19.2567, -21.0085, -16.7748, -21.4938, -19.4652, -19.8205, -17.2875,
        -21.7145, -21.4860])
Final projection likelihood: -19.9205
1 mode projection succeeded
New goal: tensor([ 0.0713,  0.5780,  0.5993,  0.5486, -0.1440,  0.6614,  0.7448,  0.7897,
         1.4463,  0.1415,  0.2242,  1.0864,  0.0188,  0.0179, -0.5952],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -20.50237464904785
Adjusted likelihood: -20.50237464904785
Likelihood residual: 0.0
Original likelihood: -26.52940559387207
Adjusted likelihood: -26.52940559387207
Likelihood residual: 0.0
{'index': 26.52940559387207, 'thumb_middle': 20.50237464904785}
Current yaw: tensor([0.0217, 0.0180, 0.0069], device='cuda:0')
6 thumb_middle
tensor([ 0.0629,  0.5563,  0.5520,  0.6132, -0.1964,  0.7604,  0.7168,  0.8831,
         1.4489,  0.1149,  0.2473,  0.8714,  0.0217,  0.0180,  0.0069,  1.7596],
       device='cuda:0')
Solve time for step 1 8.891713302000426
Current ori: tensor([0.0217, 0.0180, 0.0069], device='cuda:0')
Index force: tensor([0.5919, 0.5935, 0.5878, 0.5949], device='cuda:0')
tensor([ 0.0580,  0.5453,  0.5871,  0.5634, -0.2275,  0.6736,  0.7178,  0.7895,
         1.3787,  0.1012,  0.1237,  1.0000,  0.0192,  0.0199,  0.0069,  1.7246],
       device='cuda:0')
Solve time for step 2 3.5171769649605267
Current ori: tensor([0.0192, 0.0199, 0.0069], device='cuda:0')
Index force: tensor([0.5844, 0.5802, 0.5872], device='cuda:0')
tensor([ 0.0521,  0.5726,  0.5664,  0.5211, -0.2265,  0.6712,  0.7272,  0.7759,
         1.3735,  0.0975,  0.1051,  1.0237,  0.0095,  0.0219,  0.0069,  1.6969],
       device='cuda:0')
Solve time for step 3 3.5335592449991964
Current ori: tensor([0.0095, 0.0219, 0.0069], device='cuda:0')
Index force: tensor([0.5702, 0.5784], device='cuda:0')
tensor([ 0.0524,  0.5526,  0.5822,  0.5445, -0.2266,  0.6708,  0.7276,  0.7772,
         1.3756,  0.1020,  0.1015,  1.0273,  0.0160,  0.0225,  0.0069,  1.7087],
       device='cuda:0')
Solve time for step 4 3.3111466530244797
Current ori: tensor([0.0160, 0.0225, 0.0069], device='cuda:0')
Index force: tensor([0.5974], device='cuda:0')
Storing RECOVERY transition: reward=-0.0012 (scaled=-0.0004), steps=3
Reward stats updated: mean 0.0122 -> 0.0118, std: 0.0383
Collected 33 transitions for RL
Original likelihood: -26.657032012939453
Adjusted likelihood: -26.657032012939453
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9659)
Current yaw: tensor([0.0136, 0.0261, 0.0080], device='cuda:0')
7 turn
Sampling time 3.6659085039864294
tensor([ 0.0429,  0.5597,  0.5718,  0.5285, -0.1681,  0.7081,  0.7572,  0.7937,
         1.4422,  0.1280,  0.1540,  1.0556,  0.0136,  0.0261,  0.0080,  1.7188],
       device='cuda:0')
Original likelihood: -25.78765106201172
Adjusted likelihood: -25.78765106201172
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9892)
Solve time for step 1 13.57022697100183
Current ori: tensor([0.0136, 0.0261, 0.0080], device='cuda:0')
Middle force: tensor([0.8123, 0.6506, 0.5725, 0.9210, 0.9190, 0.8903, 0.8648, 0.8186, 0.5697,
        0.5226, 0.5764, 0.5806], device='cuda:0')
Thumb force: tensor([0.8368, 3.2705, 1.3873, 0.5912, 0.6992, 0.9472, 0.8271, 0.5827, 1.2252,
        0.5523, 0.8633, 0.6011], device='cuda:0')
Index force: tensor([0.7327, 0.5117, 0.5566, 0.8115, 0.5590, 0.5588, 0.5651, 0.6159, 0.5246,
        0.7150, 0.6009, 0.6354], device='cuda:0')
Storing NORMAL transition: reward=-0.0467 (scaled=-0.0467), steps=1
Reward stats updated: mean 0.0118 -> 0.0101, std: 0.0390
Collected 34 transitions for RL
tensor([-0.0995,  0.3818,  0.6571,  0.6136, -0.2173,  0.6661,  0.7697,  0.8714,
         1.3464,  0.3237,  0.2935,  1.0011,  0.0542,  0.0574,  0.0500,  1.2064],
       device='cuda:0')
Original likelihood: -30.770477294921875
Adjusted likelihood: -30.770477294921875
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.3371)
Solve time for step 2 5.704714749997947
Current ori: tensor([0.0542, 0.0574, 0.0500], device='cuda:0')
Middle force: tensor([0.6640, 0.5876, 0.9999, 0.9307, 0.9135, 0.8946, 0.8611, 0.5686, 0.5258,
        0.5946, 0.5960], device='cuda:0')
Thumb force: tensor([3.2040, 1.3777, 0.5723, 0.6991, 0.9279, 0.7945, 0.5597, 1.2003, 0.5405,
        0.8304, 0.5840], device='cuda:0')
Index force: tensor([0.5130, 0.5679, 0.7984, 0.5551, 0.5600, 0.5643, 0.6120, 0.5250, 0.7128,
        0.5876, 0.6234], device='cuda:0')
Storing NORMAL transition: reward=0.1319 (scaled=0.1319), steps=1
Reward stats updated: mean 0.0101 -> 0.0136, std: 0.0435
Collected 35 transitions for RL
tensor([-0.0839,  0.4626,  0.5391,  0.6555, -0.2555,  0.6314,  0.8065,  0.7460,
         1.2484,  0.5728,  0.5408,  0.6769,  0.0448,  0.0926, -0.0887,  2.1071],
       device='cuda:0')
Original likelihood: -29.462743759155273
Adjusted likelihood: -29.462743759155273
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.6153)
Solve time for step 3 5.169196030998137
Current ori: tensor([ 0.0448,  0.0926, -0.0887], device='cuda:0')
Middle force: tensor([0.5813, 0.9414, 0.9218, 0.8883, 0.8647, 0.8259, 0.5635, 0.5195, 0.5808,
        0.5739], device='cuda:0')
Thumb force: tensor([1.3626, 0.5897, 0.6975, 0.9253, 0.8003, 0.5639, 1.1852, 0.5435, 0.8314,
        0.5901], device='cuda:0')
Index force: tensor([0.5779, 0.8229, 0.5556, 0.5614, 0.5655, 0.6128, 0.5231, 0.7129, 0.5876,
        0.6302], device='cuda:0')
Storing NORMAL transition: reward=-0.0292 (scaled=-0.0292), steps=1
Reward stats updated: mean 0.0136 -> 0.0124, std: 0.0434
Collected 36 transitions for RL
tensor([-0.0254,  0.3665,  0.6106,  0.4239, -0.2370,  0.5463,  0.9996,  0.8212,
         1.3630,  0.4514,  0.4327,  0.6822,  0.1157,  0.0525, -0.0649,  1.0676],
       device='cuda:0')
Original likelihood: -27.982650756835938
Adjusted likelihood: -27.982650756835938
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.8645)
State is out of distribution
Projection step: 0, Loss: 31.297595977783203
Projection step: 1, Loss: 30.212779998779297
Projection step: 2, Loss: 24.666950225830078
Projection step: 3, Loss: 27.74053382873535
Projection step: 4, Loss: 28.38441276550293
Projection step: 5, Loss: 27.533489227294922
Projection step: 6, Loss: 31.456130981445312
Projection step: 7, Loss: 24.81332778930664
Projection step: 8, Loss: 26.865211486816406
Projection step: 9, Loss: 23.895998001098633
Projection step: 10, Loss: 24.969194412231445
Projection step: 11, Loss: 23.089115142822266
Projection step: 12, Loss: 25.81293296813965
Projection step: 13, Loss: 27.930091857910156
Projection step: 14, Loss: 25.657867431640625
Projection step: 15, Loss: 27.79847526550293
Projection step: 16, Loss: 25.156415939331055
Projection step: 17, Loss: 28.52924346923828
Projection step: 18, Loss: 26.710834503173828
Projection step: 19, Loss: 23.275508880615234
Projection step: 20, Loss: 27.563859939575195
Projection step: 21, Loss: 22.64092254638672
Projection step: 22, Loss: 20.76150894165039
Projection step: 23, Loss: 23.157970428466797
Projection step: 24, Loss: 24.89417266845703
Final likelihood: tensor([-19.2865, -44.5859, -22.5824, -19.5621, -19.0681, -11.2088, -13.1941,
        -28.5076, -19.2322, -21.3294, -40.2297, -10.1625, -19.4786, -24.0998,
        -27.0101, -21.4314])
Final projection likelihood: -22.5606
1 mode projection succeeded
New goal: tensor([-0.0414,  0.3592,  0.6631,  0.5224, -0.2129,  0.5206,  0.9338,  0.8069,
         1.3597,  0.4247,  0.4553,  0.8128,  0.1132,  0.0565, -0.7266],
       device='cuda:0')
tensor([[0.0053]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0133]], device='cuda:0')
Original likelihood: -25.167728424072266
Adjusted likelihood: -25.167728424072266
Likelihood residual: 0.0
Original likelihood: -27.82413101196289
Adjusted likelihood: -27.82413101196289
Likelihood residual: 0.0
{'index': 27.82413101196289, 'thumb_middle': 25.167728424072266}
Current yaw: tensor([ 0.1157,  0.0525, -0.0649], device='cuda:0')
8 thumb_middle
tensor([-0.0254,  0.3665,  0.6106,  0.4239, -0.2370,  0.5463,  0.9996,  0.8212,
         1.3630,  0.4514,  0.4327,  0.6822,  0.1157,  0.0525, -0.0649,  1.0676],
       device='cuda:0')
Solve time for step 1 8.999998048995622
Current ori: tensor([ 0.1157,  0.0525, -0.0649], device='cuda:0')
Index force: tensor([0.5877, 0.6060, 0.6049, 0.5921], device='cuda:0')
tensor([-0.0927,  0.4101,  0.6843,  0.5031, -0.2825,  0.5340,  0.9236,  0.7934,
         1.3140,  0.4224,  0.3613,  0.7432,  0.1772,  0.1128, -0.0649,  1.2532],
       device='cuda:0')
Solve time for step 2 3.5885190709959716
Current ori: tensor([ 0.1772,  0.1128, -0.0649], device='cuda:0')
Index force: tensor([0.6006, 0.5994, 0.5856], device='cuda:0')
tensor([-0.0772,  0.4004,  0.7053,  0.5349, -0.3113,  0.5254,  0.8993,  0.7867,
         1.3481,  0.4227,  0.3959,  0.7804,  0.1896,  0.1211, -0.0164,  1.2396],
       device='cuda:0')
Solve time for step 3 3.418647267972119
Current ori: tensor([ 0.1896,  0.1211, -0.0164], device='cuda:0')
Index force: tensor([0.5962, 0.5813], device='cuda:0')
tensor([-0.0631,  0.3889,  0.7294,  0.5581, -0.3231,  0.5089,  0.9045,  0.8001,
         1.3574,  0.4233,  0.4057,  0.7912,  0.1917,  0.1262,  0.0114,  1.3935],
       device='cuda:0')
Solve time for step 4 3.202733808022458
Current ori: tensor([0.1917, 0.1262, 0.0114], device='cuda:0')
Index force: tensor([0.5000], device='cuda:0')
Storing RECOVERY transition: reward=-0.1425 (scaled=-0.0475), steps=3
Reward stats updated: mean 0.0124 -> 0.0108, std: 0.0439
Collected 37 transitions for RL
Original likelihood: -30.306102752685547
Adjusted likelihood: -30.306102752685547
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.4337)
State is out of distribution
Projection step: 0, Loss: 32.60017395019531
Projection step: 1, Loss: 29.84329605102539
Projection step: 2, Loss: 27.842514038085938
Projection step: 3, Loss: 31.49554443359375
Projection step: 4, Loss: 30.22720718383789
Projection step: 5, Loss: 29.383329391479492
Projection step: 6, Loss: 31.142681121826172
Projection step: 7, Loss: 30.517362594604492
Projection step: 8, Loss: 29.833953857421875
Projection step: 9, Loss: 29.323020935058594
Projection step: 10, Loss: 28.183773040771484
Projection step: 11, Loss: 29.8646240234375
Projection step: 12, Loss: 28.719518661499023
Projection step: 13, Loss: 28.38672637939453
Projection step: 14, Loss: 28.20258331298828
Projection step: 15, Loss: 28.852113723754883
Projection step: 16, Loss: 29.21759605407715
Projection step: 17, Loss: 26.630373001098633
Projection step: 18, Loss: 26.421001434326172
Projection step: 19, Loss: 27.217041015625
Projection step: 20, Loss: 28.796796798706055
Projection step: 21, Loss: 28.058883666992188
Projection step: 22, Loss: 28.19960594177246
Projection step: 23, Loss: 28.237545013427734
Projection step: 24, Loss: 26.71424102783203
Final likelihood: tensor([-22.4059, -25.4037, -25.2572, -22.2000, -24.4051, -31.4574, -25.4894,
        -25.7091, -29.0945, -32.0183, -32.3739, -25.5661, -25.2854, -25.2955,
        -36.0816, -28.3192])
Final projection likelihood: -27.2726
1 mode projection succeeded
New goal: tensor([-0.0472,  0.3812,  0.7315,  0.5312, -0.2379,  0.5211,  0.9246,  0.8558,
         1.3363,  0.4476,  0.4611,  0.8454,  0.1926,  0.0982,  0.1053],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -31.472354888916016
Adjusted likelihood: -31.472354888916016
Likelihood residual: 0.0
Original likelihood: -26.114883422851562
Adjusted likelihood: -26.114883422851562
Likelihood residual: 0.0
{'index': 26.114883422851562, 'thumb_middle': 31.472354888916016}
Current yaw: tensor([0.1961, 0.0950, 0.0550], device='cuda:0')
9 index
tensor([-0.0564,  0.3887,  0.7287,  0.5636, -0.2477,  0.5515,  0.9366,  0.7928,
         1.3956,  0.4486,  0.4723,  0.8447,  0.1961,  0.0950,  0.0550,  1.0694],
       device='cuda:0')
Solve time for step 1 10.356785591051448
Current ori: tensor([0.1961, 0.0950, 0.0550], device='cuda:0')
Middle force: tensor([0.6036, 0.5142, 0.5666, 0.6160], device='cuda:0')
Thumb force: tensor([0.5862, 0.6154, 0.6184, 0.5670], device='cuda:0')
tensor([-0.0346,  0.2429,  0.6980,  0.5384, -0.2547,  0.5483,  0.9366,  0.8521,
         1.3970,  0.4502,  0.4769,  0.8363,  0.1961,  0.0974,  0.0490, -0.6220],
       device='cuda:0')
Solve time for step 2 4.055151305976324
Current ori: tensor([0.1961, 0.0974, 0.0490], device='cuda:0')
Middle force: tensor([0.5122, 0.5618, 0.6099], device='cuda:0')
Thumb force: tensor([0.6075, 0.6145, 0.5642], device='cuda:0')
tensor([-0.0321,  0.2475,  0.6956,  0.5329, -0.2384,  0.5580,  0.9349,  0.8428,
         1.4016,  0.4460,  0.4733,  0.8206,  0.1923,  0.0927,  0.0190, -1.8018],
       device='cuda:0')
Solve time for step 3 3.964186373981647
Current ori: tensor([0.1923, 0.0927, 0.0190], device='cuda:0')
Middle force: tensor([0.5583, 0.6043], device='cuda:0')
Thumb force: tensor([0.6074, 0.5611], device='cuda:0')
tensor([-0.0332,  0.2494,  0.6958,  0.5306, -0.2215,  0.5688,  0.9328,  0.8501,
         1.4069,  0.4502,  0.4619,  0.8206,  0.1915,  0.0868,  0.0019, -1.6870],
       device='cuda:0')
Solve time for step 4 3.5627198980073445
Current ori: tensor([0.1915, 0.0868, 0.0019], device='cuda:0')
Middle force: tensor([0.5218], device='cuda:0')
Thumb force: tensor([0.5038], device='cuda:0')
Storing RECOVERY transition: reward=-0.1068 (scaled=-0.0356), steps=3
Reward stats updated: mean 0.0108 -> 0.0096, std: 0.0440
Collected 38 transitions for RL
Original likelihood: -31.01390838623047
Adjusted likelihood: -31.01390838623047
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.2900)
State is out of distribution
Projection step: 0, Loss: 29.464309692382812
Projection step: 1, Loss: 31.266868591308594
Projection step: 2, Loss: 30.65375518798828
Projection step: 3, Loss: 30.43622589111328
Projection step: 4, Loss: 29.597732543945312
Projection step: 5, Loss: 29.526304244995117
Projection step: 6, Loss: 30.026329040527344
Projection step: 7, Loss: 29.91705322265625
Projection step: 8, Loss: 29.234397888183594
Projection step: 9, Loss: 29.42829132080078
Projection step: 10, Loss: 31.065139770507812
Projection step: 11, Loss: 28.401939392089844
Projection step: 12, Loss: 28.272262573242188
Projection step: 13, Loss: 28.172765731811523
Projection step: 14, Loss: 28.706464767456055
Projection step: 15, Loss: 28.56963348388672
Projection step: 16, Loss: 28.67682647705078
Projection step: 17, Loss: 27.850513458251953
Projection step: 18, Loss: 27.410619735717773
Projection step: 19, Loss: 27.662630081176758
Projection step: 20, Loss: 26.623769760131836
Projection step: 21, Loss: 29.368885040283203
Projection step: 22, Loss: 27.199371337890625
Projection step: 23, Loss: 27.277965545654297
Projection step: 24, Loss: 26.938072204589844
Final likelihood: tensor([-24.9955, -25.5488, -28.4499, -22.0576, -22.2182, -28.1848, -34.4136,
        -26.6882, -23.9381, -24.8391, -24.2928, -22.6602, -22.3255, -30.0585,
        -24.9114, -28.6610])
Final projection likelihood: -25.8902
1 mode projection succeeded
New goal: tensor([-0.0640,  0.3618,  0.7451,  0.5152, -0.2092,  0.5286,  0.9283,  0.9034,
         1.3319,  0.4686,  0.4435,  0.8425,  0.1930,  0.0871,  0.1160],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0021]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -28.046932220458984
Adjusted likelihood: -28.046932220458984
Likelihood residual: 0.0
Original likelihood: -30.626506805419922
Adjusted likelihood: -30.626506805419922
Likelihood residual: 0.0
{'index': 30.626506805419922, 'thumb_middle': 28.046932220458984}
Current yaw: tensor([0.1965, 0.0834, 0.0184], device='cuda:0')
10 thumb_middle
tensor([-0.0788,  0.3681,  0.7458,  0.5438, -0.2162,  0.5679,  0.9469,  0.8498,
         1.4008,  0.4705,  0.4547,  0.8413,  0.1965,  0.0834,  0.0184, -1.4415],
       device='cuda:0')
Solve time for step 1 8.89702156902058
Current ori: tensor([0.1965, 0.0834, 0.0184], device='cuda:0')
Index force: tensor([0.5843, 0.5675, 0.5488, 0.5001], device='cuda:0')
tensor([-0.0598,  0.3497,  0.7965,  0.5665, -0.2961,  0.5127,  0.9055,  0.8784,
         1.3348,  0.4615,  0.3977,  0.8133,  0.2163,  0.1091,  0.0489, -1.0589],
       device='cuda:0')
Solve time for step 2 3.661024675006047
Current ori: tensor([0.2163, 0.1091, 0.0489], device='cuda:0')
Index force: tensor([0.5000, 0.6004, 0.5939], device='cuda:0')
tensor([-0.0664,  0.3803,  0.8222,  0.5805, -0.3358,  0.4727,  0.9084,  0.9012,
         1.3496,  0.4723,  0.4071,  0.8233,  0.2985,  0.1086,  0.0628, -0.4262],
       device='cuda:0')
Solve time for step 3 3.743530326988548
Current ori: tensor([0.2985, 0.1086, 0.0628], device='cuda:0')
Index force: tensor([0.5612, 0.5000], device='cuda:0')
tensor([-0.0714,  0.3796,  0.9456,  0.6967, -0.3243,  0.4766,  0.9072,  0.9080,
         1.3683,  0.4807,  0.4578,  0.8492,  0.2974,  0.1073,  0.0731, -0.4621],
       device='cuda:0')
Solve time for step 4 3.363215066958219
Current ori: tensor([0.2974, 0.1073, 0.0731], device='cuda:0')
Index force: tensor([0.5000], device='cuda:0')
Storing RECOVERY transition: reward=-0.2087 (scaled=-0.0696), steps=3
Reward stats updated: mean 0.0096 -> 0.0075, std: 0.0452
Collected 39 transitions for RL
Original likelihood: -108.94737243652344
Adjusted likelihood: -108.94737243652344
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 108.33306121826172
Projection step: 1, Loss: 113.74842834472656
Projection step: 2, Loss: 102.8585205078125
Projection step: 3, Loss: 107.04173278808594
Projection step: 4, Loss: 107.41932678222656
Projection step: 5, Loss: 108.08785247802734
Projection step: 6, Loss: 111.05827331542969
Projection step: 7, Loss: 108.12751007080078
Projection step: 8, Loss: 101.449462890625
Projection step: 9, Loss: 107.84129333496094
Projection step: 10, Loss: 117.08673858642578
Projection step: 11, Loss: 107.00755310058594
Projection step: 12, Loss: 109.63002014160156
Projection step: 13, Loss: 105.98191833496094
Projection step: 14, Loss: 102.72701263427734
Projection step: 15, Loss: 100.18820190429688
Projection step: 16, Loss: 106.99860382080078
Projection step: 17, Loss: 97.51771545410156
Projection step: 18, Loss: 114.61864471435547
Projection step: 19, Loss: 105.11127471923828
Projection step: 20, Loss: 100.94940948486328
Projection step: 21, Loss: 109.77267456054688
Projection step: 22, Loss: 107.1570053100586
Projection step: 23, Loss: 96.69112396240234
Projection step: 24, Loss: 108.50697326660156
Final likelihood: tensor([-102.8801, -125.6173, -111.4842,  -87.9813, -109.0485, -114.7023,
         -88.0298,  -79.5149,  -98.5677, -111.3913,  -91.5361,  -93.5180,
         -85.8941,  -94.8621, -113.6780,  -86.4533])
Final projection likelihood: -99.6974
1 mode projection failed, trying anyway
New goal: tensor([-0.0734,  0.3617,  0.9792,  0.6291, -0.2872,  0.4835,  0.9613,  0.9444,
         1.3910,  0.4785,  0.5337,  0.9016,  0.2888,  0.1014,  0.2457],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[-0.0006]], device='cuda:0') tensor([[0.0018]], device='cuda:0')
Original likelihood: -112.44898986816406
Adjusted likelihood: -112.44898986816406
Likelihood residual: 0.0
Original likelihood: -96.50570678710938
Adjusted likelihood: -96.50570678710938
Likelihood residual: 0.0
{'index': 96.50570678710938, 'thumb_middle': 112.44898986816406}
Current yaw: tensor([0.2916, 0.0997, 0.0872], device='cuda:0')
11 index
tensor([-0.0618,  0.3719,  0.9629,  0.6082, -0.2913,  0.4829,  0.8930,  0.9058,
         1.4225,  0.4720,  0.5309,  0.8972,  0.2916,  0.0997,  0.0872, -0.1835],
       device='cuda:0')
Solve time for step 1 10.407130223989952
Current ori: tensor([0.2916, 0.0997, 0.0872], device='cuda:0')
Middle force: tensor([0.5033, 0.5751, 0.5601, 0.5585], device='cuda:0')
Thumb force: tensor([0.6011, 0.5821, 0.5662, 0.5911], device='cuda:0')
tensor([-0.1361,  0.0792,  0.9319,  0.5974, -0.2708,  0.5012,  0.9259,  0.9398,
         1.4596,  0.5061,  0.5374,  0.8870,  0.3292,  0.1027,  0.1184,  1.8670],
       device='cuda:0')
Solve time for step 2 4.161987387982663
Current ori: tensor([0.3292, 0.1027, 0.1184], device='cuda:0')
Middle force: tensor([0.5761, 0.5559, 0.5544], device='cuda:0')
Thumb force: tensor([0.5798, 0.5645, 0.5880], device='cuda:0')
tensor([-0.1619,  0.0281,  0.9603,  0.6101, -0.2714,  0.4973,  0.9813,  0.9456,
         1.4694,  0.5050,  0.5264,  0.8834,  0.3282,  0.0975,  0.1468,  2.6208],
       device='cuda:0')
Solve time for step 3 3.7248902919818647
Current ori: tensor([0.3282, 0.0975, 0.1468], device='cuda:0')
Middle force: tensor([0.5491, 0.5511], device='cuda:0')
Thumb force: tensor([0.5728, 0.5871], device='cuda:0')
tensor([-0.1713,  0.0102,  0.9677,  0.6122, -0.2349,  0.5231,  0.9588,  0.9466,
         1.4721,  0.5129,  0.5217,  0.8746,  0.3285,  0.0914,  0.1434,  4.1408],
       device='cuda:0')
Solve time for step 4 3.7776126930257306
Current ori: tensor([0.3285, 0.0914, 0.1434], device='cuda:0')
Middle force: tensor([0.5421], device='cuda:0')
Thumb force: tensor([0.5535], device='cuda:0')
Storing RECOVERY transition: reward=-0.2720 (scaled=-0.0907), steps=3
Reward stats updated: mean 0.0075 -> 0.0051, std: 0.0472
Collected 40 transitions for RL
Original likelihood: -124.21715545654297
Adjusted likelihood: -124.21715545654297
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 137.95196533203125
Projection step: 1, Loss: 129.24261474609375
Projection step: 2, Loss: 121.64047241210938
Projection step: 3, Loss: 117.69528198242188
Projection step: 4, Loss: 121.32014465332031
Projection step: 5, Loss: 124.14762878417969
Projection step: 6, Loss: 125.07940673828125
Projection step: 7, Loss: 133.2232208251953
Projection step: 8, Loss: 128.63926696777344
Projection step: 9, Loss: 125.61422729492188
Projection step: 10, Loss: 129.66061401367188
Projection step: 11, Loss: 126.8126220703125
Projection step: 12, Loss: 120.24655151367188
Projection step: 13, Loss: 126.58063507080078
Projection step: 14, Loss: 123.74761962890625
Projection step: 15, Loss: 119.015380859375
Projection step: 16, Loss: 126.11112976074219
Projection step: 17, Loss: 126.5772705078125
Projection step: 18, Loss: 133.81607055664062
Projection step: 19, Loss: 121.65636444091797
Projection step: 20, Loss: 120.34375762939453
Projection step: 21, Loss: 123.6234359741211
Projection step: 22, Loss: 125.34051513671875
Projection step: 23, Loss: 115.36598205566406
Projection step: 24, Loss: 131.12890625
Final likelihood: tensor([-150.0385, -108.2503, -106.8337, -133.7372, -104.2317, -152.5795,
        -116.7525, -137.4654, -109.0018, -113.5235, -114.0657, -142.3573,
        -105.1408, -109.6747, -159.8417, -109.5276])
Final projection likelihood: -123.3139
1 mode projection failed, trying anyway
New goal: tensor([-0.1384,  0.2062,  1.0580,  0.6604, -0.2406,  0.5193,  1.0209,  0.9768,
         1.4251,  0.5447,  0.5061,  0.8830,  0.3107,  0.0894,  0.2774],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[-0.0040]], device='cuda:0') tensor([[0.0013]], device='cuda:0')
Original likelihood: -118.74690246582031
Adjusted likelihood: -118.74690246582031
Likelihood residual: 0.0
Original likelihood: -108.89016723632812
Adjusted likelihood: -108.89016723632812
Likelihood residual: 0.0
{'index': 108.89016723632812, 'thumb_middle': 118.74690246582031}
Current yaw: tensor([0.3128, 0.0875, 0.1497], device='cuda:0')
12 index
tensor([-0.1275,  0.2072,  1.0441,  0.6452, -0.2410,  0.5255,  0.9545,  0.9471,
         1.4575,  0.5400,  0.5072,  0.8699,  0.3128,  0.0875,  0.1497,  4.9956],
       device='cuda:0')
Solve time for step 1 10.970686323998962
Current ori: tensor([0.3128, 0.0875, 0.1497], device='cuda:0')
Middle force: tensor([0.6053, 0.5662, 0.5622, 0.5788], device='cuda:0')
Thumb force: tensor([0.6027, 0.5793, 0.5600, 0.6017], device='cuda:0')
tensor([-0.1856, -0.0573,  1.0090,  0.6294, -0.2430,  0.5310,  1.0300,  0.9730,
         1.4724,  0.5430,  0.5120,  0.8640,  0.3274,  0.0862,  0.1857, -4.8531],
       device='cuda:0')
Solve time for step 2 4.126959913002793
Current ori: tensor([0.3274, 0.0862, 0.1857], device='cuda:0')
Middle force: tensor([0.5683, 0.5595, 0.5752], device='cuda:0')
Thumb force: tensor([0.5789, 0.5581, 0.5976], device='cuda:0')
tensor([-0.2054, -0.1050,  1.0208,  0.6335, -0.2436,  0.5308,  1.0415,  0.9784,
         1.4747,  0.5588,  0.5005,  0.8677,  0.3273,  0.0853,  0.1951,  0.2663],
       device='cuda:0')
Solve time for step 3 4.001135341008194
Current ori: tensor([0.3273, 0.0853, 0.1951], device='cuda:0')
Middle force: tensor([0.5586, 0.5699], device='cuda:0')
Thumb force: tensor([0.5568, 0.5938], device='cuda:0')
tensor([-0.1827, -0.0374,  1.0121,  0.6304, -0.2384,  0.5358,  1.0273,  0.9789,
         1.4836,  0.5561,  0.4963,  0.8558,  0.3274,  0.0852,  0.1907, -4.2246],
       device='cuda:0')
Solve time for step 4 3.9250182740506716
Current ori: tensor([0.3274, 0.0852, 0.1907], device='cuda:0')
Middle force: tensor([0.5436], device='cuda:0')
Thumb force: tensor([0.5426], device='cuda:0')
Storing RECOVERY transition: reward=-0.3077 (scaled=-0.1026), steps=3
Reward stats updated: mean 0.0051 -> 0.0025, std: 0.0494
Collected 41 transitions for RL
Original likelihood: -143.1771240234375
Adjusted likelihood: -143.1771240234375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 155.12594604492188
Projection step: 1, Loss: 127.11100006103516
Projection step: 2, Loss: 143.1448211669922
Projection step: 3, Loss: 135.29220581054688
Projection step: 4, Loss: 153.8206787109375
Projection step: 5, Loss: 150.17095947265625
Projection step: 6, Loss: 140.51907348632812
Projection step: 7, Loss: 129.2269744873047
Projection step: 8, Loss: 139.5305633544922
Projection step: 9, Loss: 154.4802703857422
Projection step: 10, Loss: 133.91177368164062
Projection step: 11, Loss: 132.8392791748047
Projection step: 12, Loss: 145.09072875976562
Projection step: 13, Loss: 151.242919921875
Projection step: 14, Loss: 149.678955078125
Projection step: 15, Loss: 139.65103149414062
Projection step: 16, Loss: 153.72763061523438
Projection step: 17, Loss: 153.21755981445312
Projection step: 18, Loss: 142.6290740966797
Projection step: 19, Loss: 133.29080200195312
Projection step: 20, Loss: 146.06631469726562
Projection step: 21, Loss: 147.04254150390625
Projection step: 22, Loss: 138.18466186523438
Projection step: 23, Loss: 137.59368896484375
Projection step: 24, Loss: 133.40093994140625
Final likelihood: tensor([-148.2634, -102.1673, -156.1820, -149.4942, -137.7674, -151.3440,
        -106.7087, -143.7106, -168.4314, -119.2237, -168.4224, -128.5443,
        -146.6458, -149.8273, -116.1896, -169.9518])
Final projection likelihood: -141.4296
1 mode projection failed, trying anyway
New goal: tensor([-0.1913,  0.1599,  1.1074,  0.6878, -0.2428,  0.5320,  1.0798,  1.0025,
         1.4418,  0.5866,  0.4872,  0.8874,  0.3234,  0.0868,  0.2905],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[-0.0058]], device='cuda:0') tensor([[0.0017]], device='cuda:0')
Original likelihood: -120.73616027832031
Adjusted likelihood: -120.73616027832031
Likelihood residual: 0.0
Original likelihood: -103.34613037109375
Adjusted likelihood: -103.34613037109375
Likelihood residual: 0.0
{'index': 103.34613037109375, 'thumb_middle': 120.73616027832031}
Current yaw: tensor([0.3249, 0.0850, 0.1839], device='cuda:0')
13 index
tensor([-0.1809,  0.1586,  1.0981,  0.6749, -0.2416,  0.5389,  1.0241,  0.9799,
         1.4742,  0.5830,  0.4881,  0.8675,  0.3249,  0.0850,  0.1839, -5.1885],
       device='cuda:0')
Solve time for step 1 10.388860278006177
Current ori: tensor([0.3249, 0.0850, 0.1839], device='cuda:0')
Middle force: tensor([0.5549, 0.5833, 0.5230, 0.5472], device='cuda:0')
Thumb force: tensor([0.5266, 0.5988, 0.5728, 0.5635], device='cuda:0')
tensor([-0.2111, -0.0850,  1.0238,  0.6402, -0.1845,  0.5608,  1.1026,  1.0045,
         1.4837,  0.5866,  0.4755,  0.8635,  0.3289,  0.0665,  0.1904, -5.3195],
       device='cuda:0')
Solve time for step 2 4.099080785992555
Current ori: tensor([0.3289, 0.0665, 0.1904], device='cuda:0')
Middle force: tensor([0.5724, 0.5200, 0.5429], device='cuda:0')
Thumb force: tensor([0.5959, 0.5701, 0.5584], device='cuda:0')
tensor([-0.2140, -0.0923,  1.0405,  0.6447, -0.1426,  0.5882,  1.1321,  1.0100,
         1.4999,  0.5979,  0.4727,  0.8601,  0.3302,  0.0538,  0.2163,  5.7250],
       device='cuda:0')
Solve time for step 3 3.9539235699921846
Current ori: tensor([0.3302, 0.0538, 0.2163], device='cuda:0')
Middle force: tensor([0.5214, 0.5365], device='cuda:0')
Thumb force: tensor([0.5631, 0.5533], device='cuda:0')
tensor([-0.2095, -0.0350,  1.0655,  0.6616, -0.1407,  0.5902,  1.1331,  1.0140,
         1.4999,  0.6154,  0.4671,  0.8477,  0.3303,  0.0520,  0.2296,  4.0200],
       device='cuda:0')
Solve time for step 4 3.9454090479994193
Current ori: tensor([0.3303, 0.0520, 0.2296], device='cuda:0')
Middle force: tensor([0.5262], device='cuda:0')
Thumb force: tensor([0.5523], device='cuda:0')
Storing RECOVERY transition: reward=-0.3480 (scaled=-0.1160), steps=3
Reward stats updated: mean 0.0025 -> -0.0004, std: 0.0521
Collected 42 transitions for RL
Original likelihood: -119.48017883300781
Adjusted likelihood: -119.48017883300781
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 115.57603454589844
Projection step: 1, Loss: 113.36219787597656
Projection step: 2, Loss: 112.16752624511719
Projection step: 3, Loss: 112.44725036621094
Projection step: 4, Loss: 120.77745819091797
Projection step: 5, Loss: 120.97164916992188
Projection step: 6, Loss: 119.77310180664062
Projection step: 7, Loss: 117.99002075195312
Projection step: 8, Loss: 122.79263305664062
Projection step: 9, Loss: 113.1844482421875
Projection step: 10, Loss: 112.54196166992188
Projection step: 11, Loss: 123.49449157714844
Projection step: 12, Loss: 124.76318359375
Projection step: 13, Loss: 114.43014526367188
Projection step: 14, Loss: 118.11087799072266
Projection step: 15, Loss: 123.62325286865234
Projection step: 16, Loss: 117.34768676757812
Projection step: 17, Loss: 113.55268859863281
Projection step: 18, Loss: 113.95610809326172
Projection step: 19, Loss: 133.6278076171875
Projection step: 20, Loss: 115.84916687011719
Projection step: 21, Loss: 110.23577117919922
Projection step: 22, Loss: 123.39923858642578
Projection step: 23, Loss: 113.59741973876953
Projection step: 24, Loss: 123.4158706665039
Final likelihood: tensor([-153.9182, -118.8121, -106.0335,  -85.1786, -162.2596, -126.5730,
         -95.3702, -104.6779, -109.1285, -120.2942, -101.3874,  -77.6895,
         -94.8020, -114.5685, -129.4478,  -94.0128])
Final projection likelihood: -112.1346
1 mode projection failed, trying anyway
New goal: tensor([-0.2127,  0.0963,  1.1446,  0.7071, -0.1222,  0.5916,  1.1583,  1.0192,
         1.4734,  0.6223,  0.4472,  0.8411,  0.3245,  0.0510,  0.4177],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[-0.0046]], device='cuda:0') tensor([[0.0020]], device='cuda:0')
Original likelihood: -87.48136901855469
Adjusted likelihood: -87.48136901855469
Likelihood residual: 0.0
Original likelihood: -93.17294311523438
Adjusted likelihood: -93.17294311523438
Likelihood residual: 0.0
{'index': 93.17294311523438, 'thumb_middle': 87.48136901855469}
Current yaw: tensor([0.3270, 0.0489, 0.2307], device='cuda:0')
14 thumb_middle
tensor([-0.2074,  0.1019,  1.1297,  0.7015, -0.1196,  0.6084,  1.1026,  0.9996,
         1.4999,  0.6302,  0.4534,  0.8553,  0.3270,  0.0489,  0.2307,  3.9245],
       device='cuda:0')
Solve time for step 1 8.852580933016725
Current ori: tensor([0.3270, 0.0489, 0.2307], device='cuda:0')
Index force: tensor([0.5693, 0.6057, 0.5840, 0.5284], device='cuda:0')
tensor([-0.1911,  0.0705,  1.1758,  0.6956, -0.2930,  0.3529,  1.1731,  1.0171,
         1.4481,  0.6309,  0.3788,  0.8014,  0.3334,  0.0582,  0.2296,  3.8786],
       device='cuda:0')
Solve time for step 2 3.5654887609998696
Current ori: tensor([0.3334, 0.0582, 0.2296], device='cuda:0')
Index force: tensor([0.6018, 0.5819, 0.5217], device='cuda:0')
tensor([-0.1540,  0.0904,  1.1750,  0.7333, -0.2852,  0.3113,  1.2005,  1.0161,
         1.4528,  0.6243,  0.3671,  0.7992,  0.3340,  0.0801,  0.1927,  4.2211],
       device='cuda:0')
Solve time for step 3 3.643000520998612
Current ori: tensor([0.3340, 0.0801, 0.1927], device='cuda:0')
Index force: tensor([0.5751, 0.5104], device='cuda:0')
tensor([-0.1540,  0.0949,  1.2068,  0.7369, -0.2505,  0.1848,  1.2027,  1.0017,
         1.4593,  0.6249,  0.3797,  0.8033,  0.3397,  0.0875,  0.1823,  4.0762],
       device='cuda:0')
Solve time for step 4 3.3327521579922177
Current ori: tensor([0.3397, 0.0875, 0.1823], device='cuda:0')
Index force: tensor([0.5093], device='cuda:0')
Storing RECOVERY transition: reward=-0.3223 (scaled=-0.1074), steps=3
Reward stats updated: mean -0.0004 -> -0.0029, std: 0.0539
Collected 43 transitions for RL
Original likelihood: -180.60223388671875
Adjusted likelihood: -180.60223388671875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 177.99172973632812
Projection step: 1, Loss: 158.87759399414062
Projection step: 2, Loss: 171.43154907226562
Projection step: 3, Loss: 155.84158325195312
Projection step: 4, Loss: 157.02902221679688
Projection step: 5, Loss: 158.0947265625
Projection step: 6, Loss: 168.73818969726562
Projection step: 7, Loss: 162.42044067382812
Projection step: 8, Loss: 162.7012939453125
Projection step: 9, Loss: 172.57769775390625
Projection step: 10, Loss: 166.51568603515625
Projection step: 11, Loss: 163.58851623535156
Projection step: 12, Loss: 166.19778442382812
Projection step: 13, Loss: 170.81979370117188
Projection step: 14, Loss: 161.98190307617188
Projection step: 15, Loss: 167.3411865234375
Projection step: 16, Loss: 173.8681640625
Projection step: 17, Loss: 159.03466796875
Projection step: 18, Loss: 159.54827880859375
Projection step: 19, Loss: 170.95706176757812
Projection step: 20, Loss: 160.24729919433594
Projection step: 21, Loss: 165.9999237060547
Projection step: 22, Loss: 164.9586181640625
Projection step: 23, Loss: 157.32850646972656
Projection step: 24, Loss: 162.55990600585938
Final likelihood: tensor([-115.1971, -130.1582, -139.1930, -136.0470, -143.3062, -203.5709,
        -169.8664, -140.7820, -157.7922, -164.5395, -185.1899, -122.1682,
        -162.6223, -196.7203, -157.3604, -201.9106])
Final projection likelihood: -157.9015
1 mode projection failed, trying anyway
New goal: tensor([-0.1663,  0.0843,  1.2238,  0.7291, -0.2328,  0.2170,  1.1865,  0.9286,
         1.4722,  0.6290,  0.4812,  0.8876,  0.3389,  0.0899,  0.2863],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[-0.0022]], device='cuda:0') tensor([[0.0021]], device='cuda:0')
Original likelihood: -150.550048828125
Adjusted likelihood: -150.550048828125
Likelihood residual: 0.0
Original likelihood: -139.9053955078125
Adjusted likelihood: -139.9053955078125
Likelihood residual: 0.0
{'index': 139.9053955078125, 'thumb_middle': 150.550048828125}
Current yaw: tensor([0.3393, 0.0885, 0.1930], device='cuda:0')
15 index
tensor([-0.1552,  0.0826,  1.2148,  0.7129, -0.2326,  0.2160,  1.1399,  0.9136,
         1.5000,  0.6322,  0.4777,  0.8587,  0.3393,  0.0885,  0.1930,  3.9917],
       device='cuda:0')
Solve time for step 1 10.12360751198139
Current ori: tensor([0.3393, 0.0885, 0.1930], device='cuda:0')
Middle force: tensor([0.5370, 0.5659, 0.5377, 0.5927], device='cuda:0')
Thumb force: tensor([0.5528, 0.5588, 0.5398, 0.5442], device='cuda:0')
tensor([-0.2062, -0.1690,  1.1448,  0.6855, -0.2094,  0.2298,  1.1426,  0.9286,
         1.4999,  0.6293,  0.4761,  0.8692,  0.3397,  0.0844,  0.1967,  2.6419],
       device='cuda:0')
Solve time for step 2 4.521307246002834
Current ori: tensor([0.3397, 0.0844, 0.1967], device='cuda:0')
Middle force: tensor([0.5615, 0.5343, 0.5863], device='cuda:0')
Thumb force: tensor([0.5546, 0.5370, 0.5414], device='cuda:0')
tensor([-0.2066, -0.1882,  1.1366,  0.6776, -0.2166,  0.2198,  1.1854,  0.9311,
         1.4999,  0.6304,  0.4798,  0.8628,  0.3400,  0.0832,  0.2000,  3.1370],
       device='cuda:0')
Solve time for step 3 4.126115586026572
Current ori: tensor([0.3400, 0.0832, 0.2000], device='cuda:0')
Middle force: tensor([0.5295, 0.5774], device='cuda:0')
Thumb force: tensor([0.5329, 0.5383], device='cuda:0')
tensor([-0.1719, -0.0882,  1.1403,  0.6840, -0.2190,  0.2184,  1.2004,  0.9302,
         1.4999,  0.6350,  0.4759,  0.8688,  0.3399,  0.0825,  0.2093,  4.9717],
       device='cuda:0')
Solve time for step 4 3.840752262971364
Current ori: tensor([0.3399, 0.0825, 0.2093], device='cuda:0')
Middle force: tensor([0.5128], device='cuda:0')
Thumb force: tensor([0.5341], device='cuda:0')
Storing RECOVERY transition: reward=-0.3386 (scaled=-0.1129), steps=3
Reward stats updated: mean -0.0029 -> -0.0054, std: 0.0558
Collected 44 transitions for RL
Original likelihood: -150.91839599609375
Adjusted likelihood: -150.91839599609375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 143.26898193359375
Projection step: 1, Loss: 151.11392211914062
Projection step: 2, Loss: 166.23983764648438
Projection step: 3, Loss: 164.44186401367188
Projection step: 4, Loss: 161.67147827148438
Projection step: 5, Loss: 142.13796997070312
Projection step: 6, Loss: 167.95135498046875
Projection step: 7, Loss: 149.71090698242188
Projection step: 8, Loss: 140.9158477783203
Projection step: 9, Loss: 185.6976318359375
Projection step: 10, Loss: 153.01223754882812
Projection step: 11, Loss: 137.51260375976562
Projection step: 12, Loss: 147.72947692871094
Projection step: 13, Loss: 146.16749572753906
Projection step: 14, Loss: 154.1747283935547
Projection step: 15, Loss: 163.93295288085938
Projection step: 16, Loss: 160.91641235351562
Projection step: 17, Loss: 163.10670471191406
Projection step: 18, Loss: 144.46128845214844
Projection step: 19, Loss: 159.2975616455078
Projection step: 20, Loss: 145.058837890625
Projection step: 21, Loss: 149.31973266601562
Projection step: 22, Loss: 152.38006591796875
Projection step: 23, Loss: 154.12286376953125
Projection step: 24, Loss: 146.00784301757812
Final likelihood: tensor([-155.3011, -142.6625, -188.7337, -186.2368, -127.1484, -116.3635,
        -124.1565, -142.1635, -172.6355, -147.4469, -109.5959, -163.1786,
        -184.7941, -132.0690, -173.6870, -125.4266])
Final projection likelihood: -149.4750
1 mode projection failed, trying anyway
New goal: tensor([-0.2154,  0.0505,  1.2027,  0.7286, -0.1853,  0.2464,  1.2314,  0.9472,
         1.4721,  0.6386,  0.4719,  0.8968,  0.3394,  0.0799,  0.3225],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[-0.0021]], device='cuda:0') tensor([[0.0018]], device='cuda:0')
Original likelihood: -157.46498107910156
Adjusted likelihood: -157.46498107910156
Likelihood residual: 0.0
Original likelihood: -114.58067321777344
Adjusted likelihood: -114.58067321777344
Likelihood residual: 0.0
{'index': 114.58067321777344, 'thumb_middle': 157.46498107910156}
Current yaw: tensor([0.3401, 0.0782, 0.2119], device='cuda:0')
16 index
tensor([-0.2055,  0.0501,  1.1911,  0.7134, -0.1848,  0.2438,  1.1765,  0.9291,
         1.5000,  0.6409,  0.4696,  0.8777,  0.3401,  0.0782,  0.2119,  5.5625],
       device='cuda:0')
Solve time for step 1 10.40725577698322
Current ori: tensor([0.3401, 0.0782, 0.2119], device='cuda:0')
Middle force: tensor([0.5512, 0.5522, 0.6028, 0.5751], device='cuda:0')
Thumb force: tensor([0.5798, 0.5503, 0.5892, 0.5191], device='cuda:0')
tensor([-0.2405, -0.1704,  1.1232,  0.6844, -0.1633,  0.2410,  1.3386,  0.9727,
         1.4992,  0.6443,  0.4815,  0.8519,  0.3418,  0.0650,  0.2377,  4.8869],
       device='cuda:0')
Solve time for step 2 4.271376810967922
Current ori: tensor([0.3418, 0.0650, 0.2377], device='cuda:0')
Middle force: tensor([0.5455, 0.5897, 0.5674], device='cuda:0')
Thumb force: tensor([0.5451, 0.5839, 0.5147], device='cuda:0')
tensor([-0.2449, -0.1504,  1.1289,  0.6846, -0.1091,  0.2772,  1.3287,  0.9893,
         1.5000,  0.6270,  0.4653,  0.8662,  0.3435,  0.0510,  0.2789,  4.7044],
       device='cuda:0')
Solve time for step 3 4.004561820998788
Current ori: tensor([0.3435, 0.0510, 0.2789], device='cuda:0')
Middle force: tensor([0.5856, 0.5610], device='cuda:0')
Thumb force: tensor([0.5793, 0.5122], device='cuda:0')
tensor([-0.2330, -0.1209,  1.1379,  0.6921, -0.0994,  0.2848,  1.3256,  0.9935,
         1.5000,  0.6310,  0.4825,  0.8410,  0.3439,  0.0482,  0.2937,  4.8766],
       device='cuda:0')
Solve time for step 4 3.909186463977676
Current ori: tensor([0.3439, 0.0482, 0.2937], device='cuda:0')
Middle force: tensor([0.5508], device='cuda:0')
Thumb force: tensor([0.5089], device='cuda:0')
Storing RECOVERY transition: reward=-0.4289 (scaled=-0.1430), steps=3
Reward stats updated: mean -0.0054 -> -0.0084, std: 0.0588
Collected 45 transitions for RL
Original likelihood: -124.96208190917969
Adjusted likelihood: -124.96208190917969
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 146.6915283203125
Projection step: 1, Loss: 146.2262420654297
Projection step: 2, Loss: 139.21714782714844
Projection step: 3, Loss: 138.5657958984375
Projection step: 4, Loss: 136.0252685546875
Projection step: 5, Loss: 140.9192657470703
Projection step: 6, Loss: 125.2060546875
Projection step: 7, Loss: 128.92739868164062
Projection step: 8, Loss: 136.32635498046875
Projection step: 9, Loss: 127.82331085205078
Projection step: 10, Loss: 136.471435546875
Projection step: 11, Loss: 136.55264282226562
Projection step: 12, Loss: 129.85031127929688
Projection step: 13, Loss: 121.84471130371094
Projection step: 14, Loss: 127.31568908691406
Projection step: 15, Loss: 136.92074584960938
Projection step: 16, Loss: 126.60137176513672
Projection step: 17, Loss: 125.6112060546875
Projection step: 18, Loss: 140.92771911621094
Projection step: 19, Loss: 125.33255767822266
Projection step: 20, Loss: 130.10870361328125
Projection step: 21, Loss: 131.48983764648438
Projection step: 22, Loss: 127.94573211669922
Projection step: 23, Loss: 144.40139770507812
Projection step: 24, Loss: 120.07421112060547
Final likelihood: tensor([-192.2007, -120.6942, -117.4494,  -97.6289,  -94.3687, -148.7701,
        -118.8943, -183.7122, -116.9289, -138.3131, -101.1799, -117.7054,
         -91.5289, -108.1632, -123.2873, -128.1884])
Final projection likelihood: -124.9383
1 mode projection failed, trying anyway
New goal: tensor([-0.2371,  0.0046,  1.2315,  0.7563, -0.0899,  0.2997,  1.3729,  0.9987,
         1.4761,  0.6246,  0.4808,  0.8342,  0.3427,  0.0486,  0.4585],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[-0.0008]], device='cuda:0') tensor([[0.0022]], device='cuda:0')
Original likelihood: -132.80003356933594
Adjusted likelihood: -132.80003356933594
Likelihood residual: 0.0
Original likelihood: -113.63592529296875
Adjusted likelihood: -113.63592529296875
Likelihood residual: 0.0
{'index': 113.63592529296875, 'thumb_middle': 132.80003356933594}
Current yaw: tensor([0.3441, 0.0463, 0.3117], device='cuda:0')
17 index
tensor([-0.2315,  0.0088,  1.2228,  0.7464, -0.0886,  0.2983,  1.3222,  0.9853,
         1.5000,  0.6276,  0.4827,  0.8394,  0.3441,  0.0463,  0.3117,  4.9300],
       device='cuda:0')
Solve time for step 1 10.2697022529901
Current ori: tensor([0.3441, 0.0463, 0.3117], device='cuda:0')
Middle force: tensor([0.5534, 0.5113, 0.5239, 0.5696], device='cuda:0')
Thumb force: tensor([0.5215, 0.7317, 0.5058, 0.5099], device='cuda:0')
tensor([-0.2508, -0.1960,  1.1865,  0.7269, -0.0166,  0.3402,  1.3186,  0.9843,
         1.4954,  0.6596,  0.4874,  0.8326,  0.3474,  0.0304,  0.3344,  5.0076],
       device='cuda:0')
Solve time for step 2 3.9171907589770854
Current ori: tensor([0.3474, 0.0304, 0.3344], device='cuda:0')
Middle force: tensor([0.5103, 0.5193, 0.5593], device='cuda:0')
Thumb force: tensor([0.7185, 0.5049, 0.5083], device='cuda:0')
tensor([-0.2505, -0.1960,  1.2094,  0.7337,  0.0138,  0.3634,  1.3135,  0.9906,
         1.5000,  0.6649,  0.4770,  0.8404,  0.3497,  0.0211,  0.3757,  4.6791],
       device='cuda:0')
Solve time for step 3 3.950302270008251
Current ori: tensor([0.3497, 0.0211, 0.3757], device='cuda:0')
Middle force: tensor([0.5237, 0.5380], device='cuda:0')
Thumb force: tensor([0.5054, 0.5601], device='cuda:0')
tensor([-0.2885, -0.1960,  1.2471,  0.7509,  0.0202,  0.3692,  1.3132,  0.9931,
         1.4999,  0.6847,  0.4816,  0.8362,  0.3506,  0.0182,  0.3936,  4.5312],
       device='cuda:0')
Solve time for step 4 4.11339091003174
Current ori: tensor([0.3506, 0.0182, 0.3936], device='cuda:0')
Middle force: tensor([0.5127], device='cuda:0')
Thumb force: tensor([0.5055], device='cuda:0')
Storing RECOVERY transition: reward=-0.5352 (scaled=-0.1784), steps=3
Reward stats updated: mean -0.0084 -> -0.0121, std: 0.0632
Collected 46 transitions for RL
Original likelihood: -129.01742553710938
Adjusted likelihood: -129.01742553710938
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 127.66546630859375
Projection step: 1, Loss: 123.77029418945312
Projection step: 2, Loss: 129.86729431152344
Projection step: 3, Loss: 118.26490020751953
Projection step: 4, Loss: 120.60698699951172
Projection step: 5, Loss: 147.41268920898438
Projection step: 6, Loss: 124.87923431396484
Projection step: 7, Loss: 116.60507202148438
Projection step: 8, Loss: 110.86839294433594
Projection step: 9, Loss: 128.67311096191406
Projection step: 10, Loss: 113.49032592773438
Projection step: 11, Loss: 119.91658020019531
Projection step: 12, Loss: 124.97151184082031
Projection step: 13, Loss: 119.96356964111328
Projection step: 14, Loss: 119.35012817382812
Projection step: 15, Loss: 117.31240844726562
Projection step: 16, Loss: 123.15607452392578
Projection step: 17, Loss: 120.59996032714844
Projection step: 18, Loss: 128.16542053222656
Projection step: 19, Loss: 133.60897827148438
Projection step: 20, Loss: 112.61772155761719
Projection step: 21, Loss: 119.68821716308594
Projection step: 22, Loss: 117.97473907470703
Projection step: 23, Loss: 118.90280151367188
Projection step: 24, Loss: 124.91203308105469
Final likelihood: tensor([-127.2209, -116.3056, -128.8736,  -88.2845, -150.0913, -107.6530,
         -90.0837, -112.5868, -107.9406, -120.1266, -181.7323, -133.2678,
        -234.3776, -173.2222, -104.1914,  -85.1760])
Final projection likelihood: -128.8209
1 mode projection failed, trying anyway
New goal: tensor([-0.2670, -0.0462,  1.2795,  0.7784,  0.0147,  0.3630,  1.3599,  0.9765,
         1.4751,  0.6682,  0.4720,  0.8163,  0.3462,  0.0205,  0.6228],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0005]], device='cuda:0') tensor([[0.0021]], device='cuda:0')
Original likelihood: -124.98709106445312
Adjusted likelihood: -124.98709106445312
Likelihood residual: 0.0
Original likelihood: -119.11770629882812
Adjusted likelihood: -119.11770629882812
Likelihood residual: 0.0
{'index': 119.11770629882812, 'thumb_middle': 124.98709106445312}
Current yaw: tensor([0.3478, 0.0180, 0.4255], device='cuda:0')
18 index
tensor([-0.2651, -0.0415,  1.2772,  0.7742,  0.0155,  0.3629,  1.3239,  0.9695,
         1.5000,  0.6754,  0.4774,  0.8384,  0.3478,  0.0180,  0.4255,  4.4741],
       device='cuda:0')
Solve time for step 1 10.283558163966518
Current ori: tensor([0.3478, 0.0180, 0.4255], device='cuda:0')
Middle force: tensor([0.5482, 0.5872, 0.5579, 0.5587], device='cuda:0')
Thumb force: tensor([0.5588, 0.5788, 0.5029, 0.5845], device='cuda:0')
tensor([-2.5861e-01, -1.9577e-01,  1.2909e+00,  7.6262e-01,  8.7182e-02,
         3.5693e-01,  1.2394e+00,  7.4558e-01,  1.5000e+00,  6.8144e-01,
         4.6726e-01,  8.4246e-01,  3.5909e-01,  2.7253e-03,  4.4567e-01,
         4.5794e+00], device='cuda:0')
Solve time for step 2 4.3727140519768
Current ori: tensor([0.3591, 0.0027, 0.4457], device='cuda:0')
Middle force: tensor([0.5684, 0.5451, 0.5553], device='cuda:0')
Thumb force: tensor([0.6003, 0.5021, 0.5815], device='cuda:0')
tensor([-0.1946, -0.1959,  1.3125,  0.7789,  0.1468,  0.3840,  1.2769,  0.9409,
         1.5000,  0.6954,  0.4753,  0.8268,  0.3592,  0.0064,  0.4457,  4.5113],
       device='cuda:0')
Solve time for step 3 4.142578892991878
Current ori: tensor([0.3592, 0.0064, 0.4457], device='cuda:0')
Middle force: tensor([0.5003, 0.5532], device='cuda:0')
Thumb force: tensor([0.6830, 0.5030], device='cuda:0')
tensor([-0.2136, -0.1854,  1.3040,  0.7828,  0.1279,  0.3737,  1.2953,  0.9588,
         1.5000,  0.6962,  0.4991,  0.8190,  0.3585,  0.0081,  0.4520,  4.4966],
       device='cuda:0')
Solve time for step 4 3.9320148089900613
Current ori: tensor([0.3585, 0.0081, 0.4520], device='cuda:0')
Middle force: tensor([0.5364], device='cuda:0')
Thumb force: tensor([0.5157], device='cuda:0')
Storing RECOVERY transition: reward=-0.5635 (scaled=-0.1878), steps=3
Reward stats updated: mean -0.0121 -> -0.0158, std: 0.0675
Collected 47 transitions for RL
Original likelihood: -129.99380493164062
Adjusted likelihood: -129.99380493164062
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 120.423583984375
Projection step: 1, Loss: 122.01513671875
Projection step: 2, Loss: 124.03439331054688
Projection step: 3, Loss: 131.60464477539062
Projection step: 4, Loss: 121.32386016845703
Projection step: 5, Loss: 133.60760498046875
Projection step: 6, Loss: 123.74197387695312
Projection step: 7, Loss: 124.5251235961914
Projection step: 8, Loss: 133.8655242919922
Projection step: 9, Loss: 126.3876953125
Projection step: 10, Loss: 123.5294189453125
Projection step: 11, Loss: 145.03762817382812
Projection step: 12, Loss: 128.40753173828125
Projection step: 13, Loss: 120.10690307617188
Projection step: 14, Loss: 129.72213745117188
Projection step: 15, Loss: 118.75559997558594
Projection step: 16, Loss: 118.9034423828125
Projection step: 17, Loss: 128.67874145507812
Projection step: 18, Loss: 119.69288635253906
Projection step: 19, Loss: 120.71858215332031
Projection step: 20, Loss: 127.74823760986328
Projection step: 21, Loss: 125.93521118164062
Projection step: 22, Loss: 127.9336166381836
Projection step: 23, Loss: 127.38703918457031
Projection step: 24, Loss: 131.158935546875
Final likelihood: tensor([-116.7238, -111.8022, -116.8151, -133.7260, -113.6855, -136.0075,
        -122.5848, -118.9197, -159.6877, -126.1144, -116.9665, -122.1147,
        -104.6037, -121.1415,  -93.4641, -173.1390])
Final projection likelihood: -124.2185
1 mode projection failed, trying anyway
New goal: tensor([-0.2271, -0.1510,  1.3002,  0.7999,  0.1053,  0.3831,  1.3562,  0.9663,
         1.4737,  0.6893,  0.4834,  0.8184,  0.3555,  0.0114,  0.6203],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[-0.0001]], device='cuda:0') tensor([[0.0018]], device='cuda:0')
Original likelihood: -118.71419525146484
Adjusted likelihood: -118.71419525146484
Likelihood residual: 0.0
Original likelihood: -151.23837280273438
Adjusted likelihood: -151.23837280273438
Likelihood residual: 0.0
{'index': 151.23837280273438, 'thumb_middle': 118.71419525146484}
Current yaw: tensor([0.3567, 0.0088, 0.4526], device='cuda:0')
19 thumb_middle
tensor([-0.2247, -0.1541,  1.2992,  0.7941,  0.1059,  0.3820,  1.3290,  0.9638,
         1.4999,  0.7005,  0.4887,  0.8384,  0.3567,  0.0088,  0.4526,  4.4896],
       device='cuda:0')
Solve time for step 1 8.791024708014447
Current ori: tensor([0.3567, 0.0088, 0.4526], device='cuda:0')
Index force: tensor([0.5736, 0.5799, 0.5473, 0.5750], device='cuda:0')
tensor([-0.1120, -0.1028,  1.3144,  0.8004,  0.0262,  0.2063,  1.3357,  0.9619,
         1.4406,  0.6919,  0.4032,  0.7716,  0.3504,  0.0411,  0.4415,  4.0869],
       device='cuda:0')
Solve time for step 2 3.5889675130019896
Current ori: tensor([0.3504, 0.0411, 0.4415], device='cuda:0')
Index force: tensor([0.5704, 0.5386, 0.5660], device='cuda:0')
tensor([-0.0773, -0.1088,  1.3269,  0.8479,  0.0398,  0.1881,  1.3236,  0.9284,
         1.4561,  0.7110,  0.3861,  0.7600,  0.3508,  0.0450,  0.4449,  3.9696],
       device='cuda:0')
Solve time for step 3 3.53019298001891
Current ori: tensor([0.3508, 0.0450, 0.4449], device='cuda:0')
Index force: tensor([0.5530, 0.5772], device='cuda:0')
tensor([-8.0234e-02, -1.1675e-01,  1.3458e+00,  8.1862e-01, -3.0185e-03,
         8.4310e-02,  1.3060e+00,  9.2421e-01,  1.4605e+00,  7.0765e-01,
         3.8864e-01,  7.6267e-01,  3.5677e-01,  5.9312e-02,  4.2670e-01,
         3.6587e+00], device='cuda:0')
Solve time for step 4 3.1921607749536633
Current ori: tensor([0.3568, 0.0593, 0.4267], device='cuda:0')
Index force: tensor([0.5124], device='cuda:0')
Storing RECOVERY transition: reward=-0.5619 (scaled=-0.1873), steps=3
Reward stats updated: mean -0.0158 -> -0.0194, std: 0.0711
Collected 48 transitions for RL
Original likelihood: -157.87109375
Adjusted likelihood: -157.87109375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 139.97601318359375
Projection step: 1, Loss: 145.7586669921875
Projection step: 2, Loss: 139.766357421875
Projection step: 3, Loss: 157.10122680664062
Projection step: 4, Loss: 138.72171020507812
Projection step: 5, Loss: 158.90823364257812
Projection step: 6, Loss: 132.46255493164062
Projection step: 7, Loss: 157.9981689453125
Projection step: 8, Loss: 147.1208953857422
Projection step: 9, Loss: 144.07763671875
Projection step: 10, Loss: 152.94281005859375
Projection step: 11, Loss: 143.97012329101562
Projection step: 12, Loss: 142.80520629882812
Projection step: 13, Loss: 138.23681640625
Projection step: 14, Loss: 150.90045166015625
Projection step: 15, Loss: 142.9453125
Projection step: 16, Loss: 144.6157684326172
Projection step: 17, Loss: 147.344970703125
Projection step: 18, Loss: 138.26016235351562
Projection step: 19, Loss: 145.88400268554688
Projection step: 20, Loss: 163.67239379882812
Projection step: 21, Loss: 148.76495361328125
Projection step: 22, Loss: 158.58544921875
Projection step: 23, Loss: 154.69200134277344
Projection step: 24, Loss: 151.62863159179688
Final likelihood: tensor([-122.1036, -120.6994, -132.5181, -122.5125, -163.4177, -120.1832,
        -120.4459, -174.7732, -152.4479, -187.8192, -111.0450, -152.7692,
        -204.7088, -154.2351, -157.4921, -119.7675])
Final projection likelihood: -144.8086
1 mode projection failed, trying anyway
New goal: tensor([-0.0962, -0.1050,  1.3329,  0.8510, -0.0216,  0.1650,  1.4177,  0.9856,
         1.4748,  0.6940,  0.5072,  0.8432,  0.3521,  0.0534,  0.5274],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0002]], device='cuda:0') tensor([[0.0018]], device='cuda:0')
Original likelihood: -152.08285522460938
Adjusted likelihood: -152.08285522460938
Likelihood residual: 0.0
Original likelihood: -175.739013671875
Adjusted likelihood: -175.739013671875
Likelihood residual: 0.0
{'index': 175.739013671875, 'thumb_middle': 152.08285522460938}
Current yaw: tensor([0.3527, 0.0515, 0.4509], device='cuda:0')
20 thumb_middle
tensor([-0.0863, -0.1061,  1.3271,  0.8379, -0.0215,  0.1604,  1.3717,  0.9753,
         1.5000,  0.6986,  0.5036,  0.8327,  0.3527,  0.0515,  0.4509,  3.6937],
       device='cuda:0')
Solve time for step 1 8.670343820995186
Current ori: tensor([0.3527, 0.0515, 0.4509], device='cuda:0')
Index force: tensor([0.5703, 0.5872, 0.5732, 0.5596], device='cuda:0')
tensor([-0.0866, -0.0948,  1.3336,  0.8574, -0.0890, -0.0585,  1.3476,  0.9293,
         1.4477,  0.6896,  0.4094,  0.7759,  0.3623,  0.0751,  0.3904,  3.4060],
       device='cuda:0')
Solve time for step 2 3.450465057976544
Current ori: tensor([0.3623, 0.0751, 0.3904], device='cuda:0')
Index force: tensor([0.5779, 0.5659, 0.5536], device='cuda:0')
tensor([-0.0860, -0.0585,  1.3734,  0.8488, -0.0944, -0.1035,  1.3152,  0.9363,
         1.4543,  0.7173,  0.4139,  0.7876,  0.3676,  0.0998,  0.3308,  3.9009],
       device='cuda:0')
Solve time for step 3 3.441617154981941
Current ori: tensor([0.3676, 0.0998, 0.3308], device='cuda:0')
Index force: tensor([0.5553, 0.5459], device='cuda:0')
tensor([-0.0855, -0.0468,  1.3858,  0.8616, -0.0503, -0.0702,  1.2925,  0.9535,
         1.4615,  0.7161,  0.4306,  0.7970,  0.3685,  0.1013,  0.2933,  3.8755],
       device='cuda:0')
Solve time for step 4 3.2594748139963485
Current ori: tensor([0.3685, 0.1013, 0.2933], device='cuda:0')
Index force: tensor([0.5463], device='cuda:0')
Storing RECOVERY transition: reward=-0.4305 (scaled=-0.1435), steps=3
Reward stats updated: mean -0.0194 -> -0.0220, std: 0.0725
Collected 49 transitions for RL
Original likelihood: -212.44418334960938
Adjusted likelihood: -212.44418334960938
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 200.49862670898438
Projection step: 1, Loss: 177.42767333984375
Projection step: 2, Loss: 218.9046630859375
Projection step: 3, Loss: 190.10931396484375
Projection step: 4, Loss: 190.9134063720703
Projection step: 5, Loss: 187.95281982421875
Projection step: 6, Loss: 192.2203826904297
Projection step: 7, Loss: 204.10372924804688
Projection step: 8, Loss: 195.82333374023438
Projection step: 9, Loss: 199.9104766845703
Projection step: 10, Loss: 206.88592529296875
Projection step: 11, Loss: 203.96878051757812
Projection step: 12, Loss: 195.5772705078125
Projection step: 13, Loss: 203.5279998779297
Projection step: 14, Loss: 209.58802795410156
Projection step: 15, Loss: 205.65029907226562
Projection step: 16, Loss: 200.2351837158203
Projection step: 17, Loss: 189.0305633544922
Projection step: 18, Loss: 178.91592407226562
Projection step: 19, Loss: 196.85702514648438
Projection step: 20, Loss: 193.02542114257812
Projection step: 21, Loss: 197.79420471191406
Projection step: 22, Loss: 202.77476501464844
Projection step: 23, Loss: 194.0703125
Projection step: 24, Loss: 208.87448120117188
Final likelihood: tensor([-167.7956, -155.0404, -194.0038, -158.5823, -184.8436, -235.3807,
        -159.9209, -254.6919, -156.0294, -157.7912, -239.3850, -142.0086,
        -158.6333, -173.2932, -192.3641, -166.4087])
Final projection likelihood: -181.0108
1 mode projection failed, trying anyway
New goal: tensor([-0.0834, -0.0478,  1.3971,  0.8782, -0.0240, -0.0525,  1.3285,  0.9699,
         1.4778,  0.6979,  0.5312,  0.8849,  0.3688,  0.1001,  0.3523],
       device='cuda:0')
Marked last transition as done (final step)
{}

Trial 3
Loaded trajectory sampler
Current yaw: tensor([-0.0005,  0.0146, -0.0441], device='cuda:0')
Current yaw: tensor([-0.0005,  0.0146, -0.0441], device='cuda:0')
1 turn
Sampling time 3.5755602570134215
tensor([ 1.6883e-01,  6.1485e-01,  5.7889e-01,  6.0914e-01, -1.4500e-01,
         5.9462e-01,  8.7389e-01,  8.9368e-01,  1.2169e+00,  3.2973e-01,
         2.3960e-01,  1.2012e+00, -4.7459e-04,  1.4550e-02, -4.4092e-02,
         2.0146e-01], device='cuda:0')
Original likelihood: -24.153059005737305
Adjusted likelihood: -24.153059005737305
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9993)
Solve time for step 1 13.581675648980308
Current ori: tensor([-0.0005,  0.0146, -0.0441], device='cuda:0')
Middle force: tensor([0.5042, 0.5085, 0.5014, 1.4776, 0.6340, 0.4954, 0.5707, 0.5156, 0.5380,
        1.8077, 0.5821, 0.5687], device='cuda:0')
Thumb force: tensor([1.2873, 0.9284, 1.3797, 0.6998, 0.5312, 1.0568, 1.1823, 0.5524, 0.9016,
        1.8171, 0.7223, 0.7223], device='cuda:0')
Index force: tensor([0.5259, 0.8080, 0.7302, 0.7415, 0.5550, 0.6811, 0.6649, 0.5936, 0.5885,
        0.5536, 0.5900, 0.5241], device='cuda:0')
Storing NORMAL transition: reward=0.0886 (scaled=0.0886), steps=1
Reward stats updated: mean -0.0220 -> -0.0197, std: 0.0735
Collected 50 transitions for RL
tensor([ 0.1740,  0.6443,  0.5919,  0.4878, -0.1593,  0.5549,  0.8836,  0.9754,
         1.2714,  0.2667,  0.2391,  1.1717,  0.0107,  0.0165, -0.1330, -0.4925],
       device='cuda:0')
Original likelihood: -23.619712829589844
Adjusted likelihood: -23.619712829589844
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9998)
Solve time for step 2 5.592392053978983
Current ori: tensor([ 0.0107,  0.0165, -0.1330], device='cuda:0')
Middle force: tensor([0.5656, 1.1436, 0.5572, 1.0681, 0.6231, 0.5289, 0.5120, 0.6776, 0.5410,
        0.5154, 0.5525], device='cuda:0')
Thumb force: tensor([0.7948, 0.7309, 0.9974, 0.9384, 0.6367, 0.5205, 0.9063, 0.5834, 1.3845,
        0.6635, 0.8656], device='cuda:0')
Index force: tensor([0.5900, 0.5434, 0.5599, 0.7450, 0.5194, 0.9665, 0.8841, 0.5479, 0.5811,
        0.5968, 0.5854], device='cuda:0')
Storing NORMAL transition: reward=0.0678 (scaled=0.0678), steps=1
Reward stats updated: mean -0.0197 -> -0.0180, std: 0.0737
Collected 51 transitions for RL
tensor([ 0.0636,  0.6278,  0.5349,  0.4313, -0.1383,  0.5620,  0.8869,  0.9845,
         1.3058,  0.1953,  0.2841,  0.9880,  0.0022,  0.0030, -0.2004, -0.2877],
       device='cuda:0')
Original likelihood: -16.88436508178711
Adjusted likelihood: -16.88436508178711
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.018605248013046
Current ori: tensor([ 0.0022,  0.0030, -0.2004], device='cuda:0')
Middle force: tensor([1.1085, 0.5582, 1.1086, 0.6237, 0.5321, 0.5445, 0.6790, 0.5392, 0.5205,
        0.5496], device='cuda:0')
Thumb force: tensor([0.7000, 0.9460, 0.8065, 0.6130, 0.5149, 0.7282, 0.5735, 1.3446, 0.6088,
        0.8455], device='cuda:0')
Index force: tensor([0.5376, 0.5553, 0.7426, 0.5188, 0.9270, 0.8279, 0.5429, 0.5738, 0.6005,
        0.5795], device='cuda:0')
Storing NORMAL transition: reward=0.0272 (scaled=0.0272), steps=1
Reward stats updated: mean -0.0180 -> -0.0172, std: 0.0733
Collected 52 transitions for RL
tensor([ 0.0879,  0.6264,  0.5257,  0.5124, -0.0919,  0.5879,  0.9005,  1.0252,
         1.2702,  0.2427,  0.2822,  0.8905,  0.0073, -0.0409, -0.2301,  0.1139],
       device='cuda:0')
Original likelihood: -24.10750961303711
Adjusted likelihood: -24.10750961303711
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9993)
Solve time for step 4 4.830718168988824
Current ori: tensor([ 0.0073, -0.0409, -0.2301], device='cuda:0')
Middle force: tensor([0.9358, 0.7243, 0.5503, 0.5765, 0.5004, 0.5528, 0.6365, 0.5551, 0.5168],
       device='cuda:0')
Thumb force: tensor([1.2459, 0.9888, 1.4020, 1.1133, 0.5047, 0.5083, 0.5424, 1.0109, 0.5400],
       device='cuda:0')
Index force: tensor([0.5556, 0.8956, 0.7583, 0.7515, 0.8493, 0.7840, 0.5371, 0.5990, 0.5342],
       device='cuda:0')
Storing NORMAL transition: reward=-0.1047 (scaled=-0.1047), steps=1
Reward stats updated: mean -0.0172 -> -0.0188, std: 0.0736
Collected 53 transitions for RL
tensor([ 0.0751,  0.6054,  0.5325,  0.5301, -0.1009,  0.5849,  0.9403,  0.9261,
         1.2890,  0.2011,  0.2089,  1.0648,  0.0129, -0.0331, -0.1242, -0.0262],
       device='cuda:0')
Original likelihood: -18.53919219970703
Adjusted likelihood: -18.53919219970703
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.632233953976538
Current ori: tensor([ 0.0129, -0.0331, -0.1242], device='cuda:0')
Middle force: tensor([0.6620, 0.5094, 0.5841, 0.5209, 0.5371, 1.7100, 0.5731, 0.6174],
       device='cuda:0')
Thumb force: tensor([0.5134, 0.9092, 1.0492, 0.5241, 0.8285, 1.6596, 0.6846, 0.6351],
       device='cuda:0')
Index force: tensor([0.5380, 0.5969, 0.6441, 0.5745, 0.5752, 0.5414, 0.5770, 0.5145],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0452 (scaled=-0.0452), steps=1
Reward stats updated: mean -0.0188 -> -0.0193, std: 0.0730
Collected 54 transitions for RL
tensor([ 8.5152e-03,  6.1500e-01,  4.5543e-01,  5.3361e-01, -1.3948e-01,
         6.0136e-01,  8.8838e-01,  8.8297e-01,  1.2496e+00,  1.7074e-01,
         1.7107e-01,  9.9420e-01,  1.0724e-04, -1.6216e-03, -7.7645e-02,
        -2.2594e-01], device='cuda:0')
Original likelihood: -23.385498046875
Adjusted likelihood: -23.385498046875
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9998)
Solve time for step 6 4.6494048470049165
Current ori: tensor([ 0.0001, -0.0016, -0.0776], device='cuda:0')
Middle force: tensor([0.5090, 0.5814, 0.5206, 0.5358, 1.6826, 0.5702, 0.6195],
       device='cuda:0')
Thumb force: tensor([0.8882, 1.0315, 0.5224, 0.8174, 1.6270, 0.6778, 0.6290],
       device='cuda:0')
Index force: tensor([0.5877, 0.6374, 0.5707, 0.5724, 0.5387, 0.5746, 0.5129],
       device='cuda:0')
Storing NORMAL transition: reward=0.0589 (scaled=0.0589), steps=1
Reward stats updated: mean -0.0193 -> -0.0179, std: 0.0731
Collected 55 transitions for RL
tensor([ 0.0521,  0.6064,  0.4968,  0.5331, -0.1315,  0.6648,  0.9498,  0.9794,
         1.3012,  0.2795,  0.1485,  0.9752, -0.0142, -0.0657, -0.1418, -0.2668],
       device='cuda:0')
Original likelihood: -28.172779083251953
Adjusted likelihood: -28.172779083251953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.8406)
Solve time for step 7 4.382817750039976
Current ori: tensor([-0.0142, -0.0657, -0.1418], device='cuda:0')
Middle force: tensor([0.5262, 0.5835, 0.6682, 0.5318, 0.5356, 0.5392], device='cuda:0')
Thumb force: tensor([0.5085, 0.6357, 0.5528, 1.2402, 0.5633, 0.7902], device='cuda:0')
Index force: tensor([0.8521, 0.6805, 0.5295, 0.5471, 0.5513, 0.5601], device='cuda:0')
Storing NORMAL transition: reward=-0.0121 (scaled=-0.0121), steps=1
Reward stats updated: mean -0.0179 -> -0.0178, std: 0.0724
Collected 56 transitions for RL
tensor([ 6.0937e-02,  5.9261e-01,  4.5570e-01,  4.8643e-01, -9.8889e-02,
         7.1553e-01,  8.7518e-01,  1.0796e+00,  1.2059e+00,  2.8953e-01,
         2.0366e-01,  9.7200e-01,  9.3477e-04, -9.4217e-02, -1.3464e-01,
         6.6908e-02], device='cuda:0')
Original likelihood: -33.33302307128906
Adjusted likelihood: -33.33302307128906
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0345)
State is out of distribution
Projection step: 0, Loss: 33.50914001464844
Projection step: 1, Loss: 33.72882843017578
Projection step: 2, Loss: 34.916358947753906
Projection step: 3, Loss: 32.589969635009766
Projection step: 4, Loss: 32.68616485595703
Projection step: 5, Loss: 31.51742172241211
Projection step: 6, Loss: 34.242942810058594
Projection step: 7, Loss: 31.536767959594727
Projection step: 8, Loss: 31.813514709472656
Projection step: 9, Loss: 30.589893341064453
Projection step: 10, Loss: 31.043556213378906
Projection step: 11, Loss: 32.00410079956055
Projection step: 12, Loss: 29.80518913269043
Projection step: 13, Loss: 30.077720642089844
Projection step: 14, Loss: 29.08749008178711
Projection step: 15, Loss: 29.544815063476562
Projection step: 16, Loss: 29.027841567993164
Projection step: 17, Loss: 29.664352416992188
Projection step: 18, Loss: 28.41525650024414
Projection step: 19, Loss: 29.443756103515625
Projection step: 20, Loss: 29.089242935180664
Projection step: 21, Loss: 28.151458740234375
Projection step: 22, Loss: 29.221731185913086
Projection step: 23, Loss: 27.430828094482422
Projection step: 24, Loss: 27.730567932128906
Final likelihood: tensor([-35.1938, -26.9480, -24.4483, -26.1669, -26.3594, -24.5128, -32.2686,
        -24.8211, -29.2969, -27.9462, -28.2048, -26.7563, -28.4913, -29.5355,
        -27.5797, -29.9529])
Final projection likelihood: -28.0301
1 mode projection succeeded
New goal: tensor([ 0.0740,  0.5780,  0.5470,  0.5359, -0.0611,  0.6752,  0.8163,  0.9974,
         1.2672,  0.3143,  0.2099,  0.9572, -0.0019, -0.0874, -0.0693],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0090]], device='cuda:0')
Original likelihood: -29.182823181152344
Adjusted likelihood: -29.182823181152344
Likelihood residual: 0.0
{'index': 29.182823181152344, 'thumb_middle': inf}
Current yaw: tensor([ 0.0009, -0.0942, -0.1346], device='cuda:0')
2 index
tensor([ 6.0937e-02,  5.9261e-01,  4.5570e-01,  4.8643e-01, -9.8889e-02,
         7.1553e-01,  8.7518e-01,  1.0796e+00,  1.2059e+00,  2.8953e-01,
         2.0366e-01,  9.7200e-01,  9.3477e-04, -9.4217e-02, -1.3464e-01,
         6.6908e-02], device='cuda:0')
Solve time for step 1 10.778062512981705
Current ori: tensor([ 0.0009, -0.0942, -0.1346], device='cuda:0')
Middle force: tensor([0.6054, 0.5569, 0.5571, 0.5888], device='cuda:0')
Thumb force: tensor([0.5904, 0.5528, 0.6113, 0.5711], device='cuda:0')
tensor([ 0.0725,  0.5430,  0.4942,  0.5069, -0.0856,  0.7449,  0.8794,  1.0430,
         1.2150,  0.2810,  0.1756,  0.9487, -0.0137, -0.1113, -0.1741,  0.1334],
       device='cuda:0')
Solve time for step 2 4.3668376469868235
Current ori: tensor([-0.0137, -0.1113, -0.1741], device='cuda:0')
Middle force: tensor([0.5520, 0.5544, 0.5862], device='cuda:0')
Thumb force: tensor([0.5510, 0.6073, 0.5683], device='cuda:0')
tensor([ 0.0725,  0.5381,  0.5042,  0.5104, -0.0944,  0.7573,  0.8861,  1.0341,
         1.2200,  0.2724,  0.1699,  0.9456, -0.0147, -0.1141, -0.1876,  0.0182],
       device='cuda:0')
Solve time for step 3 3.6694252739544027
Current ori: tensor([-0.0147, -0.1141, -0.1876], device='cuda:0')
Middle force: tensor([0.5966, 0.5035], device='cuda:0')
Thumb force: tensor([0.5399, 0.5201], device='cuda:0')
tensor([ 0.0756,  0.5313,  0.5027,  0.5103, -0.1076,  0.7584,  0.8764,  1.0260,
         1.2366,  0.2559,  0.1876,  0.9132, -0.0187, -0.1041, -0.1898, -0.2670],
       device='cuda:0')
Solve time for step 4 3.9803606679779477
Current ori: tensor([-0.0187, -0.1041, -0.1898], device='cuda:0')
Middle force: tensor([0.5027], device='cuda:0')
Thumb force: tensor([0.5170], device='cuda:0')
Storing RECOVERY transition: reward=0.0728 (scaled=0.0104), steps=7
Reward stats updated: mean -0.0178 -> -0.0173, std: 0.0719
Collected 57 transitions for RL
Original likelihood: -32.86613082885742
Adjusted likelihood: -32.86613082885742
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0589)
State is out of distribution
Projection step: 0, Loss: 32.291831970214844
Projection step: 1, Loss: 32.33187484741211
Projection step: 2, Loss: 32.72895431518555
Projection step: 3, Loss: 32.30175018310547
Projection step: 4, Loss: 32.4874267578125
Projection step: 5, Loss: 31.87308692932129
Projection step: 6, Loss: 31.38034439086914
Projection step: 7, Loss: 30.167238235473633
Projection step: 8, Loss: 31.829830169677734
Projection step: 9, Loss: 31.251060485839844
Projection step: 10, Loss: 30.666133880615234
Projection step: 11, Loss: 30.678306579589844
Projection step: 12, Loss: 29.974834442138672
Projection step: 13, Loss: 30.411840438842773
Projection step: 14, Loss: 30.611207962036133
Projection step: 15, Loss: 29.470645904541016
Projection step: 16, Loss: 29.236461639404297
Projection step: 17, Loss: 29.629222869873047
Projection step: 18, Loss: 28.737451553344727
Projection step: 19, Loss: 28.531784057617188
Projection step: 20, Loss: 28.754863739013672
Projection step: 21, Loss: 28.276710510253906
Projection step: 22, Loss: 29.57470703125
Projection step: 23, Loss: 27.99921989440918
Projection step: 24, Loss: 27.882272720336914
Final likelihood: tensor([-26.6771, -27.6330, -27.9707, -26.2246, -30.1337, -29.7997, -28.5941,
        -27.1020, -27.2107, -27.2312, -27.0257, -27.4871, -28.4341, -26.5853,
        -29.6277, -27.8005])
Final projection likelihood: -27.8461
1 mode projection succeeded
New goal: tensor([ 0.0669,  0.5561,  0.6121,  0.5498, -0.0872,  0.7613,  0.8528,  0.9701,
         1.2689,  0.3330,  0.1844,  0.8876, -0.0155, -0.1063, -0.2217],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0023]], device='cuda:0')
Original likelihood: -28.735599517822266
Adjusted likelihood: -28.735599517822266
Likelihood residual: 0.0
Original likelihood: -30.903966903686523
Adjusted likelihood: -30.903966903686523
Likelihood residual: 0.0
{'index': 30.903966903686523, 'thumb_middle': 28.735599517822266}
Current yaw: tensor([-0.0135, -0.1130, -0.2152], device='cuda:0')
3 thumb_middle
tensor([ 0.0527,  0.6010,  0.5500,  0.5348, -0.1229,  0.7889,  0.8815,  1.0175,
         1.2155,  0.2878,  0.1866,  0.9227, -0.0135, -0.1130, -0.2152, -0.3475],
       device='cuda:0')
Solve time for step 1 8.963695509999525
Current ori: tensor([-0.0135, -0.1130, -0.2152], device='cuda:0')
Index force: tensor([0.5172, 0.5889, 0.5936, 0.5749], device='cuda:0')
tensor([ 0.0321,  0.6248,  0.5732,  0.5209, -0.1645,  0.7314,  0.8125,  0.9541,
         1.2118,  0.3111,  0.0895,  0.8417, -0.0310, -0.2068, -0.2408, -0.4273],
       device='cuda:0')
Solve time for step 2 3.537716950988397
Current ori: tensor([-0.0310, -0.2068, -0.2408], device='cuda:0')
Index force: tensor([0.5786, 0.5855, 0.5678], device='cuda:0')
tensor([ 0.0022,  0.6473,  0.6736,  0.5870, -0.1455,  0.7697,  0.8421,  0.9577,
         1.2034,  0.3297,  0.0229,  0.8210, -0.0521, -0.2538, -0.2884, -1.3277],
       device='cuda:0')
Solve time for step 3 3.415809646015987
Current ori: tensor([-0.0521, -0.2538, -0.2884], device='cuda:0')
Index force: tensor([0.5635, 0.5874], device='cuda:0')
tensor([-0.0157,  0.6943,  0.7324,  0.6194, -0.1091,  0.7921,  0.8517,  0.9619,
         1.1961,  0.3288, -0.0243,  0.7916, -0.0615, -0.2956, -0.3377, -2.9664],
       device='cuda:0')
Solve time for step 4 3.3070162090007216
Current ori: tensor([-0.0615, -0.2956, -0.3377], device='cuda:0')
Index force: tensor([0.5605], device='cuda:0')
Storing RECOVERY transition: reward=0.1297 (scaled=0.0185), steps=7
Reward stats updated: mean -0.0173 -> -0.0167, std: 0.0714
Collected 58 transitions for RL
Original likelihood: -86.24942779541016
Adjusted likelihood: -86.24942779541016
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 93.20616149902344
Projection step: 1, Loss: 104.50361633300781
Projection step: 2, Loss: 114.58052062988281
Projection step: 3, Loss: 107.33625793457031
Projection step: 4, Loss: 111.94711303710938
Projection step: 5, Loss: 111.56327819824219
Projection step: 6, Loss: 107.71832275390625
Projection step: 7, Loss: 109.46785736083984
Projection step: 8, Loss: 99.51197052001953
Projection step: 9, Loss: 123.30807495117188
Projection step: 10, Loss: 119.78287506103516
Projection step: 11, Loss: 106.51222229003906
Projection step: 12, Loss: 117.87134552001953
Projection step: 13, Loss: 101.46487426757812
Projection step: 14, Loss: 120.23548889160156
Projection step: 15, Loss: 87.36734008789062
Projection step: 16, Loss: 97.23243713378906
Projection step: 17, Loss: 103.94972229003906
Projection step: 18, Loss: 95.15155029296875
Projection step: 19, Loss: 79.60546875
Projection step: 20, Loss: 98.74250793457031
Projection step: 21, Loss: 92.99069213867188
Projection step: 22, Loss: 100.48417663574219
Projection step: 23, Loss: 114.14057159423828
Projection step: 24, Loss: 100.30377197265625
Final likelihood: tensor([ -38.4327, -195.5046, -115.4258, -136.0857,  -79.7272,  -91.3105,
        -141.5632, -131.0217, -194.2659, -111.1203, -113.7114, -161.5806,
        -108.9661, -158.0277,  -95.5989,  -47.5225])
Final projection likelihood: -119.9916
1 mode projection failed, trying anyway
New goal: tensor([-0.0322,  0.7446,  0.7650,  0.6478,  0.0160,  0.9082,  0.8835,  0.9445,
         1.1953,  0.3084,  0.0304,  0.7947, -0.0301, -0.2655, -0.2243],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0049]], device='cuda:0') tensor([[0.0057]], device='cuda:0')
Original likelihood: -89.52499389648438
Adjusted likelihood: -89.52499389648438
Likelihood residual: 0.0
{'index': 89.52499389648438, 'thumb_middle': inf}
Current yaw: tensor([-0.0306, -0.2667, -0.4058], device='cuda:0')
4 index
tensor([-0.0383,  0.7560,  0.7777,  0.6482,  0.0200,  0.9155,  0.8817,  0.9393,
         1.2020,  0.3090,  0.0304,  0.8085, -0.0306, -0.2667, -0.4058, -3.5935],
       device='cuda:0')
Solve time for step 1 10.140514271974098
Current ori: tensor([-0.0306, -0.2667, -0.4058], device='cuda:0')
Middle force: tensor([0.6680, 0.5432, 0.5687, 0.5747], device='cuda:0')
Thumb force: tensor([0.5139, 0.5998, 0.5079, 0.5715], device='cuda:0')
tensor([-9.7717e-02,  7.4061e-01,  7.4054e-01,  6.2976e-01,  1.9793e-02,
         9.5990e-01,  8.8949e-01,  9.3410e-01,  1.1969e+00,  2.9813e-01,
        -1.9368e-03,  8.0903e-01, -2.1014e-02, -2.9779e-01, -4.8171e-01,
        -2.8028e+00], device='cuda:0')
Solve time for step 2 4.128070990962442
Current ori: tensor([-0.0210, -0.2978, -0.4817], device='cuda:0')
Middle force: tensor([0.5098, 0.6127, 0.5847], device='cuda:0')
Thumb force: tensor([0.5663, 0.5298, 0.5757], device='cuda:0')
tensor([-9.4089e-02,  7.5285e-01,  7.4671e-01,  6.3092e-01,  1.8623e-02,
         9.9432e-01,  8.9483e-01,  9.2461e-01,  1.1979e+00,  2.8806e-01,
        -1.0351e-02,  8.4824e-01,  1.0105e-03, -3.0223e-01, -5.4897e-01,
        -1.8243e+00], device='cuda:0')
Solve time for step 3 4.046710196009371
Current ori: tensor([ 0.0010, -0.3022, -0.5490], device='cuda:0')
Middle force: tensor([0.6159, 0.5308], device='cuda:0')
Thumb force: tensor([0.5503, 0.5083], device='cuda:0')
tensor([-0.0836,  0.7582,  0.7501,  0.6329,  0.0043,  1.0214,  0.8912,  0.9105,
         1.1763,  0.3300, -0.0131,  0.8780,  0.0187, -0.3124, -0.6150, -1.7862],
       device='cuda:0')
Solve time for step 4 3.8138510860153474
Current ori: tensor([ 0.0187, -0.3124, -0.6150], device='cuda:0')
Middle force: tensor([0.5464], device='cuda:0')
Thumb force: tensor([0.5338], device='cuda:0')
Storing RECOVERY transition: reward=0.0741 (scaled=0.0106), steps=7
Reward stats updated: mean -0.0167 -> -0.0162, std: 0.0709
Collected 59 transitions for RL
Original likelihood: -274.2081298828125
Adjusted likelihood: -274.2081298828125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 273.2867431640625
Projection step: 1, Loss: 267.99261474609375
Projection step: 2, Loss: 288.633544921875
Projection step: 3, Loss: 269.9139404296875
Projection step: 4, Loss: 289.61767578125
Projection step: 5, Loss: 266.86492919921875
Projection step: 6, Loss: 273.84197998046875
Projection step: 7, Loss: 276.0570068359375
Projection step: 8, Loss: 277.78631591796875
Projection step: 9, Loss: 288.39385986328125
Projection step: 10, Loss: 284.8070373535156
Projection step: 11, Loss: 274.53192138671875
Projection step: 12, Loss: 292.9398498535156
Projection step: 13, Loss: 273.01190185546875
Projection step: 14, Loss: 291.092041015625
Projection step: 15, Loss: 283.8079833984375
Projection step: 16, Loss: 283.5293273925781
Projection step: 17, Loss: 270.43023681640625
Projection step: 18, Loss: 281.45379638671875
Projection step: 19, Loss: 276.9371337890625
Projection step: 20, Loss: 293.2933349609375
Projection step: 21, Loss: 277.4288330078125
Projection step: 22, Loss: 290.61572265625
Projection step: 23, Loss: 279.5614013671875
Projection step: 24, Loss: 287.160400390625
Final likelihood: tensor([-281.5388, -278.5698, -292.5429, -328.9308, -270.6478, -291.9779,
        -289.7452, -274.1280, -308.6438, -273.2993, -305.6644, -258.9467,
        -319.2501, -207.1377, -266.0913, -265.6223])
Final projection likelihood: -282.0461
1 mode projection failed, trying anyway
New goal: tensor([ 0.0042,  0.8523,  0.8254,  0.6698,  0.0053,  1.0624,  0.8835,  0.8877,
         1.1572,  0.3128, -0.0346,  0.9396,  0.0541, -0.3338, -0.6989],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0032]], device='cuda:0') tensor([[0.0078]], device='cuda:0')
Original likelihood: -278.3460388183594
Adjusted likelihood: -278.3460388183594
Likelihood residual: 0.0
{'index': 278.3460388183594, 'thumb_middle': inf}
Current yaw: tensor([ 0.0542, -0.3334, -0.7164], device='cuda:0')
5 index
tensor([ 1.5413e-03,  8.5348e-01,  8.2337e-01,  6.7013e-01,  3.6679e-03,
         1.0637e+00,  8.8420e-01,  8.8662e-01,  1.1671e+00,  3.1281e-01,
        -3.3874e-02,  9.3764e-01,  5.4206e-02, -3.3342e-01, -7.1640e-01,
        -2.1674e+00], device='cuda:0')
Solve time for step 1 9.961420314968564
Current ori: tensor([ 0.0542, -0.3334, -0.7164], device='cuda:0')
Middle force: tensor([0.5923, 0.5438, 0.6282, 0.6074], device='cuda:0')
Thumb force: tensor([0.5684, 0.5557, 0.5526, 0.5358], device='cuda:0')
tensor([-0.0456,  0.8537,  0.7929,  0.6478, -0.0166,  1.1187,  0.9088,  0.8922,
         1.1544,  0.3069, -0.0429,  0.9529,  0.1023, -0.3751, -0.8922, -1.9368],
       device='cuda:0')
Solve time for step 2 4.160811281995848
Current ori: tensor([ 0.1023, -0.3751, -0.8922], device='cuda:0')
Middle force: tensor([0.5477, 0.5460, 0.6191], device='cuda:0')
Thumb force: tensor([0.5396, 0.5811, 0.5256], device='cuda:0')
tensor([-0.0270,  0.8896,  0.8133,  0.6578, -0.0336,  1.1900,  0.8986,  0.8585,
         1.1786,  0.3091, -0.0422,  1.0034,  0.1942, -0.4528, -1.0436, -1.2775],
       device='cuda:0')
Solve time for step 3 3.899895373964682
Current ori: tensor([ 0.1942, -0.4528, -1.0436], device='cuda:0')
Middle force: tensor([0.5520, 0.5962], device='cuda:0')
Thumb force: tensor([0.5864, 0.5300], device='cuda:0')
tensor([ 0.0207,  0.9433,  0.8625,  0.6808, -0.0476,  1.2602,  0.8894,  0.8236,
         1.1734,  0.3109,  0.0635,  1.0351,  0.2278, -0.4207, -1.1658, -1.0085],
       device='cuda:0')
Solve time for step 4 3.867696448985953
Current ori: tensor([ 0.2278, -0.4207, -1.1658], device='cuda:0')
Middle force: tensor([0.6603], device='cuda:0')
Thumb force: tensor([0.5450], device='cuda:0')
Storing RECOVERY transition: reward=-0.3873 (scaled=-0.0553), steps=7
Reward stats updated: mean -0.0162 -> -0.0168, std: 0.0705
Collected 60 transitions for RL
Original likelihood: -448.32122802734375
Adjusted likelihood: -448.32122802734375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 4
Loaded trajectory sampler
Current yaw: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
Current yaw: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
1 turn
Sampling time 3.5800451879622415
tensor([ 1.0971e-01,  6.3288e-01,  5.3578e-01,  5.2803e-01, -9.9625e-02,
         5.5386e-01,  8.5874e-01,  8.9161e-01,  1.2381e+00,  3.0705e-01,
         2.2905e-01,  1.1840e+00, -9.3357e-04,  1.4822e-02, -2.8876e-02,
         2.6114e-01], device='cuda:0')
Original likelihood: -15.606945991516113
Adjusted likelihood: -15.606945991516113
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.30924860399682
Current ori: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
Middle force: tensor([1.2223, 1.8170, 0.7987, 0.5195, 0.5809, 0.8910, 1.1507, 0.5095, 0.6036,
        0.5203, 0.5147, 0.5805], device='cuda:0')
Thumb force: tensor([0.8599, 1.2898, 0.5675, 0.5501, 0.5277, 1.2746, 0.6531, 0.5410, 0.6966,
        0.6227, 0.7996, 0.6269], device='cuda:0')
Index force: tensor([0.9356, 1.8264, 0.5657, 0.5965, 0.5975, 0.8420, 0.5440, 0.5887, 0.5206,
        0.5987, 0.6677, 0.6198], device='cuda:0')
Storing NORMAL transition: reward=-0.0396 (scaled=-0.0396), steps=1
Reward stats updated: mean -0.0168 -> -0.0172, std: 0.0699
Collected 61 transitions for RL
tensor([ 0.1368,  0.6730,  0.5196,  0.4926, -0.1056,  0.5313,  0.8582,  0.9685,
         1.1330,  0.3900,  0.2986,  1.2651,  0.0215,  0.0067,  0.0105, -0.8996],
       device='cuda:0')
Original likelihood: -22.857444763183594
Adjusted likelihood: -22.857444763183594
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.680124495993368
Current ori: tensor([0.0215, 0.0067, 0.0105], device='cuda:0')
Middle force: tensor([1.7419, 0.7902, 0.5173, 0.5776, 0.8663, 1.1390, 0.5074, 0.6007, 0.5177,
        0.5114, 0.5787], device='cuda:0')
Thumb force: tensor([1.3161, 0.5629, 0.5466, 0.5249, 1.2621, 0.6480, 0.5415, 0.6881, 0.6187,
        0.7944, 0.6205], device='cuda:0')
Index force: tensor([1.7673, 0.5625, 0.5934, 0.5936, 0.8349, 0.5401, 0.5868, 0.5187, 0.5972,
        0.6725, 0.6142], device='cuda:0')
Storing NORMAL transition: reward=0.0067 (scaled=0.0067), steps=1
Reward stats updated: mean -0.0172 -> -0.0168, std: 0.0694
Collected 62 transitions for RL
tensor([ 0.1242,  0.6891,  0.4916,  0.4790, -0.0735,  0.5422,  0.8644,  0.9829,
         1.1376,  0.2920,  0.3004,  1.2612,  0.0239, -0.0149,  0.0035, -0.5014],
       device='cuda:0')
Original likelihood: -24.94912338256836
Adjusted likelihood: -24.94912338256836
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9971)
Solve time for step 3 5.087690663000103
Current ori: tensor([ 0.0239, -0.0149,  0.0035], device='cuda:0')
Middle force: tensor([0.7745, 0.5167, 0.5779, 0.8715, 1.1320, 0.5070, 0.6092, 0.5190, 0.5137,
        0.5780], device='cuda:0')
Thumb force: tensor([0.5581, 0.5380, 0.5205, 1.2147, 0.6357, 0.5305, 0.6590, 0.5972, 0.7615,
        0.6118], device='cuda:0')
Index force: tensor([0.5578, 0.5893, 0.5883, 0.8309, 0.5360, 0.5920, 0.5166, 0.5953, 0.6581,
        0.6083], device='cuda:0')
Storing NORMAL transition: reward=-0.0775 (scaled=-0.0775), steps=1
Reward stats updated: mean -0.0168 -> -0.0178, std: 0.0693
Collected 63 transitions for RL
tensor([ 0.0618,  0.7005,  0.3758,  0.5316, -0.1779,  0.5130,  0.8881,  0.9905,
         1.2256,  0.2160,  0.3535,  1.3101,  0.0456,  0.0356,  0.0791,  0.2630],
       device='cuda:0')
Original likelihood: -30.627315521240234
Adjusted likelihood: -30.627315521240234
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.3661)
State is out of distribution
Projection step: 0, Loss: 27.39162254333496
Projection step: 1, Loss: 27.851444244384766
Projection step: 2, Loss: 27.20880889892578
Projection step: 3, Loss: 27.199951171875
Projection step: 4, Loss: 26.50864028930664
Projection step: 5, Loss: 26.149974822998047
Projection step: 6, Loss: 23.993732452392578
Projection step: 7, Loss: 25.570709228515625
Projection step: 8, Loss: 23.520477294921875
Projection step: 9, Loss: 23.975326538085938
Projection step: 10, Loss: 22.70525360107422
Projection step: 11, Loss: 21.746004104614258
Projection step: 12, Loss: 22.86983299255371
Projection step: 13, Loss: 22.18094253540039
Projection step: 14, Loss: 21.974895477294922
Projection step: 15, Loss: 20.831439971923828
Projection step: 16, Loss: 20.13373565673828
Projection step: 17, Loss: 20.576515197753906
Projection step: 18, Loss: 20.543180465698242
Projection step: 19, Loss: 20.172077178955078
Projection step: 20, Loss: 20.792858123779297
Projection step: 21, Loss: 19.44165802001953
Projection step: 22, Loss: 21.141563415527344
Projection step: 23, Loss: 19.31403350830078
Projection step: 24, Loss: 19.908716201782227
Final likelihood: tensor([-16.1236, -16.9794, -17.5187, -20.3081, -20.9001, -17.3798, -20.3065,
        -16.2304, -23.3697, -17.7090, -21.6552, -19.1886, -16.8402, -16.6762,
        -22.2229, -20.7291])
Final projection likelihood: -19.0086
1 mode projection succeeded
New goal: tensor([ 0.0602,  0.6798,  0.4282,  0.5236, -0.1328,  0.4924,  0.8655,  0.9126,
         1.2856,  0.2332,  0.2537,  1.1978,  0.0418,  0.0303, -0.1339],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -21.858421325683594
Adjusted likelihood: -21.858421325683594
Likelihood residual: 0.0
Original likelihood: -23.90382194519043
Adjusted likelihood: -23.90382194519043
Likelihood residual: 0.0
{'index': 23.90382194519043, 'thumb_middle': 21.858421325683594}
Current yaw: tensor([0.0456, 0.0356, 0.0791], device='cuda:0')
2 thumb_middle
tensor([ 0.0618,  0.7005,  0.3758,  0.5316, -0.1779,  0.5130,  0.8881,  0.9905,
         1.2256,  0.2160,  0.3535,  1.3101,  0.0456,  0.0356,  0.0791,  0.2630],
       device='cuda:0')
Solve time for step 1 8.908462287043221
Current ori: tensor([0.0456, 0.0356, 0.0791], device='cuda:0')
Index force: tensor([0.5840, 0.5927, 0.6047, 0.6008], device='cuda:0')
tensor([ 0.0614,  0.6884,  0.4038,  0.5119, -0.2235,  0.4778,  0.8452,  0.9103,
         1.2395,  0.2066,  0.1957,  1.1877,  0.0462,  0.0355,  0.0792,  0.2471],
       device='cuda:0')
Solve time for step 2 3.536017954000272
Current ori: tensor([0.0462, 0.0355, 0.0792], device='cuda:0')
Index force: tensor([0.5839, 0.5967, 0.5934], device='cuda:0')
tensor([ 0.0577,  0.6865,  0.4042,  0.5091, -0.2254,  0.4856,  0.8410,  0.8945,
         1.2574,  0.2150,  0.1755,  1.1672,  0.0465,  0.0373,  0.0792,  0.2416],
       device='cuda:0')
Solve time for step 3 3.461297106987331
Current ori: tensor([0.0465, 0.0373, 0.0792], device='cuda:0')
Index force: tensor([0.5877, 0.5868], device='cuda:0')
tensor([ 0.0558,  0.6782,  0.4095,  0.5182, -0.2272,  0.4876,  0.8418,  0.8886,
         1.2620,  0.2108,  0.1730,  1.1683,  0.0489,  0.0389,  0.0792,  0.2430],
       device='cuda:0')
Solve time for step 4 3.3237222260213457
Current ori: tensor([0.0489, 0.0389, 0.0792], device='cuda:0')
Index force: tensor([0.5946], device='cuda:0')
Storing RECOVERY transition: reward=0.0035 (scaled=0.0012), steps=3
Reward stats updated: mean -0.0178 -> -0.0175, std: 0.0688
Collected 64 transitions for RL
SAC Update 1/5: Actor Loss=-0.0050, Q1 Loss=1.0828, Q2 Loss=1.0828, Entropy=0.3633, Time=0.10sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0321
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.0795, Q2 Loss=1.0795, Entropy=0.0029, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0592
SAC Update 3/5: Actor Loss=-0.0251, Q1 Loss=1.0741, Q2 Loss=1.0741, Entropy=0.5363, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0541
SAC Update 4/5: Actor Loss=-0.0201, Q1 Loss=1.0740, Q2 Loss=1.0740, Entropy=0.1975, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0860
SAC Update 5/5: Actor Loss=-0.0202, Q1 Loss=1.0676, Q2 Loss=1.0676, Entropy=0.1954, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0681

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.06s (20.6%)
Q1 update: 0.08s (26.5%)
Q2 update: 0.05s (16.5%)
Actor update: 0.10s (33.8%)
Target update: 0.00s (1.1%)
Priority update: 0.00s (0.1%)
Actor loss: -0.014071
Q1 loss: 1.075599
Q2 loss: 1.075598
Current threshold: -30.0414
Global Scale Offset: 0.9959
Reward stats: mean=-0.0175, std=0.0688, count=64
----------------------------------------------
SAC Update - Actor Loss: -0.0141, Q1 Loss: 1.0756, Q2 Loss: 1.0756, Entropy: 0.2591, Mean TD Error: 0.0599, Threshold: -30.0414
Original likelihood: -25.69448471069336
Adjusted likelihood: -25.69448471069336
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9913)
Current yaw: tensor([0.0473, 0.0371, 0.0754], device='cuda:0')
3 turn
Sampling time 3.5702996449545026
tensor([ 0.0512,  0.6837,  0.4075,  0.4984, -0.1638,  0.5274,  0.8741,  0.9075,
         1.3291,  0.2409,  0.2274,  1.1924,  0.0473,  0.0371,  0.0754,  0.1288],
       device='cuda:0')
Original likelihood: -20.900409698486328
Adjusted likelihood: -20.900409698486328
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.02496281801723
Current ori: tensor([0.0473, 0.0371, 0.0754], device='cuda:0')
Middle force: tensor([1.4375, 0.8348, 0.9204, 1.5130, 1.3291, 0.6970, 0.5639, 0.5492, 0.7976,
        0.5860, 1.0140, 1.4152], device='cuda:0')
Thumb force: tensor([0.7142, 0.5168, 0.5748, 0.6503, 1.0057, 1.1176, 0.8969, 0.6302, 0.8812,
        0.5633, 0.5826, 0.9043], device='cuda:0')
Index force: tensor([0.7978, 0.6736, 0.6652, 0.5277, 0.9074, 0.7877, 0.5721, 0.5564, 0.5840,
        0.6039, 0.5673, 0.5562], device='cuda:0')
Storing NORMAL transition: reward=0.0008 (scaled=0.0008), steps=1
Reward stats updated: mean -0.0175 -> -0.0172, std: 0.0683
Collected 65 transitions for RL
SAC Update 1/5: Actor Loss=-0.0001, Q1 Loss=1.0597, Q2 Loss=1.0597, Entropy=0.0171, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0048
SAC Update 2/5: Actor Loss=-0.0206, Q1 Loss=1.0613, Q2 Loss=1.0613, Entropy=0.1913, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1241
SAC Update 3/5: Actor Loss=-0.0234, Q1 Loss=1.0576, Q2 Loss=1.0576, Entropy=0.4616, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1136
SAC Update 4/5: Actor Loss=-0.0210, Q1 Loss=1.0500, Q2 Loss=1.0500, Entropy=0.1951, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0696
SAC Update 5/5: Actor Loss=-0.0051, Q1 Loss=1.0434, Q2 Loss=1.0434, Entropy=0.3438, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0558

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.3%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (19.8%)
Actor update: 0.11s (40.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.014026
Q1 loss: 1.054407
Q2 loss: 1.054407
Current threshold: -30.0831
Global Scale Offset: 0.9571
Reward stats: mean=-0.0172, std=0.0683, count=65
----------------------------------------------
SAC Update - Actor Loss: -0.0140, Q1 Loss: 1.0544, Q2 Loss: 1.0544, Entropy: 0.2418, Mean TD Error: 0.0735, Threshold: -30.0831
tensor([ 0.0447,  0.7615,  0.2630,  0.5314, -0.2236,  0.5451,  0.8856,  1.0464,
         1.3363,  0.3342,  0.1855,  1.0513,  0.0356,  0.0398,  0.0752,  0.1361],
       device='cuda:0')
Original likelihood: -30.490402221679688
Adjusted likelihood: -30.490402221679688
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.4093)
State is out of distribution
Projection step: 0, Loss: 31.2011775970459
Projection step: 1, Loss: 28.933422088623047
Projection step: 2, Loss: 29.157581329345703
Projection step: 3, Loss: 29.07720184326172
Projection step: 4, Loss: 28.00920867919922
Projection step: 5, Loss: 28.75182342529297
Projection step: 6, Loss: 27.520343780517578
Projection step: 7, Loss: 26.51578140258789
Projection step: 8, Loss: 27.04131317138672
Projection step: 9, Loss: 27.431575775146484
Projection step: 10, Loss: 25.758562088012695
Projection step: 11, Loss: 25.640975952148438
Projection step: 12, Loss: 25.42974853515625
Projection step: 13, Loss: 25.479572296142578
Projection step: 14, Loss: 24.435550689697266
Projection step: 15, Loss: 25.526527404785156
Projection step: 16, Loss: 24.300582885742188
Projection step: 17, Loss: 23.968103408813477
Projection step: 18, Loss: 23.4720401763916
Projection step: 19, Loss: 22.847702026367188
Projection step: 20, Loss: 23.376327514648438
Projection step: 21, Loss: 22.185253143310547
Projection step: 22, Loss: 25.586711883544922
Projection step: 23, Loss: 22.328414916992188
Projection step: 24, Loss: 22.577714920043945
Final likelihood: tensor([-21.0049, -21.5798, -21.4861, -20.1046, -16.6032, -22.4141, -20.1901,
        -20.6244, -19.9512, -21.6888, -22.8349, -25.0127, -17.6271, -22.3208,
        -24.4608, -20.1151])
Final projection likelihood: -21.1262
1 mode projection succeeded
New goal: tensor([ 0.0514,  0.7286,  0.3422,  0.5129, -0.1602,  0.5057,  0.7995,  0.9667,
         1.3466,  0.2455,  0.1821,  1.1626,  0.0343,  0.0343,  0.4636],
       device='cuda:0')
tensor([[0.0071]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -25.616737365722656
Adjusted likelihood: -25.616737365722656
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 25.616737365722656}
Current yaw: tensor([0.0356, 0.0398, 0.0752], device='cuda:0')
4 thumb_middle
tensor([ 0.0447,  0.7615,  0.2630,  0.5314, -0.2236,  0.5451,  0.8856,  1.0464,
         1.3363,  0.3342,  0.1855,  1.0513,  0.0356,  0.0398,  0.0752,  0.1361],
       device='cuda:0')
Solve time for step 1 8.555155338020995
Current ori: tensor([0.0356, 0.0398, 0.0752], device='cuda:0')
Index force: tensor([0.5740, 0.5838, 0.5487, 0.5924], device='cuda:0')
tensor([ 0.0530,  0.7348,  0.3248,  0.5130, -0.2330,  0.5064,  0.8088,  0.9691,
         1.3142,  0.2446,  0.1128,  1.1156,  0.0384,  0.0357,  0.0752,  0.1517],
       device='cuda:0')
Solve time for step 2 3.4312961410032585
Current ori: tensor([0.0384, 0.0357, 0.0752], device='cuda:0')
Index force: tensor([0.5731, 0.5420, 0.5823], device='cuda:0')
tensor([ 0.0454,  0.7213,  0.3432,  0.5019, -0.2325,  0.5174,  0.7926,  0.9492,
         1.3234,  0.2305,  0.1046,  1.1318,  0.0404,  0.0396,  0.0752,  0.1421],
       device='cuda:0')
Solve time for step 3 3.4022967609926127
Current ori: tensor([0.0404, 0.0396, 0.0752], device='cuda:0')
Index force: tensor([0.5328, 0.5701], device='cuda:0')
tensor([ 0.0566,  0.7273,  0.3382,  0.5194, -0.2296,  0.5246,  0.7876,  0.9584,
         1.3379,  0.2271,  0.0916,  1.1047,  0.0401,  0.0341,  0.0752,  0.1614],
       device='cuda:0')
Solve time for step 4 3.3151878300122917
Current ori: tensor([0.0401, 0.0341, 0.0752], device='cuda:0')
Index force: tensor([0.5533], device='cuda:0')
Storing RECOVERY transition: reward=0.0097 (scaled=0.0097), steps=1
Reward stats updated: mean -0.0172 -> -0.0168, std: 0.0679
Collected 66 transitions for RL
SAC Update 1/5: Actor Loss=-0.0003, Q1 Loss=1.0376, Q2 Loss=1.0376, Entropy=0.0737, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0110
SAC Update 2/5: Actor Loss=-0.0013, Q1 Loss=1.0335, Q2 Loss=1.0335, Entropy=0.1855, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0271
SAC Update 3/5: Actor Loss=-0.0039, Q1 Loss=1.0293, Q2 Loss=1.0293, Entropy=0.3781, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0372
SAC Update 4/5: Actor Loss=-0.0039, Q1 Loss=1.0242, Q2 Loss=1.0242, Entropy=0.3777, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0125
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.0204, Q2 Loss=1.0204, Entropy=0.0003, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0359

------ SAC Update Summary (5 iterations) ------
Total time: 0.19s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (18.7%)
Actor update: 0.08s (39.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.001888
Q1 loss: 1.028996
Q2 loss: 1.028996
Current threshold: -30.1186
Global Scale Offset: 0.9341
Reward stats: mean=-0.0168, std=0.0679, count=66
----------------------------------------------
SAC Update - Actor Loss: -0.0019, Q1 Loss: 1.0290, Q2 Loss: 1.0290, Entropy: 0.2031, Mean TD Error: 0.0247, Threshold: -30.1186
Original likelihood: -26.397043228149414
Adjusted likelihood: -26.397043228149414
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9835)
Current yaw: tensor([0.0420, 0.0418, 0.0648], device='cuda:0')
5 turn
Sampling time 3.6256131550180726
tensor([ 0.0394,  0.7201,  0.3360,  0.5100, -0.1759,  0.5406,  0.8251,  0.9850,
         1.3949,  0.2631,  0.1487,  1.1612,  0.0420,  0.0418,  0.0648,  0.1105],
       device='cuda:0')
Original likelihood: -25.247379302978516
Adjusted likelihood: -25.247379302978516
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9974)
Solve time for step 1 14.301233745005447
Current ori: tensor([0.0420, 0.0418, 0.0648], device='cuda:0')
Middle force: tensor([1.3732, 0.8162, 0.9143, 1.3968, 1.2424, 0.7033, 0.5392, 0.6879, 0.5123,
        0.5108, 0.5676, 0.5561], device='cuda:0')
Thumb force: tensor([0.7294, 0.5215, 0.5652, 0.6034, 0.9929, 1.0608, 0.8651, 0.5802, 0.5078,
        0.5475, 0.5026, 0.5733], device='cuda:0')
Index force: tensor([0.7615, 0.6297, 0.6702, 0.5251, 0.8867, 0.7665, 0.5586, 0.5210, 0.4900,
        0.5564, 0.7213, 0.6320], device='cuda:0')
Storing NORMAL transition: reward=0.0635 (scaled=0.0635), steps=1
Reward stats updated: mean -0.0168 -> -0.0156, std: 0.0681
Collected 67 transitions for RL
SAC Update 1/5: Actor Loss=-0.0086, Q1 Loss=1.0152, Q2 Loss=1.0152, Entropy=0.6448, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0142
SAC Update 2/5: Actor Loss=-0.0221, Q1 Loss=1.0146, Q2 Loss=1.0146, Entropy=0.1728, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0970
SAC Update 3/5: Actor Loss=-0.0056, Q1 Loss=1.0072, Q2 Loss=1.0072, Entropy=0.3806, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0468
SAC Update 4/5: Actor Loss=-0.0053, Q1 Loss=1.0025, Q2 Loss=1.0025, Entropy=0.3470, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0460
SAC Update 5/5: Actor Loss=-0.0056, Q1 Loss=0.9971, Q2 Loss=0.9971, Entropy=0.4107, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0226

------ SAC Update Summary (5 iterations) ------
Total time: 0.19s, Avg iteration: 0.04s
Sampling: 0.00s (0.9%)
Target Q: 0.04s (18.5%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.8%)
Actor update: 0.08s (39.9%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.009459
Q1 loss: 1.007306
Q2 loss: 1.007306
Current threshold: -30.1521
Global Scale Offset: 0.9191
Reward stats: mean=-0.0156, std=0.0681, count=67
----------------------------------------------
SAC Update - Actor Loss: -0.0095, Q1 Loss: 1.0073, Q2 Loss: 1.0073, Entropy: 0.3912, Mean TD Error: 0.0453, Threshold: -30.1521
tensor([ 3.3257e-02,  7.0096e-01,  3.7282e-01,  4.7839e-01, -1.7188e-01,
         4.4552e-01,  8.6023e-01,  1.1302e+00,  1.3809e+00,  3.7286e-01,
         1.1930e-01,  1.1782e+00,  4.3254e-02,  4.5227e-02,  4.1522e-04,
         1.7261e-01], device='cuda:0')
Original likelihood: -27.532024383544922
Adjusted likelihood: -27.532024383544922
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9354)
Solve time for step 2 5.528950486972462
Current ori: tensor([0.0433, 0.0452, 0.0004], device='cuda:0')
Middle force: tensor([0.7692, 0.9472, 1.3975, 1.1983, 0.6616, 0.5868, 0.6997, 0.6327, 0.5124,
        0.5373, 0.5382], device='cuda:0')
Thumb force: tensor([0.5799, 0.5962, 0.6585, 1.0656, 0.9881, 0.8543, 0.5768, 0.8828, 0.5231,
        0.5133, 0.5474], device='cuda:0')
Index force: tensor([0.6297, 0.6443, 0.5233, 0.8858, 0.7618, 0.5271, 0.5128, 0.5032, 0.6858,
        0.6541, 0.6128], device='cuda:0')
Storing NORMAL transition: reward=0.2085 (scaled=0.2085), steps=1
Reward stats updated: mean -0.0156 -> -0.0123, std: 0.0727
Collected 68 transitions for RL
SAC Update 1/5: Actor Loss=-0.0281, Q1 Loss=0.9953, Q2 Loss=0.9953, Entropy=0.5085, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0773
SAC Update 2/5: Actor Loss=-0.0259, Q1 Loss=0.7932, Q2 Loss=0.7932, Entropy=0.5051, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0998
SAC Update 3/5: Actor Loss=-0.0229, Q1 Loss=0.9834, Q2 Loss=0.9834, Entropy=0.1639, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0152
SAC Update 4/5: Actor Loss=-0.0054, Q1 Loss=0.8325, Q2 Loss=0.8325, Entropy=0.3442, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0479
SAC Update 5/5: Actor Loss=-0.0050, Q1 Loss=0.7361, Q2 Loss=0.7361, Entropy=0.3292, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1248

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.3%)
Q1 update: 0.05s (20.6%)
Q2 update: 0.05s (21.1%)
Actor update: 0.09s (37.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.017486
Q1 loss: 0.868100
Q2 loss: 0.868100
Current threshold: -30.1945
Global Scale Offset: 0.8946
Reward stats: mean=-0.0123, std=0.0727, count=68
----------------------------------------------
SAC Update - Actor Loss: -0.0175, Q1 Loss: 0.8681, Q2 Loss: 0.8681, Entropy: 0.3702, Mean TD Error: 0.0730, Threshold: -30.1945
tensor([ 0.1160,  0.7239,  0.4165,  0.5000, -0.1431,  0.3566,  1.1685,  1.1654,
         1.3820,  0.3574,  0.0957,  1.1113,  0.0383, -0.0053, -0.2068,  0.3061],
       device='cuda:0')
Original likelihood: -28.822437286376953
Adjusted likelihood: -28.822437286376953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.7910)
Solve time for step 3 5.020905758021399
Current ori: tensor([ 0.0383, -0.0053, -0.2068], device='cuda:0')
Middle force: tensor([0.8424, 1.2564, 1.1189, 0.6422, 0.5516, 0.6665, 0.5073, 0.5095, 0.5535,
        0.5485], device='cuda:0')
Thumb force: tensor([0.5606, 0.5937, 0.9407, 0.9808, 0.7723, 0.5719, 0.5037, 0.5343, 0.5025,
        0.5591], device='cuda:0')
Index force: tensor([0.6536, 0.5221, 0.8209, 0.7504, 0.5379, 0.5145, 0.5033, 0.5407, 0.6944,
        0.6105], device='cuda:0')
Storing NORMAL transition: reward=0.0184 (scaled=0.0184), steps=1
Reward stats updated: mean -0.0123 -> -0.0119, std: 0.0723
Collected 69 transitions for RL
SAC Update 1/5: Actor Loss=-0.0264, Q1 Loss=0.7512, Q2 Loss=0.7512, Entropy=0.4983, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0725
SAC Update 2/5: Actor Loss=-0.0236, Q1 Loss=0.7745, Q2 Loss=0.7745, Entropy=0.1558, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0909
SAC Update 3/5: Actor Loss=-0.0008, Q1 Loss=0.9723, Q2 Loss=0.9723, Entropy=0.1627, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1346
SAC Update 4/5: Actor Loss=-0.0001, Q1 Loss=0.9572, Q2 Loss=0.9572, Entropy=0.0305, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0204
SAC Update 5/5: Actor Loss=-0.0003, Q1 Loss=0.6322, Q2 Loss=0.6322, Entropy=0.0801, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0162

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.4%)
Q1 update: 0.05s (20.9%)
Q2 update: 0.04s (19.8%)
Actor update: 0.09s (38.7%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.010261
Q1 loss: 0.817480
Q2 loss: 0.817480
Current threshold: -30.2338
Global Scale Offset: 0.8669
Reward stats: mean=-0.0119, std=0.0723, count=69
----------------------------------------------
SAC Update - Actor Loss: -0.0103, Q1 Loss: 0.8175, Q2 Loss: 0.8175, Entropy: 0.1855, Mean TD Error: 0.0669, Threshold: -30.2338
tensor([-0.0623,  0.6278,  0.4132,  0.4321, -0.2096,  0.3403,  1.1405,  1.1420,
         1.4492,  0.3013,  0.0134,  0.9420,  0.0236,  0.0434, -0.2275, -0.5738],
       device='cuda:0')
Original likelihood: -38.286041259765625
Adjusted likelihood: -38.286041259765625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.33391571044922
Projection step: 1, Loss: 37.61923599243164
Projection step: 2, Loss: 37.242706298828125
Projection step: 3, Loss: 37.06654357910156
Projection step: 4, Loss: 36.048095703125
Projection step: 5, Loss: 35.73794174194336
Projection step: 6, Loss: 34.64514923095703
Projection step: 7, Loss: 34.77861404418945
Projection step: 8, Loss: 34.361366271972656
Projection step: 9, Loss: 33.36899948120117
Projection step: 10, Loss: 32.7344970703125
Projection step: 11, Loss: 32.410804748535156
Projection step: 12, Loss: 31.170499801635742
Projection step: 13, Loss: 31.766605377197266
Projection step: 14, Loss: 30.583759307861328
Projection step: 15, Loss: 30.328933715820312
Projection step: 16, Loss: 30.262449264526367
Projection step: 17, Loss: 29.515731811523438
Projection step: 18, Loss: 28.921951293945312
Projection step: 19, Loss: 28.203758239746094
Projection step: 20, Loss: 28.18490219116211
Projection step: 21, Loss: 27.494455337524414
Projection step: 22, Loss: 27.271446228027344
Projection step: 23, Loss: 26.248382568359375
Projection step: 24, Loss: 25.328487396240234
Final likelihood: tensor([-25.2897, -24.0628, -27.9011, -26.9368, -25.2413, -26.8545, -26.9902,
        -33.1508, -30.4220, -24.7736, -27.2469, -21.8518, -22.1328, -26.9132,
        -29.3006, -24.8397])
Final projection likelihood: -26.4942
1 mode projection succeeded
New goal: tensor([-0.0345,  0.6328,  0.4840,  0.4817, -0.1654,  0.3540,  1.0099,  0.9783,
         1.4503,  0.3131,  0.0561,  1.1121,  0.0247,  0.0354,  0.5497],
       device='cuda:0')
tensor([[0.0149]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -27.63909149169922
Adjusted likelihood: -27.63909149169922
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 27.63909149169922}
Current yaw: tensor([ 0.0236,  0.0434, -0.2275], device='cuda:0')
6 thumb_middle
tensor([-0.0623,  0.6278,  0.4132,  0.4321, -0.2096,  0.3403,  1.1405,  1.1420,
         1.4492,  0.3013,  0.0134,  0.9420,  0.0236,  0.0434, -0.2275, -0.5738],
       device='cuda:0')
Solve time for step 1 8.818721026007552
Current ori: tensor([ 0.0236,  0.0434, -0.2275], device='cuda:0')
Index force: tensor([0.5834, 0.5950, 0.5429, 0.5039], device='cuda:0')
tensor([-0.0646,  0.5921,  0.4566,  0.4671, -0.2565,  0.3366,  1.0032,  0.9831,
         1.4236,  0.3000, -0.0097,  1.0658,  0.0384,  0.0472, -0.2245, -0.5032],
       device='cuda:0')
Solve time for step 2 3.7464585509733297
Current ori: tensor([ 0.0384,  0.0472, -0.2245], device='cuda:0')
Index force: tensor([0.5840, 0.5373, 0.5000], device='cuda:0')
tensor([-0.0477,  0.5720,  0.4779,  0.5074, -0.2522,  0.3438,  0.9996,  0.9730,
         1.4315,  0.2938, -0.0149,  1.0672,  0.0458,  0.0395, -0.2245, -0.4684],
       device='cuda:0')
Solve time for step 3 3.515731083054561
Current ori: tensor([ 0.0458,  0.0395, -0.2245], device='cuda:0')
Index force: tensor([0.5841, 0.5937], device='cuda:0')
tensor([-0.0433,  0.5996,  0.4584,  0.4762, -0.2483,  0.3533,  0.9917,  0.9639,
         1.4247,  0.2967, -0.0187,  1.0742,  0.0365,  0.0371, -0.2245, -0.4717],
       device='cuda:0')
Solve time for step 4 3.3265659059979953
Current ori: tensor([ 0.0365,  0.0371, -0.2245], device='cuda:0')
Index force: tensor([0.5875], device='cuda:0')
Storing RECOVERY transition: reward=-0.0011 (scaled=-0.0004), steps=3
Reward stats updated: mean -0.0119 -> -0.0117, std: 0.0718
Collected 70 transitions for RL
SAC Update 1/5: Actor Loss=-0.0244, Q1 Loss=0.7600, Q2 Loss=0.7600, Entropy=0.1480, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1128
SAC Update 2/5: Actor Loss=-0.0059, Q1 Loss=0.9444, Q2 Loss=0.9444, Entropy=0.3435, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0291
SAC Update 3/5: Actor Loss=-0.0307, Q1 Loss=0.9475, Q2 Loss=0.9475, Entropy=0.4877, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1091
SAC Update 4/5: Actor Loss=-0.0183, Q1 Loss=0.8109, Q2 Loss=0.8109, Entropy=0.1956, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1145
SAC Update 5/5: Actor Loss=-0.0061, Q1 Loss=0.6195, Q2 Loss=0.6195, Entropy=0.5039, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0149

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.9%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (18.5%)
Actor update: 0.09s (37.9%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.017062
Q1 loss: 0.816460
Q2 loss: 0.816460
Current threshold: -30.2743
Global Scale Offset: 0.8390
Reward stats: mean=-0.0117, std=0.0718, count=70
----------------------------------------------
SAC Update - Actor Loss: -0.0171, Q1 Loss: 0.8165, Q2 Loss: 0.8165, Entropy: 0.3357, Mean TD Error: 0.0761, Threshold: -30.2743
Original likelihood: -29.7499942779541
Adjusted likelihood: -29.7499942779541
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.6270)
Current yaw: tensor([ 0.0355,  0.0406, -0.2274], device='cuda:0')
7 turn
Sampling time 3.71208366000792
tensor([-0.0535,  0.6067,  0.4405,  0.4747, -0.1798,  0.4038,  1.0216,  0.9767,
         1.4879,  0.3181,  0.0369,  1.1077,  0.0355,  0.0406, -0.2274, -0.4554],
       device='cuda:0')
Original likelihood: -30.294830322265625
Adjusted likelihood: -30.294830322265625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.4949)
Solve time for step 1 13.950988163007423
Current ori: tensor([ 0.0355,  0.0406, -0.2274], device='cuda:0')
Middle force: tensor([0.6706, 0.6932, 1.6388, 0.8262, 0.5754, 0.5286, 0.6648, 0.5725, 0.5294,
        0.5710, 0.6433, 0.8937], device='cuda:0')
Thumb force: tensor([0.8778, 0.9801, 0.6353, 1.2077, 1.0008, 0.5058, 0.5368, 0.6107, 0.5854,
        0.5946, 0.5736, 0.6175], device='cuda:0')
Index force: tensor([0.7240, 0.6578, 0.7017, 0.5492, 0.6162, 0.5230, 0.5206, 0.5626, 0.6583,
        0.6084, 0.5765, 0.5262], device='cuda:0')
Storing NORMAL transition: reward=0.0789 (scaled=0.0789), steps=1
Reward stats updated: mean -0.0117 -> -0.0104, std: 0.0721
Collected 71 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.6977, Q2 Loss=0.6977, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0687
SAC Update 2/5: Actor Loss=-0.0097, Q1 Loss=0.7168, Q2 Loss=0.7168, Entropy=0.3324, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1065
SAC Update 3/5: Actor Loss=-0.0408, Q1 Loss=0.7349, Q2 Loss=0.7349, Entropy=0.2678, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1396
SAC Update 4/5: Actor Loss=-0.0293, Q1 Loss=0.7047, Q2 Loss=0.7047, Entropy=0.4763, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0940
SAC Update 5/5: Actor Loss=-0.0282, Q1 Loss=0.6999, Q2 Loss=0.6999, Entropy=0.4340, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0775

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.4%)
Q1 update: 0.06s (20.7%)
Q2 update: 0.06s (20.5%)
Actor update: 0.12s (41.5%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.021596
Q1 loss: 0.710781
Q2 loss: 0.710781
Current threshold: -30.3126
Global Scale Offset: 0.8092
Reward stats: mean=-0.0104, std=0.0721, count=71
----------------------------------------------
SAC Update - Actor Loss: -0.0216, Q1 Loss: 0.7108, Q2 Loss: 0.7108, Entropy: 0.3021, Mean TD Error: 0.0972, Threshold: -30.3126
tensor([-0.0344,  0.5710,  0.4540,  0.5800, -0.1729,  0.3776,  1.0444,  1.0408,
         1.4990,  0.3288,  0.0648,  1.0172,  0.0431,  0.0314, -0.3066, -0.4056],
       device='cuda:0')
Original likelihood: -31.21047592163086
Adjusted likelihood: -31.21047592163086
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.2847)
State is out of distribution
Projection step: 0, Loss: 31.182851791381836
Projection step: 1, Loss: 31.084774017333984
Projection step: 2, Loss: 30.8222713470459
Projection step: 3, Loss: 29.177583694458008
Projection step: 4, Loss: 28.227081298828125
Projection step: 5, Loss: 27.637531280517578
Projection step: 6, Loss: 28.086597442626953
Projection step: 7, Loss: 28.07793617248535
Projection step: 8, Loss: 26.866737365722656
Projection step: 9, Loss: 26.310897827148438
Projection step: 10, Loss: 26.533157348632812
Projection step: 11, Loss: 25.630210876464844
Projection step: 12, Loss: 26.024049758911133
Projection step: 13, Loss: 25.22601890563965
Projection step: 14, Loss: 25.16118812561035
Projection step: 15, Loss: 25.386125564575195
Projection step: 16, Loss: 24.290876388549805
Projection step: 17, Loss: 24.08852767944336
Projection step: 18, Loss: 24.307785034179688
Projection step: 19, Loss: 23.343536376953125
Projection step: 20, Loss: 23.631019592285156
Projection step: 21, Loss: 23.04167366027832
Projection step: 22, Loss: 21.798519134521484
Projection step: 23, Loss: 23.095661163330078
Projection step: 24, Loss: 21.847808837890625
Final likelihood: tensor([-23.2630, -23.3968, -22.6218, -23.7913, -20.7559, -26.1479, -27.9801,
        -26.5573, -23.1737, -20.6474, -22.9865, -21.2075, -22.1711, -26.8744,
        -21.7178, -20.9081])
Final projection likelihood: -23.3875
1 mode projection succeeded
New goal: tensor([-0.0028,  0.6002,  0.5288,  0.5495, -0.1266,  0.3889,  0.9438,  0.9274,
         1.4665,  0.3308,  0.1102,  1.0948,  0.0392,  0.0287,  0.4769],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -21.940675735473633
Adjusted likelihood: -21.940675735473633
Likelihood residual: 0.0
Original likelihood: -22.124094009399414
Adjusted likelihood: -22.124094009399414
Likelihood residual: 0.0
{'index': 22.124094009399414, 'thumb_middle': 21.940675735473633}
Current yaw: tensor([ 0.0431,  0.0314, -0.3066], device='cuda:0')
8 thumb_middle
tensor([-0.0344,  0.5710,  0.4540,  0.5800, -0.1729,  0.3776,  1.0444,  1.0408,
         1.4990,  0.3288,  0.0648,  1.0172,  0.0431,  0.0314, -0.3066, -0.4056],
       device='cuda:0')
Solve time for step 1 9.041910864994861
Current ori: tensor([ 0.0431,  0.0314, -0.3066], device='cuda:0')
Index force: tensor([0.5795, 0.6115, 0.5886, 0.6115], device='cuda:0')
tensor([-0.0176,  0.5702,  0.4910,  0.5370, -0.2011,  0.3830,  0.9392,  0.9306,
         1.4088,  0.3130,  0.0074,  1.0351,  0.0423,  0.0237, -0.3066, -0.3672],
       device='cuda:0')
Solve time for step 2 3.4777531889849342
Current ori: tensor([ 0.0423,  0.0237, -0.3066], device='cuda:0')
Index force: tensor([0.6053, 0.5835, 0.5952], device='cuda:0')
tensor([-0.0124,  0.5694,  0.5006,  0.5296, -0.1993,  0.3986,  0.9270,  0.9173,
         1.4070,  0.3100,  0.0017,  1.0404,  0.0417,  0.0215, -0.3066, -0.3696],
       device='cuda:0')
Solve time for step 3 3.3448175610392354
Current ori: tensor([ 0.0417,  0.0215, -0.3066], device='cuda:0')
Index force: tensor([0.5000, 0.5717], device='cuda:0')
tensor([-0.0152,  0.5581,  0.5230,  0.5110, -0.2095,  0.4091,  0.9244,  0.8958,
         1.4072,  0.3060,  0.0051,  1.0461,  0.0429,  0.0226, -0.3066, -0.3666],
       device='cuda:0')
Solve time for step 4 3.40773552499013
Current ori: tensor([ 0.0429,  0.0226, -0.3066], device='cuda:0')
Index force: tensor([0.5551], device='cuda:0')
Storing RECOVERY transition: reward=0.0003 (scaled=0.0003), steps=1
Reward stats updated: mean -0.0104 -> -0.0103, std: 0.0716
Collected 72 transitions for RL
SAC Update 1/5: Actor Loss=-0.0001, Q1 Loss=0.6202, Q2 Loss=0.6202, Entropy=0.0501, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0325
SAC Update 2/5: Actor Loss=-0.0337, Q1 Loss=0.9053, Q2 Loss=0.9053, Entropy=0.4709, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0975
SAC Update 3/5: Actor Loss=-0.0271, Q1 Loss=0.6733, Q2 Loss=0.6733, Entropy=0.1221, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0999
SAC Update 4/5: Actor Loss=-0.0277, Q1 Loss=0.7607, Q2 Loss=0.7607, Entropy=0.1952, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1382
SAC Update 5/5: Actor Loss=-0.0278, Q1 Loss=0.7084, Q2 Loss=0.7084, Entropy=0.1164, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0918

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.8%)
Actor update: 0.08s (40.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.023273
Q1 loss: 0.733592
Q2 loss: 0.733592
Current threshold: -30.3563
Global Scale Offset: 0.7719
Reward stats: mean=-0.0103, std=0.0716, count=72
----------------------------------------------
SAC Update - Actor Loss: -0.0233, Q1 Loss: 0.7336, Q2 Loss: 0.7336, Entropy: 0.1910, Mean TD Error: 0.0920, Threshold: -30.3563
Original likelihood: -27.744098663330078
Adjusted likelihood: -27.744098663330078
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9565)
Current yaw: tensor([ 0.0453,  0.0353, -0.3074], device='cuda:0')
9 turn
Sampling time 3.814461581001524
tensor([-0.0429,  0.5615,  0.4884,  0.5246, -0.1470,  0.4391,  0.9541,  0.9358,
         1.4776,  0.3317,  0.0607,  1.0840,  0.0453,  0.0353, -0.3074, -0.3473],
       device='cuda:0')
Original likelihood: -26.365856170654297
Adjusted likelihood: -26.365856170654297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9955)
Solve time for step 1 13.758226396981627
Current ori: tensor([ 0.0453,  0.0353, -0.3074], device='cuda:0')
Middle force: tensor([1.4036, 0.9353, 0.9207, 1.4461, 1.3249, 0.7469, 0.7868, 0.5385, 0.5605,
        0.5544, 0.5502, 1.1768], device='cuda:0')
Thumb force: tensor([0.5656, 0.5099, 0.5693, 0.6663, 0.9489, 1.0384, 0.5618, 0.5420, 0.5056,
        0.8313, 0.5152, 0.9003], device='cuda:0')
Index force: tensor([0.7525, 0.7875, 0.6493, 0.5073, 0.8769, 0.7466, 0.5002, 0.5724, 0.5612,
        0.5332, 0.5581, 0.7380], device='cuda:0')
Storing NORMAL transition: reward=-0.0331 (scaled=-0.0331), steps=1
Reward stats updated: mean -0.0103 -> -0.0106, std: 0.0712
Collected 73 transitions for RL
SAC Update 1/5: Actor Loss=-0.0173, Q1 Loss=0.7204, Q2 Loss=0.7204, Entropy=0.1846, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1272
SAC Update 2/5: Actor Loss=-0.0002, Q1 Loss=0.6171, Q2 Loss=0.6171, Entropy=0.0590, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0442
SAC Update 3/5: Actor Loss=-0.0288, Q1 Loss=0.8549, Q2 Loss=0.8549, Entropy=0.1087, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0714
SAC Update 4/5: Actor Loss=-0.0311, Q1 Loss=0.6942, Q2 Loss=0.6942, Entropy=0.4063, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0980
SAC Update 5/5: Actor Loss=-0.0001, Q1 Loss=0.5958, Q2 Loss=0.5958, Entropy=0.0618, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1061

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (16.1%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (19.1%)
Actor update: 0.10s (42.6%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.015518
Q1 loss: 0.696476
Q2 loss: 0.696476
Current threshold: -30.4032
Global Scale Offset: 0.7339
Reward stats: mean=-0.0106, std=0.0712, count=73
----------------------------------------------
SAC Update - Actor Loss: -0.0155, Q1 Loss: 0.6965, Q2 Loss: 0.6965, Entropy: 0.1641, Mean TD Error: 0.0894, Threshold: -30.4032
tensor([ 0.0023,  0.5661,  0.4493,  0.6519, -0.0450,  0.4873,  0.8983,  0.9915,
         1.4455,  0.3446, -0.0262,  1.1858,  0.0424, -0.0173, -0.2731, -0.7671],
       device='cuda:0')
Original likelihood: -28.71399688720703
Adjusted likelihood: -28.71399688720703
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.8742)
Solve time for step 2 5.523792752006557
Current ori: tensor([ 0.0424, -0.0173, -0.2731], device='cuda:0')
Middle force: tensor([0.9417, 0.9024, 1.3869, 1.3034, 0.7400, 0.7767, 0.5411, 0.5574, 0.5569,
        0.5624, 1.1833], device='cuda:0')
Thumb force: tensor([0.5079, 0.5728, 0.6643, 0.8955, 0.9771, 0.5539, 0.5363, 0.5045, 0.8129,
        0.5128, 0.8647], device='cuda:0')
Index force: tensor([0.7671, 0.6304, 0.5063, 0.8313, 0.7259, 0.5001, 0.5612, 0.5584, 0.5282,
        0.5427, 0.7154], device='cuda:0')
Storing NORMAL transition: reward=-0.0158 (scaled=-0.0158), steps=1
Reward stats updated: mean -0.0106 -> -0.0107, std: 0.0707
Collected 74 transitions for RL
SAC Update 1/5: Actor Loss=-0.0071, Q1 Loss=0.5970, Q2 Loss=0.5970, Entropy=0.5354, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0290
SAC Update 2/5: Actor Loss=-0.0008, Q1 Loss=0.5997, Q2 Loss=0.5997, Entropy=0.2434, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0411
SAC Update 3/5: Actor Loss=-0.0002, Q1 Loss=0.8191, Q2 Loss=0.8191, Entropy=0.0561, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1768
SAC Update 4/5: Actor Loss=-0.0067, Q1 Loss=0.7700, Q2 Loss=0.7700, Entropy=0.3465, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0781
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.7564, Q2 Loss=0.7564, Entropy=0.0008, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1032

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.3%)
Q1 update: 0.06s (20.7%)
Q2 update: 0.05s (19.5%)
Actor update: 0.11s (41.7%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.002947
Q1 loss: 0.708443
Q2 loss: 0.708443
Current threshold: -30.4408
Global Scale Offset: 0.7083
Reward stats: mean=-0.0107, std=0.0707, count=74
----------------------------------------------
SAC Update - Actor Loss: -0.0029, Q1 Loss: 0.7084, Q2 Loss: 0.7084, Entropy: 0.2364, Mean TD Error: 0.0856, Threshold: -30.4408
tensor([-0.0342,  0.5169,  0.4788,  0.6821, -0.0819,  0.5397,  0.7959,  0.9608,
         1.4850,  0.2941,  0.0193,  1.0691,  0.0198,  0.0108, -0.2559, -1.0462],
       device='cuda:0')
Original likelihood: -23.04964828491211
Adjusted likelihood: -23.04964828491211
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.210373738023918
Current ori: tensor([ 0.0198,  0.0108, -0.2559], device='cuda:0')
Middle force: tensor([0.8836, 1.3552, 1.2622, 0.7022, 0.7682, 0.5365, 0.5521, 0.5524, 0.5506,
        1.1691], device='cuda:0')
Thumb force: tensor([0.5735, 0.6612, 0.8785, 0.9469, 0.5493, 0.5349, 0.5039, 0.8062, 0.5123,
        0.8482], device='cuda:0')
Index force: tensor([0.6145, 0.5052, 0.8097, 0.7314, 0.5000, 0.5590, 0.5564, 0.5263, 0.5462,
        0.7049], device='cuda:0')
Storing NORMAL transition: reward=0.0410 (scaled=0.0410), steps=1
Reward stats updated: mean -0.0107 -> -0.0100, std: 0.0705
Collected 75 transitions for RL
SAC Update 1/5: Actor Loss=-0.0282, Q1 Loss=0.7997, Q2 Loss=0.7997, Entropy=0.0896, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0730
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.6226, Q2 Loss=0.6226, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0756
SAC Update 3/5: Actor Loss=-0.0068, Q1 Loss=0.7992, Q2 Loss=0.7992, Entropy=0.3466, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0597
SAC Update 4/5: Actor Loss=-0.0068, Q1 Loss=0.7372, Q2 Loss=0.7372, Entropy=0.3397, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1044
SAC Update 5/5: Actor Loss=-0.0325, Q1 Loss=0.7486, Q2 Loss=0.7486, Entropy=0.1299, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1573

------ SAC Update Summary (5 iterations) ------
Total time: 0.30s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.7%)
Q1 update: 0.06s (20.3%)
Q2 update: 0.06s (20.1%)
Actor update: 0.12s (41.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.3%)
Actor loss: -0.014868
Q1 loss: 0.741470
Q2 loss: 0.741470
Current threshold: -30.4736
Global Scale Offset: 0.6869
Reward stats: mean=-0.0100, std=0.0705, count=75
----------------------------------------------
SAC Update - Actor Loss: -0.0149, Q1 Loss: 0.7415, Q2 Loss: 0.7415, Entropy: 0.1812, Mean TD Error: 0.0940, Threshold: -30.4736
tensor([-0.0367,  0.5049,  0.4934,  0.6841, -0.0828,  0.5160,  0.8342,  0.9476,
         1.4925,  0.2977,  0.0548,  0.9856,  0.0198,  0.0111, -0.2970, -1.0302],
       device='cuda:0')
Original likelihood: -23.232711791992188
Adjusted likelihood: -23.232711791992188
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.889719851023983
Current ori: tensor([ 0.0198,  0.0111, -0.2970], device='cuda:0')
Middle force: tensor([0.5247, 0.9017, 1.2908, 0.5999, 0.5817, 0.6126, 0.6424, 0.5879, 0.5104],
       device='cuda:0')
Thumb force: tensor([0.5774, 0.5330, 0.5019, 0.5799, 1.2131, 0.5560, 0.5525, 0.5725, 0.6110],
       device='cuda:0')
Index force: tensor([0.5415, 0.8335, 0.7750, 0.5489, 0.5664, 0.5996, 0.5575, 0.6460, 0.6323],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0555 (scaled=-0.0555), steps=1
Reward stats updated: mean -0.0100 -> -0.0106, std: 0.0702
Collected 76 transitions for RL
SAC Update 1/5: Actor Loss=-0.0593, Q1 Loss=0.7521, Q2 Loss=0.7521, Entropy=0.1625, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1417
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.7859, Q2 Loss=0.7859, Entropy=0.0064, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0788
SAC Update 3/5: Actor Loss=-0.0333, Q1 Loss=0.5682, Q2 Loss=0.5682, Entropy=0.0769, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0610
SAC Update 4/5: Actor Loss=-0.0001, Q1 Loss=0.6912, Q2 Loss=0.6912, Entropy=0.0464, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1396
SAC Update 5/5: Actor Loss=-0.0612, Q1 Loss=0.7200, Q2 Loss=0.7200, Entropy=0.1453, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1034

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (17.9%)
Q1 update: 0.04s (19.6%)
Q2 update: 0.04s (19.2%)
Actor update: 0.09s (39.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.030795
Q1 loss: 0.703457
Q2 loss: 0.703457
Current threshold: -30.5160
Global Scale Offset: 0.6574
Reward stats: mean=-0.0106, std=0.0702, count=76
----------------------------------------------
SAC Update - Actor Loss: -0.0308, Q1 Loss: 0.7035, Q2 Loss: 0.7035, Entropy: 0.0875, Mean TD Error: 0.1049, Threshold: -30.5160
tensor([-0.0896,  0.5031,  0.4477,  0.7092, -0.2346,  0.5673,  0.8344,  0.8979,
         1.4998,  0.3433,  0.1126,  0.9898,  0.0340,  0.0698, -0.2461, -1.3160],
       device='cuda:0')
Original likelihood: -34.26013946533203
Adjusted likelihood: -34.26013946533203
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0030)
State is out of distribution
Projection step: 0, Loss: 30.426105499267578
Projection step: 1, Loss: 31.58144760131836
Projection step: 2, Loss: 30.292041778564453
Projection step: 3, Loss: 29.388818740844727
Projection step: 4, Loss: 27.811933517456055
Projection step: 5, Loss: 28.933258056640625
Projection step: 6, Loss: 28.09963607788086
Projection step: 7, Loss: 28.741947174072266
Projection step: 8, Loss: 27.287071228027344
Projection step: 9, Loss: 28.316139221191406
Projection step: 10, Loss: 26.32404327392578
Projection step: 11, Loss: 25.90800666809082
Projection step: 12, Loss: 26.276531219482422
Projection step: 13, Loss: 25.762386322021484
Projection step: 14, Loss: 23.242305755615234
Projection step: 15, Loss: 23.264888763427734
Projection step: 16, Loss: 24.338714599609375
Projection step: 17, Loss: 23.87833023071289
Projection step: 18, Loss: 23.234188079833984
Projection step: 19, Loss: 21.632953643798828
Projection step: 20, Loss: 22.31266212463379
Projection step: 21, Loss: 22.35132598876953
Projection step: 22, Loss: 21.86673355102539
Projection step: 23, Loss: 21.12189483642578
Projection step: 24, Loss: 21.63371467590332
Final likelihood: tensor([-21.2672, -22.0486, -20.8195, -19.8155, -22.5284, -21.9790, -19.7977,
        -21.5076, -20.4564, -20.2358, -19.1488, -20.3088, -20.7167, -22.0898,
        -21.9723, -20.2468])
Final projection likelihood: -20.9337
1 mode projection succeeded
New goal: tensor([-0.1167,  0.5057,  0.3487,  0.8395, -0.1914,  0.5900,  0.7579,  0.9421,
         1.5210,  0.3491,  0.1460,  1.1354,  0.0350,  0.0691, -0.2832],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -22.20018196105957
Adjusted likelihood: -22.20018196105957
Likelihood residual: 0.0
Original likelihood: -29.370845794677734
Adjusted likelihood: -29.370845794677734
Likelihood residual: 0.0
{'index': 29.370845794677734, 'thumb_middle': 22.20018196105957}
Current yaw: tensor([ 0.0340,  0.0698, -0.2461], device='cuda:0')
10 thumb_middle
tensor([-0.0896,  0.5031,  0.4477,  0.7092, -0.2346,  0.5673,  0.8344,  0.8979,
         1.4998,  0.3433,  0.1126,  0.9898,  0.0340,  0.0698, -0.2461, -1.3160],
       device='cuda:0')
Solve time for step 1 8.942860036040656
Current ori: tensor([ 0.0340,  0.0698, -0.2461], device='cuda:0')
Index force: tensor([0.5768, 0.5842, 0.5887, 0.5874], device='cuda:0')
tensor([-0.1272,  0.5184,  0.3544,  0.7961, -0.2799,  0.5815,  0.7372,  0.9213,
         1.4442,  0.3180,  0.0385,  1.0494,  0.0423,  0.0870, -0.2460, -1.3414],
       device='cuda:0')
Solve time for step 2 3.5857822879916057
Current ori: tensor([ 0.0423,  0.0870, -0.2460], device='cuda:0')
Index force: tensor([0.5733, 0.5794, 0.5806], device='cuda:0')
tensor([-0.1116,  0.5045,  0.3618,  0.8440, -0.2783,  0.5981,  0.7367,  0.9134,
         1.4339,  0.3208,  0.0202,  1.0803,  0.0523,  0.0796, -0.2460, -1.3029],
       device='cuda:0')
Solve time for step 3 3.517695704009384
Current ori: tensor([ 0.0523,  0.0796, -0.2460], device='cuda:0')
Index force: tensor([0.5653, 0.5680], device='cuda:0')
tensor([-0.0953,  0.5094,  0.3594,  0.8582, -0.2707,  0.5899,  0.7380,  0.9276,
         1.4279,  0.3079,  0.0237,  1.0801,  0.0513,  0.0718, -0.2460, -1.2809],
       device='cuda:0')
Solve time for step 4 3.284346773987636
Current ori: tensor([ 0.0513,  0.0718, -0.2460], device='cuda:0')
Index force: tensor([0.5540], device='cuda:0')
Storing RECOVERY transition: reward=0.0054 (scaled=0.0014), steps=4
Reward stats updated: mean -0.0106 -> -0.0104, std: 0.0697
Collected 77 transitions for RL
SAC Update 1/5: Actor Loss=-0.0346, Q1 Loss=0.5882, Q2 Loss=0.5882, Entropy=0.0805, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0575
SAC Update 2/5: Actor Loss=-0.0067, Q1 Loss=0.7078, Q2 Loss=0.7078, Entropy=0.3414, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1066
SAC Update 3/5: Actor Loss=-0.0073, Q1 Loss=0.7042, Q2 Loss=0.7042, Entropy=0.3462, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0772
SAC Update 4/5: Actor Loss=-0.0359, Q1 Loss=0.7322, Q2 Loss=0.7322, Entropy=0.0633, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0391
SAC Update 5/5: Actor Loss=-0.0075, Q1 Loss=0.6315, Q2 Loss=0.6315, Entropy=0.3773, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1276

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.9%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (21.0%)
Actor update: 0.11s (41.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.018375
Q1 loss: 0.672797
Q2 loss: 0.672797
Current threshold: -30.5616
Global Scale Offset: 0.6268
Reward stats: mean=-0.0104, std=0.0697, count=77
----------------------------------------------
SAC Update - Actor Loss: -0.0184, Q1 Loss: 0.6728, Q2 Loss: 0.6728, Entropy: 0.2417, Mean TD Error: 0.0816, Threshold: -30.5616
Original likelihood: -24.5894775390625
Adjusted likelihood: -24.5894775390625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0502,  0.0804, -0.2541], device='cuda:0')
11 turn
Sampling time 3.6045001469901763
tensor([-0.1151,  0.5062,  0.3556,  0.8467, -0.2422,  0.6104,  0.7535,  0.9345,
         1.4992,  0.3302,  0.0756,  1.1096,  0.0502,  0.0804, -0.2541, -1.3169],
       device='cuda:0')
Original likelihood: -25.21217155456543
Adjusted likelihood: -25.21217155456543
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.755036961985752
Current ori: tensor([ 0.0502,  0.0804, -0.2541], device='cuda:0')
Middle force: tensor([0.9084, 0.5144, 0.5706, 0.8213, 1.1735, 0.6518, 0.9323, 0.5427, 0.7457,
        1.1954, 0.6491, 0.7991], device='cuda:0')
Thumb force: tensor([1.3428, 0.6222, 1.2409, 1.1150, 0.8872, 0.5679, 0.7646, 0.9388, 0.7311,
        0.9172, 0.7652, 0.5349], device='cuda:0')
Index force: tensor([1.1871, 0.8022, 0.7072, 0.7263, 1.1088, 0.9690, 0.5277, 0.5551, 0.7761,
        0.6320, 0.5617, 0.5190], device='cuda:0')
Storing NORMAL transition: reward=-0.0224 (scaled=-0.0224), steps=1
Reward stats updated: mean -0.0104 -> -0.0106, std: 0.0693
Collected 78 transitions for RL
SAC Update 1/5: Actor Loss=-0.0256, Q1 Loss=0.6117, Q2 Loss=0.6117, Entropy=0.3691, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0505
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.6714, Q2 Loss=0.6714, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0275
SAC Update 3/5: Actor Loss=-0.0047, Q1 Loss=0.7098, Q2 Loss=0.7098, Entropy=0.3720, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0164
SAC Update 4/5: Actor Loss=-0.0209, Q1 Loss=0.5868, Q2 Loss=0.5868, Entropy=0.0848, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1052
SAC Update 5/5: Actor Loss=-0.0001, Q1 Loss=0.7145, Q2 Loss=0.7145, Entropy=0.0310, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0482

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (18.4%)
Actor update: 0.08s (40.2%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.010253
Q1 loss: 0.658837
Q2 loss: 0.658837
Current threshold: -30.6041
Global Scale Offset: 0.6030
Reward stats: mean=-0.0106, std=0.0693, count=78
----------------------------------------------
SAC Update - Actor Loss: -0.0103, Q1 Loss: 0.6588, Q2 Loss: 0.6588, Entropy: 0.1714, Mean TD Error: 0.0495, Threshold: -30.6041
tensor([-0.0818,  0.3744,  0.3922,  0.8905, -0.2198,  0.4325,  1.0051,  0.9403,
         1.5000,  0.2994,  0.1290,  0.8846,  0.1193,  0.0583, -0.2393, -1.6634],
       device='cuda:0')
Original likelihood: -37.286277770996094
Adjusted likelihood: -37.286277770996094
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 36.125572204589844
Projection step: 1, Loss: 36.16451644897461
Projection step: 2, Loss: 34.85694885253906
Projection step: 3, Loss: 33.60438919067383
Projection step: 4, Loss: 33.40668487548828
Projection step: 5, Loss: 35.221885681152344
Projection step: 6, Loss: 34.292640686035156
Projection step: 7, Loss: 32.702232360839844
Projection step: 8, Loss: 33.286094665527344
Projection step: 9, Loss: 33.07914733886719
Projection step: 10, Loss: 32.309165954589844
Projection step: 11, Loss: 32.49089813232422
Projection step: 12, Loss: 30.833515167236328
Projection step: 13, Loss: 30.841121673583984
Projection step: 14, Loss: 31.07836151123047
Projection step: 15, Loss: 30.979082107543945
Projection step: 16, Loss: 30.8565616607666
Projection step: 17, Loss: 30.581958770751953
Projection step: 18, Loss: 30.212799072265625
Projection step: 19, Loss: 30.16480827331543
Projection step: 20, Loss: 29.63846206665039
Projection step: 21, Loss: 29.508995056152344
Projection step: 22, Loss: 28.506534576416016
Projection step: 23, Loss: 28.89107322692871
Projection step: 24, Loss: 27.80474090576172
Final likelihood: tensor([-30.4550, -27.9690, -29.3134, -29.5504, -28.4729, -27.4738, -29.8145,
        -29.0951, -29.1361, -27.3558, -28.3853, -27.9477, -16.0831, -29.4305,
        -30.9280, -27.6338])
Final projection likelihood: -28.0653
1 mode projection succeeded
New goal: tensor([-0.1015,  0.4072,  0.3523,  0.9708, -0.1754,  0.4542,  0.9031,  1.0353,
         1.5099,  0.3115,  0.1644,  1.0749,  0.1157,  0.0595, -0.3130],
       device='cuda:0')
tensor([[0.0118]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0109]], device='cuda:0')
Original likelihood: -28.29864501953125
Adjusted likelihood: -28.29864501953125
Likelihood residual: 0.0
Original likelihood: -31.044925689697266
Adjusted likelihood: -31.044925689697266
Likelihood residual: 0.0
{'index': 31.044925689697266, 'thumb_middle': 28.29864501953125}
Current yaw: tensor([ 0.1193,  0.0583, -0.2393], device='cuda:0')
12 thumb_middle
tensor([-0.0818,  0.3744,  0.3922,  0.8905, -0.2198,  0.4325,  1.0051,  0.9403,
         1.5000,  0.2994,  0.1290,  0.8846,  0.1193,  0.0583, -0.2393, -1.6634],
       device='cuda:0')
Solve time for step 1 8.855387142975815
Current ori: tensor([ 0.1193,  0.0583, -0.2393], device='cuda:0')
Index force: tensor([0.5723, 0.5860, 0.5746, 0.5964], device='cuda:0')
tensor([-0.1106,  0.4577,  0.3964,  0.9913, -0.2675,  0.4438,  0.8893,  0.9820,
         1.4526,  0.2969,  0.0796,  0.9973,  0.2291,  0.1090, -0.2129, -1.2292],
       device='cuda:0')
Solve time for step 2 3.5535823510144837
Current ori: tensor([ 0.2291,  0.1090, -0.2129], device='cuda:0')
Index force: tensor([0.5808, 0.5693, 0.5893], device='cuda:0')
tensor([-0.1103,  0.4284,  0.4484,  1.0205, -0.3035,  0.4269,  0.8578,  1.0063,
         1.4999,  0.3007,  0.1376,  1.0414,  0.2937,  0.1066, -0.2300, -1.9269],
       device='cuda:0')
Solve time for step 3 3.447643033985514
Current ori: tensor([ 0.2937,  0.1066, -0.2300], device='cuda:0')
Index force: tensor([0.5323, 0.5006], device='cuda:0')
tensor([-0.0524,  0.4300,  0.3875,  1.0116, -0.2778,  0.4163,  0.9048,  1.0413,
         1.5000,  0.3091,  0.1561,  1.0773,  0.2869,  0.0997, -0.2580, -1.9553],
       device='cuda:0')
Solve time for step 4 3.4179762189742178
Current ori: tensor([ 0.2869,  0.0997, -0.2580], device='cuda:0')
Index force: tensor([0.5005], device='cuda:0')
Storing RECOVERY transition: reward=-0.0444 (scaled=-0.0444), steps=1
Reward stats updated: mean -0.0106 -> -0.0110, std: 0.0690
Collected 79 transitions for RL
SAC Update 1/5: Actor Loss=-0.0002, Q1 Loss=0.6463, Q2 Loss=0.6463, Entropy=0.0426, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0279
SAC Update 2/5: Actor Loss=-0.0392, Q1 Loss=0.5159, Q2 Loss=0.5159, Entropy=0.0489, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0450
SAC Update 3/5: Actor Loss=-0.0059, Q1 Loss=0.6348, Q2 Loss=0.6348, Entropy=0.3862, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0996
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=0.6125, Q2 Loss=0.6125, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0919
SAC Update 5/5: Actor Loss=-0.0076, Q1 Loss=0.5727, Q2 Loss=0.5727, Entropy=0.3455, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0379

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.2%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (17.0%)
Actor update: 0.12s (41.9%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.010567
Q1 loss: 0.596445
Q2 loss: 0.596445
Current threshold: -30.6387
Global Scale Offset: 0.5838
Reward stats: mean=-0.0110, std=0.0690, count=79
----------------------------------------------
SAC Update - Actor Loss: -0.0106, Q1 Loss: 0.5964, Q2 Loss: 0.5964, Entropy: 0.1646, Mean TD Error: 0.0605, Threshold: -30.6387
Original likelihood: -90.63558959960938
Adjusted likelihood: -90.63558959960938
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 99.70515441894531
Projection step: 1, Loss: 92.55856323242188
Projection step: 2, Loss: 92.02140808105469
Projection step: 3, Loss: 91.73487091064453
Projection step: 4, Loss: 91.58982849121094
Projection step: 5, Loss: 92.73955535888672
Projection step: 6, Loss: 97.71329498291016
Projection step: 7, Loss: 94.22565460205078
Projection step: 8, Loss: 92.97457885742188
Projection step: 9, Loss: 96.09541320800781
Projection step: 10, Loss: 96.46487426757812
Projection step: 11, Loss: 96.29566955566406
Projection step: 12, Loss: 96.1750259399414
Projection step: 13, Loss: 93.59841918945312
Projection step: 14, Loss: 89.31227111816406
Projection step: 15, Loss: 95.94103240966797
Projection step: 16, Loss: 85.50525665283203
Projection step: 17, Loss: 87.34998321533203
Projection step: 18, Loss: 93.45643615722656
Projection step: 19, Loss: 94.28265380859375
Projection step: 20, Loss: 88.70352172851562
Projection step: 21, Loss: 91.7349853515625
Projection step: 22, Loss: 91.7361068725586
Projection step: 23, Loss: 96.64118957519531
Projection step: 24, Loss: 92.7579116821289
Final likelihood: tensor([ -81.9083,  -92.1641, -100.1172,  -70.8801,  -87.6299, -111.0370,
        -102.4277, -130.1958,  -86.8378,  -60.1117,  -85.9635,  -91.3064,
         -77.4068,  -68.5213,  -98.7058, -105.0557])
Final projection likelihood: -90.6418
1 mode projection failed, trying anyway
New goal: tensor([-0.0367,  0.4229,  0.4164,  1.0132, -0.2134,  0.4462,  0.9281,  1.0510,
         1.4689,  0.3429,  0.2748,  1.1332,  0.2746,  0.0909,  0.0921],
       device='cuda:0')
tensor([[0.0072]], device='cuda:0') tensor([[0.0017]], device='cuda:0') tensor([[0.0010]], device='cuda:0')
Original likelihood: -84.88993072509766
Adjusted likelihood: -84.88993072509766
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 84.88993072509766}
Current yaw: tensor([ 0.2773,  0.0889, -0.2544], device='cuda:0')
13 thumb_middle
tensor([-0.0310,  0.4291,  0.3802,  0.9831, -0.2217,  0.4415,  0.8906,  1.0401,
         1.5000,  0.3286,  0.2569,  1.1439,  0.2773,  0.0889, -0.2544, -1.7934],
       device='cuda:0')
Solve time for step 1 8.86209524399601
Current ori: tensor([ 0.2773,  0.0889, -0.2544], device='cuda:0')
Index force: tensor([0.5851, 0.6066, 0.5964, 0.6168], device='cuda:0')
tensor([-0.0047,  0.4342,  0.4156,  1.0009, -0.2840,  0.2383,  0.9564,  1.0938,
         1.4556,  0.3316,  0.2211,  1.1165,  0.2840,  0.0968, -0.2486, -1.9931],
       device='cuda:0')
Solve time for step 2 3.659552705998067
Current ori: tensor([ 0.2840,  0.0968, -0.2486], device='cuda:0')
Index force: tensor([0.6021, 0.5917, 0.6110], device='cuda:0')
tensor([-0.0042,  0.4368,  0.4345,  1.0184, -0.2866,  0.1896,  0.9689,  1.1141,
         1.4632,  0.3368,  0.2269,  1.1156,  0.2904,  0.1039, -0.2809, -1.9366],
       device='cuda:0')
Solve time for step 3 3.5318495579995215
Current ori: tensor([ 0.2904,  0.1039, -0.2809], device='cuda:0')
Index force: tensor([0.5860, 0.6052], device='cuda:0')
tensor([-0.0158,  0.4340,  0.4375,  1.0170, -0.2871,  0.1316,  0.9674,  1.1412,
         1.4691,  0.3396,  0.2294,  1.1201,  0.2925,  0.1060, -0.3067, -1.9311],
       device='cuda:0')
Solve time for step 4 3.3745338860317133
Current ori: tensor([ 0.2925,  0.1060, -0.3067], device='cuda:0')
Index force: tensor([0.5610], device='cuda:0')
Storing RECOVERY transition: reward=0.0181 (scaled=0.0181), steps=1
Reward stats updated: mean -0.0110 -> -0.0106, std: 0.0686
Collected 80 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.5288, Q2 Loss=0.5288, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0773
SAC Update 2/5: Actor Loss=-0.0221, Q1 Loss=0.5430, Q2 Loss=0.5430, Entropy=0.1577, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0561
SAC Update 3/5: Actor Loss=-0.0078, Q1 Loss=0.5597, Q2 Loss=0.5597, Entropy=0.3638, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1220
SAC Update 4/5: Actor Loss=-0.0310, Q1 Loss=0.6170, Q2 Loss=0.6170, Entropy=0.0686, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0986
SAC Update 5/5: Actor Loss=-0.0332, Q1 Loss=0.6260, Q2 Loss=0.6260, Entropy=0.0787, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0841

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (17.1%)
Q1 update: 0.04s (20.0%)
Q2 update: 0.04s (19.6%)
Actor update: 0.09s (39.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.018815
Q1 loss: 0.574916
Q2 loss: 0.574916
Current threshold: -30.6720
Global Scale Offset: 0.5667
Reward stats: mean=-0.0106, std=0.0686, count=80
----------------------------------------------
SAC Update - Actor Loss: -0.0188, Q1 Loss: 0.5749, Q2 Loss: 0.5749, Entropy: 0.1337, Mean TD Error: 0.0876, Threshold: -30.6720
Original likelihood: -118.35509490966797
Adjusted likelihood: -118.35509490966797
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 118.28591918945312
Projection step: 1, Loss: 115.88510131835938
Projection step: 2, Loss: 114.0724868774414
Projection step: 3, Loss: 125.1002197265625
Projection step: 4, Loss: 121.72848510742188
Projection step: 5, Loss: 123.05476379394531
Projection step: 6, Loss: 111.61071014404297
Projection step: 7, Loss: 125.69026184082031
Projection step: 8, Loss: 125.48641967773438
Projection step: 9, Loss: 114.78450775146484
Projection step: 10, Loss: 120.74909973144531
Projection step: 11, Loss: 111.57061004638672
Projection step: 12, Loss: 122.81364440917969
Projection step: 13, Loss: 113.4473648071289
Projection step: 14, Loss: 122.1626968383789
Projection step: 15, Loss: 113.94346618652344
Projection step: 16, Loss: 123.66221618652344
Projection step: 17, Loss: 118.73949432373047
Projection step: 18, Loss: 124.64427185058594
Projection step: 19, Loss: 112.0555191040039
Projection step: 20, Loss: 112.52691650390625
Projection step: 21, Loss: 113.90394592285156
Projection step: 22, Loss: 113.47773742675781
Projection step: 23, Loss: 115.89796447753906
Projection step: 24, Loss: 114.80426025390625
Final likelihood: tensor([-112.4100, -117.9239, -114.3921, -103.9918, -136.8647,  -98.5067,
         -94.1842, -129.6679, -110.3748, -101.9965, -128.7011, -118.6046,
        -131.6504, -133.9538, -101.7072,  -86.1373])
Final projection likelihood: -113.8167
1 mode projection failed, trying anyway
New goal: tensor([-0.0334,  0.4285,  0.4505,  1.0392, -0.2566,  0.1259,  1.0802,  1.2918,
         1.4665,  0.3845,  0.3125,  1.1911,  0.2930,  0.1095, -0.1741],
       device='cuda:0')
tensor([[0.0046]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0002]], device='cuda:0')
Original likelihood: -107.93655395507812
Adjusted likelihood: -107.93655395507812
Likelihood residual: 0.0
Original likelihood: -108.26118469238281
Adjusted likelihood: -108.26118469238281
Likelihood residual: 0.0
{'index': 108.26118469238281, 'thumb_middle': 107.93655395507812}
Current yaw: tensor([ 0.2949,  0.1083, -0.3429], device='cuda:0')
14 thumb_middle
tensor([-0.0235,  0.4310,  0.4344,  1.0180, -0.2549,  0.1087,  1.0494,  1.2910,
         1.5000,  0.3716,  0.3043,  1.1690,  0.2949,  0.1083, -0.3429, -1.9840],
       device='cuda:0')
Solve time for step 1 8.781726902001537
Current ori: tensor([ 0.2949,  0.1083, -0.3429], device='cuda:0')
Index force: tensor([0.5850, 0.5511, 0.5761, 0.6172], device='cuda:0')
tensor([-0.0276,  0.4293,  0.4099,  0.9966, -0.3297,  0.0219,  1.0053,  1.2555,
         1.4431,  0.3592,  0.2579,  1.1618,  0.2966,  0.1097, -0.3512, -1.9805],
       device='cuda:0')
Solve time for step 2 3.3225907029700466
Current ori: tensor([ 0.2966,  0.1097, -0.3512], device='cuda:0')
Index force: tensor([0.5447, 0.5679, 0.6055], device='cuda:0')
tensor([-0.0381,  0.4285,  0.4388,  1.0127, -0.3231,  0.0442,  1.0067,  1.2523,
         1.4445,  0.3772,  0.2492,  1.1553,  0.2952,  0.1089, -0.3858, -1.8707],
       device='cuda:0')
Solve time for step 3 3.5245325620053336
Current ori: tensor([ 0.2952,  0.1089, -0.3858], device='cuda:0')
Index force: tensor([0.5869, 0.5573], device='cuda:0')
tensor([-0.0516,  0.4290,  0.4549,  1.0371, -0.3145,  0.0618,  1.0076,  1.2589,
         1.4421,  0.3739,  0.2510,  1.1646,  0.3033,  0.1161, -0.4131, -1.9313],
       device='cuda:0')
Solve time for step 4 3.413919201993849
Current ori: tensor([ 0.3033,  0.1161, -0.4131], device='cuda:0')
Index force: tensor([0.5561], device='cuda:0')
Storing RECOVERY transition: reward=0.0396 (scaled=0.0396), steps=1
Reward stats updated: mean -0.0106 -> -0.0100, std: 0.0684
Collected 81 transitions for RL
SAC Update 1/5: Actor Loss=-0.0776, Q1 Loss=0.6466, Q2 Loss=0.6466, Entropy=0.0751, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1486
SAC Update 2/5: Actor Loss=-0.0431, Q1 Loss=0.5020, Q2 Loss=0.5020, Entropy=0.0357, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0620
SAC Update 3/5: Actor Loss=-0.0314, Q1 Loss=0.5958, Q2 Loss=0.5958, Entropy=0.0348, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1276
SAC Update 4/5: Actor Loss=-0.0001, Q1 Loss=0.4629, Q2 Loss=0.4629, Entropy=0.0262, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0474
SAC Update 5/5: Actor Loss=-0.0735, Q1 Loss=0.5515, Q2 Loss=0.5515, Entropy=0.0606, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0883

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (18.4%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.04s (18.2%)
Actor update: 0.09s (39.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.045112
Q1 loss: 0.551754
Q2 loss: 0.551754
Current threshold: -30.7267
Global Scale Offset: 0.5368
Reward stats: mean=-0.0100, std=0.0684, count=81
----------------------------------------------
SAC Update - Actor Loss: -0.0451, Q1 Loss: 0.5518, Q2 Loss: 0.5518, Entropy: 0.0465, Mean TD Error: 0.0948, Threshold: -30.7267
Original likelihood: -123.69029998779297
Adjusted likelihood: -123.69029998779297
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 132.66424560546875
Projection step: 1, Loss: 133.3544464111328
Projection step: 2, Loss: 117.8188705444336
Projection step: 3, Loss: 120.39759063720703
Projection step: 4, Loss: 124.09912109375
Projection step: 5, Loss: 122.73577880859375
Projection step: 6, Loss: 132.47450256347656
Projection step: 7, Loss: 127.58042907714844
Projection step: 8, Loss: 132.554443359375
Projection step: 9, Loss: 123.95989990234375
Projection step: 10, Loss: 128.53485107421875
Projection step: 11, Loss: 128.08457946777344
Projection step: 12, Loss: 125.45877075195312
Projection step: 13, Loss: 121.77131652832031
Projection step: 14, Loss: 129.74684143066406
Projection step: 15, Loss: 120.31021118164062
Projection step: 16, Loss: 121.93453979492188
Projection step: 17, Loss: 126.40579223632812
Projection step: 18, Loss: 121.55342102050781
Projection step: 19, Loss: 125.93623352050781
Projection step: 20, Loss: 121.56680297851562
Projection step: 21, Loss: 121.6434097290039
Projection step: 22, Loss: 127.69607543945312
Projection step: 23, Loss: 130.71983337402344
Projection step: 24, Loss: 123.84701538085938
Final likelihood: tensor([ -97.4428,  -91.4675, -116.0008, -117.3732, -147.2690, -105.5054,
         -91.4641, -121.2246, -106.4926, -128.9719, -120.4543, -134.1225,
         -85.1695, -122.5777, -117.2083, -104.2923])
Final projection likelihood: -112.9398
1 mode projection failed, trying anyway
New goal: tensor([-0.0704,  0.4298,  0.4825,  1.0564, -0.2796,  0.1256,  1.0976,  1.2947,
         1.4669,  0.4034,  0.3120,  1.2313,  0.2963,  0.1130, -0.2118],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0002]], device='cuda:0')
Original likelihood: -115.7772216796875
Adjusted likelihood: -115.7772216796875
Likelihood residual: 0.0
Original likelihood: -121.32453918457031
Adjusted likelihood: -121.32453918457031
Likelihood residual: 0.0
{'index': 121.32453918457031, 'thumb_middle': 115.7772216796875}
Current yaw: tensor([ 0.2980,  0.1118, -0.3729], device='cuda:0')
15 thumb_middle
tensor([-0.0600,  0.4313,  0.4688,  1.0406, -0.2782,  0.1105,  1.0621,  1.2907,
         1.4999,  0.3932,  0.3044,  1.2076,  0.2980,  0.1118, -0.3729, -1.8530],
       device='cuda:0')
Solve time for step 1 8.966485993005335
Current ori: tensor([ 0.2980,  0.1118, -0.3729], device='cuda:0')
Index force: tensor([0.5726, 0.5775, 0.5999, 0.5982], device='cuda:0')
tensor([-0.0706,  0.4321,  0.4956,  1.0573, -0.3387,  0.0138,  1.0207,  1.2445,
         1.4363,  0.3836,  0.2479,  1.1976,  0.3072,  0.1199, -0.3958, -1.9549],
       device='cuda:0')
Solve time for step 2 3.5324696169700474
Current ori: tensor([ 0.3072,  0.1199, -0.3958], device='cuda:0')
Index force: tensor([0.6011, 0.5810, 0.5728], device='cuda:0')
tensor([-0.0847,  0.4299,  0.5041,  1.0566, -0.3318,  0.0108,  1.0252,  1.2450,
         1.4416,  0.3922,  0.2472,  1.2058,  0.3113,  0.1235, -0.4121, -1.9924],
       device='cuda:0')
Solve time for step 3 3.3643737829988822
Current ori: tensor([ 0.3113,  0.1235, -0.4121], device='cuda:0')
Index force: tensor([0.5699, 0.5921], device='cuda:0')
tensor([-0.1004,  0.4269,  0.4904,  1.0608, -0.3179,  0.0298,  1.0256,  1.2557,
         1.4458,  0.3848,  0.2586,  1.1938,  0.3134,  0.1252, -0.4186, -1.9389],
       device='cuda:0')
Solve time for step 4 3.2180969860055484
Current ori: tensor([ 0.3134,  0.1252, -0.4186], device='cuda:0')
Index force: tensor([0.5508], device='cuda:0')
Storing RECOVERY transition: reward=0.0562 (scaled=0.0562), steps=1
Reward stats updated: mean -0.0100 -> -0.0092, std: 0.0684
Collected 82 transitions for RL
SAC Update 1/5: Actor Loss=-0.0530, Q1 Loss=0.6795, Q2 Loss=0.6795, Entropy=0.3748, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1440
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.5568, Q2 Loss=0.5568, Entropy=0.0005, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0434
SAC Update 3/5: Actor Loss=-0.0160, Q1 Loss=0.4429, Q2 Loss=0.4429, Entropy=0.0250, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0936
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=0.4993, Q2 Loss=0.4993, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0337
SAC Update 5/5: Actor Loss=-0.0489, Q1 Loss=0.4773, Q2 Loss=0.4773, Entropy=0.0222, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0856

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (18.3%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (40.1%)
Target update: 0.00s (1.9%)
Priority update: 0.00s (0.2%)
Actor loss: -0.023590
Q1 loss: 0.531155
Q2 loss: 0.531155
Current threshold: -30.7800
Global Scale Offset: 0.5075
Reward stats: mean=-0.0092, std=0.0684, count=82
----------------------------------------------
SAC Update - Actor Loss: -0.0236, Q1 Loss: 0.5312, Q2 Loss: 0.5312, Entropy: 0.0845, Mean TD Error: 0.0801, Threshold: -30.7800
Original likelihood: -147.1493682861328
Adjusted likelihood: -147.1493682861328
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 146.2967071533203
Projection step: 1, Loss: 145.92913818359375
Projection step: 2, Loss: 153.15325927734375
Projection step: 3, Loss: 149.91928100585938
Projection step: 4, Loss: 147.7698974609375
Projection step: 5, Loss: 142.98223876953125
Projection step: 6, Loss: 137.3367919921875
Projection step: 7, Loss: 144.72586059570312
Projection step: 8, Loss: 133.99192810058594
Projection step: 9, Loss: 139.80459594726562
Projection step: 10, Loss: 136.2295379638672
Projection step: 11, Loss: 138.18536376953125
Projection step: 12, Loss: 144.76434326171875
Projection step: 13, Loss: 138.95567321777344
Projection step: 14, Loss: 145.61048889160156
Projection step: 15, Loss: 136.285400390625
Projection step: 16, Loss: 146.6584014892578
Projection step: 17, Loss: 156.89694213867188
Projection step: 18, Loss: 142.1684112548828
Projection step: 19, Loss: 136.64404296875
Projection step: 20, Loss: 136.40017700195312
Projection step: 21, Loss: 155.57308959960938
Projection step: 22, Loss: 143.19436645507812
Projection step: 23, Loss: 141.11981201171875
Projection step: 24, Loss: 141.97093200683594
Final likelihood: tensor([-129.5088, -119.3995, -162.8578, -121.0021, -132.1355, -100.3148,
        -117.6596, -135.1299, -155.5176, -128.9987, -143.3718, -132.2287,
        -163.7212, -125.2200, -119.8286, -165.4404])
Final projection likelihood: -134.5209
1 mode projection failed, trying anyway
New goal: tensor([-0.1059,  0.4309,  0.5130,  1.0751, -0.2779,  0.0430,  1.0847,  1.2908,
         1.4694,  0.4146,  0.3079,  1.2707,  0.3082,  0.1225, -0.2791],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[1.8569e-05]], device='cuda:0')
Original likelihood: -128.26585388183594
Adjusted likelihood: -128.26585388183594
Likelihood residual: 0.0
Original likelihood: -136.34591674804688
Adjusted likelihood: -136.34591674804688
Likelihood residual: 0.0
{'index': 136.34591674804688, 'thumb_middle': 128.26585388183594}
Current yaw: tensor([ 0.3092,  0.1216, -0.4063], device='cuda:0')
16 thumb_middle
tensor([-0.0967,  0.4286,  0.5031,  1.0587, -0.2756,  0.0339,  1.0557,  1.2835,
         1.4999,  0.4098,  0.3009,  1.2387,  0.3092,  0.1216, -0.4063, -1.9788],
       device='cuda:0')
Solve time for step 1 8.626593254040927
Current ori: tensor([ 0.3092,  0.1216, -0.4063], device='cuda:0')
Index force: tensor([0.5928, 0.6667, 0.5378, 0.5881], device='cuda:0')
tensor([-0.1079,  0.4296,  0.5152,  1.0654, -0.3526, -0.0546,  0.9990,  1.2230,
         1.4268,  0.4043,  0.2413,  1.2357,  0.3157,  0.1272, -0.4262, -2.0000],
       device='cuda:0')
Solve time for step 2 3.467058511043433
Current ori: tensor([ 0.3157,  0.1272, -0.4262], device='cuda:0')
Index force: tensor([0.5388, 0.5504, 0.5530], device='cuda:0')
tensor([-0.1248,  0.4278,  0.5245,  1.0752, -0.3616, -0.0740,  0.9965,  1.2194,
         1.4288,  0.4016,  0.2377,  1.2383,  0.3199,  0.1306, -0.4362, -2.0366],
       device='cuda:0')
Solve time for step 3 3.3878205589717254
Current ori: tensor([ 0.3199,  0.1306, -0.4362], device='cuda:0')
Index force: tensor([0.5332, 0.5974], device='cuda:0')
tensor([-0.1411,  0.4276,  0.5310,  1.0762, -0.3661, -0.0912,  0.9835,  1.2110,
         1.4350,  0.3980,  0.2388,  1.2406,  0.3224,  0.1325, -0.4607, -2.0147],
       device='cuda:0')
Solve time for step 4 3.2623765770113096
Current ori: tensor([ 0.3224,  0.1325, -0.4607], device='cuda:0')
Index force: tensor([0.5176], device='cuda:0')
Storing RECOVERY transition: reward=0.0823 (scaled=0.0823), steps=1
Reward stats updated: mean -0.0092 -> -0.0081, std: 0.0687
Collected 83 transitions for RL
SAC Update 1/5: Actor Loss=-0.0012, Q1 Loss=0.4561, Q2 Loss=0.4561, Entropy=0.1853, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1087
SAC Update 2/5: Actor Loss=-0.0313, Q1 Loss=0.5240, Q2 Loss=0.5240, Entropy=0.0196, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1093
SAC Update 3/5: Actor Loss=-0.0511, Q1 Loss=0.4594, Q2 Loss=0.4594, Entropy=0.0184, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0921
SAC Update 4/5: Actor Loss=-0.1037, Q1 Loss=0.6414, Q2 Loss=0.6414, Entropy=0.0342, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1426
SAC Update 5/5: Actor Loss=-0.0358, Q1 Loss=0.5263, Q2 Loss=0.5263, Entropy=0.0157, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0932

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.6%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (19.3%)
Actor update: 0.12s (43.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.044628
Q1 loss: 0.521447
Q2 loss: 0.521447
Current threshold: -30.8367
Global Scale Offset: 0.4788
Reward stats: mean=-0.0081, std=0.0687, count=83
----------------------------------------------
SAC Update - Actor Loss: -0.0446, Q1 Loss: 0.5214, Q2 Loss: 0.5214, Entropy: 0.0546, Mean TD Error: 0.1092, Threshold: -30.8367
Original likelihood: -163.0320281982422
Adjusted likelihood: -163.0320281982422
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 174.39976501464844
Projection step: 1, Loss: 164.01385498046875
Projection step: 2, Loss: 164.89044189453125
Projection step: 3, Loss: 164.743408203125
Projection step: 4, Loss: 164.77008056640625
Projection step: 5, Loss: 174.72674560546875
Projection step: 6, Loss: 167.6055908203125
Projection step: 7, Loss: 178.08377075195312
Projection step: 8, Loss: 172.88092041015625
Projection step: 9, Loss: 171.36354064941406
Projection step: 10, Loss: 161.1088409423828
Projection step: 11, Loss: 179.96127319335938
Projection step: 12, Loss: 175.91763305664062
Projection step: 13, Loss: 169.87777709960938
Projection step: 14, Loss: 167.38473510742188
Projection step: 15, Loss: 172.1293182373047
Projection step: 16, Loss: 175.74781799316406
Projection step: 17, Loss: 172.63433837890625
Projection step: 18, Loss: 162.53366088867188
Projection step: 19, Loss: 173.59555053710938
Projection step: 20, Loss: 163.24365234375
Projection step: 21, Loss: 186.31536865234375
Projection step: 22, Loss: 167.49057006835938
Projection step: 23, Loss: 173.24551391601562
Projection step: 24, Loss: 174.61065673828125
Final likelihood: tensor([-125.6378, -149.6492, -178.8244, -182.6631, -202.0861, -173.2554,
        -166.7347, -161.9965, -178.2715, -183.4756, -164.0098, -179.8190,
        -128.1060, -176.4311, -128.8226, -161.1068])
Final projection likelihood: -165.0556
1 mode projection failed, trying anyway
New goal: tensor([-0.1576,  0.4307,  0.5390,  1.0954, -0.2978,  0.0167,  1.0698,  1.2899,
         1.4711,  0.4235,  0.3009,  1.3155,  0.3228,  0.1335, -0.3566],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0018]], device='cuda:0') tensor([[-2.5366e-05]], device='cuda:0')
Original likelihood: -158.0166015625
Adjusted likelihood: -158.0166015625
Likelihood residual: 0.0
Original likelihood: -184.3195343017578
Adjusted likelihood: -184.3195343017578
Likelihood residual: 0.0
{'index': 184.3195343017578, 'thumb_middle': 158.0166015625}
Current yaw: tensor([ 0.3230,  0.1330, -0.4600], device='cuda:0')
17 thumb_middle
tensor([-0.1493,  0.4270,  0.5328,  1.0774, -0.2957,  0.0138,  1.0458,  1.2828,
         1.4999,  0.4244,  0.2965,  1.2792,  0.3230,  0.1330, -0.4600, -2.0056],
       device='cuda:0')
Solve time for step 1 8.94181596499402
Current ori: tensor([ 0.3230,  0.1330, -0.4600], device='cuda:0')
Index force: tensor([0.5928, 0.5818, 0.6079, 0.6180], device='cuda:0')
tensor([-0.1631,  0.4307,  0.5545,  1.0949, -0.3575, -0.1957,  0.9154,  1.1732,
         1.4299,  0.4048,  0.2361,  1.2801,  0.3269,  0.1361, -0.4860, -2.0062],
       device='cuda:0')
Solve time for step 2 3.7018943480215967
Current ori: tensor([ 0.3269,  0.1361, -0.4860], device='cuda:0')
Index force: tensor([0.5767, 0.6014, 0.6120], device='cuda:0')
tensor([-0.1786,  0.4314,  0.5625,  1.0970, -0.3415, -0.1960,  0.8902,  1.1535,
         1.4337,  0.4060,  0.2307,  1.2855,  0.3296,  0.1381, -0.5016, -1.9992],
       device='cuda:0')
Solve time for step 3 3.6109714680351317
Current ori: tensor([ 0.3296,  0.1381, -0.5016], device='cuda:0')
Index force: tensor([0.5672, 0.5505], device='cuda:0')
tensor([-0.1957,  0.4325,  0.5767,  1.1041, -0.3285, -0.0749,  0.9705,  1.2051,
         1.4450,  0.4079,  0.2302,  1.2817,  0.3328,  0.1405, -0.5098, -2.0039],
       device='cuda:0')
Solve time for step 4 3.5741967979702167
Current ori: tensor([ 0.3328,  0.1405, -0.5098], device='cuda:0')
Index force: tensor([0.5340], device='cuda:0')
Storing RECOVERY transition: reward=0.1019 (scaled=0.1019), steps=1
Reward stats updated: mean -0.0081 -> -0.0068, std: 0.0693
Collected 84 transitions for RL
SAC Update 1/5: Actor Loss=-0.0541, Q1 Loss=0.4492, Q2 Loss=0.4492, Entropy=0.0144, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1019
SAC Update 2/5: Actor Loss=-0.0100, Q1 Loss=0.6094, Q2 Loss=0.6094, Entropy=0.3278, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0445
SAC Update 3/5: Actor Loss=-0.0103, Q1 Loss=0.5465, Q2 Loss=0.5465, Entropy=0.3261, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0777
SAC Update 4/5: Actor Loss=-0.0001, Q1 Loss=0.5288, Q2 Loss=0.5288, Entropy=0.0290, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0065
SAC Update 5/5: Actor Loss=-0.0582, Q1 Loss=0.6014, Q2 Loss=0.6014, Entropy=0.0101, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1034

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (21.0%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (17.4%)
Actor update: 0.10s (38.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.026530
Q1 loss: 0.547061
Q2 loss: 0.547061
Current threshold: -30.8992
Global Scale Offset: 0.4506
Reward stats: mean=-0.0068, std=0.0693, count=84
----------------------------------------------
SAC Update - Actor Loss: -0.0265, Q1 Loss: 0.5471, Q2 Loss: 0.5471, Entropy: 0.1415, Mean TD Error: 0.0668, Threshold: -30.8992
Original likelihood: -189.30987548828125
Adjusted likelihood: -189.30987548828125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 199.16700744628906
Projection step: 1, Loss: 172.6795654296875
Projection step: 2, Loss: 194.98948669433594
Projection step: 3, Loss: 190.71810913085938
Projection step: 4, Loss: 180.45358276367188
Projection step: 5, Loss: 186.83697509765625
Projection step: 6, Loss: 179.33551025390625
Projection step: 7, Loss: 199.42410278320312
Projection step: 8, Loss: 198.11959838867188
Projection step: 9, Loss: 187.34271240234375
Projection step: 10, Loss: 190.39027404785156
Projection step: 11, Loss: 186.39613342285156
Projection step: 12, Loss: 186.3360595703125
Projection step: 13, Loss: 191.065673828125
Projection step: 14, Loss: 190.66702270507812
Projection step: 15, Loss: 194.71624755859375
Projection step: 16, Loss: 181.82644653320312
Projection step: 17, Loss: 180.62548828125
Projection step: 18, Loss: 194.00929260253906
Projection step: 19, Loss: 186.17996215820312
Projection step: 20, Loss: 178.59361267089844
Projection step: 21, Loss: 189.6898956298828
Projection step: 22, Loss: 182.5802764892578
Projection step: 23, Loss: 188.6768798828125
Projection step: 24, Loss: 184.58543395996094
Final likelihood: tensor([-185.0480, -161.9579, -204.8929, -254.0795, -162.8754, -202.8684,
        -209.2474, -193.5567, -145.5018, -153.9804, -171.7244, -202.0451,
        -187.5026, -156.2298, -194.4542, -149.8320])
Final projection likelihood: -183.4873
1 mode projection failed, trying anyway
New goal: tensor([-0.2155,  0.4334,  0.5697,  1.1152, -0.2925, -0.0045,  1.0457,  1.2774,
         1.4716,  0.4344,  0.2899,  1.3587,  0.3349,  0.1424, -0.4206],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0008]], device='cuda:0') tensor([[6.5135e-05]], device='cuda:0')
Original likelihood: -181.40151977539062
Adjusted likelihood: -181.40151977539062
Likelihood residual: 0.0
Original likelihood: -191.92547607421875
Adjusted likelihood: -191.92547607421875
Likelihood residual: 0.0
{'index': 191.92547607421875, 'thumb_middle': 181.40151977539062}
Current yaw: tensor([ 0.3348,  0.1420, -0.5083], device='cuda:0')
18 thumb_middle
tensor([-0.2086,  0.4301,  0.5663,  1.0966, -0.2905, -0.0053,  1.0250,  1.2720,
         1.4980,  0.4343,  0.2868,  1.3255,  0.3348,  0.1420, -0.5083, -2.0096],
       device='cuda:0')
Solve time for step 1 8.77485817699926
Current ori: tensor([ 0.3348,  0.1420, -0.5083], device='cuda:0')
Index force: tensor([0.5962, 0.6001, 0.5462, 0.5372], device='cuda:0')
tensor([-0.2234,  0.4293,  0.5644,  1.0988, -0.2681, -0.1960,  0.8747,  1.1547,
         1.4330,  0.4152,  0.2295,  1.3264,  0.3376,  0.1440, -0.5118, -2.0217],
       device='cuda:0')
Solve time for step 2 3.516484687977936
Current ori: tensor([ 0.3376,  0.1440, -0.5118], device='cuda:0')
Index force: tensor([0.5379, 0.6269, 0.5331], device='cuda:0')
tensor([-0.2417,  0.4295,  0.5761,  1.1130, -0.3002, -0.1960,  0.8873,  1.1608,
         1.4355,  0.4113,  0.2240,  1.3333,  0.3410,  0.1464, -0.5245, -2.0212],
       device='cuda:0')
Solve time for step 3 3.3690846439567395
Current ori: tensor([ 0.3410,  0.1464, -0.5245], device='cuda:0')
Index force: tensor([0.6119, 0.5270], device='cuda:0')
tensor([-0.2588,  0.4332,  0.5910,  1.1070, -0.3109, -0.1222,  0.9351,  1.1871,
         1.4342,  0.4195,  0.2269,  1.3349,  0.3438,  0.1484, -0.5377, -2.0177],
       device='cuda:0')
Solve time for step 4 3.566951798973605
Current ori: tensor([ 0.3438,  0.1484, -0.5377], device='cuda:0')
Index force: tensor([0.5173], device='cuda:0')
Storing RECOVERY transition: reward=0.1115 (scaled=0.1115), steps=1
Reward stats updated: mean -0.0068 -> -0.0054, std: 0.0701
Collected 85 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.5815, Q2 Loss=0.5815, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0416
SAC Update 2/5: Actor Loss=-0.0109, Q1 Loss=0.4657, Q2 Loss=0.4657, Entropy=0.3220, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1221
SAC Update 3/5: Actor Loss=-0.0104, Q1 Loss=0.5156, Q2 Loss=0.5156, Entropy=0.3244, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0456
SAC Update 4/5: Actor Loss=-0.0096, Q1 Loss=0.4329, Q2 Loss=0.4329, Entropy=0.3329, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0786
SAC Update 5/5: Actor Loss=-0.0026, Q1 Loss=0.5263, Q2 Loss=0.5263, Entropy=0.3053, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0726

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (17.9%)
Q1 update: 0.05s (18.4%)
Q2 update: 0.05s (17.4%)
Actor update: 0.11s (42.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.006704
Q1 loss: 0.504398
Q2 loss: 0.504398
Current threshold: -30.9504
Global Scale Offset: 0.4312
Reward stats: mean=-0.0054, std=0.0701, count=85
----------------------------------------------
SAC Update - Actor Loss: -0.0067, Q1 Loss: 0.5044, Q2 Loss: 0.5044, Entropy: 0.2569, Mean TD Error: 0.0721, Threshold: -30.9504
Original likelihood: -206.23135375976562
Adjusted likelihood: -206.23135375976562
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 201.69131469726562
Projection step: 1, Loss: 212.30239868164062
Projection step: 2, Loss: 222.262451171875
Projection step: 3, Loss: 194.76705932617188
Projection step: 4, Loss: 208.49449157714844
Projection step: 5, Loss: 205.9508514404297
Projection step: 6, Loss: 215.16082763671875
Projection step: 7, Loss: 198.64254760742188
Projection step: 8, Loss: 212.46298217773438
Projection step: 9, Loss: 210.74716186523438
Projection step: 10, Loss: 217.72671508789062
Projection step: 11, Loss: 203.15057373046875
Projection step: 12, Loss: 214.2724151611328
Projection step: 13, Loss: 220.85926818847656
Projection step: 14, Loss: 210.69766235351562
Projection step: 15, Loss: 208.919189453125
Projection step: 16, Loss: 213.16302490234375
Projection step: 17, Loss: 203.277099609375
Projection step: 18, Loss: 208.939453125
Projection step: 19, Loss: 220.84991455078125
Projection step: 20, Loss: 211.97433471679688
Projection step: 21, Loss: 211.19912719726562
Projection step: 22, Loss: 216.08135986328125
Projection step: 23, Loss: 216.51661682128906
Projection step: 24, Loss: 213.60562133789062
Final likelihood: tensor([-213.1535, -246.0970, -258.0988, -173.9770, -161.0948, -175.0920,
        -214.5007, -190.0562, -218.0800, -211.7562, -183.6284, -258.4960,
        -209.2571, -179.8006, -209.5016, -196.8094])
Final projection likelihood: -206.2124
1 mode projection failed, trying anyway
New goal: tensor([-0.2779,  0.4351,  0.5983,  1.1165, -0.2863, -0.0427,  1.0099,  1.2670,
         1.4748,  0.4445,  0.2789,  1.4005,  0.3464,  0.1502, -0.4791],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[-0.0002]], device='cuda:0') tensor([[1.6543e-05]], device='cuda:0')
Original likelihood: -233.6413116455078
Adjusted likelihood: -233.6413116455078
Likelihood residual: 0.0
Original likelihood: -193.33975219726562
Adjusted likelihood: -193.33975219726562
Likelihood residual: 0.0
{'index': 193.33975219726562, 'thumb_middle': 233.6413116455078}
Current yaw: tensor([ 0.3462,  0.1500, -0.5456], device='cuda:0')
19 index
tensor([-0.2718,  0.4333,  0.5973,  1.0984, -0.2838, -0.0428,  0.9913,  1.2620,
         1.4995,  0.4396,  0.2762,  1.3710,  0.3462,  0.1500, -0.5456, -2.0191],
       device='cuda:0')
Solve time for step 1 10.79527872800827
Current ori: tensor([ 0.3462,  0.1500, -0.5456], device='cuda:0')
Middle force: tensor([0.6036, 0.5483, 0.5239, 0.5933], device='cuda:0')
Thumb force: tensor([0.5733, 0.5756, 0.5306, 0.5804], device='cuda:0')
tensor([-0.3279,  0.1436,  0.5627,  1.0811, -0.2698, -0.0036,  0.9504,  1.3906,
         1.5000,  0.4720,  0.2862,  1.3420,  0.3653,  0.1490, -0.4910, -0.0607],
       device='cuda:0')
Solve time for step 2 4.318330641021021
Current ori: tensor([ 0.3653,  0.1490, -0.4910], device='cuda:0')
Middle force: tensor([0.5532, 0.5203, 0.5883], device='cuda:0')
Thumb force: tensor([0.5755, 0.5296, 0.5756], device='cuda:0')
tensor([-0.3331,  0.1031,  0.5790,  1.0918, -0.2764, -0.0196,  1.0013,  1.2926,
         1.5000,  0.4801,  0.2775,  1.3177,  0.3685,  0.1533, -0.4873,  0.4644],
       device='cuda:0')
Solve time for step 3 4.326253964973148
Current ori: tensor([ 0.3685,  0.1533, -0.4873], device='cuda:0')
Middle force: tensor([0.5132, 0.5506], device='cuda:0')
Thumb force: tensor([0.5007, 0.5645], device='cuda:0')
tensor([-0.2755,  0.1760,  0.6093,  1.1118, -0.2792, -0.0178,  0.9738,  1.2611,
         1.5000,  0.4596,  0.2870,  1.2877,  0.3732,  0.1616, -0.5018,  0.5895],
       device='cuda:0')
Solve time for step 4 3.720452398993075
Current ori: tensor([ 0.3732,  0.1616, -0.5018], device='cuda:0')
Middle force: tensor([0.5446], device='cuda:0')
Thumb force: tensor([0.5605], device='cuda:0')
Storing RECOVERY transition: reward=0.0849 (scaled=0.0849), steps=1
Reward stats updated: mean -0.0054 -> -0.0044, std: 0.0703
Collected 86 transitions for RL
SAC Update 1/5: Actor Loss=-0.0050, Q1 Loss=0.5542, Q2 Loss=0.5542, Entropy=0.3363, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0126
SAC Update 2/5: Actor Loss=-0.0494, Q1 Loss=0.4952, Q2 Loss=0.4952, Entropy=0.0060, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0650
SAC Update 3/5: Actor Loss=-0.0065, Q1 Loss=0.3498, Q2 Loss=0.3498, Entropy=0.3461, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0675
SAC Update 4/5: Actor Loss=-0.0001, Q1 Loss=0.4917, Q2 Loss=0.4917, Entropy=0.0264, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0823
SAC Update 5/5: Actor Loss=-0.1269, Q1 Loss=0.5417, Q2 Loss=0.5417, Entropy=0.0099, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1655

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.3%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.4%)
Actor update: 0.08s (40.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.037568
Q1 loss: 0.486519
Q2 loss: 0.486519
Current threshold: -30.9970
Global Scale Offset: 0.4155
Reward stats: mean=-0.0044, std=0.0703, count=86
----------------------------------------------
SAC Update - Actor Loss: -0.0376, Q1 Loss: 0.4865, Q2 Loss: 0.4865, Entropy: 0.1449, Mean TD Error: 0.0786, Threshold: -30.9970
Original likelihood: -289.6027526855469
Adjusted likelihood: -289.6027526855469
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 282.11273193359375
Projection step: 1, Loss: 280.3759765625
Projection step: 2, Loss: 279.21563720703125
Projection step: 3, Loss: 274.874267578125
Projection step: 4, Loss: 271.41497802734375
Projection step: 5, Loss: 293.89794921875
Projection step: 6, Loss: 283.1416015625
Projection step: 7, Loss: 282.45770263671875
Projection step: 8, Loss: 282.87091064453125
Projection step: 9, Loss: 271.44873046875
Projection step: 10, Loss: 276.6254577636719
Projection step: 11, Loss: 264.95001220703125
Projection step: 12, Loss: 270.7081604003906
Projection step: 13, Loss: 282.28631591796875
Projection step: 14, Loss: 267.7214050292969
Projection step: 15, Loss: 270.3978271484375
Projection step: 16, Loss: 275.28021240234375
Projection step: 17, Loss: 278.4707336425781
Projection step: 18, Loss: 279.77685546875
Projection step: 19, Loss: 277.1668701171875
Projection step: 20, Loss: 280.234375
Projection step: 21, Loss: 271.50830078125
Projection step: 22, Loss: 277.17236328125
Projection step: 23, Loss: 285.9112243652344
Projection step: 24, Loss: 271.5140380859375
Final likelihood: tensor([-264.6570, -194.5075, -311.3266, -283.9368, -275.7225, -246.0201,
        -323.0132, -268.6187, -338.5197, -255.2995, -232.4419, -280.9968,
        -260.5229, -251.8887, -271.2638, -245.4393])
Final projection likelihood: -269.0110
1 mode projection failed, trying anyway
New goal: tensor([-0.2859,  0.4330,  0.6263,  1.1357, -0.2822, -0.0187,  0.9490,  1.1991,
         1.4842,  0.4594,  0.2966,  1.3197,  0.3842,  0.1790, -0.5532],
       device='cuda:0')
tensor([[0.0089]], device='cuda:0') tensor([[-0.0062]], device='cuda:0') tensor([[0.0022]], device='cuda:0')
Original likelihood: -286.1038818359375
Adjusted likelihood: -286.1038818359375
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 286.1038818359375}
Current yaw: tensor([ 0.3839,  0.1792, -0.5750], device='cuda:0')
20 thumb_middle
tensor([-0.2828,  0.4325,  0.6300,  1.1240, -0.2803, -0.0179,  0.9388,  1.1937,
         1.5000,  0.4529,  0.2955,  1.3006,  0.3839,  0.1792, -0.5750,  0.8331],
       device='cuda:0')
Solve time for step 1 8.856514017970767
Current ori: tensor([ 0.3839,  0.1792, -0.5750], device='cuda:0')
Index force: tensor([0.5639, 0.5246, 0.5185, 0.5962], device='cuda:0')
tensor([-0.3058,  0.4992,  0.6033,  1.1284, -0.1978, -0.1960,  0.8707,  1.1209,
         1.4756,  0.4459,  0.2534,  1.3119,  0.4145,  0.2065, -0.5879,  5.6008],
       device='cuda:0')
Solve time for step 2 3.5992996339919046
Current ori: tensor([ 0.4145,  0.2065, -0.5879], device='cuda:0')
Index force: tensor([0.5234, 0.5117, 0.5870], device='cuda:0')
tensor([-0.3511,  0.5668,  0.6267,  1.1372, -0.1954, -0.1960,  0.8858,  1.1415,
         1.4819,  0.4563,  0.2630,  1.3280,  0.4146,  0.2067, -0.5873, -3.4253],
       device='cuda:0')
Solve time for step 3 3.4939759959816
Current ori: tensor([ 0.4146,  0.2067, -0.5873], device='cuda:0')
Index force: tensor([0.5108, 0.5728], device='cuda:0')
tensor([-0.3820,  0.6191,  0.6321,  1.1351, -0.2226, -0.1962,  0.8957,  1.1666,
         1.4865,  0.4669,  0.2693,  1.3169,  0.4144,  0.2066, -0.5886, -4.3049],
       device='cuda:0')
Solve time for step 4 3.2735832459875382
Current ori: tensor([ 0.4144,  0.2066, -0.5886], device='cuda:0')
Index force: tensor([0.5532], device='cuda:0')
Storing RECOVERY transition: reward=0.0542 (scaled=0.0542), steps=1
Reward stats updated: mean -0.0044 -> -0.0037, std: 0.0702
Collected 87 transitions for RL
SAC Update 1/5: Actor Loss=-0.0668, Q1 Loss=0.5441, Q2 Loss=0.5441, Entropy=0.0045, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1137
SAC Update 2/5: Actor Loss=-0.0116, Q1 Loss=0.4325, Q2 Loss=0.4325, Entropy=0.3105, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0638
SAC Update 3/5: Actor Loss=-0.0699, Q1 Loss=0.3987, Q2 Loss=0.3987, Entropy=0.0037, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0985
SAC Update 4/5: Actor Loss=-0.0036, Q1 Loss=0.4536, Q2 Loss=0.4536, Entropy=0.3350, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1007
SAC Update 5/5: Actor Loss=-0.0042, Q1 Loss=0.4473, Q2 Loss=0.4473, Entropy=0.3419, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0076

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.5%)
Q1 update: 0.04s (18.5%)
Q2 update: 0.04s (19.1%)
Actor update: 0.08s (40.2%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.031238
Q1 loss: 0.455223
Q2 loss: 0.455223
Current threshold: -31.0552
Global Scale Offset: 0.3946
Reward stats: mean=-0.0037, std=0.0702, count=87
----------------------------------------------
SAC Update - Actor Loss: -0.0312, Q1 Loss: 0.4552, Q2 Loss: 0.4552, Entropy: 0.1991, Mean TD Error: 0.0768, Threshold: -31.0552
Original likelihood: -332.13592529296875
Adjusted likelihood: -332.13592529296875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 5
Loaded trajectory sampler
Current yaw: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
Current yaw: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
1 turn
Sampling time 3.9007715730112977
tensor([ 1.3942e-01,  6.2413e-01,  5.4466e-01,  5.9316e-01, -1.0719e-01,
         5.1486e-01,  9.2985e-01,  8.6782e-01,  1.2323e+00,  2.8186e-01,
         2.5891e-01,  1.1671e+00, -9.3357e-04,  1.4822e-02, -2.8876e-02,
         2.6114e-01], device='cuda:0')
Original likelihood: -17.125333786010742
Adjusted likelihood: -17.125333786010742
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.05584354698658
Current ori: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
Middle force: tensor([2.0868, 1.2078, 1.2552, 0.5526, 0.8600, 1.0755, 1.0218, 0.5228, 1.1172,
        0.5798, 0.5378, 0.6017], device='cuda:0')
Thumb force: tensor([1.0567, 0.5753, 1.5067, 0.7812, 0.8038, 1.3167, 0.5401, 0.6155, 0.7861,
        1.8136, 0.5438, 0.5914], device='cuda:0')
Index force: tensor([0.7459, 0.5002, 0.5570, 0.5500, 0.8321, 1.1005, 0.5175, 0.5901, 0.8037,
        0.5778, 0.5736, 0.6071], device='cuda:0')
Storing NORMAL transition: reward=0.0240 (scaled=0.0240), steps=1
Reward stats updated: mean -0.0037 -> -0.0034, std: 0.0699
Collected 88 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.2954, Q2 Loss=0.2954, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0425
SAC Update 2/5: Actor Loss=-0.0481, Q1 Loss=0.4260, Q2 Loss=0.4260, Entropy=0.0268, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1211
SAC Update 3/5: Actor Loss=-0.0756, Q1 Loss=0.4749, Q2 Loss=0.4749, Entropy=0.0027, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1622
SAC Update 4/5: Actor Loss=-0.0038, Q1 Loss=0.4646, Q2 Loss=0.4646, Entropy=0.3290, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1168
SAC Update 5/5: Actor Loss=-0.0723, Q1 Loss=0.4761, Q2 Loss=0.4761, Entropy=0.0018, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0924

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.3%)
Q1 update: 0.06s (19.9%)
Q2 update: 0.06s (20.0%)
Actor update: 0.12s (42.0%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.039952
Q1 loss: 0.427411
Q2 loss: 0.427411
Current threshold: -31.1069
Global Scale Offset: 0.3759
Reward stats: mean=-0.0034, std=0.0699, count=88
----------------------------------------------
SAC Update - Actor Loss: -0.0400, Q1 Loss: 0.4274, Q2 Loss: 0.4274, Entropy: 0.0720, Mean TD Error: 0.1070, Threshold: -31.1069
tensor([ 0.1003,  0.5523,  0.6165,  0.5717, -0.1245,  0.4992,  0.8420,  1.0115,
         1.2800,  0.2333,  0.2721,  1.1594,  0.0124,  0.0269, -0.0535,  0.4211],
       device='cuda:0')
Original likelihood: -20.046611785888672
Adjusted likelihood: -20.046611785888672
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.755149934033398
Current ori: tensor([ 0.0124,  0.0269, -0.0535], device='cuda:0')
Middle force: tensor([1.2077, 1.2610, 0.5567, 0.8624, 1.0769, 1.0148, 0.5227, 1.1129, 0.5800,
        0.5369, 0.6000], device='cuda:0')
Thumb force: tensor([0.5819, 1.4474, 0.7522, 0.7922, 1.2785, 0.5371, 0.5993, 0.7646, 1.7720,
        0.5408, 0.5865], device='cuda:0')
Index force: tensor([0.5002, 0.5607, 0.5518, 0.8107, 1.0816, 0.5164, 0.5924, 0.7947, 0.5756,
        0.5699, 0.6026], device='cuda:0')
Storing NORMAL transition: reward=0.0259 (scaled=0.0259), steps=1
Reward stats updated: mean -0.0034 -> -0.0030, std: 0.0695
Collected 89 transitions for RL
SAC Update 1/5: Actor Loss=-0.0794, Q1 Loss=0.3660, Q2 Loss=0.3660, Entropy=0.0016, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0846
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.3825, Q2 Loss=0.3825, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0314
SAC Update 3/5: Actor Loss=-0.0824, Q1 Loss=0.3028, Q2 Loss=0.3028, Entropy=0.0012, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0298
SAC Update 4/5: Actor Loss=-0.0840, Q1 Loss=0.4098, Q2 Loss=0.4098, Entropy=0.0011, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0589
SAC Update 5/5: Actor Loss=-0.0858, Q1 Loss=0.3998, Q2 Loss=0.3998, Entropy=0.0009, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0795

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (16.2%)
Q1 update: 0.04s (19.5%)
Q2 update: 0.05s (20.2%)
Actor update: 0.09s (40.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.066321
Q1 loss: 0.372202
Q2 loss: 0.372202
Current threshold: -31.1701
Global Scale Offset: 0.3528
Reward stats: mean=-0.0030, std=0.0695, count=89
----------------------------------------------
SAC Update - Actor Loss: -0.0663, Q1 Loss: 0.3722, Q2 Loss: 0.3722, Entropy: 0.0010, Mean TD Error: 0.0569, Threshold: -31.1701
tensor([ 0.0665,  0.6085,  0.5209,  0.5379, -0.1808,  0.5324,  0.8181,  1.0248,
         1.3206,  0.2725,  0.2461,  1.1164, -0.0082,  0.0477, -0.0810,  0.3180],
       device='cuda:0')
Original likelihood: -26.19394874572754
Adjusted likelihood: -26.19394874572754
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.122803281992674
Current ori: tensor([-0.0082,  0.0477, -0.0810], device='cuda:0')
Middle force: tensor([1.2279, 0.5523, 0.8607, 1.0556, 1.0000, 0.5199, 1.1013, 0.5732, 0.5355,
        0.5971], device='cuda:0')
Thumb force: tensor([1.4276, 0.7473, 0.7834, 1.2763, 0.5367, 0.6050, 0.7587, 1.7544, 0.5395,
        0.5830], device='cuda:0')
Index force: tensor([0.5557, 0.5508, 0.7954, 1.0466, 0.5152, 0.5847, 0.7796, 0.5678, 0.5657,
        0.5987], device='cuda:0')
Storing NORMAL transition: reward=0.1641 (scaled=0.1641), steps=1
Reward stats updated: mean -0.0030 -> -0.0012, std: 0.0713
Collected 90 transitions for RL
SAC Update 1/5: Actor Loss=-0.0723, Q1 Loss=0.4382, Q2 Loss=0.4382, Entropy=0.0008, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1497
SAC Update 2/5: Actor Loss=-0.0030, Q1 Loss=0.4152, Q2 Loss=0.4152, Entropy=0.3126, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1109
SAC Update 3/5: Actor Loss=-0.0160, Q1 Loss=0.2988, Q2 Loss=0.2988, Entropy=0.2511, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1050
SAC Update 4/5: Actor Loss=-0.0696, Q1 Loss=0.3862, Q2 Loss=0.3862, Entropy=0.0004, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0763
SAC Update 5/5: Actor Loss=-0.0221, Q1 Loss=0.4043, Q2 Loss=0.4043, Entropy=0.6141, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0177

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.8%)
Q1 update: 0.04s (19.6%)
Q2 update: 0.04s (19.5%)
Actor update: 0.09s (40.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.036633
Q1 loss: 0.388530
Q2 loss: 0.388530
Current threshold: -31.2373
Global Scale Offset: 0.3302
Reward stats: mean=-0.0012, std=0.0713, count=90
----------------------------------------------
SAC Update - Actor Loss: -0.0366, Q1 Loss: 0.3885, Q2 Loss: 0.3885, Entropy: 0.2358, Mean TD Error: 0.0919, Threshold: -31.2373
tensor([ 0.0385,  0.5708,  0.5432,  0.5471, -0.1612,  0.4629,  0.9158,  1.0837,
         1.2725,  0.3224,  0.2881,  1.0507,  0.0071,  0.0296, -0.2441,  0.9491],
       device='cuda:0')
Original likelihood: -20.92955780029297
Adjusted likelihood: -20.92955780029297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.984564993996173
Current ori: tensor([ 0.0071,  0.0296, -0.2441], device='cuda:0')
Middle force: tensor([0.5518, 0.8530, 1.1177, 0.9934, 0.5241, 1.1257, 0.5810, 0.5358, 0.5954],
       device='cuda:0')
Thumb force: tensor([0.7255, 0.7739, 1.1690, 0.5331, 0.5780, 0.7117, 1.7043, 0.5363, 0.5783],
       device='cuda:0')
Index force: tensor([0.5480, 0.7838, 1.0398, 0.5141, 0.5863, 0.7744, 0.5641, 0.5619, 0.5942],
       device='cuda:0')
Storing NORMAL transition: reward=0.0292 (scaled=0.0292), steps=1
Reward stats updated: mean -0.0012 -> -0.0008, std: 0.0710
Collected 91 transitions for RL
SAC Update 1/5: Actor Loss=-0.0730, Q1 Loss=0.3931, Q2 Loss=0.3931, Entropy=0.0003, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1361
SAC Update 2/5: Actor Loss=-0.0164, Q1 Loss=0.3618, Q2 Loss=0.3618, Entropy=0.2458, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0511
SAC Update 3/5: Actor Loss=-0.1019, Q1 Loss=0.4133, Q2 Loss=0.4133, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1401
SAC Update 4/5: Actor Loss=-0.0783, Q1 Loss=0.3623, Q2 Loss=0.3623, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0651
SAC Update 5/5: Actor Loss=-0.0001, Q1 Loss=0.3304, Q2 Loss=0.3304, Entropy=0.0197, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0330

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.1%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.9%)
Actor update: 0.10s (39.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.053942
Q1 loss: 0.372179
Q2 loss: 0.372179
Current threshold: -31.3082
Global Scale Offset: 0.3095
Reward stats: mean=-0.0008, std=0.0710, count=91
----------------------------------------------
SAC Update - Actor Loss: -0.0539, Q1 Loss: 0.3722, Q2 Loss: 0.3722, Entropy: 0.0532, Mean TD Error: 0.0851, Threshold: -31.3082
tensor([ 1.7335e-01,  6.0356e-01,  5.5557e-01,  6.9003e-01, -7.1875e-02,
         4.0012e-01,  9.3137e-01,  1.0184e+00,  1.3165e+00,  2.6113e-01,
         2.1240e-01,  1.0992e+00,  9.6047e-04,  1.6060e-02, -2.7228e-01,
         5.3058e-01], device='cuda:0')
Original likelihood: -20.68981170654297
Adjusted likelihood: -20.68981170654297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.670528465008829
Current ori: tensor([ 0.0010,  0.0161, -0.2723], device='cuda:0')
Middle force: tensor([0.8335, 0.9829, 0.9645, 0.5154, 1.0439, 0.5623, 0.5314, 0.5887],
       device='cuda:0')
Thumb force: tensor([0.7647, 1.2668, 0.5337, 0.6078, 0.7637, 1.7002, 0.5357, 0.5774],
       device='cuda:0')
Index force: tensor([0.7769, 1.0174, 0.5134, 0.5779, 0.7643, 0.5632, 0.5628, 0.5920],
       device='cuda:0')
Storing NORMAL transition: reward=0.0430 (scaled=0.0430), steps=1
Reward stats updated: mean -0.0008 -> -0.0004, std: 0.0708
Collected 92 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.2842, Q2 Loss=0.2842, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0432
SAC Update 2/5: Actor Loss=-0.1110, Q1 Loss=0.2785, Q2 Loss=0.2785, Entropy=0.0001, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0668
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.3325, Q2 Loss=0.3325, Entropy=0.0096, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0179
SAC Update 4/5: Actor Loss=-0.0641, Q1 Loss=0.3037, Q2 Loss=0.3037, Entropy=0.0001, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0844
SAC Update 5/5: Actor Loss=-0.0194, Q1 Loss=0.3121, Q2 Loss=0.3121, Entropy=0.2055, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0518

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (19.7%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (39.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.038910
Q1 loss: 0.302197
Q2 loss: 0.302197
Current threshold: -31.3721
Global Scale Offset: 0.2915
Reward stats: mean=-0.0004, std=0.0708, count=92
----------------------------------------------
SAC Update - Actor Loss: -0.0389, Q1 Loss: 0.3022, Q2 Loss: 0.3022, Entropy: 0.0431, Mean TD Error: 0.0528, Threshold: -31.3721
tensor([ 0.2459,  0.6690,  0.5962,  0.6232, -0.0717,  0.3832,  0.8689,  1.0822,
         1.3800,  0.1757,  0.2050,  1.0934,  0.0138,  0.0295, -0.3171,  0.0653],
       device='cuda:0')
Original likelihood: -28.72726058959961
Adjusted likelihood: -28.72726058959961
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9997)
Solve time for step 6 4.686764385027345
Current ori: tensor([ 0.0138,  0.0295, -0.3171], device='cuda:0')
Middle force: tensor([0.9004, 0.9437, 0.5120, 0.9913, 0.5535, 0.5291, 0.5840],
       device='cuda:0')
Thumb force: tensor([1.3276, 0.5335, 0.6391, 0.8220, 1.6983, 0.5347, 0.5753],
       device='cuda:0')
Index force: tensor([0.9581, 0.5128, 0.5625, 0.7281, 0.5545, 0.5609, 0.5893],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0221 (scaled=-0.0221), steps=1
Reward stats updated: mean -0.0004 -> -0.0006, std: 0.0704
Collected 93 transitions for RL
SAC Update 1/5: Actor Loss=-0.0025, Q1 Loss=0.2855, Q2 Loss=0.2855, Entropy=0.2638, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0822
SAC Update 2/5: Actor Loss=-0.1226, Q1 Loss=0.2662, Q2 Loss=0.2662, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0683
SAC Update 3/5: Actor Loss=-0.0206, Q1 Loss=0.2666, Q2 Loss=0.2666, Entropy=0.1907, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0533
SAC Update 4/5: Actor Loss=-0.1276, Q1 Loss=0.3580, Q2 Loss=0.3580, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0977
SAC Update 5/5: Actor Loss=-0.0194, Q1 Loss=0.2953, Q2 Loss=0.2953, Entropy=0.2061, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0265

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.5%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (19.0%)
Actor update: 0.09s (38.5%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.058513
Q1 loss: 0.294324
Q2 loss: 0.294324
Current threshold: -31.4364
Global Scale Offset: 0.2749
Reward stats: mean=-0.0006, std=0.0704, count=93
----------------------------------------------
SAC Update - Actor Loss: -0.0585, Q1 Loss: 0.2943, Q2 Loss: 0.2943, Entropy: 0.1321, Mean TD Error: 0.0656, Threshold: -31.4364
tensor([ 0.2311,  0.6288,  0.5802,  0.6948, -0.1291,  0.3590,  0.9469,  1.0403,
         1.3947,  0.2631,  0.1932,  1.0539,  0.0189,  0.0467, -0.2977,  0.1371],
       device='cuda:0')
Original likelihood: -33.9295654296875
Adjusted likelihood: -33.9295654296875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0004)
State is out of distribution
Projection step: 0, Loss: 36.794517517089844
Projection step: 1, Loss: 35.627418518066406
Projection step: 2, Loss: 33.65314483642578
Projection step: 3, Loss: 35.00566101074219
Projection step: 4, Loss: 32.305564880371094
Projection step: 5, Loss: 31.109086990356445
Projection step: 6, Loss: 31.651229858398438
Projection step: 7, Loss: 31.934288024902344
Projection step: 8, Loss: 30.3421630859375
Projection step: 9, Loss: 30.75305938720703
Projection step: 10, Loss: 28.56073760986328
Projection step: 11, Loss: 28.77086639404297
Projection step: 12, Loss: 28.017196655273438
Projection step: 13, Loss: 27.434127807617188
Projection step: 14, Loss: 26.880237579345703
Projection step: 15, Loss: 26.998884201049805
Projection step: 16, Loss: 25.902938842773438
Projection step: 17, Loss: 26.045635223388672
Projection step: 18, Loss: 25.614601135253906
Projection step: 19, Loss: 24.937786102294922
Projection step: 20, Loss: 24.13481903076172
Projection step: 21, Loss: 24.381980895996094
Projection step: 22, Loss: 23.49602699279785
Projection step: 23, Loss: 23.87923812866211
Projection step: 24, Loss: 23.900436401367188
Final likelihood: tensor([-21.8443, -19.3423, -20.0857, -24.1979, -19.0730, -26.0378, -24.5006,
        -24.7540, -23.6047, -19.0630, -38.6424, -23.5039, -20.6935, -24.1450,
        -23.0103, -27.6765])
Final projection likelihood: -23.7609
1 mode projection succeeded
New goal: tensor([ 0.1885,  0.6290,  0.5712,  0.6333, -0.1080,  0.3933,  0.8727,  1.0482,
         1.3656,  0.2478,  0.1727,  1.0654,  0.0256,  0.0310, -0.6737],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -35.194358825683594
Adjusted likelihood: -35.194358825683594
Likelihood residual: 0.0
Original likelihood: -28.17287826538086
Adjusted likelihood: -28.17287826538086
Likelihood residual: 0.0
{'index': 28.17287826538086, 'thumb_middle': 35.194358825683594}
Current yaw: tensor([ 0.0189,  0.0467, -0.2977], device='cuda:0')
2 index
tensor([ 0.2311,  0.6288,  0.5802,  0.6948, -0.1291,  0.3590,  0.9469,  1.0403,
         1.3947,  0.2631,  0.1932,  1.0539,  0.0189,  0.0467, -0.2977,  0.1371],
       device='cuda:0')
Solve time for step 1 10.065500528959092
Current ori: tensor([ 0.0189,  0.0467, -0.2977], device='cuda:0')
Middle force: tensor([0.5508, 0.5031, 0.5072, 0.5043], device='cuda:0')
Thumb force: tensor([0.6452, 0.5359, 0.5007, 0.6109], device='cuda:0')
tensor([ 0.2397,  0.5716,  0.5237,  0.6197, -0.1320,  0.3895,  0.9063,  1.0673,
         1.4121,  0.2492,  0.1708,  1.0481,  0.0082,  0.0465, -0.3127,  0.2861],
       device='cuda:0')
Solve time for step 2 4.027631914010271
Current ori: tensor([ 0.0082,  0.0465, -0.3127], device='cuda:0')
Middle force: tensor([0.5027, 0.5064, 0.5037], device='cuda:0')
Thumb force: tensor([0.5327, 0.5005, 0.6088], device='cuda:0')
tensor([ 0.2366,  0.5726,  0.5206,  0.6116, -0.1306,  0.3976,  0.8962,  1.0679,
         1.4084,  0.2610,  0.1683,  1.0473,  0.0042,  0.0463, -0.3233,  0.4120],
       device='cuda:0')
Solve time for step 3 3.88994055299554
Current ori: tensor([ 0.0042,  0.0463, -0.3233], device='cuda:0')
Middle force: tensor([0.5058, 0.5031], device='cuda:0')
Thumb force: tensor([0.5003, 0.6045], device='cuda:0')
tensor([ 2.3567e-01,  5.7311e-01,  5.1853e-01,  6.0646e-01, -1.2213e-01,
         4.1446e-01,  8.8438e-01,  1.0522e+00,  1.4240e+00,  2.2952e-01,
         1.4620e-01,  1.0553e+00, -1.1622e-03,  4.2036e-02, -3.1026e-01,
         5.0581e-01], device='cuda:0')
Solve time for step 4 3.769543930015061
Current ori: tensor([-0.0012,  0.0420, -0.3103], device='cuda:0')
Middle force: tensor([0.5029], device='cuda:0')
Thumb force: tensor([0.5937], device='cuda:0')
Storing RECOVERY transition: reward=0.0324 (scaled=0.0054), steps=6
Reward stats updated: mean -0.0006 -> -0.0005, std: 0.0701
Collected 94 transitions for RL
SAC Update 1/5: Actor Loss=-0.0001, Q1 Loss=0.3340, Q2 Loss=0.3340, Entropy=0.0166, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0617
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.2647, Q2 Loss=0.2647, Entropy=0.0082, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0939
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.3054, Q2 Loss=0.3054, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0645
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.2922, Q2 Loss=0.2922, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0757
SAC Update 5/5: Actor Loss=-0.0239, Q1 Loss=0.2575, Q2 Loss=0.2575, Entropy=0.2566, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0470

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.0%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (19.2%)
Actor update: 0.12s (43.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.004797
Q1 loss: 0.290738
Q2 loss: 0.290738
Current threshold: -31.4888
Global Scale Offset: 0.2628
Reward stats: mean=-0.0005, std=0.0701, count=94
----------------------------------------------
SAC Update - Actor Loss: -0.0048, Q1 Loss: 0.2907, Q2 Loss: 0.2907, Entropy: 0.0563, Mean TD Error: 0.0685, Threshold: -31.4888
Original likelihood: -31.50680160522461
Adjusted likelihood: -31.50680160522461
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.4900)
State is out of distribution
Projection step: 0, Loss: 30.32093048095703
Projection step: 1, Loss: 29.839256286621094
Projection step: 2, Loss: 29.822017669677734
Projection step: 3, Loss: 29.513320922851562
Projection step: 4, Loss: 29.294418334960938
Projection step: 5, Loss: 28.653724670410156
Projection step: 6, Loss: 30.187793731689453
Projection step: 7, Loss: 30.056596755981445
Projection step: 8, Loss: 27.772619247436523
Projection step: 9, Loss: 26.09380340576172
Projection step: 10, Loss: 26.750751495361328
Projection step: 11, Loss: 27.588287353515625
Projection step: 12, Loss: 29.371139526367188
Projection step: 13, Loss: 23.44170379638672
Projection step: 14, Loss: 24.585601806640625
Projection step: 15, Loss: 25.17425537109375
Projection step: 16, Loss: 24.26938247680664
Projection step: 17, Loss: 23.242597579956055
Projection step: 18, Loss: 22.49484634399414
Projection step: 19, Loss: 21.481510162353516
Projection step: 20, Loss: 21.245037078857422
Projection step: 21, Loss: 21.893964767456055
Projection step: 22, Loss: 21.048059463500977
Projection step: 23, Loss: 19.99167251586914
Projection step: 24, Loss: 19.314075469970703
Final likelihood: tensor([-18.8044, -21.1592, -19.3490, -16.5755, -21.8006, -21.3852, -19.5113,
        -17.7750, -16.7488, -18.7284, -17.8920, -16.2015, -18.0806, -20.4692,
        -16.8100, -17.8767])
Final projection likelihood: -18.6980
1 mode projection succeeded
New goal: tensor([ 0.1291,  0.6135,  0.5273,  0.6117, -0.1033,  0.4415,  0.8382,  1.0407,
         1.3542,  0.2867,  0.1332,  1.0883, -0.0069,  0.0262, -1.9138],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -29.04488182067871
Adjusted likelihood: -29.04488182067871
Likelihood residual: 0.0
Original likelihood: -20.757389068603516
Adjusted likelihood: -20.757389068603516
Likelihood residual: 0.0
{'index': 20.757389068603516, 'thumb_middle': 29.04488182067871}
Current yaw: tensor([-0.0007,  0.0407, -0.3289], device='cuda:0')
3 index
tensor([ 1.8330e-01,  6.2569e-01,  5.6005e-01,  6.2845e-01, -1.1993e-01,
         4.0989e-01,  8.8921e-01,  1.0603e+00,  1.4062e+00,  2.6406e-01,
         1.4856e-01,  1.0660e+00, -6.6767e-04,  4.0707e-02, -3.2889e-01,
         6.2107e-01], device='cuda:0')
Solve time for step 1 10.46883071900811
Current ori: tensor([-0.0007,  0.0407, -0.3289], device='cuda:0')
Middle force: tensor([0.5249, 0.5747, 0.5660, 0.5471], device='cuda:0')
Thumb force: tensor([0.6237, 0.5930, 0.5267, 0.5058], device='cuda:0')
tensor([ 0.1832,  0.5616,  0.4871,  0.5930, -0.0898,  0.4361,  0.8730,  1.0709,
         1.3860,  0.2819,  0.1254,  1.0854, -0.0051,  0.0217, -0.3353,  0.9157],
       device='cuda:0')
Solve time for step 2 4.635770060005598
Current ori: tensor([-0.0051,  0.0217, -0.3353], device='cuda:0')
Middle force: tensor([0.5680, 0.5617, 0.5436], device='cuda:0')
Thumb force: tensor([0.5843, 0.5245, 0.5048], device='cuda:0')
tensor([ 0.1775,  0.5611,  0.4806,  0.5886, -0.0945,  0.4384,  0.8663,  1.0700,
         1.3871,  0.2832,  0.1315,  1.0776, -0.0063,  0.0248, -0.3321,  1.0926],
       device='cuda:0')
Solve time for step 3 4.2694156329962425
Current ori: tensor([-0.0063,  0.0248, -0.3321], device='cuda:0')
Middle force: tensor([0.5936, 0.5053], device='cuda:0')
Thumb force: tensor([0.5839, 0.5037], device='cuda:0')
tensor([ 0.1766,  0.5604,  0.4786,  0.5873, -0.0887,  0.4549,  0.8540,  1.0524,
         1.3944,  0.2744,  0.1281,  1.0555, -0.0155,  0.0226, -0.3432,  1.1998],
       device='cuda:0')
Solve time for step 4 4.056244578969199
Current ori: tensor([-0.0155,  0.0226, -0.3432], device='cuda:0')
Middle force: tensor([0.5000], device='cuda:0')
Thumb force: tensor([0.5565], device='cuda:0')
Storing RECOVERY transition: reward=0.0556 (scaled=0.0093), steps=6
Reward stats updated: mean -0.0005 -> -0.0004, std: 0.0697
Collected 95 transitions for RL
SAC Update 1/5: Actor Loss=-0.0572, Q1 Loss=0.2309, Q2 Loss=0.2309, Entropy=0.0162, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0994
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.2799, Q2 Loss=0.2799, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1009
SAC Update 3/5: Actor Loss=-0.0950, Q1 Loss=0.2682, Q2 Loss=0.2682, Entropy=0.0161, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1166
SAC Update 4/5: Actor Loss=-0.0461, Q1 Loss=0.2034, Q2 Loss=0.2034, Entropy=0.0003, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0946
SAC Update 5/5: Actor Loss=-0.1531, Q1 Loss=0.2000, Q2 Loss=0.2000, Entropy=0.0014, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0394

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.9%)
Target Q: 0.04s (14.5%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.06s (20.5%)
Actor update: 0.12s (41.9%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.070299
Q1 loss: 0.236484
Q2 loss: 0.236484
Current threshold: -31.5407
Global Scale Offset: 0.2510
Reward stats: mean=-0.0004, std=0.0697, count=95
----------------------------------------------
SAC Update - Actor Loss: -0.0703, Q1 Loss: 0.2365, Q2 Loss: 0.2365, Entropy: 0.0068, Mean TD Error: 0.0902, Threshold: -31.5407
Original likelihood: -18.106048583984375
Adjusted likelihood: -18.106048583984375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0155,  0.0242, -0.3504], device='cuda:0')
4 turn
Sampling time 3.60656934697181
tensor([ 0.1204,  0.6211,  0.5210,  0.6088, -0.0930,  0.4536,  0.8559,  1.0564,
         1.3896,  0.2888,  0.1190,  1.0763, -0.0155,  0.0242, -0.3504,  1.2398],
       device='cuda:0')
Original likelihood: -17.396345138549805
Adjusted likelihood: -17.396345138549805
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.208520607964601
Current ori: tensor([-0.0155,  0.0242, -0.3504], device='cuda:0')
Middle force: tensor([0.5694, 0.5077, 0.5891, 0.6803, 2.0773, 0.5752, 0.6016, 0.5268, 0.9115,
        0.5763, 0.5794, 0.5143], device='cuda:0')
Thumb force: tensor([0.5515, 1.8981, 1.3968, 1.7960, 0.7670, 0.6063, 0.6262, 0.6758, 0.6201,
        0.6020, 0.8202, 0.6455], device='cuda:0')
Index force: tensor([0.6138, 0.8709, 0.6564, 0.8476, 0.8351, 0.5906, 0.6297, 0.6650, 0.5530,
        0.6170, 0.5376, 0.6175], device='cuda:0')
Storing NORMAL transition: reward=-0.0007 (scaled=-0.0007), steps=1
Reward stats updated: mean -0.0004 -> -0.0004, std: 0.0693
Collected 96 transitions for RL
SAC Update 1/5: Actor Loss=-0.0977, Q1 Loss=0.2549, Q2 Loss=0.2549, Entropy=0.0159, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1213
SAC Update 2/5: Actor Loss=-0.0283, Q1 Loss=0.1837, Q2 Loss=0.1837, Entropy=0.1117, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0205
SAC Update 3/5: Actor Loss=-0.0290, Q1 Loss=0.1818, Q2 Loss=0.1818, Entropy=0.1078, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0238
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.2073, Q2 Loss=0.2073, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0454
SAC Update 5/5: Actor Loss=-0.0563, Q1 Loss=0.1832, Q2 Loss=0.1832, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0722

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.1%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (19.3%)
Actor update: 0.09s (37.7%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.042274
Q1 loss: 0.202166
Q2 loss: 0.202166
Current threshold: -31.6048
Global Scale Offset: 0.2372
Reward stats: mean=-0.0004, std=0.0693, count=96
----------------------------------------------
SAC Update - Actor Loss: -0.0423, Q1 Loss: 0.2022, Q2 Loss: 0.2022, Entropy: 0.0471, Mean TD Error: 0.0566, Threshold: -31.6048
tensor([ 0.1005,  0.6206,  0.5116,  0.5892, -0.1838,  0.4384,  0.8611,  1.0170,
         1.3386,  0.3881,  0.1710,  0.9698, -0.0159,  0.0360, -0.3504,  1.1887],
       device='cuda:0')
Original likelihood: -26.573394775390625
Adjusted likelihood: -26.573394775390625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.608793817984406
Current ori: tensor([-0.0159,  0.0360, -0.3504], device='cuda:0')
Middle force: tensor([1.5498, 0.5430, 0.5189, 0.6438, 1.5342, 0.5481, 0.5000, 0.5353, 0.5330,
        0.5311, 0.5996], device='cuda:0')
Thumb force: tensor([0.8664, 0.5004, 0.8777, 0.8218, 1.0010, 0.7451, 0.5356, 1.0279, 0.8430,
        0.7867, 0.6046], device='cuda:0')
Index force: tensor([0.7251, 0.7479, 0.5897, 0.5074, 0.5243, 0.5043, 0.5372, 0.5282, 0.5554,
        0.5046, 0.5034], device='cuda:0')
Storing NORMAL transition: reward=0.1082 (scaled=0.1082), steps=1
Reward stats updated: mean -0.0004 -> 0.0007, std: 0.0698
Collected 97 transitions for RL
SAC Update 1/5: Actor Loss=-0.1727, Q1 Loss=0.2691, Q2 Loss=0.2691, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1333
SAC Update 2/5: Actor Loss=-0.1767, Q1 Loss=0.2093, Q2 Loss=0.2093, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1256
SAC Update 3/5: Actor Loss=-0.0856, Q1 Loss=0.2005, Q2 Loss=0.2005, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1059
SAC Update 4/5: Actor Loss=-0.0179, Q1 Loss=0.2001, Q2 Loss=0.2001, Entropy=0.2253, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0652
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.2341, Q2 Loss=0.2341, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0902

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.1%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.4%)
Actor update: 0.10s (43.6%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.090569
Q1 loss: 0.222642
Q2 loss: 0.222642
Current threshold: -31.6799
Global Scale Offset: 0.2213
Reward stats: mean=0.0007, std=0.0698, count=97
----------------------------------------------
SAC Update - Actor Loss: -0.0906, Q1 Loss: 0.2226, Q2 Loss: 0.2226, Entropy: 0.0451, Mean TD Error: 0.1040, Threshold: -31.6799
tensor([ 0.1340,  0.6100,  0.5390,  0.6322, -0.1116,  0.4645,  0.8956,  1.1421,
         1.4420,  0.2493,  0.1368,  0.9357, -0.0108,  0.0134, -0.4575,  1.4584],
       device='cuda:0')
Original likelihood: -19.73342514038086
Adjusted likelihood: -19.73342514038086
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.1654852330102585
Current ori: tensor([-0.0108,  0.0134, -0.4575], device='cuda:0')
Middle force: tensor([0.5783, 0.6993, 1.9862, 0.5717, 0.5931, 0.5279, 0.8927, 0.5742, 0.5722,
        0.5137], device='cuda:0')
Thumb force: tensor([1.3409, 1.7054, 0.7514, 0.5924, 0.6147, 0.6487, 0.6099, 0.5906, 0.8029,
        0.6289], device='cuda:0')
Index force: tensor([0.6431, 0.8140, 0.8170, 0.5826, 0.6200, 0.6507, 0.5469, 0.6066, 0.5335,
        0.6074], device='cuda:0')
Storing NORMAL transition: reward=0.1376 (scaled=0.1376), steps=1
Reward stats updated: mean 0.0007 -> 0.0021, std: 0.0708
Collected 98 transitions for RL
SAC Update 1/5: Actor Loss=-0.1947, Q1 Loss=0.2528, Q2 Loss=0.2528, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1385
SAC Update 2/5: Actor Loss=-0.1995, Q1 Loss=0.2488, Q2 Loss=0.2488, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1385
SAC Update 3/5: Actor Loss=-0.1268, Q1 Loss=0.2058, Q2 Loss=0.2058, Entropy=0.0150, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1213
SAC Update 4/5: Actor Loss=-0.1429, Q1 Loss=0.1868, Q2 Loss=0.1868, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0460
SAC Update 5/5: Actor Loss=-0.2156, Q1 Loss=0.2568, Q2 Loss=0.2568, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1760

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (18.0%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (18.9%)
Actor update: 0.11s (41.1%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.175896
Q1 loss: 0.230206
Q2 loss: 0.230206
Current threshold: -31.7665
Global Scale Offset: 0.2036
Reward stats: mean=0.0021, std=0.0708, count=98
----------------------------------------------
SAC Update - Actor Loss: -0.1759, Q1 Loss: 0.2302, Q2 Loss: 0.2302, Entropy: 0.0030, Mean TD Error: 0.1240, Threshold: -31.7665
tensor([ 0.0668,  0.4684,  0.6153,  0.5707,  0.0132,  0.4959,  0.9751,  1.0917,
         1.4024,  0.2838,  0.0703,  0.9167, -0.0306, -0.0628, -0.6037, -0.9459],
       device='cuda:0')
Original likelihood: -21.218677520751953
Adjusted likelihood: -21.218677520751953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.197729784005787
Current ori: tensor([-0.0306, -0.0628, -0.6037], device='cuda:0')
Middle force: tensor([0.5026, 0.5046, 0.9432, 0.5124, 1.0569, 0.5815, 0.7492, 0.5198, 0.6049],
       device='cuda:0')
Thumb force: tensor([0.5678, 0.7447, 0.8732, 1.0275, 0.7453, 0.5374, 0.5420, 0.5415, 0.5446],
       device='cuda:0')
Index force: tensor([0.7785, 0.6531, 0.6181, 0.5279, 0.5074, 0.5248, 0.5019, 0.6595, 0.5195],
       device='cuda:0')
Storing NORMAL transition: reward=0.0377 (scaled=0.0377), steps=1
Reward stats updated: mean 0.0021 -> 0.0024, std: 0.0706
Collected 99 transitions for RL
SAC Update 1/5: Actor Loss=-0.0391, Q1 Loss=0.1282, Q2 Loss=0.1282, Entropy=0.0489, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0273
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.2060, Q2 Loss=0.2060, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1126
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.1805, Q2 Loss=0.1805, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1276
SAC Update 4/5: Actor Loss=-0.0033, Q1 Loss=0.1441, Q2 Loss=0.1441, Entropy=0.2981, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1110
SAC Update 5/5: Actor Loss=-0.2289, Q1 Loss=0.2117, Q2 Loss=0.2117, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1580

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.9%)
Q1 update: 0.04s (19.5%)
Q2 update: 0.04s (19.1%)
Actor update: 0.09s (41.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.054269
Q1 loss: 0.174078
Q2 loss: 0.174078
Current threshold: -31.8424
Global Scale Offset: 0.1898
Reward stats: mean=0.0024, std=0.0706, count=99
----------------------------------------------
SAC Update - Actor Loss: -0.0543, Q1 Loss: 0.1741, Q2 Loss: 0.1741, Entropy: 0.0694, Mean TD Error: 0.1073, Threshold: -31.8424
tensor([ 2.7823e-02,  4.6603e-01,  6.5835e-01,  6.1025e-01,  1.3270e-03,
         5.2160e-01,  9.5354e-01,  1.1216e+00,  1.4438e+00,  2.0728e-01,
         6.8678e-02,  8.4951e-01, -4.4875e-02, -6.1591e-02, -6.4483e-01,
        -1.1760e+00], device='cuda:0')
Original likelihood: -22.56715202331543
Adjusted likelihood: -22.56715202331543
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.758881352026947
Current ori: tensor([-0.0449, -0.0616, -0.6448], device='cuda:0')
Middle force: tensor([1.8828, 0.6099, 0.5886, 0.6185, 0.8672, 0.6551, 0.5665, 0.5397],
       device='cuda:0')
Thumb force: tensor([0.7294, 0.5579, 0.6015, 0.5518, 0.5983, 0.5457, 0.7786, 0.5650],
       device='cuda:0')
Index force: tensor([0.7994, 0.5667, 0.6116, 0.5899, 0.5441, 0.5727, 0.5315, 0.5732],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0369 (scaled=-0.0369), steps=1
Reward stats updated: mean 0.0024 -> 0.0020, std: 0.0703
Collected 100 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.5040, Q2 Loss=2.5040, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1748
SAC Update 2/5: Actor Loss=-0.0460, Q1 Loss=0.8946, Q2 Loss=0.8946, Entropy=0.0280, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5865
SAC Update 3/5: Actor Loss=-0.2261, Q1 Loss=5.5615, Q2 Loss=5.5615, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5717
SAC Update 4/5: Actor Loss=-0.2223, Q1 Loss=5.4457, Q2 Loss=5.4457, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5717
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.8264, Q2 Loss=2.8264, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1632

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.7%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (19.6%)
Actor update: 0.09s (37.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.098882
Q1 loss: 3.446432
Q2 loss: 3.446432
Current threshold: -31.8966
Global Scale Offset: 0.1811
Reward stats: mean=0.0020, std=0.0703, count=100
----------------------------------------------
SAC Update - Actor Loss: -0.0989, Q1 Loss: 3.4464, Q2 Loss: 3.4464, Entropy: 0.0056, Mean TD Error: 1.2136, Threshold: -31.8966
tensor([ 0.0572,  0.4739,  0.5813,  0.5996, -0.0756,  0.6687,  0.9478,  1.1973,
         1.4070,  0.2512,  0.0048,  0.9358, -0.0463, -0.0978, -0.6175, -1.0470],
       device='cuda:0')
Original likelihood: -29.468584060668945
Adjusted likelihood: -29.468584060668945
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.60602042300161
Current ori: tensor([-0.0463, -0.0978, -0.6175], device='cuda:0')
Middle force: tensor([0.5849, 0.8240, 0.6820, 0.5669, 0.5419, 0.6722, 0.5741],
       device='cuda:0')
Thumb force: tensor([0.5340, 0.5346, 0.7234, 0.5878, 0.5838, 0.5462, 0.5087],
       device='cuda:0')
Index force: tensor([0.5063, 0.6066, 0.5461, 0.5706, 0.5507, 0.5822, 0.5728],
       device='cuda:0')
Storing NORMAL transition: reward=0.0244 (scaled=0.0244), steps=1
Reward stats updated: mean 0.0020 -> 0.0023, std: 0.0700
Collected 101 transitions for RL
SAC Update 1/5: Actor Loss=-0.0497, Q1 Loss=0.8632, Q2 Loss=0.8632, Entropy=0.0207, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5467
SAC Update 2/5: Actor Loss=-0.0535, Q1 Loss=0.5982, Q2 Loss=0.5982, Entropy=0.0151, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4252
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=4.3058, Q2 Loss=4.3058, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3712
SAC Update 4/5: Actor Loss=-0.1707, Q1 Loss=2.8454, Q2 Loss=2.8454, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2347
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.6213, Q2 Loss=0.6213, Entropy=0.0062, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4836

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.1%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.06s (20.8%)
Actor update: 0.11s (42.1%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.054785
Q1 loss: 1.846789
Q2 loss: 1.846789
Current threshold: -31.9446
Global Scale Offset: 0.1750
Reward stats: mean=0.0023, std=0.0700, count=101
----------------------------------------------
SAC Update - Actor Loss: -0.0548, Q1 Loss: 1.8468, Q2 Loss: 1.8468, Entropy: 0.0084, Mean TD Error: 0.8123, Threshold: -31.9446
tensor([ 0.1066,  0.4690,  0.5471,  0.6291,  0.0153,  0.7144,  0.8483,  1.1439,
         1.4637,  0.1078, -0.0187,  0.7954, -0.0927, -0.1231, -0.6721, -1.5354],
       device='cuda:0')
Original likelihood: -37.317657470703125
Adjusted likelihood: -37.317657470703125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.655059814453125
Projection step: 1, Loss: 36.7357177734375
Projection step: 2, Loss: 36.94783020019531
Projection step: 3, Loss: 36.4732666015625
Projection step: 4, Loss: 36.98780822753906
Projection step: 5, Loss: 36.75634002685547
Projection step: 6, Loss: 36.47563934326172
Projection step: 7, Loss: 35.9700813293457
Projection step: 8, Loss: 35.69992446899414
Projection step: 9, Loss: 34.50869369506836
Projection step: 10, Loss: 37.20113754272461
Projection step: 11, Loss: 35.30248260498047
Projection step: 12, Loss: 36.098854064941406
Projection step: 13, Loss: 35.345458984375
Projection step: 14, Loss: 35.223602294921875
Projection step: 15, Loss: 34.64973449707031
Projection step: 16, Loss: 35.96110534667969
Projection step: 17, Loss: 34.77275466918945
Projection step: 18, Loss: 34.70602798461914
Projection step: 19, Loss: 34.14027404785156
Projection step: 20, Loss: 34.38301086425781
Projection step: 21, Loss: 33.51811218261719
Projection step: 22, Loss: 33.88768005371094
Projection step: 23, Loss: 34.071144104003906
Projection step: 24, Loss: 35.191986083984375
Final likelihood: tensor([-32.0934, -32.5548, -39.0771, -34.3853, -33.5384, -32.4443, -32.2769,
        -32.4409, -36.6679, -32.6288, -34.5768, -33.3101, -32.7268, -33.3699,
        -32.0337, -32.0538])
Final projection likelihood: -33.5112
1 mode projection failed, trying anyway
New goal: tensor([ 0.1145,  0.4505,  0.6410,  0.6778,  0.0045,  0.6837,  0.8368,  1.1046,
         1.4734,  0.0691,  0.0110,  0.7812, -0.0905, -0.1177, -0.3490],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0029]], device='cuda:0') tensor([[0.0171]], device='cuda:0')
Original likelihood: -38.27424240112305
Adjusted likelihood: -38.27424240112305
Likelihood residual: 0.0
{'index': 38.27424240112305, 'thumb_middle': inf}
Current yaw: tensor([-0.0927, -0.1231, -0.6721], device='cuda:0')
5 index
tensor([ 0.1066,  0.4690,  0.5471,  0.6291,  0.0153,  0.7144,  0.8483,  1.1439,
         1.4637,  0.1078, -0.0187,  0.7954, -0.0927, -0.1231, -0.6721, -1.5354],
       device='cuda:0')
Solve time for step 1 10.33319804398343
Current ori: tensor([-0.0927, -0.1231, -0.6721], device='cuda:0')
Middle force: tensor([0.5388, 0.5263, 0.5036, 0.5282], device='cuda:0')
Thumb force: tensor([0.5776, 0.5015, 0.5113, 0.5024], device='cuda:0')
tensor([ 0.0879,  0.4630,  0.6130,  0.6580,  0.0305,  0.7309,  0.8576,  1.1128,
         1.4743,  0.0669, -0.0493,  0.8002, -0.1011, -0.1366, -0.6933, -2.5549],
       device='cuda:0')
Solve time for step 2 4.169094693032093
Current ori: tensor([-0.1011, -0.1366, -0.6933], device='cuda:0')
Middle force: tensor([0.5225, 0.5028, 0.5255], device='cuda:0')
Thumb force: tensor([0.5013, 0.5106, 0.5021], device='cuda:0')
tensor([ 0.0802,  0.4642,  0.6232,  0.6639,  0.0212,  0.7573,  0.8539,  1.1039,
         1.4750,  0.0577, -0.0559,  0.7904, -0.1098, -0.1413, -0.6764, -3.3639],
       device='cuda:0')
Solve time for step 3 4.092304933001287
Current ori: tensor([-0.1098, -0.1413, -0.6764], device='cuda:0')
Middle force: tensor([0.5021, 0.5231], device='cuda:0')
Thumb force: tensor([0.5096, 0.5019], device='cuda:0')
tensor([ 0.0769,  0.4703,  0.6308,  0.6676,  0.0144,  0.7854,  0.8615,  1.0910,
         1.4688,  0.0641, -0.0700,  0.7901, -0.1196, -0.1500, -0.6550, -3.4134],
       device='cuda:0')
Solve time for step 4 3.927562225027941
Current ori: tensor([-0.1196, -0.1500, -0.6550], device='cuda:0')
Middle force: tensor([0.5010], device='cuda:0')
Thumb force: tensor([0.5107], device='cuda:0')
Storing RECOVERY transition: reward=-0.0655 (scaled=-0.0109), steps=6
Reward stats updated: mean 0.0023 -> 0.0021, std: 0.0697
Collected 102 transitions for RL
SAC Update 1/5: Actor Loss=-0.0424, Q1 Loss=0.6763, Q2 Loss=0.6763, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9419
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.3439, Q2 Loss=2.3439, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0466
SAC Update 3/5: Actor Loss=-0.0045, Q1 Loss=0.2903, Q2 Loss=0.2903, Entropy=0.3270, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4162
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.6544, Q2 Loss=1.6544, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7374
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5979, Q2 Loss=1.5979, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9829

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.1%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (18.9%)
Actor update: 0.10s (39.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.009379
Q1 loss: 1.312544
Q2 loss: 1.312544
Current threshold: -31.9757
Global Scale Offset: 0.1714
Reward stats: mean=0.0021, std=0.0697, count=102
----------------------------------------------
SAC Update - Actor Loss: -0.0094, Q1 Loss: 1.3125, Q2 Loss: 1.3125, Entropy: 0.0654, Mean TD Error: 0.8250, Threshold: -31.9757
Original likelihood: -43.11882781982422
Adjusted likelihood: -43.11882781982422
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 42.7527961730957
Projection step: 1, Loss: 43.3481330871582
Projection step: 2, Loss: 42.48637390136719
Projection step: 3, Loss: 41.66951370239258
Projection step: 4, Loss: 42.792964935302734
Projection step: 5, Loss: 42.393096923828125
Projection step: 6, Loss: 42.325584411621094
Projection step: 7, Loss: 41.935569763183594
Projection step: 8, Loss: 42.15657043457031
Projection step: 9, Loss: 41.339683532714844
Projection step: 10, Loss: 41.86490249633789
Projection step: 11, Loss: 41.39887237548828
Projection step: 12, Loss: 41.36342239379883
Projection step: 13, Loss: 40.82083511352539
Projection step: 14, Loss: 40.59262466430664
Projection step: 15, Loss: 40.28644943237305
Projection step: 16, Loss: 40.756492614746094
Projection step: 17, Loss: 40.37681198120117
Projection step: 18, Loss: 41.2071533203125
Projection step: 19, Loss: 41.43288803100586
Projection step: 20, Loss: 41.071102142333984
Projection step: 21, Loss: 40.8109016418457
Projection step: 22, Loss: 39.85456466674805
Projection step: 23, Loss: 39.309600830078125
Projection step: 24, Loss: 40.53942108154297
Final likelihood: tensor([-37.2818, -39.2983, -40.7102, -41.9937, -41.6025, -39.0604, -38.3950,
        -40.1812, -39.0756, -37.1279, -43.5113, -41.4085, -40.2442, -37.9363,
        -41.5897, -39.6616])
Final projection likelihood: -39.9424
1 mode projection failed, trying anyway
New goal: tensor([ 0.0985,  0.5282,  0.6694,  0.6883,  0.0057,  0.7832,  0.8321,  1.1143,
         1.4725,  0.0663, -0.0435,  0.6920, -0.1339, -0.1559, -0.5787],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0022]], device='cuda:0')
Original likelihood: -51.556602478027344
Adjusted likelihood: -51.556602478027344
Likelihood residual: 0.0
Original likelihood: -40.568939208984375
Adjusted likelihood: -40.568939208984375
Likelihood residual: 0.0
{'index': 40.568939208984375, 'thumb_middle': 51.556602478027344}
Current yaw: tensor([-0.1349, -0.1589, -0.6318], device='cuda:0')
6 index
tensor([ 0.0761,  0.5641,  0.6891,  0.6941,  0.0091,  0.8182,  0.8647,  1.0824,
         1.4646,  0.0678, -0.0789,  0.7707, -0.1349, -0.1589, -0.6318, -3.1557],
       device='cuda:0')
Solve time for step 1 10.377815669984557
Current ori: tensor([-0.1349, -0.1589, -0.6318], device='cuda:0')
Middle force: tensor([0.5267, 0.5489, 0.5191, 0.5091], device='cuda:0')
Thumb force: tensor([0.5847, 0.5644, 0.5366, 0.6109], device='cuda:0')
tensor([ 0.0475,  0.5221,  0.6488,  0.6720,  0.0174,  0.8740,  0.8331,  1.0781,
         1.4677,  0.0608, -0.1006,  0.7331, -0.1705, -0.1704, -0.6049, -3.3796],
       device='cuda:0')
Solve time for step 2 4.204041524033528
Current ori: tensor([-0.1705, -0.1704, -0.6049], device='cuda:0')
Middle force: tensor([0.5444, 0.5152, 0.5070], device='cuda:0')
Thumb force: tensor([0.5608, 0.5368, 0.6082], device='cuda:0')
tensor([ 0.0444,  0.5311,  0.6507,  0.6757,  0.0123,  0.9030,  0.8435,  1.0675,
         1.4721,  0.0520, -0.0917,  0.7042, -0.1942, -0.1629, -0.5316, -3.1880],
       device='cuda:0')
Solve time for step 3 3.7748892190284096
Current ori: tensor([-0.1942, -0.1629, -0.5316], device='cuda:0')
Middle force: tensor([0.5222, 0.5002], device='cuda:0')
Thumb force: tensor([0.5102, 0.5062], device='cuda:0')
tensor([ 4.3065e-02,  5.5779e-01,  6.6088e-01,  6.7767e-01,  1.6058e-03,
         9.6814e-01,  8.4762e-01,  1.0625e+00,  1.4843e+00,  9.0467e-04,
        -8.2144e-02,  6.3184e-01, -2.4603e-01, -1.6082e-01, -4.1881e-01,
        -3.1514e+00], device='cuda:0')
Solve time for step 4 3.840195710014086
Current ori: tensor([-0.2460, -0.1608, -0.4188], device='cuda:0')
Middle force: tensor([0.5004], device='cuda:0')
Thumb force: tensor([0.5069], device='cuda:0')
Storing RECOVERY transition: reward=-0.4219 (scaled=-0.0703), steps=6
Reward stats updated: mean 0.0021 -> 0.0014, std: 0.0697
Collected 103 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.0971, Q2 Loss=1.0971, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5980
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.3308, Q2 Loss=1.3308, Entropy=0.0065, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8929
SAC Update 3/5: Actor Loss=-0.0387, Q1 Loss=0.3376, Q2 Loss=0.3376, Entropy=0.0006, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5655
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5903, Q2 Loss=1.5903, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1336
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.1210, Q2 Loss=1.1210, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8831

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (21.3%)
Q1 update: 0.04s (19.7%)
Q2 update: 0.04s (17.2%)
Actor update: 0.09s (37.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.053799
Q1 loss: 1.095362
Q2 loss: 1.095362
Current threshold: -31.9949
Global Scale Offset: 0.1692
Reward stats: mean=0.0014, std=0.0697, count=103
----------------------------------------------
SAC Update - Actor Loss: -0.0538, Q1 Loss: 1.0954, Q2 Loss: 1.0954, Entropy: 0.0014, Mean TD Error: 0.8146, Threshold: -31.9949
Original likelihood: -240.4447021484375
Adjusted likelihood: -240.4447021484375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 244.88992309570312
Projection step: 1, Loss: 241.705078125
Projection step: 2, Loss: 245.42160034179688
Projection step: 3, Loss: 248.71376037597656
Projection step: 4, Loss: 243.76669311523438
Projection step: 5, Loss: 242.82681274414062
Projection step: 6, Loss: 241.42526245117188
Projection step: 7, Loss: 243.05596923828125
Projection step: 8, Loss: 240.35989379882812
Projection step: 9, Loss: 239.2848358154297
Projection step: 10, Loss: 246.65585327148438
Projection step: 11, Loss: 246.93377685546875
Projection step: 12, Loss: 240.3289337158203
Projection step: 13, Loss: 241.4091339111328
Projection step: 14, Loss: 239.5838165283203
Projection step: 15, Loss: 240.0348663330078
Projection step: 16, Loss: 242.13833618164062
Projection step: 17, Loss: 240.252685546875
Projection step: 18, Loss: 236.99530029296875
Projection step: 19, Loss: 237.3245086669922
Projection step: 20, Loss: 241.74026489257812
Projection step: 21, Loss: 243.333740234375
Projection step: 22, Loss: 239.884033203125
Projection step: 23, Loss: 239.86802673339844
Projection step: 24, Loss: 242.797607421875
Final likelihood: tensor([-242.1223, -239.5291, -241.5971, -251.8647, -245.6546, -229.7524,
        -246.6341, -231.3956, -251.2309, -254.5295, -208.6226, -248.7882,
        -266.6538, -243.2351, -222.1065, -245.6947])
Final projection likelihood: -241.8382
1 mode projection failed, trying anyway
New goal: tensor([ 0.0299,  0.7476,  0.7033,  0.6749, -0.0029,  1.0422,  0.8515,  1.0548,
         1.4810,  0.0622, -0.0593,  0.5429, -0.3201, -0.1499, -0.3043],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0099]], device='cuda:0') tensor([[0.0140]], device='cuda:0')
Original likelihood: -225.6387481689453
Adjusted likelihood: -225.6387481689453
Likelihood residual: 0.0
Original likelihood: -208.73342895507812
Adjusted likelihood: -208.73342895507812
Likelihood residual: 0.0
{'index': 208.73342895507812, 'thumb_middle': 225.6387481689453}
Current yaw: tensor([-0.3204, -0.1502, -0.2982], device='cuda:0')
7 index
tensor([ 2.8924e-02,  7.4736e-01,  7.0717e-01,  6.7060e-01, -2.8493e-03,
         1.0433e+00,  8.5050e-01,  1.0497e+00,  1.4817e+00,  6.2847e-02,
        -5.7947e-02,  5.4189e-01, -3.2037e-01, -1.5019e-01, -2.9815e-01,
        -3.5430e+00], device='cuda:0')
Solve time for step 1 10.329241280036513
Current ori: tensor([-0.3204, -0.1502, -0.2982], device='cuda:0')
Middle force: tensor([0.5808, 0.5154, 0.5086, 0.5262], device='cuda:0')
Thumb force: tensor([0.5275, 0.5543, 0.5478, 0.5639], device='cuda:0')
tensor([-0.0607,  0.6734,  0.6602,  0.6556, -0.0333,  1.1057,  0.8233,  1.0065,
         1.4954,  0.0432, -0.0878,  0.5896, -0.4622, -0.1424, -0.1135, -4.0912],
       device='cuda:0')
Solve time for step 2 4.16022363799857
Current ori: tensor([-0.4622, -0.1424, -0.1135], device='cuda:0')
Middle force: tensor([0.5012, 0.5983, 0.5332], device='cuda:0')
Thumb force: tensor([0.5336, 0.5958, 0.5669], device='cuda:0')
tensor([-0.0587,  0.6987,  0.6699,  0.6607, -0.0699,  1.2261,  0.7925,  0.9253,
         1.5000,  0.0562, -0.0437,  0.7072, -1.0269, -0.2411, -0.0875, -4.2264],
       device='cuda:0')
Solve time for step 3 3.8984851319692098
Current ori: tensor([-1.0269, -0.2411, -0.0875], device='cuda:0')
Middle force: tensor([0.5137, 0.5053], device='cuda:0')
Thumb force: tensor([0.5923, 0.5058], device='cuda:0')
tensor([-0.0781,  0.5822,  0.6243,  0.6515, -0.0929,  1.3976,  0.8307,  0.7826,
         1.5000, -0.1050, -0.0255,  0.6176, -1.8158, -0.2738, -0.0876, -4.3714],
       device='cuda:0')
Solve time for step 4 3.7779935630387627
Current ori: tensor([-1.8158, -0.2738, -0.0876], device='cuda:0')
Middle force: tensor([0.5061], device='cuda:0')
Thumb force: tensor([0.5052], device='cuda:0')
Storing RECOVERY transition: reward=-1.9699 (scaled=-0.3283), steps=6
Reward stats updated: mean 0.0014 -> -0.0017, std: 0.0765
Collected 104 transitions for RL
SAC Update 1/5: Actor Loss=-0.1864, Q1 Loss=2.3698, Q2 Loss=2.3698, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1748
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=7.6306, Q2 Loss=7.6306, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0587
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.3727, Q2 Loss=0.3727, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3921
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9204, Q2 Loss=0.9204, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1930
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.7611, Q2 Loss=1.7611, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0248

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (18.5%)
Actor update: 0.08s (39.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.129387
Q1 loss: 2.610921
Q2 loss: 2.610921
Current threshold: -32.0064
Global Scale Offset: 0.1679
Reward stats: mean=-0.0017, std=0.0765, count=104
----------------------------------------------
SAC Update - Actor Loss: -0.1294, Q1 Loss: 2.6109, Q2 Loss: 2.6109, Entropy: 0.0000, Mean TD Error: 1.3687, Threshold: -32.0064
Original likelihood: -1284.1787109375
Adjusted likelihood: -1284.1787109375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 6
Loaded trajectory sampler
Current yaw: tensor([-0.0028,  0.0149, -0.0447], device='cuda:0')
Current yaw: tensor([-0.0028,  0.0149, -0.0447], device='cuda:0')
1 turn
Sampling time 3.723703275027219
tensor([ 0.1385,  0.6279,  0.5349,  0.6003, -0.0931,  0.5146,  0.8562,  0.9795,
         1.2349,  0.2631,  0.2434,  1.2133, -0.0028,  0.0149, -0.0447,  0.1288],
       device='cuda:0')
Original likelihood: -19.43201446533203
Adjusted likelihood: -19.43201446533203
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.752155976952054
Current ori: tensor([-0.0028,  0.0149, -0.0447], device='cuda:0')
Middle force: tensor([0.6483, 0.6798, 0.5283, 0.5217, 0.6276, 0.9450, 0.9746, 0.5709, 0.5029,
        0.5461, 0.5669, 0.6319], device='cuda:0')
Thumb force: tensor([0.5517, 2.2756, 0.6447, 1.4888, 1.0327, 0.8418, 1.8871, 0.6154, 0.6780,
        0.6318, 0.5604, 0.7219], device='cuda:0')
Index force: tensor([0.5865, 0.5090, 0.5968, 0.5521, 0.5687, 0.5150, 0.5819, 0.5981, 0.7126,
        0.6713, 0.6229, 0.5490], device='cuda:0')
Storing NORMAL transition: reward=-0.0397 (scaled=-0.0397), steps=1
Reward stats updated: mean -0.0017 -> -0.0021, std: 0.0762
Collected 105 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=4.4359, Q2 Loss=4.4359, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6750
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=12.5357, Q2 Loss=12.5357, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0917
SAC Update 3/5: Actor Loss=-0.2062, Q1 Loss=4.1268, Q2 Loss=4.1268, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6483
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4182, Q2 Loss=1.4182, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8422
SAC Update 5/5: Actor Loss=-0.1486, Q1 Loss=2.2106, Q2 Loss=2.2106, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2749

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.2%)
Actor update: 0.08s (39.3%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.070953
Q1 loss: 4.945448
Q2 loss: 4.945448
Current threshold: -32.0133
Global Scale Offset: 0.1672
Reward stats: mean=-0.0021, std=0.0762, count=105
----------------------------------------------
SAC Update - Actor Loss: -0.0710, Q1 Loss: 4.9454, Q2 Loss: 4.9454, Entropy: 0.0000, Mean TD Error: 1.7064, Threshold: -32.0133
tensor([ 0.1052,  0.6137,  0.5025,  0.6372, -0.1003,  0.5858,  0.7538,  0.9672,
         1.3372,  0.1335,  0.2017,  1.1999, -0.0216,  0.0144, -0.0053,  0.2469],
       device='cuda:0')
Original likelihood: -20.607213973999023
Adjusted likelihood: -20.607213973999023
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.488040085998364
Current ori: tensor([-0.0216,  0.0144, -0.0053], device='cuda:0')
Middle force: tensor([0.6710, 0.5275, 0.5202, 0.6179, 0.9224, 0.9426, 0.5675, 0.5026, 0.5437,
        0.5692, 0.6268], device='cuda:0')
Thumb force: tensor([2.1687, 0.6319, 1.4295, 0.9988, 0.8156, 1.8089, 0.6048, 0.6518, 0.6219,
        0.5419, 0.7120], device='cuda:0')
Index force: tensor([0.5075, 0.5875, 0.5477, 0.5635, 0.5134, 0.5751, 0.5907, 0.7106, 0.6595,
        0.6285, 0.5447], device='cuda:0')
Storing NORMAL transition: reward=-0.0307 (scaled=-0.0307), steps=1
Reward stats updated: mean -0.0021 -> -0.0024, std: 0.0759
Collected 106 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.8047, Q2 Loss=0.8047, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6714
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=8.8327, Q2 Loss=8.8327, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8218
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8923, Q2 Loss=0.8923, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6500
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5749, Q2 Loss=1.5749, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8609
SAC Update 5/5: Actor Loss=-0.0641, Q1 Loss=0.6269, Q2 Loss=0.6269, Entropy=0.0061, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4532

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (21.2%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.04s (17.5%)
Actor update: 0.10s (38.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.012826
Q1 loss: 2.546303
Q2 loss: 2.546303
Current threshold: -32.0199
Global Scale Offset: 0.1666
Reward stats: mean=-0.0024, std=0.0759, count=106
----------------------------------------------
SAC Update - Actor Loss: -0.0128, Q1 Loss: 2.5463, Q2 Loss: 2.5463, Entropy: 0.0012, Mean TD Error: 1.0915, Threshold: -32.0199
tensor([ 0.1228,  0.5492,  0.5830,  0.6781, -0.1418,  0.5597,  0.7858,  0.9023,
         1.3574,  0.1344,  0.2645,  1.1335, -0.0224,  0.0448,  0.0239, -0.1478],
       device='cuda:0')
Original likelihood: -24.202007293701172
Adjusted likelihood: -24.202007293701172
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.040877692983486
Current ori: tensor([-0.0224,  0.0448,  0.0239], device='cuda:0')
Middle force: tensor([1.1082, 0.5584, 1.0531, 0.6353, 0.5268, 0.5198, 0.5109, 0.7851, 0.5544,
        0.5007], device='cuda:0')
Thumb force: tensor([0.7511, 1.0088, 1.0134, 0.6540, 0.5271, 0.9105, 0.5337, 0.5487, 0.5587,
        0.5401], device='cuda:0')
Index force: tensor([0.5495, 0.5625, 0.7707, 0.5223, 0.9799, 0.9157, 0.5681, 0.5713, 0.5823,
        0.7821], device='cuda:0')
Storing NORMAL transition: reward=0.0311 (scaled=0.0311), steps=1
Reward stats updated: mean -0.0024 -> -0.0021, std: 0.0756
Collected 107 transitions for RL
SAC Update 1/5: Actor Loss=-0.0356, Q1 Loss=0.4300, Q2 Loss=0.4300, Entropy=0.0648, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6381
SAC Update 2/5: Actor Loss=-0.0382, Q1 Loss=0.3754, Q2 Loss=0.3754, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5374
SAC Update 3/5: Actor Loss=-0.1238, Q1 Loss=1.1537, Q2 Loss=1.1537, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9222
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2574, Q2 Loss=1.2574, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8527
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7782, Q2 Loss=0.7782, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4976

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.1%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (40.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.039508
Q1 loss: 0.798962
Q2 loss: 0.798962
Current threshold: -32.0384
Global Scale Offset: 0.1654
Reward stats: mean=-0.0021, std=0.0756, count=107
----------------------------------------------
SAC Update - Actor Loss: -0.0395, Q1 Loss: 0.7990, Q2 Loss: 0.7990, Entropy: 0.0130, Mean TD Error: 0.6896, Threshold: -32.0384
tensor([ 0.1577,  0.4973,  0.6453,  0.7564, -0.2454,  0.5783,  0.8604,  0.7858,
         1.3294,  0.2872,  0.2831,  1.1262, -0.0186,  0.0837, -0.0117, -0.0805],
       device='cuda:0')
Original likelihood: -36.85765838623047
Adjusted likelihood: -36.85765838623047
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.82688903808594
Projection step: 1, Loss: 36.33184051513672
Projection step: 2, Loss: 37.55059051513672
Projection step: 3, Loss: 36.793785095214844
Projection step: 4, Loss: 36.6468391418457
Projection step: 5, Loss: 35.23487854003906
Projection step: 6, Loss: 34.58198165893555
Projection step: 7, Loss: 34.868370056152344
Projection step: 8, Loss: 35.21720886230469
Projection step: 9, Loss: 33.584503173828125
Projection step: 10, Loss: 32.28363037109375
Projection step: 11, Loss: 33.397335052490234
Projection step: 12, Loss: 33.870635986328125
Projection step: 13, Loss: 33.98466491699219
Projection step: 14, Loss: 32.06477355957031
Projection step: 15, Loss: 31.809675216674805
Projection step: 16, Loss: 31.464492797851562
Projection step: 17, Loss: 31.857891082763672
Projection step: 18, Loss: 31.456531524658203
Projection step: 19, Loss: 30.93062973022461
Projection step: 20, Loss: 31.225616455078125
Projection step: 21, Loss: 30.852741241455078
Projection step: 22, Loss: 30.314685821533203
Projection step: 23, Loss: 30.821598052978516
Projection step: 24, Loss: 29.0003719329834
Final likelihood: tensor([-29.0514, -28.2877, -26.8400, -29.5521, -31.7443, -35.4334, -29.9066,
        -28.7581, -25.7126, -31.6743, -28.9955, -31.3113, -27.9411, -32.0047,
        -27.1672, -27.1863])
Final projection likelihood: -29.4729
1 mode projection succeeded
New goal: tensor([ 0.1273,  0.4953,  0.6255,  0.6648, -0.2102,  0.6019,  0.8444,  0.7954,
         1.3226,  0.2308,  0.3032,  1.1182, -0.0256,  0.0737,  0.5766],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -28.976245880126953
Adjusted likelihood: -28.976245880126953
Likelihood residual: 0.0
Original likelihood: -29.822887420654297
Adjusted likelihood: -29.822887420654297
Likelihood residual: 0.0
{'index': 29.822887420654297, 'thumb_middle': 28.976245880126953}
Current yaw: tensor([-0.0186,  0.0837, -0.0117], device='cuda:0')
2 thumb_middle
tensor([ 0.1577,  0.4973,  0.6453,  0.7564, -0.2454,  0.5783,  0.8604,  0.7858,
         1.3294,  0.2872,  0.2831,  1.1262, -0.0186,  0.0837, -0.0117, -0.0805],
       device='cuda:0')
Solve time for step 1 8.895663681963924
Current ori: tensor([-0.0186,  0.0837, -0.0117], device='cuda:0')
Index force: tensor([0.5781, 0.5868, 0.5838, 0.5773], device='cuda:0')
tensor([ 0.1358,  0.5567,  0.6829,  0.6984, -0.2818,  0.5851,  0.8101,  0.7707,
         1.2874,  0.2115,  0.2411,  1.1030, -0.0474,  0.1216, -0.0572, -1.2189],
       device='cuda:0')
Solve time for step 2 3.6214757369598374
Current ori: tensor([-0.0474,  0.1216, -0.0572], device='cuda:0')
Index force: tensor([0.5782, 0.5762, 0.5701], device='cuda:0')
tensor([ 0.1061,  0.6122,  0.6724,  0.6832, -0.2977,  0.5874,  0.7976,  0.7627,
         1.3188,  0.2185,  0.2614,  1.1061, -0.0595,  0.1370, -0.1046, -1.3643],
       device='cuda:0')
Solve time for step 3 3.152481523982715
Current ori: tensor([-0.0595,  0.1370, -0.1046], device='cuda:0')
Index force: tensor([0.5639, 0.5592], device='cuda:0')
tensor([ 0.0896,  0.6113,  0.7005,  0.6904, -0.3057,  0.5863,  0.7908,  0.7624,
         1.3349,  0.2228,  0.2703,  1.1146, -0.0657,  0.1450, -0.1257, -1.3857],
       device='cuda:0')
Solve time for step 4 3.518281386990566
Current ori: tensor([-0.0657,  0.1450, -0.1257], device='cuda:0')
Index force: tensor([0.5502], device='cuda:0')
Storing RECOVERY transition: reward=0.0912 (scaled=0.0304), steps=3
Reward stats updated: mean -0.0021 -> -0.0018, std: 0.0753
Collected 108 transitions for RL
SAC Update 1/5: Actor Loss=-0.0661, Q1 Loss=0.4162, Q2 Loss=0.4162, Entropy=0.0051, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3779
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.6629, Q2 Loss=2.6629, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1783
SAC Update 3/5: Actor Loss=-0.1760, Q1 Loss=0.9471, Q2 Loss=0.9471, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7024
SAC Update 4/5: Actor Loss=-0.0674, Q1 Loss=3.7159, Q2 Loss=3.7159, Entropy=0.0046, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3324
SAC Update 5/5: Actor Loss=-0.1168, Q1 Loss=1.5111, Q2 Loss=1.5111, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1509

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.3%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (18.8%)
Actor update: 0.10s (39.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.085262
Q1 loss: 1.850630
Q2 loss: 1.850630
Current threshold: -32.0641
Global Scale Offset: 0.1636
Reward stats: mean=-0.0018, std=0.0753, count=108
----------------------------------------------
SAC Update - Actor Loss: -0.0853, Q1 Loss: 1.8506, Q2 Loss: 1.8506, Entropy: 0.0019, Mean TD Error: 1.1484, Threshold: -32.0641
Original likelihood: -33.502197265625
Adjusted likelihood: -33.502197265625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0027)
State is out of distribution
Projection step: 0, Loss: 33.93925857543945
Projection step: 1, Loss: 34.771026611328125
Projection step: 2, Loss: 34.787109375
Projection step: 3, Loss: 40.0161018371582
Projection step: 4, Loss: 35.302677154541016
Projection step: 5, Loss: 36.00693893432617
Projection step: 6, Loss: 39.90855407714844
Projection step: 7, Loss: 36.929969787597656
Projection step: 8, Loss: 35.9808349609375
Projection step: 9, Loss: 38.060646057128906
Projection step: 10, Loss: 35.209815979003906
Projection step: 11, Loss: 39.05879211425781
Projection step: 12, Loss: 35.62598419189453
Projection step: 13, Loss: 33.24586486816406
Projection step: 14, Loss: 37.253997802734375
Projection step: 15, Loss: 35.981292724609375
Projection step: 16, Loss: 34.77078628540039
Projection step: 17, Loss: 35.544227600097656
Projection step: 18, Loss: 37.076263427734375
Projection step: 19, Loss: 37.66900634765625
Projection step: 20, Loss: 34.0681266784668
Projection step: 21, Loss: 30.668893814086914
Projection step: 22, Loss: 37.9989128112793
Projection step: 23, Loss: 28.490123748779297
Projection step: 24, Loss: 35.61653518676758
Final likelihood: tensor([-37.7138, -36.9975, -24.4199, -38.9099, -23.1596, -19.6866, -19.3210,
        -39.0886, -22.5225, -36.4294, -18.8371, -34.0705, -38.3020, -43.9291,
        -39.4629, -19.4617])
Final projection likelihood: -30.7695
1 mode projection succeeded
New goal: tensor([ 0.0991,  0.5597,  0.6166,  0.6445, -0.2589,  0.5801,  0.7629,  0.7656,
         1.3970,  0.2382,  0.3023,  1.0842, -0.0700,  0.1300,  0.4242],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0023]], device='cuda:0')
Original likelihood: -36.76850891113281
Adjusted likelihood: -36.76850891113281
Likelihood residual: 0.0
Original likelihood: -37.433250427246094
Adjusted likelihood: -37.433250427246094
Likelihood residual: 0.0
{'index': 37.433250427246094, 'thumb_middle': 36.76850891113281}
Current yaw: tensor([-0.0679,  0.1351, -0.1212], device='cuda:0')
3 thumb_middle
tensor([ 0.1069,  0.6151,  0.7041,  0.7069, -0.2909,  0.5894,  0.7985,  0.7560,
         1.4038,  0.2641,  0.2905,  1.1386, -0.0679,  0.1351, -0.1212, -1.2705],
       device='cuda:0')
Solve time for step 1 8.763321842998266
Current ori: tensor([-0.0679,  0.1351, -0.1212], device='cuda:0')
Index force: tensor([0.5420, 0.5879, 0.5876, 0.5732], device='cuda:0')
tensor([ 0.0631,  0.6768,  0.6497,  0.6425, -0.3117,  0.5882,  0.7589,  0.7485,
         1.3637,  0.2318,  0.2627,  1.0768, -0.0965,  0.1666, -0.1827, -1.0689],
       device='cuda:0')
Solve time for step 2 3.6041650800034404
Current ori: tensor([-0.0965,  0.1666, -0.1827], device='cuda:0')
Index force: tensor([0.5668, 0.5585, 0.5681], device='cuda:0')
tensor([ 0.0111,  0.7091,  0.6767,  0.6570, -0.3160,  0.5899,  0.7411,  0.7582,
         1.3854,  0.2155,  0.2674,  1.0583, -0.1245,  0.1825, -0.2701,  0.0066],
       device='cuda:0')
Solve time for step 3 3.4948301119729877
Current ori: tensor([-0.1245,  0.1825, -0.2701], device='cuda:0')
Index force: tensor([0.5444, 0.5568], device='cuda:0')
tensor([-0.0427,  0.7529,  0.6853,  0.6756, -0.3076,  0.5934,  0.7626,  0.7504,
         1.3871,  0.2359,  0.2621,  1.0883, -0.2232,  0.2486, -0.3405,  1.5858],
       device='cuda:0')
Solve time for step 4 3.423390125972219
Current ori: tensor([-0.2232,  0.2486, -0.3405], device='cuda:0')
Index force: tensor([0.5363], device='cuda:0')
Storing RECOVERY transition: reward=-0.2141 (scaled=-0.0714), steps=3
Reward stats updated: mean -0.0018 -> -0.0024, std: 0.0753
Collected 109 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=3.6387, Q2 Loss=3.6387, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9200
SAC Update 2/5: Actor Loss=-0.0001, Q1 Loss=1.3514, Q2 Loss=1.3514, Entropy=0.0214, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3047
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.8298, Q2 Loss=2.8298, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5349
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=3.7986, Q2 Loss=3.7986, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6903
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=2.2378, Q2 Loss=2.2378, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3766

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.2%)
Q1 update: 0.05s (20.7%)
Q2 update: 0.05s (17.9%)
Actor update: 0.10s (38.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.138170
Q1 loss: 2.771249
Q2 loss: 2.771249
Current threshold: -32.0850
Global Scale Offset: 0.1621
Reward stats: mean=-0.0024, std=0.0753, count=109
----------------------------------------------
SAC Update - Actor Loss: -0.1382, Q1 Loss: 2.7712, Q2 Loss: 2.7712, Entropy: 0.0043, Mean TD Error: 1.9653, Threshold: -32.0850
Original likelihood: -502.317138671875
Adjusted likelihood: -502.317138671875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 7
Loaded trajectory sampler
Current yaw: tensor([ 0.0004,  0.0137, -0.0504], device='cuda:0')
Current yaw: tensor([ 0.0004,  0.0137, -0.0504], device='cuda:0')
1 turn
Sampling time 3.607410768046975
tensor([ 1.4720e-01,  6.1819e-01,  5.5203e-01,  6.0971e-01, -1.1759e-01,
         5.5622e-01,  8.5493e-01,  9.4147e-01,  1.1889e+00,  3.3571e-01,
         2.7453e-01,  1.1989e+00,  4.0844e-04,  1.3733e-02, -5.0365e-02,
         2.3955e-01], device='cuda:0')
Original likelihood: -17.99958038330078
Adjusted likelihood: -17.99958038330078
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.166630605992395
Current ori: tensor([ 0.0004,  0.0137, -0.0504], device='cuda:0')
Middle force: tensor([0.6673, 0.6602, 0.5164, 0.5191, 0.6441, 0.9929, 1.0075, 0.5685, 0.5612,
        0.5099, 0.5241, 0.5092], device='cuda:0')
Thumb force: tensor([0.5404, 2.3729, 0.6571, 1.5442, 1.0430, 0.8753, 1.9872, 0.6228, 0.6449,
        0.6732, 0.5885, 0.6652], device='cuda:0')
Index force: tensor([0.5888, 0.5096, 0.5988, 0.5583, 0.5755, 0.5105, 0.5772, 0.5967, 0.5550,
        0.6115, 0.6088, 0.5380], device='cuda:0')
Storing NORMAL transition: reward=-0.0328 (scaled=-0.0328), steps=1
Reward stats updated: mean -0.0024 -> -0.0027, std: 0.0750
Collected 110 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.0721, Q2 Loss=1.0721, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7875
SAC Update 2/5: Actor Loss=-0.0396, Q1 Loss=0.5161, Q2 Loss=0.5161, Entropy=0.0470, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2713
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.7525, Q2 Loss=0.7525, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4660
SAC Update 4/5: Actor Loss=-0.0681, Q1 Loss=0.7922, Q2 Loss=0.7922, Entropy=0.0054, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5983
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.7290, Q2 Loss=1.7290, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1315

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (16.3%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (20.2%)
Actor update: 0.09s (39.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.113643
Q1 loss: 0.972379
Q2 loss: 0.972379
Current threshold: -32.1086
Global Scale Offset: 0.1606
Reward stats: mean=-0.0027, std=0.0750, count=110
----------------------------------------------
SAC Update - Actor Loss: -0.1136, Q1 Loss: 0.9724, Q2 Loss: 0.9724, Entropy: 0.0105, Mean TD Error: 0.6509, Threshold: -32.1086
tensor([ 0.0131,  0.5366,  0.5801,  0.5234, -0.2220,  0.5072,  0.8761,  0.9232,
         1.3798,  0.1590,  0.2469,  1.2386,  0.0134,  0.0727, -0.0224,  0.3442],
       device='cuda:0')
Original likelihood: -38.577049255371094
Adjusted likelihood: -38.577049255371094
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 38.524375915527344
Projection step: 1, Loss: 36.4628791809082
Projection step: 2, Loss: 34.9573974609375
Projection step: 3, Loss: 34.419456481933594
Projection step: 4, Loss: 34.13428497314453
Projection step: 5, Loss: 33.95069122314453
Projection step: 6, Loss: 32.84823226928711
Projection step: 7, Loss: 33.55802536010742
Projection step: 8, Loss: 32.33866882324219
Projection step: 9, Loss: 33.416542053222656
Projection step: 10, Loss: 32.63188171386719
Projection step: 11, Loss: 33.36876678466797
Projection step: 12, Loss: 33.33133316040039
Projection step: 13, Loss: 32.44794464111328
Projection step: 14, Loss: 32.89933776855469
Projection step: 15, Loss: 31.65408706665039
Projection step: 16, Loss: 31.729516983032227
Projection step: 17, Loss: 31.1209716796875
Projection step: 18, Loss: 31.343711853027344
Projection step: 19, Loss: 31.487558364868164
Projection step: 20, Loss: 30.378795623779297
Projection step: 21, Loss: 30.428043365478516
Projection step: 22, Loss: 29.156349182128906
Projection step: 23, Loss: 29.466644287109375
Projection step: 24, Loss: 30.43441390991211
Final likelihood: tensor([-30.7838, -32.9940, -27.9760, -30.8506, -33.8732, -30.9385, -29.0607,
        -30.5930, -30.9456, -28.1366, -30.3808, -28.0034, -27.1422, -31.8772,
        -31.2041, -29.3933])
Final projection likelihood: -30.2596
1 mode projection succeeded
New goal: tensor([ 0.0026,  0.5512,  0.5431,  0.6028, -0.1819,  0.5146,  0.8085,  0.8280,
         1.3297,  0.1312,  0.2120,  1.2237,  0.0113,  0.0609, -0.0365],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -34.262672424316406
Adjusted likelihood: -34.262672424316406
Likelihood residual: 0.0
Original likelihood: -25.748153686523438
Adjusted likelihood: -25.748153686523438
Likelihood residual: 0.0
{'index': 25.748153686523438, 'thumb_middle': 34.262672424316406}
Current yaw: tensor([ 0.0134,  0.0727, -0.0224], device='cuda:0')
2 index
tensor([ 0.0131,  0.5366,  0.5801,  0.5234, -0.2220,  0.5072,  0.8761,  0.9232,
         1.3798,  0.1590,  0.2469,  1.2386,  0.0134,  0.0727, -0.0224,  0.3442],
       device='cuda:0')
Solve time for step 1 10.482535416027531
Current ori: tensor([ 0.0134,  0.0727, -0.0224], device='cuda:0')
Middle force: tensor([0.5699, 0.5579, 0.5414, 0.5342], device='cuda:0')
Thumb force: tensor([0.5819, 0.5072, 0.5524, 0.5536], device='cuda:0')
tensor([ 0.0581,  0.4812,  0.4935,  0.5604, -0.2067,  0.5465,  0.8517,  0.8578,
         1.3878,  0.1504,  0.2377,  1.1896, -0.0111,  0.0681, -0.0354,  0.7732],
       device='cuda:0')
Solve time for step 2 4.1902520370204
Current ori: tensor([-0.0111,  0.0681, -0.0354], device='cuda:0')
Middle force: tensor([0.5556, 0.5380, 0.5322], device='cuda:0')
Thumb force: tensor([0.5052, 0.5503, 0.5512], device='cuda:0')
tensor([ 0.0614,  0.4867,  0.4810,  0.5719, -0.2015,  0.5477,  0.8524,  0.8595,
         1.3976,  0.1331,  0.2240,  1.1981, -0.0120,  0.0645, -0.0417,  0.9437],
       device='cuda:0')
Solve time for step 3 3.6898624469758943
Current ori: tensor([-0.0120,  0.0645, -0.0417], device='cuda:0')
Middle force: tensor([0.5617, 0.5301], device='cuda:0')
Thumb force: tensor([0.5536, 0.6160], device='cuda:0')
tensor([ 0.0608,  0.4845,  0.4824,  0.5732, -0.2073,  0.5448,  0.8513,  0.8630,
         1.3993,  0.1396,  0.2161,  1.2046, -0.0142,  0.0685, -0.0647,  0.9329],
       device='cuda:0')
Solve time for step 4 3.7977616250282153
Current ori: tensor([-0.0142,  0.0685, -0.0647], device='cuda:0')
Middle force: tensor([0.5002], device='cuda:0')
Thumb force: tensor([0.5559], device='cuda:0')
Storing RECOVERY transition: reward=0.0452 (scaled=0.0452), steps=1
Reward stats updated: mean -0.0027 -> -0.0022, std: 0.0748
Collected 111 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4776, Q2 Loss=1.4776, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1884
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.6561, Q2 Loss=2.6561, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5358
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=2.4295, Q2 Loss=2.4295, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2584
SAC Update 4/5: Actor Loss=-0.1803, Q1 Loss=1.8507, Q2 Loss=1.8507, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2159
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=6.9477, Q2 Loss=6.9477, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8656

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.2%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (18.6%)
Actor update: 0.09s (40.1%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.082112
Q1 loss: 3.072307
Q2 loss: 3.072307
Current threshold: -32.1294
Global Scale Offset: 0.1593
Reward stats: mean=-0.0022, std=0.0748, count=111
----------------------------------------------
SAC Update - Actor Loss: -0.0821, Q1 Loss: 3.0723, Q2 Loss: 3.0723, Entropy: 0.0000, Mean TD Error: 1.6128, Threshold: -32.1294
Original likelihood: -28.898448944091797
Adjusted likelihood: -28.898448944091797
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0170,  0.0702, -0.0678], device='cuda:0')
3 turn
Sampling time 3.592351360945031
tensor([ 0.0108,  0.5482,  0.5261,  0.5951, -0.2088,  0.5487,  0.8469,  0.8562,
         1.3961,  0.1519,  0.2135,  1.2136, -0.0170,  0.0702, -0.0678,  0.8933],
       device='cuda:0')
Original likelihood: -28.042652130126953
Adjusted likelihood: -28.042652130126953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.388754067011178
Current ori: tensor([-0.0170,  0.0702, -0.0678], device='cuda:0')
Middle force: tensor([1.1695, 0.4921, 0.5509, 0.5261, 0.5611, 0.9296, 1.5642, 0.5362, 0.5007,
        0.4904, 0.5307, 0.9305], device='cuda:0')
Thumb force: tensor([1.1544, 1.6794, 1.0638, 0.5173, 0.5470, 1.2632, 1.0034, 0.9181, 0.7991,
        0.6075, 0.5605, 0.7865], device='cuda:0')
Index force: tensor([1.0139, 0.6772, 0.5014, 0.6589, 0.5633, 1.1884, 0.7781, 0.5580, 0.6261,
        0.5694, 0.5522, 0.5422], device='cuda:0')
Storing NORMAL transition: reward=0.1234 (scaled=0.1234), steps=1
Reward stats updated: mean -0.0022 -> -0.0011, std: 0.0754
Collected 112 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=2.3675, Q2 Loss=2.3675, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5671
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0256, Q2 Loss=1.0256, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8834
SAC Update 3/5: Actor Loss=-0.0757, Q1 Loss=0.7286, Q2 Loss=0.7286, Entropy=0.0022, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0844
SAC Update 4/5: Actor Loss=-0.1133, Q1 Loss=2.0136, Q2 Loss=2.0136, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4760
SAC Update 5/5: Actor Loss=-0.2039, Q1 Loss=1.5646, Q2 Loss=1.5646, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0531

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.1%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (20.3%)
Actor update: 0.11s (42.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.124623
Q1 loss: 1.539991
Q2 loss: 1.539991
Current threshold: -32.1492
Global Scale Offset: 0.1579
Reward stats: mean=-0.0011, std=0.0754, count=112
----------------------------------------------
SAC Update - Actor Loss: -0.1246, Q1 Loss: 1.5400, Q2 Loss: 1.5400, Entropy: 0.0004, Mean TD Error: 1.2128, Threshold: -32.1492
tensor([ 0.0354,  0.5221,  0.5675,  0.6284, -0.1976,  0.5028,  0.9112,  0.9105,
         1.3756,  0.1747,  0.2058,  1.1968, -0.0098,  0.0566, -0.1903,  1.0575],
       device='cuda:0')
Original likelihood: -28.21272087097168
Adjusted likelihood: -28.21272087097168
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.609158353006933
Current ori: tensor([-0.0098,  0.0566, -0.1903], device='cuda:0')
Middle force: tensor([1.8145, 1.7042, 0.6098, 0.6023, 0.5225, 0.5517, 0.5310, 0.5617, 0.5066,
        0.5177, 0.5015], device='cuda:0')
Thumb force: tensor([1.8945, 0.5677, 0.6607, 0.5660, 0.5560, 0.6230, 0.8172, 0.7695, 1.2187,
        0.6178, 0.5680], device='cuda:0')
Index force: tensor([0.8479, 0.6317, 0.5003, 0.5928, 0.6575, 0.5859, 0.6056, 0.5731, 0.7275,
        0.5364, 0.7247], device='cuda:0')
Storing NORMAL transition: reward=0.1038 (scaled=0.1038), steps=1
Reward stats updated: mean -0.0011 -> -0.0002, std: 0.0757
Collected 113 transitions for RL
SAC Update 1/5: Actor Loss=-0.0760, Q1 Loss=0.4620, Q2 Loss=0.4620, Entropy=0.0022, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6109
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.7273, Q2 Loss=2.7273, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8443
SAC Update 3/5: Actor Loss=-0.0538, Q1 Loss=1.3571, Q2 Loss=1.3571, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5054
SAC Update 4/5: Actor Loss=-0.0549, Q1 Loss=0.6227, Q2 Loss=0.6227, Entropy=0.0016, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3719
SAC Update 5/5: Actor Loss=-0.1134, Q1 Loss=1.9980, Q2 Loss=1.9980, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5121

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (18.6%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (18.7%)
Actor update: 0.10s (39.5%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.105674
Q1 loss: 1.433404
Q2 loss: 1.433404
Current threshold: -32.1794
Global Scale Offset: 0.1557
Reward stats: mean=-0.0002, std=0.0757, count=113
----------------------------------------------
SAC Update - Actor Loss: -0.1057, Q1 Loss: 1.4334, Q2 Loss: 1.4334, Entropy: 0.0008, Mean TD Error: 1.3689, Threshold: -32.1794
tensor([ 1.0746e-01,  5.7473e-01,  5.1299e-01,  4.0991e-01, -1.0336e-01,
         5.1024e-01,  9.1422e-01,  1.0406e+00,  1.3380e+00,  9.9988e-02,
         2.7543e-01,  9.8055e-01, -1.7229e-03, -8.6713e-03, -2.9011e-01,
         2.6235e+00], device='cuda:0')
Original likelihood: -24.845836639404297
Adjusted likelihood: -24.845836639404297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.158631770987995
Current ori: tensor([-0.0017, -0.0087, -0.2901], device='cuda:0')
Middle force: tensor([0.5468, 0.5400, 0.5482, 0.9349, 1.4179, 0.5361, 0.5243, 0.5002, 0.5471,
        0.7851], device='cuda:0')
Thumb force: tensor([0.9363, 0.5135, 0.5520, 1.0678, 0.9007, 0.8455, 0.7306, 0.5666, 0.5485,
        0.6303], device='cuda:0')
Index force: tensor([0.5000, 0.6578, 0.5600, 1.0817, 0.7232, 0.5619, 0.5794, 0.6636, 0.5478,
        0.5455], device='cuda:0')
Storing NORMAL transition: reward=0.0684 (scaled=0.0684), steps=1
Reward stats updated: mean -0.0002 -> 0.0004, std: 0.0756
Collected 114 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.5857, Q2 Loss=0.5857, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3397
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.0261, Q2 Loss=2.0261, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4325
SAC Update 3/5: Actor Loss=-0.1017, Q1 Loss=1.2118, Q2 Loss=1.2118, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0990
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.4484, Q2 Loss=0.4484, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6237
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=3.3784, Q2 Loss=3.3784, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3920

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.8%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.05s (19.6%)
Actor update: 0.10s (40.8%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.112445
Q1 loss: 1.530106
Q2 loss: 1.530106
Current threshold: -32.2017
Global Scale Offset: 0.1541
Reward stats: mean=0.0004, std=0.0756, count=114
----------------------------------------------
SAC Update - Actor Loss: -0.1124, Q1 Loss: 1.5301, Q2 Loss: 1.5301, Entropy: 0.0000, Mean TD Error: 0.9774, Threshold: -32.2017
tensor([ 0.1013,  0.6133,  0.5357,  0.5569, -0.0500,  0.4405,  1.0373,  1.0537,
         1.1612,  0.2628,  0.3160,  0.9351,  0.0178, -0.0388, -0.3623,  2.9901],
       device='cuda:0')
Original likelihood: -33.98257064819336
Adjusted likelihood: -33.98257064819336
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0002)
State is out of distribution
Projection step: 0, Loss: 29.884937286376953
Projection step: 1, Loss: 35.091819763183594
Projection step: 2, Loss: 27.401037216186523
Projection step: 3, Loss: 26.698291778564453
Projection step: 4, Loss: 27.325685501098633
Projection step: 5, Loss: 26.532522201538086
Projection step: 6, Loss: 24.950756072998047
Projection step: 7, Loss: 25.436349868774414
Projection step: 8, Loss: 23.033023834228516
Projection step: 9, Loss: 21.902271270751953
Projection step: 10, Loss: 22.097305297851562
Projection step: 11, Loss: 23.898487091064453
Projection step: 12, Loss: 21.26716423034668
Projection step: 13, Loss: 20.45325469970703
Projection step: 14, Loss: 20.568965911865234
Projection step: 15, Loss: 19.857337951660156
Projection step: 16, Loss: 18.61028289794922
Final likelihood: tensor([-17.0125, -15.4311, -17.3202, -17.6069, -19.2874, -17.7503, -20.0631,
        -24.4443, -20.4456, -20.6681, -17.6012, -16.8250, -17.6661, -21.2135,
        -16.6967, -17.7326])
Final projection likelihood: -18.6103
1 mode projection succeeded
New goal: tensor([ 0.0804,  0.5635,  0.5470,  0.6168, -0.0472,  0.4772,  0.9158,  0.9691,
         1.2372,  0.2858,  0.2343,  0.9898,  0.0162, -0.0346, -0.6124],
       device='cuda:0')
tensor([[0.0076]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -20.205167770385742
Adjusted likelihood: -20.205167770385742
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 20.205167770385742}
Current yaw: tensor([ 0.0178, -0.0388, -0.3623], device='cuda:0')
4 thumb_middle
tensor([ 0.1013,  0.6133,  0.5357,  0.5569, -0.0500,  0.4405,  1.0373,  1.0537,
         1.1612,  0.2628,  0.3160,  0.9351,  0.0178, -0.0388, -0.3623,  2.9901],
       device='cuda:0')
Solve time for step 1 8.702936252986547
Current ori: tensor([ 0.0178, -0.0388, -0.3623], device='cuda:0')
Index force: tensor([0.5592, 0.6042, 0.5812, 0.5939], device='cuda:0')
tensor([ 0.0977,  0.6180,  0.5156,  0.5734, -0.1229,  0.4621,  0.8999,  0.9622,
         1.2021,  0.2825,  0.1872,  0.9598,  0.0178, -0.0359, -0.3623,  2.9628],
       device='cuda:0')
Solve time for step 2 3.4851308850338683
Current ori: tensor([ 0.0178, -0.0359, -0.3623], device='cuda:0')
Index force: tensor([0.5949, 0.5765, 0.5882], device='cuda:0')
tensor([ 0.0872,  0.5851,  0.5429,  0.5895, -0.1364,  0.4685,  0.8944,  0.9521,
         1.2088,  0.2721,  0.1819,  0.9798,  0.0260, -0.0286, -0.3623,  2.9571],
       device='cuda:0')
Solve time for step 3 3.4894554700003937
Current ori: tensor([ 0.0260, -0.0286, -0.3623], device='cuda:0')
Index force: tensor([0.5705, 0.5827], device='cuda:0')
tensor([ 0.0896,  0.5851,  0.5386,  0.6024, -0.1311,  0.4745,  0.8802,  0.9590,
         1.2334,  0.2874,  0.1647,  0.9531,  0.0275, -0.0296, -0.3623,  2.9656],
       device='cuda:0')
Solve time for step 4 3.3693967169965617
Current ori: tensor([ 0.0275, -0.0296, -0.3623], device='cuda:0')
Index force: tensor([0.5674], device='cuda:0')
Storing RECOVERY transition: reward=0.0064 (scaled=0.0021), steps=3
Reward stats updated: mean 0.0004 -> 0.0004, std: 0.0753
Collected 115 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.8985, Q2 Loss=2.8985, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6618
SAC Update 2/5: Actor Loss=-0.1415, Q1 Loss=2.6064, Q2 Loss=2.6064, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7165
SAC Update 3/5: Actor Loss=-0.1478, Q1 Loss=1.1800, Q2 Loss=1.1800, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9536
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=6.5651, Q2 Loss=6.5651, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9566
SAC Update 5/5: Actor Loss=-0.1346, Q1 Loss=1.5848, Q2 Loss=1.5848, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2499

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.5%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (18.1%)
Actor update: 0.11s (39.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.130835
Q1 loss: 2.966950
Q2 loss: 2.966950
Current threshold: -32.2150
Global Scale Offset: 0.1531
Reward stats: mean=0.0004, std=0.0753, count=115
----------------------------------------------
SAC Update - Actor Loss: -0.1308, Q1 Loss: 2.9670, Q2 Loss: 2.9670, Entropy: 0.0000, Mean TD Error: 1.7077, Threshold: -32.2150
Original likelihood: -19.474658966064453
Adjusted likelihood: -19.474658966064453
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0282, -0.0290, -0.3683], device='cuda:0')
5 turn
Sampling time 3.616087089991197
tensor([ 0.0868,  0.5862,  0.5332,  0.6042, -0.0679,  0.5292,  0.9294,  0.9682,
         1.3010,  0.3218,  0.2146,  0.9861,  0.0282, -0.0290, -0.3683,  3.0016],
       device='cuda:0')
Original likelihood: -18.477447509765625
Adjusted likelihood: -18.477447509765625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.233027889044024
Current ori: tensor([ 0.0282, -0.0290, -0.3683], device='cuda:0')
Middle force: tensor([0.8532, 2.2643, 0.5632, 1.4268, 0.7049, 1.0513, 0.5798, 0.5892, 0.5655,
        0.5434, 0.5901, 0.9168], device='cuda:0')
Thumb force: tensor([0.6580, 1.1131, 1.6160, 0.5712, 0.5400, 0.6535, 0.5153, 0.6225, 0.6405,
        1.2208, 1.1135, 0.5025], device='cuda:0')
Index force: tensor([0.6463, 1.6978, 0.6578, 0.7219, 0.7454, 0.7451, 0.6628, 0.6111, 0.5780,
        0.6518, 0.5546, 0.7270], device='cuda:0')
Storing NORMAL transition: reward=-0.0078 (scaled=-0.0078), steps=1
Reward stats updated: mean 0.0004 -> 0.0004, std: 0.0750
Collected 116 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6824, Q2 Loss=0.6824, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4908
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=2.1451, Q2 Loss=2.1451, Entropy=0.0061, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7932
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.5328, Q2 Loss=0.5328, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2940
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.1634, Q2 Loss=2.1634, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4092
SAC Update 5/5: Actor Loss=-0.0377, Q1 Loss=1.0472, Q2 Loss=1.0472, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6275

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (16.6%)
Q1 update: 0.05s (18.9%)
Q2 update: 0.06s (21.2%)
Actor update: 0.11s (40.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.053597
Q1 loss: 1.314187
Q2 loss: 1.314187
Current threshold: -32.2229
Global Scale Offset: 0.1525
Reward stats: mean=0.0004, std=0.0750, count=116
----------------------------------------------
SAC Update - Actor Loss: -0.0536, Q1 Loss: 1.3142, Q2 Loss: 1.3142, Entropy: 0.0012, Mean TD Error: 1.1230, Threshold: -32.2229
tensor([-0.0102,  0.5413,  0.4843,  0.6172,  0.0319,  0.4551,  0.8871,  0.9263,
         1.3411,  0.2053,  0.1542,  1.1112,  0.0817, -0.0516, -0.3678,  3.1504],
       device='cuda:0')
Original likelihood: -31.368026733398438
Adjusted likelihood: -31.368026733398438
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9591)
Solve time for step 2 5.567472382972483
Current ori: tensor([ 0.0817, -0.0516, -0.3678], device='cuda:0')
Middle force: tensor([1.2034, 1.4762, 0.5725, 0.5179, 0.5094, 0.5041, 0.5311, 0.5764, 0.5820,
        0.6144, 0.6830], device='cuda:0')
Thumb force: tensor([1.1299, 3.3028, 0.5340, 0.5740, 1.1448, 0.5034, 0.8903, 0.5138, 0.5645,
        0.5718, 0.5492], device='cuda:0')
Index force: tensor([0.5273, 1.3746, 0.5464, 0.5833, 0.7199, 0.5716, 0.7020, 0.5722, 0.5699,
        0.5808, 0.5735], device='cuda:0')
Storing NORMAL transition: reward=0.0412 (scaled=0.0412), steps=1
Reward stats updated: mean 0.0004 -> 0.0007, std: 0.0747
Collected 117 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=3.2056, Q2 Loss=3.2056, Entropy=0.0008, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2671
SAC Update 2/5: Actor Loss=-0.1998, Q1 Loss=2.7940, Q2 Loss=2.7940, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6805
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=2.7243, Q2 Loss=2.7243, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6014
SAC Update 4/5: Actor Loss=-0.2210, Q1 Loss=2.4471, Q2 Loss=2.4471, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5001
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.3925, Q2 Loss=1.3925, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0213

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.6%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (19.8%)
Actor update: 0.09s (38.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.222322
Q1 loss: 2.512700
Q2 loss: 2.512700
Current threshold: -32.2276
Global Scale Offset: 0.1522
Reward stats: mean=0.0007, std=0.0747, count=117
----------------------------------------------
SAC Update - Actor Loss: -0.2223, Q1 Loss: 2.5127, Q2 Loss: 2.5127, Entropy: 0.0002, Mean TD Error: 1.4141, Threshold: -32.2276
tensor([ 0.0374,  0.5161,  0.5119,  0.6704,  0.0556,  0.5564,  0.8216,  0.8468,
         1.2972,  0.3054,  0.2782,  1.0694,  0.1190, -0.0170, -0.4152,  3.8467],
       device='cuda:0')
Original likelihood: -30.867813110351562
Adjusted likelihood: -30.867813110351562
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9972)
Solve time for step 3 5.089714228990488
Current ori: tensor([ 0.1190, -0.0170, -0.4152], device='cuda:0')
Middle force: tensor([0.5459, 1.2996, 0.7206, 1.0236, 0.5809, 0.9173, 0.5220, 0.7381, 0.5692,
        0.6271], device='cuda:0')
Thumb force: tensor([1.6497, 0.5576, 0.5333, 0.6206, 0.5102, 0.5954, 1.1415, 0.5924, 0.5079,
        0.5759], device='cuda:0')
Index force: tensor([0.6362, 0.7398, 0.6726, 0.7131, 0.6114, 0.7482, 0.5099, 0.7161, 0.5939,
        0.5879], device='cuda:0')
Storing NORMAL transition: reward=-0.0094 (scaled=-0.0094), steps=1
Reward stats updated: mean 0.0007 -> 0.0006, std: 0.0744
Collected 118 transitions for RL
SAC Update 1/5: Actor Loss=-0.0823, Q1 Loss=0.4920, Q2 Loss=0.4920, Entropy=0.0012, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2391
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9027, Q2 Loss=0.9027, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0307
SAC Update 3/5: Actor Loss=-0.1342, Q1 Loss=0.9720, Q2 Loss=0.9720, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8375
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.2906, Q2 Loss=1.2906, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0683
SAC Update 5/5: Actor Loss=-0.1738, Q1 Loss=3.2985, Q2 Loss=3.2985, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9852

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.8%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (18.9%)
Actor update: 0.10s (40.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.124111
Q1 loss: 1.391159
Q2 loss: 1.391159
Current threshold: -32.2428
Global Scale Offset: 0.1511
Reward stats: mean=0.0006, std=0.0744, count=118
----------------------------------------------
SAC Update - Actor Loss: -0.1241, Q1 Loss: 1.3912, Q2 Loss: 1.3912, Entropy: 0.0002, Mean TD Error: 1.0322, Threshold: -32.2428
tensor([-0.0095,  0.5702,  0.5164,  0.6472,  0.0461,  0.5929,  0.7929,  0.8828,
         1.4182,  0.2197,  0.3119,  0.9720,  0.1898, -0.0314, -0.4292,  4.6666],
       device='cuda:0')
Original likelihood: -34.750083923339844
Adjusted likelihood: -34.750083923339844
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 33.94981002807617
Projection step: 1, Loss: 33.76427459716797
Projection step: 2, Loss: 33.86998748779297
Projection step: 3, Loss: 33.312767028808594
Projection step: 4, Loss: 33.75238037109375
Projection step: 5, Loss: 33.637168884277344
Projection step: 6, Loss: 33.383888244628906
Projection step: 7, Loss: 33.429222106933594
Projection step: 8, Loss: 32.35897445678711
Projection step: 9, Loss: 32.2695426940918
Projection step: 10, Loss: 30.74288558959961
Projection step: 11, Loss: 32.45514678955078
Projection step: 12, Loss: 32.75760269165039
Projection step: 13, Loss: 30.299335479736328
Projection step: 14, Loss: 31.176616668701172
Projection step: 15, Loss: 31.086490631103516
Projection step: 16, Loss: 31.265336990356445
Projection step: 17, Loss: 31.546588897705078
Projection step: 18, Loss: 29.56731414794922
Projection step: 19, Loss: 29.76633644104004
Projection step: 20, Loss: 31.060091018676758
Projection step: 21, Loss: 30.64110565185547
Projection step: 22, Loss: 31.179149627685547
Projection step: 23, Loss: 31.411285400390625
Projection step: 24, Loss: 30.431320190429688
Final likelihood: tensor([-30.8278, -32.8108, -31.0229, -26.3912, -30.3693, -32.2188, -30.6544,
        -28.1186, -29.8365, -27.9034, -32.0924, -31.4738, -28.5338, -27.5476,
        -33.1504, -30.9086])
Final projection likelihood: -30.2413
1 mode projection succeeded
New goal: tensor([ 0.0212,  0.5598,  0.5267,  0.6792,  0.0156,  0.5424,  0.7467,  0.8928,
         1.3949,  0.2049,  0.3225,  1.0100,  0.1850, -0.0314, -0.0759],
       device='cuda:0')
tensor([[0.0031]], device='cuda:0') tensor([[0.0097]], device='cuda:0') tensor([[0.0021]], device='cuda:0')
Original likelihood: -35.67036437988281
Adjusted likelihood: -35.67036437988281
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 35.67036437988281}
Current yaw: tensor([ 0.1898, -0.0314, -0.4292], device='cuda:0')
6 thumb_middle
tensor([-0.0095,  0.5702,  0.5164,  0.6472,  0.0461,  0.5929,  0.7929,  0.8828,
         1.4182,  0.2197,  0.3119,  0.9720,  0.1898, -0.0314, -0.4292,  4.6666],
       device='cuda:0')
Solve time for step 1 8.887733699986711
Current ori: tensor([ 0.1898, -0.0314, -0.4292], device='cuda:0')
Index force: tensor([0.5924, 0.6011, 0.5356, 0.5517], device='cuda:0')
tensor([ 0.0227,  0.5991,  0.5939,  0.7347,  0.0101,  0.6101,  0.7605,  0.8789,
         1.3552,  0.1924,  0.2336,  0.9569,  0.2311, -0.0802, -0.4138,  5.1251],
       device='cuda:0')
Solve time for step 2 3.613370878971182
Current ori: tensor([ 0.2311, -0.0802, -0.4138], device='cuda:0')
Index force: tensor([0.5929, 0.5247, 0.5463], device='cuda:0')
tensor([ 0.0351,  0.6744,  0.6623,  0.7835,  0.0656,  0.6445,  0.7780,  0.9079,
         1.3504,  0.1845,  0.2205,  0.9735,  0.2706, -0.1864, -0.3726,  6.0778],
       device='cuda:0')
Solve time for step 3 3.4913845430128276
Current ori: tensor([ 0.2706, -0.1864, -0.3726], device='cuda:0')
Index force: tensor([0.5219, 0.5404], device='cuda:0')
tensor([-0.0137,  0.7304,  0.6496,  0.6772,  0.1350,  0.7079,  0.8128,  0.8980,
         1.3424,  0.1879,  0.1973,  0.9555,  0.3594, -0.2905, -0.2969, -5.0673],
       device='cuda:0')
Solve time for step 4 3.3587564890040085
Current ori: tensor([ 0.3594, -0.2905, -0.2969], device='cuda:0')
Index force: tensor([0.5346], device='cuda:0')
Storing RECOVERY transition: reward=-0.2383 (scaled=-0.0794), steps=3
Reward stats updated: mean 0.0006 -> -0.0001, std: 0.0745
Collected 119 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.5216, Q2 Loss=0.5216, Entropy=0.0085, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3559
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1346, Q2 Loss=1.1346, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7683
SAC Update 3/5: Actor Loss=-0.1058, Q1 Loss=1.7448, Q2 Loss=1.7448, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5329
SAC Update 4/5: Actor Loss=-0.0850, Q1 Loss=0.4312, Q2 Loss=0.4312, Entropy=0.0771, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3161
SAC Update 5/5: Actor Loss=-0.1781, Q1 Loss=1.6804, Q2 Loss=1.6804, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2502

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.9%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (17.9%)
Actor update: 0.10s (38.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.073777
Q1 loss: 1.102512
Q2 loss: 1.102512
Current threshold: -32.2577
Global Scale Offset: 0.1499
Reward stats: mean=-0.0001, std=0.0745, count=119
----------------------------------------------
SAC Update - Actor Loss: -0.0738, Q1 Loss: 1.1025, Q2 Loss: 1.1025, Entropy: 0.0171, Mean TD Error: 0.8447, Threshold: -32.2577
Original likelihood: -208.52035522460938
Adjusted likelihood: -208.52035522460938
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 205.2799835205078
Projection step: 1, Loss: 197.61056518554688
Projection step: 2, Loss: 209.4679412841797
Projection step: 3, Loss: 215.7224884033203
Projection step: 4, Loss: 210.04193115234375
Projection step: 5, Loss: 188.3055419921875
Projection step: 6, Loss: 222.5178985595703
Projection step: 7, Loss: 189.65771484375
Projection step: 8, Loss: 221.34674072265625
Projection step: 9, Loss: 200.64932250976562
Projection step: 10, Loss: 214.23165893554688
Projection step: 11, Loss: 213.66769409179688
Projection step: 12, Loss: 208.80142211914062
Projection step: 13, Loss: 215.5574951171875
Projection step: 14, Loss: 172.87460327148438
Projection step: 15, Loss: 189.69541931152344
Projection step: 16, Loss: 199.16973876953125
Projection step: 17, Loss: 218.38412475585938
Projection step: 18, Loss: 223.67861938476562
Projection step: 19, Loss: 196.6158905029297
Projection step: 20, Loss: 192.31556701660156
Projection step: 21, Loss: 189.4410400390625
Projection step: 22, Loss: 215.33270263671875
Projection step: 23, Loss: 218.37307739257812
Projection step: 24, Loss: 216.25070190429688
Final likelihood: tensor([-214.2328, -151.1974, -153.6178, -195.7327, -154.2555, -245.3868,
        -223.5422, -217.8667, -204.6460, -216.1466, -231.2798, -255.5185,
        -210.3133, -229.0992, -212.6112, -213.0621])
Final projection likelihood: -208.0318
1 mode projection failed, trying anyway
New goal: tensor([ 0.0444,  0.8956,  0.5438,  0.7365,  0.2251,  0.7651,  0.9454,  1.0274,
         1.3094,  0.1937,  0.2729,  1.0997,  0.3587, -0.2865, -0.2989],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0267]], device='cuda:0') tensor([[0.0048]], device='cuda:0')
Original likelihood: -195.09579467773438
Adjusted likelihood: -195.09579467773438
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 195.09579467773438}
Current yaw: tensor([ 0.3584, -0.2858, -0.3119], device='cuda:0')
7 thumb_middle
tensor([ 0.0302,  0.9004,  0.5457,  0.7312,  0.2296,  0.7686,  0.9479,  1.0059,
         1.3221,  0.1825,  0.2729,  1.0894,  0.3584, -0.2858, -0.3119, -4.9452],
       device='cuda:0')
Solve time for step 1 9.064011887006927
Current ori: tensor([ 0.3584, -0.2858, -0.3119], device='cuda:0')
Index force: tensor([0.5917, 0.5661, 0.5746, 0.5948], device='cuda:0')
tensor([ 0.0381,  1.0584,  0.5521,  0.7448,  0.2234,  0.8677,  0.9879,  1.0215,
         1.2087,  0.2236,  0.1614,  1.1000,  0.3809, -0.3507, -0.1537, -5.6264],
       device='cuda:0')
Solve time for step 2 3.6394285719725303
Current ori: tensor([ 0.3809, -0.3507, -0.1537], device='cuda:0')
Index force: tensor([0.5626, 0.5678, 0.5881], device='cuda:0')
tensor([ 0.0284,  1.0772,  0.5416,  0.7307,  0.2121,  0.8759,  0.9910,  1.0271,
         1.2087,  0.2266,  0.1557,  1.1051,  0.3813, -0.3516, -0.1579,  4.9428],
       device='cuda:0')
Solve time for step 3 3.5005237890291028
Current ori: tensor([ 0.3813, -0.3516, -0.1579], device='cuda:0')
Index force: tensor([0.5634, 0.5808], device='cuda:0')
tensor([ 0.0341,  1.0569,  0.5522,  0.7229,  0.2191,  0.8647,  0.9887,  1.0194,
         1.2110,  0.2155,  0.1538,  1.0930,  0.3808, -0.3503, -0.1618,  3.1967],
       device='cuda:0')
Solve time for step 4 3.5503843940095976
Current ori: tensor([ 0.3808, -0.3503, -0.1618], device='cuda:0')
Index force: tensor([0.5756], device='cuda:0')
Storing RECOVERY transition: reward=-0.3091 (scaled=-0.1030), steps=3
Reward stats updated: mean -0.0001 -> -0.0009, std: 0.0747
Collected 120 transitions for RL
SAC Update 1/5: Actor Loss=-0.0905, Q1 Loss=0.7608, Q2 Loss=0.7608, Entropy=0.0006, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3693
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=5.4579, Q2 Loss=5.4579, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9026
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=2.4367, Q2 Loss=2.4367, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6446
SAC Update 4/5: Actor Loss=-0.0714, Q1 Loss=0.7519, Q2 Loss=0.7519, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8368
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.6566, Q2 Loss=0.6566, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9041

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.3%)
Q1 update: 0.05s (17.8%)
Q2 update: 0.05s (19.0%)
Actor update: 0.12s (44.7%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.078441
Q1 loss: 2.012776
Q2 loss: 2.012776
Current threshold: -32.2859
Global Scale Offset: 0.1477
Reward stats: mean=-0.0009, std=0.0747, count=120
----------------------------------------------
SAC Update - Actor Loss: -0.0784, Q1 Loss: 2.0128, Q2 Loss: 2.0128, Entropy: 0.0001, Mean TD Error: 1.3315, Threshold: -32.2859
Original likelihood: -274.33245849609375
Adjusted likelihood: -274.33245849609375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 306.75018310546875
Projection step: 1, Loss: 299.082275390625
Projection step: 2, Loss: 307.1465759277344
Projection step: 3, Loss: 290.67596435546875
Projection step: 4, Loss: 292.0569152832031
Projection step: 5, Loss: 305.3104248046875
Projection step: 6, Loss: 300.425537109375
Projection step: 7, Loss: 308.63299560546875
Projection step: 8, Loss: 293.3753356933594
Projection step: 9, Loss: 298.7701416015625
Projection step: 10, Loss: 307.68353271484375
Projection step: 11, Loss: 306.0207824707031
Projection step: 12, Loss: 295.7144775390625
Projection step: 13, Loss: 290.0569152832031
Projection step: 14, Loss: 283.4095458984375
Projection step: 15, Loss: 286.2132568359375
Projection step: 16, Loss: 291.49822998046875
Projection step: 17, Loss: 302.9438171386719
Projection step: 18, Loss: 292.71307373046875
Projection step: 19, Loss: 295.65045166015625
Projection step: 20, Loss: 296.286865234375
Projection step: 21, Loss: 292.14239501953125
Projection step: 22, Loss: 313.090576171875
Projection step: 23, Loss: 297.1100769042969
Projection step: 24, Loss: 289.2093811035156
Final likelihood: tensor([-255.7827, -247.4487, -316.5014, -261.7071, -300.9711, -339.9145,
        -344.2176, -338.4605, -294.3407, -299.8738, -301.5709, -376.2635,
        -286.9913, -302.5253, -297.9562, -328.5873])
Final projection likelihood: -305.8195
1 mode projection failed, trying anyway
New goal: tensor([ 0.0444,  1.0278,  0.5837,  0.7431,  0.2291,  0.8433,  1.0039,  1.0350,
         1.2378,  0.1616,  0.1862,  1.0780,  0.3731, -0.3320, -0.2378],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0222]], device='cuda:0') tensor([[0.0063]], device='cuda:0')
Original likelihood: -231.24063110351562
Adjusted likelihood: -231.24063110351562
Likelihood residual: 0.0
Original likelihood: -277.34161376953125
Adjusted likelihood: -277.34161376953125
Likelihood residual: 0.0
{'index': 277.34161376953125, 'thumb_middle': 231.24063110351562}
Current yaw: tensor([ 0.3731, -0.3316, -0.2538], device='cuda:0')
8 thumb_middle
tensor([ 0.0407,  1.0283,  0.5792,  0.7417,  0.2289,  0.8422,  1.0072,  1.0312,
         1.2445,  0.1593,  0.1870,  1.0757,  0.3731, -0.3316, -0.2538,  4.3551],
       device='cuda:0')
Solve time for step 1 9.063477671996225
Current ori: tensor([ 0.3731, -0.3316, -0.2538], device='cuda:0')
Index force: tensor([0.5992, 0.5784, 0.5896, 0.6024], device='cuda:0')
tensor([ 0.0247,  1.0808,  0.5916,  0.7377,  0.2096,  0.8796,  1.0369,  1.0265,
         1.2038,  0.1894,  0.0611,  1.0734,  0.3841, -0.3615, -0.1943,  5.2822],
       device='cuda:0')
Solve time for step 2 3.6063348880270496
Current ori: tensor([ 0.3841, -0.3615, -0.1943], device='cuda:0')
Index force: tensor([0.5882, 0.5946, 0.5957], device='cuda:0')
tensor([ 0.0045,  1.1094,  0.5890,  0.7391,  0.1873,  0.9033,  1.0475,  1.0291,
         1.2049,  0.1839,  0.0591,  1.0763,  0.3839, -0.3609, -0.1963,  4.4194],
       device='cuda:0')
Solve time for step 3 3.3794544839765877
Current ori: tensor([ 0.3839, -0.3609, -0.1963], device='cuda:0')
Index force: tensor([0.5765, 0.5802], device='cuda:0')
tensor([ 7.5028e-04,  1.1246e+00,  5.9156e-01,  7.3921e-01,  1.8249e-01,
         9.1589e-01,  1.0521e+00,  1.0338e+00,  1.2060e+00,  1.7574e-01,
         7.9169e-02,  1.0761e+00,  3.8299e-01, -3.5819e-01, -2.0111e-01,
         4.0702e+00], device='cuda:0')
Solve time for step 4 3.4550791849615052
Current ori: tensor([ 0.3830, -0.3582, -0.2011], device='cuda:0')
Index force: tensor([0.5868], device='cuda:0')
Storing RECOVERY transition: reward=-0.3502 (scaled=-0.1167), steps=3
Reward stats updated: mean -0.0009 -> -0.0019, std: 0.0752
Collected 121 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8591, Q2 Loss=0.8591, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5080
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.6694, Q2 Loss=1.6694, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1998
SAC Update 3/5: Actor Loss=-0.1735, Q1 Loss=3.1083, Q2 Loss=3.1083, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9691
SAC Update 4/5: Actor Loss=-0.0674, Q1 Loss=0.7237, Q2 Loss=0.7237, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8194
SAC Update 5/5: Actor Loss=-0.1382, Q1 Loss=2.2263, Q2 Loss=2.2263, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6992

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (19.9%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (18.1%)
Actor update: 0.09s (39.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.075826
Q1 loss: 1.717384
Q2 loss: 1.717384
Current threshold: -32.3028
Global Scale Offset: 0.1465
Reward stats: mean=-0.0019, std=0.0752, count=121
----------------------------------------------
SAC Update - Actor Loss: -0.0758, Q1 Loss: 1.7174, Q2 Loss: 1.7174, Entropy: 0.0000, Mean TD Error: 1.2391, Threshold: -32.3028
Original likelihood: -336.3017272949219
Adjusted likelihood: -336.3017272949219
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 8
Loaded trajectory sampler
Current yaw: tensor([-0.0013,  0.0144, -0.0310], device='cuda:0')
Current yaw: tensor([-0.0013,  0.0144, -0.0310], device='cuda:0')
1 turn
Sampling time 3.643367294978816
tensor([ 0.1267,  0.6016,  0.5836,  0.5531, -0.1420,  0.5286,  0.9172,  0.9954,
         1.1918,  0.3556,  0.2798,  1.1508, -0.0013,  0.0144, -0.0310,  0.2416],
       device='cuda:0')
Original likelihood: -24.078868865966797
Adjusted likelihood: -24.078868865966797
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.782340609002858
Current ori: tensor([-0.0013,  0.0144, -0.0310], device='cuda:0')
Middle force: tensor([1.3051, 1.2196, 0.6476, 0.7614, 0.7544, 0.7551, 0.9864, 0.4978, 1.2040,
        0.6494, 1.3447, 0.5660], device='cuda:0')
Thumb force: tensor([1.2065, 1.3040, 1.2503, 1.2125, 0.5894, 0.9378, 1.0872, 0.6151, 0.9453,
        0.6002, 0.5665, 0.5785], device='cuda:0')
Index force: tensor([1.6113, 0.5478, 0.5670, 0.6889, 0.6117, 1.0827, 1.4072, 0.7523, 0.5066,
        0.5667, 0.5631, 0.5883], device='cuda:0')
Storing NORMAL transition: reward=0.1249 (scaled=0.1249), steps=1
Reward stats updated: mean -0.0019 -> -0.0008, std: 0.0757
Collected 122 transitions for RL
SAC Update 1/5: Actor Loss=-0.1992, Q1 Loss=1.4375, Q2 Loss=1.4375, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0589
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=7.8478, Q2 Loss=7.8478, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.3931
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=3.1690, Q2 Loss=3.1690, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8626
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0679, Q2 Loss=1.0679, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8104
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.4809, Q2 Loss=0.4809, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9166

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.0%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (42.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.085883
Q1 loss: 2.800620
Q2 loss: 2.800620
Current threshold: -32.3128
Global Scale Offset: 0.1457
Reward stats: mean=-0.0008, std=0.0757, count=122
----------------------------------------------
SAC Update - Actor Loss: -0.0859, Q1 Loss: 2.8006, Q2 Loss: 2.8006, Entropy: 0.0000, Mean TD Error: 1.6083, Threshold: -32.3128
tensor([ 0.1495,  0.6589,  0.5881,  0.4129, -0.0669,  0.4550,  0.9452,  1.1016,
         1.3216,  0.1443,  0.2297,  1.0652, -0.0199, -0.0072, -0.1563,  0.2544],
       device='cuda:0')
Original likelihood: -24.138160705566406
Adjusted likelihood: -24.138160705566406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.602095806971192
Current ori: tensor([-0.0199, -0.0072, -0.1563], device='cuda:0')
Middle force: tensor([1.2037, 0.6436, 0.7512, 0.7547, 0.7542, 0.9926, 0.5011, 1.1843, 0.6432,
        1.3179, 0.5630], device='cuda:0')
Thumb force: tensor([1.2719, 1.2207, 1.1800, 0.5766, 0.8950, 1.0092, 0.6072, 0.9289, 0.5926,
        0.5628, 0.5742], device='cuda:0')
Index force: tensor([0.5439, 0.5653, 0.6863, 0.6126, 1.0840, 1.4061, 0.7659, 0.5059, 0.5654,
        0.5617, 0.5852], device='cuda:0')
Storing NORMAL transition: reward=0.3144 (scaled=0.3144), steps=1
Reward stats updated: mean -0.0008 -> 0.0017, std: 0.0806
Collected 123 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.8492, Q2 Loss=2.8492, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2953
SAC Update 2/5: Actor Loss=-0.0986, Q1 Loss=0.8523, Q2 Loss=0.8523, Entropy=0.0003, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7929
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8896, Q2 Loss=0.8896, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4835
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7529, Q2 Loss=0.7529, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4877
SAC Update 5/5: Actor Loss=-0.0444, Q1 Loss=0.5492, Q2 Loss=0.5492, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6132

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (18.4%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (18.6%)
Actor update: 0.10s (39.6%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.028605
Q1 loss: 1.178631
Q2 loss: 1.178631
Current threshold: -32.3305
Global Scale Offset: 0.1443
Reward stats: mean=0.0017, std=0.0806, count=123
----------------------------------------------
SAC Update - Actor Loss: -0.0286, Q1 Loss: 1.1786, Q2 Loss: 1.1786, Entropy: 0.0001, Mean TD Error: 0.9345, Threshold: -32.3305
tensor([ 0.2106,  0.6457,  0.6192,  0.5505,  0.0583,  0.3544,  1.1415,  1.2517,
         1.3877,  0.0381,  0.2001,  0.8930, -0.0121, -0.0543, -0.6182,  1.0347],
       device='cuda:0')
Original likelihood: -34.10371017456055
Adjusted likelihood: -34.10371017456055
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0001)
State is out of distribution
Projection step: 0, Loss: 34.93155288696289
Projection step: 1, Loss: 33.66687774658203
Projection step: 2, Loss: 33.42659378051758
Projection step: 3, Loss: 31.105207443237305
Projection step: 4, Loss: 33.601322174072266
Projection step: 5, Loss: 31.31106185913086
Projection step: 6, Loss: 30.766950607299805
Projection step: 7, Loss: 28.890108108520508
Projection step: 8, Loss: 29.692825317382812
Projection step: 9, Loss: 29.060243606567383
Projection step: 10, Loss: 28.717567443847656
Projection step: 11, Loss: 28.229881286621094
Projection step: 12, Loss: 28.14240074157715
Projection step: 13, Loss: 28.152339935302734
Projection step: 14, Loss: 24.580944061279297
Projection step: 15, Loss: 25.57114028930664
Projection step: 16, Loss: 26.06512451171875
Projection step: 17, Loss: 27.264419555664062
Projection step: 18, Loss: 26.355632781982422
Projection step: 19, Loss: 25.348587036132812
Projection step: 20, Loss: 24.355743408203125
Projection step: 21, Loss: 24.15695571899414
Projection step: 22, Loss: 24.069435119628906
Projection step: 23, Loss: 23.82095718383789
Projection step: 24, Loss: 22.550792694091797
Final likelihood: tensor([-19.5885, -28.5285, -29.9600, -23.6455, -23.8444, -26.7144, -26.3869,
        -23.2207, -23.5127, -20.0832, -25.2691, -21.5726, -23.1112, -31.6188,
        -28.0572, -21.4475])
Final projection likelihood: -24.7851
1 mode projection succeeded
New goal: tensor([ 0.1771,  0.5904,  0.5802,  0.7003,  0.0273,  0.3820,  0.9364,  1.2015,
         1.3761,  0.1301,  0.0878,  0.8851, -0.0122, -0.0508, -0.7881],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -28.172439575195312
Adjusted likelihood: -28.172439575195312
Likelihood residual: 0.0
Original likelihood: -34.50006866455078
Adjusted likelihood: -34.50006866455078
Likelihood residual: 0.0
{'index': 34.50006866455078, 'thumb_middle': 28.172439575195312}
Current yaw: tensor([-0.0121, -0.0543, -0.6182], device='cuda:0')
2 thumb_middle
tensor([ 0.2106,  0.6457,  0.6192,  0.5505,  0.0583,  0.3544,  1.1415,  1.2517,
         1.3877,  0.0381,  0.2001,  0.8930, -0.0121, -0.0543, -0.6182,  1.0347],
       device='cuda:0')
Solve time for step 1 9.008117716992274
Current ori: tensor([-0.0121, -0.0543, -0.6182], device='cuda:0')
Index force: tensor([0.5669, 0.5814, 0.5689, 0.5959], device='cuda:0')
tensor([ 0.2246,  0.6210,  0.6019,  0.6777, -0.0392,  0.3563,  0.9326,  1.2018,
         1.3608,  0.1111,  0.0745,  0.8717,  0.0041, -0.0563, -0.6161,  1.0668],
       device='cuda:0')
Solve time for step 2 3.5739011410041712
Current ori: tensor([ 0.0041, -0.0563, -0.6161], device='cuda:0')
Index force: tensor([0.5711, 0.5728, 0.5566], device='cuda:0')
tensor([ 0.2163,  0.6373,  0.5766,  0.6654, -0.0690,  0.3817,  0.9212,  1.2040,
         1.3468,  0.1236,  0.0692,  0.8951, -0.0015, -0.0530, -0.6161,  1.0472],
       device='cuda:0')
Solve time for step 3 3.414052441017702
Current ori: tensor([-0.0015, -0.0530, -0.6161], device='cuda:0')
Index force: tensor([0.5968, 0.5888], device='cuda:0')
tensor([ 0.2091,  0.6091,  0.5896,  0.6986, -0.0627,  0.3607,  0.9055,  1.2075,
         1.3834,  0.1210,  0.0585,  0.8691,  0.0071, -0.0459, -0.6161,  1.0517],
       device='cuda:0')
Solve time for step 4 3.3256109610083513
Current ori: tensor([ 0.0071, -0.0459, -0.6161], device='cuda:0')
Index force: tensor([0.5616], device='cuda:0')
Storing RECOVERY transition: reward=0.0108 (scaled=0.0054), steps=2
Reward stats updated: mean 0.0017 -> 0.0018, std: 0.0802
Collected 124 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.7084, Q2 Loss=1.7084, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4527
SAC Update 2/5: Actor Loss=-0.1884, Q1 Loss=1.4438, Q2 Loss=1.4438, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1058
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3395, Q2 Loss=1.3395, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9499
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.2585, Q2 Loss=1.2585, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2479
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.5471, Q2 Loss=0.5471, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6642

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (16.4%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (18.8%)
Actor update: 0.09s (41.9%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.083736
Q1 loss: 1.259460
Q2 loss: 1.259460
Current threshold: -32.3433
Global Scale Offset: 0.1433
Reward stats: mean=0.0018, std=0.0802, count=124
----------------------------------------------
SAC Update - Actor Loss: -0.0837, Q1 Loss: 1.2595, Q2 Loss: 1.2595, Entropy: 0.0000, Mean TD Error: 1.0841, Threshold: -32.3433
Original likelihood: -30.160160064697266
Adjusted likelihood: -30.160160064697266
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0049, -0.0358, -0.6188], device='cuda:0')
3 turn
Sampling time 3.5968889590003528
tensor([ 0.1904,  0.6080,  0.5761,  0.6867, -0.0035,  0.4328,  0.9379,  1.1998,
         1.4543,  0.1493,  0.1219,  0.8990,  0.0049, -0.0358, -0.6188,  1.0375],
       device='cuda:0')
Original likelihood: -25.990079879760742
Adjusted likelihood: -25.990079879760742
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.090916509972885
Current ori: tensor([ 0.0049, -0.0358, -0.6188], device='cuda:0')
Middle force: tensor([0.4956, 0.5377, 0.7223, 0.6593, 0.5637, 0.8623, 0.8861, 0.5620, 0.6449,
        0.7581, 1.2496, 0.7975], device='cuda:0')
Thumb force: tensor([0.6250, 0.5202, 0.8176, 0.5472, 0.5893, 0.5652, 0.8871, 1.2732, 0.5535,
        0.7842, 1.6814, 0.5938], device='cuda:0')
Index force: tensor([0.5652, 0.7156, 1.0762, 0.6112, 0.6145, 0.5936, 0.5621, 0.5542, 0.5809,
        0.5271, 0.5148, 0.7679], device='cuda:0')
Storing NORMAL transition: reward=0.0501 (scaled=0.0501), steps=1
Reward stats updated: mean 0.0018 -> 0.0022, std: 0.0800
Collected 125 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.2908, Q2 Loss=1.2908, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9841
SAC Update 2/5: Actor Loss=-0.0001, Q1 Loss=1.1591, Q2 Loss=1.1591, Entropy=0.0170, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8349
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0051, Q2 Loss=1.0051, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8778
SAC Update 4/5: Actor Loss=-0.1384, Q1 Loss=2.0095, Q2 Loss=2.0095, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6397
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=5.3642, Q2 Loss=5.3642, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9712

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (19.0%)
Q2 update: 0.05s (18.7%)
Actor update: 0.10s (39.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.119791
Q1 loss: 2.165744
Q2 loss: 2.165744
Current threshold: -32.3509
Global Scale Offset: 0.1427
Reward stats: mean=0.0022, std=0.0800, count=125
----------------------------------------------
SAC Update - Actor Loss: -0.1198, Q1 Loss: 2.1657, Q2 Loss: 2.1657, Entropy: 0.0034, Mean TD Error: 1.4615, Threshold: -32.3509
tensor([ 0.2346,  0.5523,  0.7484,  0.5839,  0.0908,  0.5343,  0.9035,  1.1813,
         1.4961, -0.0465,  0.0899,  0.7779, -0.0157, -0.1014, -0.6788,  0.8673],
       device='cuda:0')
Original likelihood: -27.103219985961914
Adjusted likelihood: -27.103219985961914
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.485381646023598
Current ori: tensor([-0.0157, -0.1014, -0.6788], device='cuda:0')
Middle force: tensor([0.5519, 0.7919, 0.6551, 0.5620, 0.8452, 0.8878, 0.5737, 0.6695, 0.7815,
        1.2384, 0.8586], device='cuda:0')
Thumb force: tensor([0.5099, 0.7014, 0.5435, 0.5837, 0.5625, 0.8597, 1.2359, 0.5364, 0.7484,
        1.6328, 0.5491], device='cuda:0')
Index force: tensor([0.7139, 1.0722, 0.6187, 0.6185, 0.5902, 0.5574, 0.5451, 0.5734, 0.5235,
        0.5140, 0.7698], device='cuda:0')
Storing NORMAL transition: reward=0.0693 (scaled=0.0693), steps=1
Reward stats updated: mean 0.0022 -> 0.0027, std: 0.0799
Collected 126 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=3.5247, Q2 Loss=3.5247, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5722
SAC Update 2/5: Actor Loss=-0.1030, Q1 Loss=1.3751, Q2 Loss=1.3751, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1146
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8729, Q2 Loss=0.8729, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5728
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.4713, Q2 Loss=2.4713, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9847
SAC Update 5/5: Actor Loss=-0.0001, Q1 Loss=0.9541, Q2 Loss=0.9541, Entropy=0.0179, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5877

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (20.1%)
Q1 update: 0.04s (18.1%)
Q2 update: 0.04s (20.4%)
Actor update: 0.08s (37.9%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.066664
Q1 loss: 1.839619
Q2 loss: 1.839619
Current threshold: -32.3678
Global Scale Offset: 0.1413
Reward stats: mean=0.0027, std=0.0799, count=126
----------------------------------------------
SAC Update - Actor Loss: -0.0667, Q1 Loss: 1.8396, Q2 Loss: 1.8396, Entropy: 0.0036, Mean TD Error: 1.7664, Threshold: -32.3678
tensor([ 0.1966,  0.5875,  0.6931,  0.5191, -0.0669,  0.6392,  0.8784,  1.2420,
         1.4617,  0.1579,  0.1828,  0.5961, -0.0356, -0.0806, -0.7466,  0.8571],
       device='cuda:0')
Original likelihood: -25.75892448425293
Adjusted likelihood: -25.75892448425293
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.284473161038477
Current ori: tensor([-0.0356, -0.0806, -0.7466], device='cuda:0')
Middle force: tensor([0.8059, 0.6613, 0.5672, 0.8548, 0.8947, 0.5839, 0.6814, 0.8079, 1.2313,
        0.8708], device='cuda:0')
Thumb force: tensor([0.6837, 0.5411, 0.5784, 0.5562, 0.8371, 1.2107, 0.5325, 0.7322, 1.5947,
        0.5459], device='cuda:0')
Index force: tensor([1.0264, 0.6122, 0.6111, 0.5826, 0.5514, 0.5369, 0.5611, 0.5176, 0.5125,
        0.7420], device='cuda:0')
Storing NORMAL transition: reward=0.0073 (scaled=0.0073), steps=1
Reward stats updated: mean 0.0027 -> 0.0027, std: 0.0796
Collected 127 transitions for RL
SAC Update 1/5: Actor Loss=-0.0554, Q1 Loss=2.2273, Q2 Loss=2.2273, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2066
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.3958, Q2 Loss=2.3958, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6377
SAC Update 3/5: Actor Loss=-0.2072, Q1 Loss=1.6373, Q2 Loss=1.6373, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2076
SAC Update 4/5: Actor Loss=-0.1086, Q1 Loss=2.3337, Q2 Loss=2.3337, Entropy=0.0001, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2994
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=2.4579, Q2 Loss=2.4579, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2220

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.2%)
Q1 update: 0.04s (20.1%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (40.2%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.120285
Q1 loss: 2.210390
Q2 loss: 2.210390
Current threshold: -32.3870
Global Scale Offset: 0.1398
Reward stats: mean=0.0027, std=0.0796, count=127
----------------------------------------------
SAC Update - Actor Loss: -0.1203, Q1 Loss: 2.2104, Q2 Loss: 2.2104, Entropy: 0.0000, Mean TD Error: 1.7147, Threshold: -32.3870
tensor([ 0.1593,  0.5911,  0.6411,  0.5317, -0.0902,  0.6136,  0.9242,  1.1917,
         1.4646,  0.1852,  0.2374,  0.5149, -0.0392, -0.0591, -0.7505,  0.8314],
       device='cuda:0')
Original likelihood: -18.465457916259766
Adjusted likelihood: -18.465457916259766
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.0817369889700785
Current ori: tensor([-0.0392, -0.0591, -0.7505], device='cuda:0')
Middle force: tensor([0.6461, 0.5636, 0.8502, 0.8887, 0.5860, 0.6807, 0.8056, 1.2133, 0.8641],
       device='cuda:0')
Thumb force: tensor([0.5404, 0.5768, 0.5534, 0.8235, 1.1901, 0.5297, 0.7243, 1.5669, 0.5435],
       device='cuda:0')
Index force: tensor([0.6181, 0.6129, 0.5790, 0.5488, 0.5337, 0.5560, 0.5157, 0.5115, 0.7316],
       device='cuda:0')
Storing NORMAL transition: reward=-0.1848 (scaled=-0.1848), steps=1
Reward stats updated: mean 0.0027 -> 0.0013, std: 0.0810
Collected 128 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3592, Q2 Loss=1.3592, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5115
SAC Update 2/5: Actor Loss=-0.0001, Q1 Loss=2.4311, Q2 Loss=2.4311, Entropy=0.0192, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2596
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.3328, Q2 Loss=2.3328, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6277
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2205, Q2 Loss=1.2205, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9887
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.4596, Q2 Loss=1.4596, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1117

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.1%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.7%)
Actor update: 0.09s (40.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000013
Q1 loss: 1.760638
Q2 loss: 1.760638
Current threshold: -32.4066
Global Scale Offset: 0.1382
Reward stats: mean=0.0013, std=0.0810, count=128
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.7606, Q2 Loss: 1.7606, Entropy: 0.0038, Mean TD Error: 1.4998, Threshold: -32.4066
tensor([ 0.0279,  0.4750,  0.6749,  0.5522, -0.0985,  0.6950,  0.8604,  1.0179,
         1.5000,  0.0679,  0.2786,  0.3371, -0.0736, -0.0596, -0.5671,  1.7830],
       device='cuda:0')
Original likelihood: -26.503263473510742
Adjusted likelihood: -26.503263473510742
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.643343101954088
Current ori: tensor([-0.0736, -0.0596, -0.5671], device='cuda:0')
Middle force: tensor([0.5073, 0.5095, 1.1662, 0.6211, 0.5966, 0.5456, 0.6167, 0.5915],
       device='cuda:0')
Thumb force: tensor([0.8027, 0.5282, 0.6700, 0.5127, 0.5215, 0.5787, 0.5341, 0.5582],
       device='cuda:0')
Index force: tensor([0.6708, 0.5425, 1.0266, 0.5567, 0.5020, 0.5624, 0.5578, 0.5374],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0302 (scaled=-0.0302), steps=1
Reward stats updated: mean 0.0013 -> 0.0010, std: 0.0807
Collected 129 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.3002, Q2 Loss=1.3002, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9176
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.3958, Q2 Loss=2.3958, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7346
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.9596, Q2 Loss=1.9596, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4007
SAC Update 4/5: Actor Loss=-0.1089, Q1 Loss=1.4813, Q2 Loss=1.4813, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4700
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=2.1718, Q2 Loss=2.1718, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7184

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.2%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.7%)
Actor update: 0.08s (39.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.205982
Q1 loss: 1.861730
Q2 loss: 1.861730
Current threshold: -32.4182
Global Scale Offset: 0.1373
Reward stats: mean=0.0010, std=0.0807, count=129
----------------------------------------------
SAC Update - Actor Loss: -0.2060, Q1 Loss: 1.8617, Q2 Loss: 1.8617, Entropy: 0.0000, Mean TD Error: 1.4483, Threshold: -32.4182
tensor([ 0.0331,  0.4218,  0.6794,  0.5758, -0.1747,  0.6416,  0.9070,  1.0735,
         1.4196,  0.2543,  0.2663,  0.3855, -0.0939, -0.0785, -0.5415,  2.4277],
       device='cuda:0')
Original likelihood: -24.52463722229004
Adjusted likelihood: -24.52463722229004
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.381724727980327
Current ori: tensor([-0.0939, -0.0785, -0.5415], device='cuda:0')
Middle force: tensor([0.5585, 0.5919, 0.6405, 0.5650, 0.5211, 0.5109, 0.5348],
       device='cuda:0')
Thumb force: tensor([0.5121, 0.8455, 0.7178, 0.5156, 0.5293, 0.5442, 0.5261],
       device='cuda:0')
Index force: tensor([0.6074, 0.5211, 0.5047, 0.5014, 0.5255, 0.5527, 0.5539],
       device='cuda:0')
Storing NORMAL transition: reward=0.0297 (scaled=0.0297), steps=1
Reward stats updated: mean 0.0010 -> 0.0012, std: 0.0805
Collected 130 transitions for RL
SAC Update 1/5: Actor Loss=-0.1899, Q1 Loss=1.4051, Q2 Loss=1.4051, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0960
SAC Update 2/5: Actor Loss=-0.2302, Q1 Loss=2.6149, Q2 Loss=2.6149, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7467
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.9708, Q2 Loss=0.9708, Entropy=0.0005, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5690
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=3.2406, Q2 Loss=3.2406, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5167
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1651, Q2 Loss=1.1651, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8353

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (19.2%)
Actor update: 0.08s (39.7%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.130070
Q1 loss: 1.879292
Q2 loss: 1.879292
Current threshold: -32.4251
Global Scale Offset: 0.1367
Reward stats: mean=0.0012, std=0.0805, count=130
----------------------------------------------
SAC Update - Actor Loss: -0.1301, Q1 Loss: 1.8793, Q2 Loss: 1.8793, Entropy: 0.0001, Mean TD Error: 1.3527, Threshold: -32.4251
tensor([-0.0357,  0.4850,  0.5715,  0.8670, -0.1951,  0.8167,  0.9100,  0.9864,
         1.4810,  0.2689,  0.1657,  0.4385, -0.1197, -0.0676, -0.5754,  2.9952],
       device='cuda:0')
Original likelihood: -45.41801452636719
Adjusted likelihood: -45.41801452636719
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 41.781219482421875
Projection step: 1, Loss: 43.68406677246094
Projection step: 2, Loss: 41.99604797363281
Projection step: 3, Loss: 39.785484313964844
Projection step: 4, Loss: 40.3164176940918
Projection step: 5, Loss: 36.007347106933594
Projection step: 6, Loss: 39.344451904296875
Projection step: 7, Loss: 36.068626403808594
Projection step: 8, Loss: 39.240169525146484
Projection step: 9, Loss: 41.33745193481445
Projection step: 10, Loss: 35.92718505859375
Projection step: 11, Loss: 38.342681884765625
Projection step: 12, Loss: 38.34836196899414
Projection step: 13, Loss: 37.02186965942383
Projection step: 14, Loss: 34.88758087158203
Projection step: 15, Loss: 36.029273986816406
Projection step: 16, Loss: 33.79316711425781
Projection step: 17, Loss: 36.253028869628906
Projection step: 18, Loss: 34.65397262573242
Projection step: 19, Loss: 35.54241180419922
Projection step: 20, Loss: 33.785987854003906
Projection step: 21, Loss: 35.80907440185547
Projection step: 22, Loss: 33.656349182128906
Projection step: 23, Loss: 30.7561092376709
Projection step: 24, Loss: 33.01691818237305
Final likelihood: tensor([-36.7245, -36.7391, -46.2093, -24.6416, -46.6437, -28.6591, -35.1069,
        -25.2083, -33.9855, -32.3150, -23.5996, -32.5931, -24.6035, -34.6524,
        -45.1583, -34.9495])
Final projection likelihood: -33.8618
1 mode projection failed, trying anyway
New goal: tensor([ 0.0051,  0.4468,  0.6204,  0.8500, -0.1722,  0.7821,  0.8696,  1.0760,
         1.4475,  0.1891,  0.1965,  0.4489, -0.1182, -0.0615, -0.1552],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0018]], device='cuda:0')
Original likelihood: -38.60871505737305
Adjusted likelihood: -38.60871505737305
Likelihood residual: 0.0
Original likelihood: -43.217262268066406
Adjusted likelihood: -43.217262268066406
Likelihood residual: 0.0
{'index': 43.217262268066406, 'thumb_middle': 38.60871505737305}
Current yaw: tensor([-0.1197, -0.0676, -0.5754], device='cuda:0')
4 thumb_middle
tensor([-0.0357,  0.4850,  0.5715,  0.8670, -0.1951,  0.8167,  0.9100,  0.9864,
         1.4810,  0.2689,  0.1657,  0.4385, -0.1197, -0.0676, -0.5754,  2.9952],
       device='cuda:0')
Solve time for step 1 9.351517650997266
Current ori: tensor([-0.1197, -0.0676, -0.5754], device='cuda:0')
Index force: tensor([0.5950, 0.6034, 0.5921, 0.5860], device='cuda:0')
tensor([-0.0341,  0.5345,  0.6435,  0.8518, -0.2583,  0.7650,  0.8484,  1.0483,
         1.4061,  0.1949,  0.1048,  0.4132, -0.2494, -0.1450, -0.5414,  3.2338],
       device='cuda:0')
Solve time for step 2 3.6420735209831037
Current ori: tensor([-0.2494, -0.1450, -0.5414], device='cuda:0')
Index force: tensor([0.5864, 0.5794, 0.5754], device='cuda:0')
tensor([-0.0454,  0.6249,  0.6848,  0.8560, -0.2332,  0.8001,  0.8536,  1.0643,
         1.3823,  0.1818,  0.0537,  0.3931, -0.3775, -0.1936, -0.4090,  2.8661],
       device='cuda:0')
Solve time for step 3 3.4236861830577254
Current ori: tensor([-0.3775, -0.1936, -0.4090], device='cuda:0')
Index force: tensor([0.5030, 0.5000], device='cuda:0')
tensor([-0.0818,  0.7038,  0.7267,  0.8425, -0.2270,  0.8260,  0.8397,  1.0467,
         1.3959,  0.1745,  0.0849,  0.4069, -0.8084, -0.3215, -0.3630,  2.3908],
       device='cuda:0')
Solve time for step 4 3.199743881006725
Current ori: tensor([-0.8084, -0.3215, -0.3630], device='cuda:0')
Index force: tensor([0.5214], device='cuda:0')
Storing RECOVERY transition: reward=-1.2397 (scaled=-0.2066), steps=6
Reward stats updated: mean 0.0012 -> -0.0004, std: 0.0822
Collected 131 transitions for RL
SAC Update 1/5: Actor Loss=-0.1270, Q1 Loss=1.0130, Q2 Loss=1.0130, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9067
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8085, Q2 Loss=0.8085, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7842
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6335, Q2 Loss=0.6335, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3807
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8696, Q2 Loss=0.8696, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6509
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.7346, Q2 Loss=1.7346, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2595

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.7%)
Actor update: 0.08s (40.1%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.025395
Q1 loss: 1.011839
Q2 loss: 1.011839
Current threshold: -32.4293
Global Scale Offset: 0.1364
Reward stats: mean=-0.0004, std=0.0822, count=131
----------------------------------------------
SAC Update - Actor Loss: -0.0254, Q1 Loss: 1.0118, Q2 Loss: 1.0118, Entropy: 0.0000, Mean TD Error: 0.7964, Threshold: -32.4293
Original likelihood: -1130.544677734375
Adjusted likelihood: -1130.544677734375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 9
Loaded trajectory sampler
Current yaw: tensor([ 0.0004,  0.0138, -0.0500], device='cuda:0')
Current yaw: tensor([ 0.0004,  0.0138, -0.0500], device='cuda:0')
1 turn
Sampling time 3.6465716819511726
tensor([ 1.6087e-01,  5.7749e-01,  6.1222e-01,  6.2793e-01, -1.2468e-01,
         5.4456e-01,  9.2242e-01,  8.4558e-01,  1.2148e+00,  3.2006e-01,
         2.4042e-01,  1.2249e+00,  4.0869e-04,  1.3767e-02, -4.9980e-02,
         2.4401e-01], device='cuda:0')
Original likelihood: -20.663318634033203
Adjusted likelihood: -20.663318634033203
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.184393237985205
Current ori: tensor([ 0.0004,  0.0138, -0.0500], device='cuda:0')
Middle force: tensor([0.5107, 0.5736, 1.2626, 0.5662, 1.2189, 0.6397, 0.5385, 0.5146, 0.5099,
        0.5082, 0.4995, 0.5591], device='cuda:0')
Thumb force: tensor([0.8820, 0.8638, 0.8089, 1.0523, 1.0050, 0.6648, 0.5184, 0.8981, 0.5274,
        0.7929, 0.6889, 0.5585], device='cuda:0')
Index force: tensor([0.6276, 0.6233, 0.5384, 0.5658, 0.7969, 0.5256, 1.0094, 0.9605, 0.5622,
        0.6317, 0.7928, 0.5762], device='cuda:0')
Storing NORMAL transition: reward=0.0008 (scaled=0.0008), steps=1
Reward stats updated: mean -0.0004 -> -0.0003, std: 0.0819
Collected 132 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1616, Q2 Loss=1.1616, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1107
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=3.4442, Q2 Loss=3.4442, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5762
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.6115, Q2 Loss=2.6115, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0407
SAC Update 4/5: Actor Loss=-0.1400, Q1 Loss=1.0499, Q2 Loss=1.0499, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8946
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=2.7356, Q2 Loss=2.7356, Entropy=0.0006, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9344

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.2%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (19.4%)
Actor update: 0.10s (40.8%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.074058
Q1 loss: 2.200569
Q2 loss: 2.200569
Current threshold: -32.4317
Global Scale Offset: 0.1362
Reward stats: mean=-0.0003, std=0.0819, count=132
----------------------------------------------
SAC Update - Actor Loss: -0.0741, Q1 Loss: 2.2006, Q2 Loss: 2.2006, Entropy: 0.0001, Mean TD Error: 1.7113, Threshold: -32.4317
tensor([ 0.1772,  0.5616,  0.6403,  0.6472, -0.1740,  0.5285,  0.8953,  0.9039,
         1.1893,  0.3738,  0.1445,  1.2596,  0.0050,  0.0032, -0.0507,  0.3079],
       device='cuda:0')
Original likelihood: -31.439523696899414
Adjusted likelihood: -31.439523696899414
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9861)
Solve time for step 2 5.606452361971606
Current ori: tensor([ 0.0050,  0.0032, -0.0507], device='cuda:0')
Middle force: tensor([0.6380, 0.8062, 1.3569, 0.5029, 0.5997, 0.5005, 0.5575, 0.6470, 0.5021,
        0.5667, 0.5938], device='cuda:0')
Thumb force: tensor([0.6911, 1.1210, 1.2528, 1.7604, 0.5867, 0.5388, 0.5249, 0.5776, 0.9764,
        0.5471, 0.5644], device='cuda:0')
Index force: tensor([0.5469, 0.5673, 0.8358, 0.6483, 0.5459, 0.8204, 0.5105, 0.5794, 0.6530,
        0.5602, 0.5956], device='cuda:0')
Storing NORMAL transition: reward=-0.0455 (scaled=-0.0455), steps=1
Reward stats updated: mean -0.0003 -> -0.0007, std: 0.0816
Collected 133 transitions for RL
SAC Update 1/5: Actor Loss=-0.1186, Q1 Loss=1.1674, Q2 Loss=1.1674, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2450
SAC Update 2/5: Actor Loss=-0.1194, Q1 Loss=1.1293, Q2 Loss=1.1293, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4969
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.9062, Q2 Loss=1.9062, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3979
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=0.5601, Q2 Loss=0.5601, Entropy=0.0014, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1502
SAC Update 5/5: Actor Loss=-0.1308, Q1 Loss=1.6912, Q2 Loss=1.6912, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5710

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.0%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (19.8%)
Actor update: 0.11s (42.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.073753
Q1 loss: 1.290831
Q2 loss: 1.290831
Current threshold: -32.4629
Global Scale Offset: 0.1336
Reward stats: mean=-0.0007, std=0.0816, count=133
----------------------------------------------
SAC Update - Actor Loss: -0.0738, Q1 Loss: 1.2908, Q2 Loss: 1.2908, Entropy: 0.0003, Mean TD Error: 1.1722, Threshold: -32.4629
tensor([ 0.1961,  0.5566,  0.6544,  0.6807, -0.0944,  0.5954,  0.8439,  0.8965,
         1.2880,  0.1719,  0.2351,  1.1295,  0.0085, -0.0101, -0.0053,  0.3615],
       device='cuda:0')
Original likelihood: -26.544750213623047
Adjusted likelihood: -26.544750213623047
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.267023039981723
Current ori: tensor([ 0.0085, -0.0101, -0.0053], device='cuda:0')
Middle force: tensor([1.2296, 0.5596, 1.1531, 0.6299, 0.5359, 0.5080, 0.5080, 0.5032, 0.5003,
        0.5493], device='cuda:0')
Thumb force: tensor([0.8087, 1.0107, 0.9891, 0.6432, 0.5128, 0.8632, 0.5205, 0.7691, 0.6680,
        0.5502], device='cuda:0')
Index force: tensor([0.5268, 0.5577, 0.7488, 0.5198, 0.9556, 0.9477, 0.5601, 0.6378, 0.7706,
        0.5664], device='cuda:0')
Storing NORMAL transition: reward=0.1863 (scaled=0.1863), steps=1
Reward stats updated: mean -0.0007 -> 0.0007, std: 0.0829
Collected 134 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.5237, Q2 Loss=1.5237, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8701
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3453, Q2 Loss=1.3453, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0965
SAC Update 3/5: Actor Loss=-0.0872, Q1 Loss=1.1734, Q2 Loss=1.1734, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3263
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0417, Q2 Loss=1.0417, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0383
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4613, Q2 Loss=1.4613, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2853

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.0%)
Q1 update: 0.04s (18.2%)
Q2 update: 0.05s (19.4%)
Actor update: 0.10s (39.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.063501
Q1 loss: 1.309063
Q2 loss: 1.309063
Current threshold: -32.4840
Global Scale Offset: 0.1319
Reward stats: mean=0.0007, std=0.0829, count=134
----------------------------------------------
SAC Update - Actor Loss: -0.0635, Q1 Loss: 1.3091, Q2 Loss: 1.3091, Entropy: 0.0000, Mean TD Error: 1.1233, Threshold: -32.4840
tensor([ 0.1705,  0.5727,  0.5945,  0.6979, -0.2103,  0.5952,  0.9462,  0.9800,
         1.3363,  0.5743,  0.2489,  0.8404,  0.0057,  0.0066, -0.1916,  0.4772],
       device='cuda:0')
Original likelihood: -40.577545166015625
Adjusted likelihood: -40.577545166015625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 34.67076110839844
Projection step: 1, Loss: 31.980876922607422
Projection step: 2, Loss: 29.830486297607422
Projection step: 3, Loss: 26.99863052368164
Projection step: 4, Loss: 27.084983825683594
Projection step: 5, Loss: 28.153047561645508
Projection step: 6, Loss: 23.78424835205078
Projection step: 7, Loss: 24.1569766998291
Projection step: 8, Loss: 22.852386474609375
Projection step: 9, Loss: 22.00765609741211
Projection step: 10, Loss: 21.25309944152832
Projection step: 11, Loss: 20.265810012817383
Projection step: 12, Loss: 20.57972526550293
Projection step: 13, Loss: 21.16380500793457
Projection step: 14, Loss: 20.609600067138672
Projection step: 15, Loss: 19.811988830566406
Projection step: 16, Loss: 19.10434913635254
Projection step: 17, Loss: 18.506662368774414
Final likelihood: tensor([-17.6614, -17.6815, -18.2709, -18.7884, -17.5773, -20.2134, -17.0311,
        -17.5910, -24.4397, -19.5089, -16.2572, -17.7740, -19.0752, -14.6212,
        -19.3667, -20.2486])
Final projection likelihood: -18.5067
1 mode projection succeeded
New goal: tensor([ 0.1659,  0.5275,  0.5855,  0.6987, -0.1394,  0.5774,  0.9946,  0.9908,
         1.3829,  0.5298,  0.2251,  0.9320,  0.0039,  0.0105, -0.0547],
       device='cuda:0')
tensor([[0.0020]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -23.494022369384766
Adjusted likelihood: -23.494022369384766
Likelihood residual: 0.0
Original likelihood: -27.777164459228516
Adjusted likelihood: -27.777164459228516
Likelihood residual: 0.0
{'index': 27.777164459228516, 'thumb_middle': 23.494022369384766}
Current yaw: tensor([ 0.0057,  0.0066, -0.1916], device='cuda:0')
2 thumb_middle
tensor([ 0.1705,  0.5727,  0.5945,  0.6979, -0.2103,  0.5952,  0.9462,  0.9800,
         1.3363,  0.5743,  0.2489,  0.8404,  0.0057,  0.0066, -0.1916,  0.4772],
       device='cuda:0')
Solve time for step 1 9.040634185017552
Current ori: tensor([ 0.0057,  0.0066, -0.1916], device='cuda:0')
Index force: tensor([0.5944, 0.5793, 0.5937, 0.6044], device='cuda:0')
tensor([ 0.1690,  0.5666,  0.6017,  0.6937, -0.2740,  0.5351,  0.9259,  0.9582,
         1.3196,  0.5350,  0.1345,  0.8640,  0.0070,  0.0076, -0.1916,  0.4750],
       device='cuda:0')
Solve time for step 2 3.531498668016866
Current ori: tensor([ 0.0070,  0.0076, -0.1916], device='cuda:0')
Index force: tensor([0.5013, 0.5787, 0.5755], device='cuda:0')
tensor([ 0.1695,  0.5702,  0.5986,  0.6914, -0.2751,  0.5347,  0.9300,  0.9582,
         1.3271,  0.5290,  0.1179,  0.8699,  0.0060,  0.0071, -0.1916,  0.4750],
       device='cuda:0')
Solve time for step 3 3.3572157230228186
Current ori: tensor([ 0.0060,  0.0071, -0.1916], device='cuda:0')
Index force: tensor([0.5668, 0.5662], device='cuda:0')
tensor([ 0.1695,  0.5552,  0.6112,  0.7074, -0.2748,  0.5321,  0.9326,  0.9577,
         1.3280,  0.5251,  0.1143,  0.8765,  0.0107,  0.0081, -0.1916,  0.4813],
       device='cuda:0')
Solve time for step 4 3.222488933010027
Current ori: tensor([ 0.0107,  0.0081, -0.1916], device='cuda:0')
Index force: tensor([0.5743], device='cuda:0')
Storing RECOVERY transition: reward=-0.0033 (scaled=-0.0011), steps=3
Reward stats updated: mean 0.0007 -> 0.0007, std: 0.0826
Collected 135 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.2395, Q2 Loss=2.2395, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3314
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.2887, Q2 Loss=2.2887, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8815
SAC Update 3/5: Actor Loss=-0.1243, Q1 Loss=0.8324, Q2 Loss=0.8324, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2462
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.5859, Q2 Loss=1.5859, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2428
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.2815, Q2 Loss=1.2815, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9027

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (20.0%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (38.6%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.024863
Q1 loss: 1.645602
Q2 loss: 1.645602
Current threshold: -32.5075
Global Scale Offset: 0.1299
Reward stats: mean=0.0007, std=0.0826, count=135
----------------------------------------------
SAC Update - Actor Loss: -0.0249, Q1 Loss: 1.6456, Q2 Loss: 1.6456, Entropy: 0.0000, Mean TD Error: 1.5209, Threshold: -32.5075
Original likelihood: -28.6865234375
Adjusted likelihood: -28.6865234375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0150,  0.0176, -0.1889], device='cuda:0')
3 turn
Sampling time 3.630047590995673
tensor([ 0.1530,  0.5416,  0.6080,  0.7159, -0.2239,  0.5706,  0.9667,  0.9755,
         1.3876,  0.5319,  0.1762,  0.9155,  0.0150,  0.0176, -0.1889,  0.4637],
       device='cuda:0')
Original likelihood: -30.958391189575195
Adjusted likelihood: -30.958391189575195
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9998)
Solve time for step 1 13.813114559045061
Current ori: tensor([ 0.0150,  0.0176, -0.1889], device='cuda:0')
Middle force: tensor([0.5660, 0.5809, 0.5126, 2.2117, 1.1128, 0.5054, 0.6292, 0.5157, 0.5401,
        0.5557, 0.5273, 0.5236], device='cuda:0')
Thumb force: tensor([0.5387, 0.6507, 1.4233, 1.6702, 1.1603, 0.5998, 0.9444, 0.6578, 1.2664,
        0.6111, 1.0611, 0.5635], device='cuda:0')
Index force: tensor([0.5655, 0.5291, 0.5361, 1.0677, 0.5044, 0.5827, 0.8766, 0.6896, 0.5423,
        0.5578, 0.6861, 0.5783], device='cuda:0')
Storing NORMAL transition: reward=-0.0009 (scaled=-0.0009), steps=1
Reward stats updated: mean 0.0007 -> 0.0007, std: 0.0823
Collected 136 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=3.4216, Q2 Loss=3.4216, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6143
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=4.0250, Q2 Loss=4.0250, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7781
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7613, Q2 Loss=0.7613, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5313
SAC Update 4/5: Actor Loss=-0.2280, Q1 Loss=3.5462, Q2 Loss=3.5462, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2695
SAC Update 5/5: Actor Loss=-0.1873, Q1 Loss=1.3552, Q2 Loss=1.3552, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0678

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.9%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.8%)
Actor update: 0.08s (40.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.175180
Q1 loss: 2.621857
Q2 loss: 2.621857
Current threshold: -32.5271
Global Scale Offset: 0.1283
Reward stats: mean=0.0007, std=0.0823, count=136
----------------------------------------------
SAC Update - Actor Loss: -0.1752, Q1 Loss: 2.6219, Q2 Loss: 2.6219, Entropy: 0.0000, Mean TD Error: 1.8522, Threshold: -32.5271
tensor([ 0.1159,  0.5327,  0.6228,  0.6335, -0.2511,  0.5608,  0.9078,  1.0606,
         1.4817,  0.4150,  0.0233,  0.7572,  0.0107,  0.0383, -0.1890,  0.3757],
       device='cuda:0')
Original likelihood: -40.69287872314453
Adjusted likelihood: -40.69287872314453
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 40.66112518310547
Projection step: 1, Loss: 41.213958740234375
Projection step: 2, Loss: 40.26275634765625
Projection step: 3, Loss: 37.56020736694336
Projection step: 4, Loss: 37.438236236572266
Projection step: 5, Loss: 36.20649719238281
Projection step: 6, Loss: 35.67295837402344
Projection step: 7, Loss: 34.272621154785156
Projection step: 8, Loss: 34.058624267578125
Projection step: 9, Loss: 34.03602981567383
Projection step: 10, Loss: 31.6945743560791
Projection step: 11, Loss: 31.770587921142578
Projection step: 12, Loss: 30.658802032470703
Projection step: 13, Loss: 30.623445510864258
Projection step: 14, Loss: 29.334421157836914
Projection step: 15, Loss: 30.08004379272461
Projection step: 16, Loss: 28.467147827148438
Projection step: 17, Loss: 27.799640655517578
Projection step: 18, Loss: 28.18032455444336
Projection step: 19, Loss: 27.504337310791016
Projection step: 20, Loss: 27.11368179321289
Projection step: 21, Loss: 23.890682220458984
Projection step: 22, Loss: 25.809654235839844
Projection step: 23, Loss: 25.221303939819336
Projection step: 24, Loss: 24.781139373779297
Final likelihood: tensor([-25.6604, -25.5226, -24.7709, -25.2216, -25.6080, -27.3709, -21.8272,
        -22.1664, -27.1600, -26.8061, -31.7494, -22.4160, -20.5242, -28.7255,
        -26.6037, -18.4577])
Final projection likelihood: -25.0369
1 mode projection succeeded
New goal: tensor([ 0.1181,  0.5124,  0.6154,  0.7108, -0.1616,  0.5906,  0.8975,  1.0007,
         1.5168,  0.4718,  0.0609,  0.9699,  0.0109,  0.0272, -0.7569],
       device='cuda:0')
tensor([[0.0154]], device='cuda:0') tensor([[0.0031]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -28.830596923828125
Adjusted likelihood: -28.830596923828125
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 28.830596923828125}
Current yaw: tensor([ 0.0107,  0.0383, -0.1890], device='cuda:0')
4 thumb_middle
tensor([ 0.1159,  0.5327,  0.6228,  0.6335, -0.2511,  0.5608,  0.9078,  1.0606,
         1.4817,  0.4150,  0.0233,  0.7572,  0.0107,  0.0383, -0.1890,  0.3757],
       device='cuda:0')
Solve time for step 1 8.688832186977379
Current ori: tensor([ 0.0107,  0.0383, -0.1890], device='cuda:0')
Index force: tensor([0.5795, 0.5941, 0.5904, 0.5986], device='cuda:0')
tensor([ 0.1401,  0.5220,  0.6234,  0.7111, -0.2808,  0.5615,  0.8581,  0.9926,
         1.4584,  0.4437, -0.0190,  0.9016,  0.0190,  0.0263, -0.1890,  0.4303],
       device='cuda:0')
Solve time for step 2 3.488833548966795
Current ori: tensor([ 0.0190,  0.0263, -0.1890], device='cuda:0')
Index force: tensor([0.5866, 0.5852, 0.5931], device='cuda:0')
tensor([ 0.1405,  0.5164,  0.6300,  0.7135, -0.2752,  0.5691,  0.8403,  0.9750,
         1.4606,  0.4732, -0.0274,  0.9118,  0.0205,  0.0264, -0.1890,  0.4315],
       device='cuda:0')
Solve time for step 3 3.3246395999449305
Current ori: tensor([ 0.0205,  0.0264, -0.1890], device='cuda:0')
Index force: tensor([0.5763, 0.5858], device='cuda:0')
tensor([ 0.1391,  0.5277,  0.6149,  0.7105, -0.2770,  0.5693,  0.8511,  0.9678,
         1.4617,  0.4579, -0.0328,  0.9203,  0.0176,  0.0266, -0.1890,  0.4283],
       device='cuda:0')
Solve time for step 4 3.239590840996243
Current ori: tensor([ 0.0176,  0.0266, -0.1890], device='cuda:0')
Index force: tensor([0.5695], device='cuda:0')
Storing RECOVERY transition: reward=0.0154 (scaled=0.0154), steps=1
Reward stats updated: mean 0.0007 -> 0.0008, std: 0.0820
Collected 137 transitions for RL
SAC Update 1/5: Actor Loss=-0.1214, Q1 Loss=0.4748, Q2 Loss=0.4748, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0915
SAC Update 2/5: Actor Loss=-0.1537, Q1 Loss=1.4616, Q2 Loss=1.4616, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2969
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.7514, Q2 Loss=1.7514, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3731
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4579, Q2 Loss=1.4579, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4747
SAC Update 5/5: Actor Loss=-0.1857, Q1 Loss=1.3753, Q2 Loss=1.3753, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0997

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (18.2%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.6%)
Actor update: 0.10s (40.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092181
Q1 loss: 1.304182
Q2 loss: 1.304182
Current threshold: -32.5551
Global Scale Offset: 0.1261
Reward stats: mean=0.0008, std=0.0820, count=137
----------------------------------------------
SAC Update - Actor Loss: -0.0922, Q1 Loss: 1.3042, Q2 Loss: 1.3042, Entropy: 0.0000, Mean TD Error: 1.0672, Threshold: -32.5551
Original likelihood: -34.05149459838867
Adjusted likelihood: -34.05149459838867
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0002)
State is out of distribution
Projection step: 0, Loss: 33.645538330078125
Projection step: 1, Loss: 33.59056854248047
Projection step: 2, Loss: 31.737720489501953
Projection step: 3, Loss: 31.944278717041016
Projection step: 4, Loss: 30.936174392700195
Projection step: 5, Loss: 30.362449645996094
Projection step: 6, Loss: 29.059185028076172
Projection step: 7, Loss: 29.547632217407227
Projection step: 8, Loss: 30.296062469482422
Projection step: 9, Loss: 29.82889175415039
Projection step: 10, Loss: 28.953662872314453
Projection step: 11, Loss: 28.149181365966797
Projection step: 12, Loss: 27.330570220947266
Projection step: 13, Loss: 26.351123809814453
Projection step: 14, Loss: 26.739028930664062
Projection step: 15, Loss: 25.6160888671875
Projection step: 16, Loss: 25.839954376220703
Projection step: 17, Loss: 25.724092483520508
Projection step: 18, Loss: 24.638580322265625
Projection step: 19, Loss: 24.041542053222656
Projection step: 20, Loss: 24.571279525756836
Projection step: 21, Loss: 24.30147361755371
Projection step: 22, Loss: 24.30555534362793
Projection step: 23, Loss: 23.22574806213379
Projection step: 24, Loss: 22.35742950439453
Final likelihood: tensor([-18.6984, -23.8293, -26.6373, -21.8960, -19.0788, -23.7440, -27.0451,
        -20.9755, -26.7850, -20.7518, -23.1022, -22.7560, -23.8121, -21.5911,
        -18.2996, -20.0779])
Final projection likelihood: -22.4425
1 mode projection succeeded
New goal: tensor([ 0.1268,  0.5168,  0.5974,  0.7063, -0.1483,  0.5949,  0.8938,  0.9256,
         1.5164,  0.4982,  0.0811,  1.0323,  0.0188,  0.0282, -0.7970],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -25.863128662109375
Adjusted likelihood: -25.863128662109375
Likelihood residual: 0.0
Original likelihood: -24.495452880859375
Adjusted likelihood: -24.495452880859375
Likelihood residual: 0.0
{'index': 24.495452880859375, 'thumb_middle': 25.863128662109375}
Current yaw: tensor([ 0.0195,  0.0358, -0.2046], device='cuda:0')
5 index
tensor([ 0.1217,  0.5192,  0.6102,  0.7085, -0.2370,  0.5963,  0.8909,  0.9847,
         1.5000,  0.4736,  0.0387,  0.9725,  0.0195,  0.0358, -0.2046,  0.4337],
       device='cuda:0')
Solve time for step 1 10.437247909954749
Current ori: tensor([ 0.0195,  0.0358, -0.2046], device='cuda:0')
Middle force: tensor([0.5261, 0.5970, 0.5804, 0.5794], device='cuda:0')
Thumb force: tensor([0.5635, 0.5423, 0.5487, 0.5175], device='cuda:0')
tensor([ 0.1641,  0.4711,  0.5575,  0.6840, -0.2443,  0.6069,  0.9235,  0.9620,
         1.4999,  0.4945,  0.0264,  0.9884,  0.0211,  0.0287, -0.2077, -0.1495],
       device='cuda:0')
Solve time for step 2 4.227389200008474
Current ori: tensor([ 0.0211,  0.0287, -0.2077], device='cuda:0')
Middle force: tensor([0.5952, 0.5759, 0.5739], device='cuda:0')
Thumb force: tensor([0.5369, 0.5460, 0.5161], device='cuda:0')
tensor([ 0.1661,  0.4766,  0.5534,  0.6821, -0.2374,  0.6133,  0.9218,  0.9461,
         1.4998,  0.4926,  0.0200,  0.9910,  0.0185,  0.0253, -0.2035, -0.5037],
       device='cuda:0')
Solve time for step 3 4.305840680957772
Current ori: tensor([ 0.0185,  0.0253, -0.2035], device='cuda:0')
Middle force: tensor([0.5706, 0.5691], device='cuda:0')
Thumb force: tensor([0.5419, 0.5147], device='cuda:0')
tensor([ 0.1644,  0.4747,  0.5550,  0.6818, -0.2455,  0.6229,  0.9284,  0.9515,
         1.4998,  0.4911,  0.0167,  0.9973,  0.0213,  0.0230, -0.2108, -0.6513],
       device='cuda:0')
Solve time for step 4 3.9567881310358644
Current ori: tensor([ 0.0213,  0.0230, -0.2108], device='cuda:0')
Middle force: tensor([0.5668], device='cuda:0')
Thumb force: tensor([0.5112], device='cuda:0')
Storing RECOVERY transition: reward=0.0113 (scaled=0.0113), steps=1
Reward stats updated: mean 0.0008 -> 0.0009, std: 0.0817
Collected 138 transitions for RL
SAC Update 1/5: Actor Loss=-0.1479, Q1 Loss=1.1100, Q2 Loss=1.1100, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9340
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.5370, Q2 Loss=2.5370, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3977
SAC Update 3/5: Actor Loss=-0.2259, Q1 Loss=1.6983, Q2 Loss=1.6983, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2623
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1628, Q2 Loss=1.1628, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4469
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.5533, Q2 Loss=0.5533, Entropy=0.0012, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1088

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.1%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (40.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.074755
Q1 loss: 1.412284
Q2 loss: 1.412284
Current threshold: -32.5717
Global Scale Offset: 0.1248
Reward stats: mean=0.0009, std=0.0817, count=138
----------------------------------------------
SAC Update - Actor Loss: -0.0748, Q1 Loss: 1.4123, Q2 Loss: 1.4123, Entropy: 0.0002, Mean TD Error: 1.2299, Threshold: -32.5717
Original likelihood: -31.025634765625
Adjusted likelihood: -31.025634765625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9999)
Current yaw: tensor([ 0.0170,  0.0215, -0.1995], device='cuda:0')
6 turn
Sampling time 3.5854621819453314
tensor([ 0.1184,  0.5300,  0.5951,  0.7058, -0.2406,  0.6323,  0.9197,  0.9340,
         1.4999,  0.4922,  0.0146,  0.9921,  0.0170,  0.0215, -0.1995, -0.6909],
       device='cuda:0')
Original likelihood: -31.50326919555664
Adjusted likelihood: -31.50326919555664
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9943)
Solve time for step 1 14.196606365963817
Current ori: tensor([ 0.0170,  0.0215, -0.1995], device='cuda:0')
Middle force: tensor([0.5186, 0.9560, 0.4980, 0.5506, 0.7485, 1.3011, 0.5527, 0.5638, 0.5214,
        0.5039, 0.5452, 0.5534], device='cuda:0')
Thumb force: tensor([1.1778, 1.0905, 0.6522, 0.5984, 0.7511, 0.5362, 0.6542, 1.2333, 0.6787,
        0.5607, 0.6136, 0.6661], device='cuda:0')
Index force: tensor([0.6372, 1.2425, 0.6493, 0.5617, 0.7586, 0.7997, 0.5497, 0.5772, 0.6325,
        0.5378, 0.6096, 0.5900], device='cuda:0')
Storing NORMAL transition: reward=0.0098 (scaled=0.0098), steps=1
Reward stats updated: mean 0.0009 -> 0.0009, std: 0.0814
Collected 139 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.5187, Q2 Loss=0.5187, Entropy=0.0121, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4377
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.8694, Q2 Loss=1.8694, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4309
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.1047, Q2 Loss=1.1047, Entropy=0.0169, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7036
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8131, Q2 Loss=0.8131, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7661
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.8454, Q2 Loss=1.8454, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4171

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (20.6%)
Q1 update: 0.04s (19.4%)
Q2 update: 0.04s (18.0%)
Actor update: 0.08s (38.2%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046057
Q1 loss: 1.230262
Q2 loss: 1.230262
Current threshold: -32.5816
Global Scale Offset: 0.1240
Reward stats: mean=0.0009, std=0.0814, count=139
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.2303, Q2 Loss: 1.2303, Entropy: 0.0058, Mean TD Error: 0.9511, Threshold: -32.5816
tensor([ 0.1212,  0.5761,  0.5623,  0.6526, -0.2291,  0.6512,  0.8752,  1.0010,
         1.5000,  0.4602,  0.0465,  0.9047,  0.0077,  0.0123, -0.2088, -0.9360],
       device='cuda:0')
Original likelihood: -30.250944137573242
Adjusted likelihood: -30.250944137573242
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.584447780041955
Current ori: tensor([ 0.0077,  0.0123, -0.2088], device='cuda:0')
Middle force: tensor([0.5528, 0.5022, 0.9955, 0.5272, 0.5145, 0.5606, 0.5315, 0.7651, 0.5583,
        0.5162, 0.5162], device='cuda:0')
Thumb force: tensor([1.7343, 0.6379, 1.2459, 0.9343, 0.6670, 0.5933, 0.5471, 0.7364, 0.5844,
        0.6636, 0.6709], device='cuda:0')
Index force: tensor([0.6925, 0.5498, 0.5267, 0.5395, 0.6176, 0.5088, 0.6136, 0.7095, 0.5427,
        0.6423, 0.5628], device='cuda:0')
Storing NORMAL transition: reward=-0.0044 (scaled=-0.0044), steps=1
Reward stats updated: mean 0.0009 -> 0.0009, std: 0.0811
Collected 140 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4347, Q2 Loss=1.4347, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5419
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.1413, Q2 Loss=2.1413, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7949
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=3.5190, Q2 Loss=3.5190, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3038
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.1847, Q2 Loss=2.1847, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9009
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7751, Q2 Loss=0.7751, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7368

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (38.8%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 2.010947
Q2 loss: 2.010947
Current threshold: -32.5875
Global Scale Offset: 0.1236
Reward stats: mean=0.0009, std=0.0811, count=140
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 2.0109, Q2 Loss: 2.0109, Entropy: 0.0000, Mean TD Error: 1.6556, Threshold: -32.5875
tensor([ 0.0993,  0.5684,  0.5195,  0.7097, -0.1865,  0.6411,  0.9393,  0.9928,
         1.4523,  0.4496,  0.0687,  0.9274,  0.0219, -0.0259, -0.2054,  0.3963],
       device='cuda:0')
Original likelihood: -31.763111114501953
Adjusted likelihood: -31.763111114501953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9754)
Solve time for step 3 5.199519285000861
Current ori: tensor([ 0.0219, -0.0259, -0.2054], device='cuda:0')
Middle force: tensor([0.5008, 0.5480, 0.8160, 1.2612, 0.5705, 0.5585, 0.5754, 0.5063, 0.5382,
        0.5619], device='cuda:0')
Thumb force: tensor([0.6334, 0.5685, 0.6102, 0.5235, 0.5949, 1.1879, 0.5566, 0.5253, 0.6009,
        0.6239], device='cuda:0')
Index force: tensor([0.6263, 0.5589, 0.7735, 0.7863, 0.5473, 0.5701, 0.6108, 0.5397, 0.6026,
        0.5820], device='cuda:0')
Storing NORMAL transition: reward=0.0146 (scaled=0.0146), steps=1
Reward stats updated: mean 0.0009 -> 0.0010, std: 0.0809
Collected 141 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=3.1427, Q2 Loss=3.1427, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6212
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.1104, Q2 Loss=1.1104, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6866
SAC Update 3/5: Actor Loss=-0.1464, Q1 Loss=0.5738, Q2 Loss=0.5738, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1871
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=3.7732, Q2 Loss=3.7732, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8310
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=3.0586, Q2 Loss=3.0586, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6600

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (20.5%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (18.8%)
Actor update: 0.09s (38.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.075329
Q1 loss: 2.331728
Q2 loss: 2.331728
Current threshold: -32.6035
Global Scale Offset: 0.1222
Reward stats: mean=0.0010, std=0.0809, count=141
----------------------------------------------
SAC Update - Actor Loss: -0.0753, Q1 Loss: 2.3317, Q2 Loss: 2.3317, Entropy: 0.0000, Mean TD Error: 1.7972, Threshold: -32.6035
tensor([ 0.1628,  0.6554,  0.5065,  0.6232, -0.1893,  0.7122,  0.7948,  1.0365,
         1.4275,  0.3536,  0.1815,  0.8045,  0.0314, -0.0350, -0.2210,  0.5476],
       device='cuda:0')
Original likelihood: -35.96028518676758
Adjusted likelihood: -35.96028518676758
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.331077575683594
Projection step: 1, Loss: 36.23286056518555
Projection step: 2, Loss: 35.81531524658203
Projection step: 3, Loss: 35.20694351196289
Projection step: 4, Loss: 35.44544219970703
Projection step: 5, Loss: 33.09986877441406
Projection step: 6, Loss: 33.195289611816406
Projection step: 7, Loss: 31.314579010009766
Projection step: 8, Loss: 32.25421142578125
Projection step: 9, Loss: 31.100948333740234
Projection step: 10, Loss: 31.02393341064453
Projection step: 11, Loss: 30.878604888916016
Projection step: 12, Loss: 31.73241424560547
Projection step: 13, Loss: 27.38112449645996
Projection step: 14, Loss: 29.876171112060547
Projection step: 15, Loss: 28.356334686279297
Projection step: 16, Loss: 27.740262985229492
Projection step: 17, Loss: 27.875652313232422
Projection step: 18, Loss: 26.608741760253906
Projection step: 19, Loss: 27.327043533325195
Projection step: 20, Loss: 25.925565719604492
Projection step: 21, Loss: 25.677175521850586
Projection step: 22, Loss: 26.245084762573242
Projection step: 23, Loss: 28.079368591308594
Projection step: 24, Loss: 25.405595779418945
Final likelihood: tensor([-27.2139, -29.4927, -25.0548, -31.2137, -21.1413, -23.4135, -27.3901,
        -22.9797, -29.3164, -25.8350, -31.6173, -21.8270, -21.6694, -28.1492,
        -21.5529, -30.3129])
Final projection likelihood: -26.1362
1 mode projection succeeded
New goal: tensor([ 0.1451,  0.6061,  0.5805,  0.6123, -0.1283,  0.6659,  0.8853,  0.9653,
         1.4689,  0.2934,  0.1786,  1.0099,  0.0272, -0.0291, -1.2250],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0051]], device='cuda:0') tensor([[0.0030]], device='cuda:0')
Original likelihood: -26.759052276611328
Adjusted likelihood: -26.759052276611328
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 26.759052276611328}
Current yaw: tensor([ 0.0314, -0.0350, -0.2210], device='cuda:0')
7 thumb_middle
tensor([ 0.1628,  0.6554,  0.5065,  0.6232, -0.1893,  0.7122,  0.7948,  1.0365,
         1.4275,  0.3536,  0.1815,  0.8045,  0.0314, -0.0350, -0.2210,  0.5476],
       device='cuda:0')
Solve time for step 1 9.022437979001552
Current ori: tensor([ 0.0314, -0.0350, -0.2210], device='cuda:0')
Index force: tensor([0.5744, 0.5743, 0.5642, 0.5802], device='cuda:0')
tensor([ 0.1536,  0.6188,  0.5581,  0.6030, -0.2337,  0.6430,  0.8228,  0.9482,
         1.3790,  0.2841,  0.0555,  0.9104,  0.0371, -0.0288, -0.2210,  0.5369],
       device='cuda:0')
Solve time for step 2 3.5429781500133686
Current ori: tensor([ 0.0371, -0.0288, -0.2210], device='cuda:0')
Index force: tensor([0.5694, 0.5573, 0.5582], device='cuda:0')
tensor([ 0.1401,  0.5842,  0.5819,  0.6204, -0.2457,  0.6462,  0.8204,  0.9316,
         1.3866,  0.2677,  0.0461,  0.9431,  0.0457, -0.0194, -0.2210,  0.5323],
       device='cuda:0')
Solve time for step 3 3.387157375982497
Current ori: tensor([ 0.0457, -0.0194, -0.2210], device='cuda:0')
Index force: tensor([0.5481, 0.5498], device='cuda:0')
tensor([ 0.1530,  0.5935,  0.5832,  0.6197, -0.2352,  0.6389,  0.8367,  0.9328,
         1.3877,  0.2752,  0.0278,  0.9476,  0.0442, -0.0265, -0.2210,  0.5481],
       device='cuda:0')
Solve time for step 4 3.3425224130041897
Current ori: tensor([ 0.0442, -0.0265, -0.2210], device='cuda:0')
Index force: tensor([0.5374], device='cuda:0')
Storing RECOVERY transition: reward=0.0019 (scaled=0.0006), steps=3
Reward stats updated: mean 0.0010 -> 0.0010, std: 0.0806
Collected 142 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7116, Q2 Loss=0.7116, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7497
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.9590, Q2 Loss=0.9590, Entropy=0.0002, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5553
SAC Update 3/5: Actor Loss=-0.0762, Q1 Loss=1.0753, Q2 Loss=1.0753, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3134
SAC Update 4/5: Actor Loss=-0.0782, Q1 Loss=1.1083, Q2 Loss=1.1083, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3650
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4228, Q2 Loss=1.4228, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4924

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.2%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.06s (20.4%)
Actor update: 0.12s (43.4%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.030877
Q1 loss: 1.055394
Q2 loss: 1.055394
Current threshold: -32.6194
Global Scale Offset: 0.1209
Reward stats: mean=0.0010, std=0.0806, count=142
----------------------------------------------
SAC Update - Actor Loss: -0.0309, Q1 Loss: 1.0554, Q2 Loss: 1.0554, Entropy: 0.0000, Mean TD Error: 1.0951, Threshold: -32.6194
Original likelihood: -31.249042510986328
Adjusted likelihood: -31.249042510986328
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9996)
Current yaw: tensor([ 0.0403, -0.0148, -0.2226], device='cuda:0')
8 turn
Sampling time 3.702106072974857
tensor([ 0.1279,  0.6009,  0.5536,  0.6066, -0.1861,  0.6849,  0.8589,  0.9635,
         1.4451,  0.2798,  0.1160,  0.9699,  0.0403, -0.0148, -0.2226,  0.5238],
       device='cuda:0')
Original likelihood: -32.087093353271484
Adjusted likelihood: -32.087093353271484
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9015)
Solve time for step 1 14.105961549037602
Current ori: tensor([ 0.0403, -0.0148, -0.2226], device='cuda:0')
Middle force: tensor([0.8346, 2.0329, 0.5541, 1.3926, 0.5992, 0.9717, 0.5165, 0.7744, 0.5012,
        0.6409, 0.5804, 0.6958], device='cuda:0')
Thumb force: tensor([0.7156, 1.3683, 1.6117, 0.5013, 0.5901, 0.6927, 0.5318, 0.6928, 1.2363,
        0.6985, 0.5959, 0.5512], device='cuda:0')
Index force: tensor([0.5507, 1.5547, 0.6222, 0.6872, 0.7114, 0.6990, 0.6526, 0.7585, 0.5655,
        0.7079, 0.5639, 0.5432], device='cuda:0')
Storing NORMAL transition: reward=0.0831 (scaled=0.0831), steps=1
Reward stats updated: mean 0.0010 -> 0.0016, std: 0.0806
Collected 143 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.5094, Q2 Loss=1.5094, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1500
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.8200, Q2 Loss=1.8200, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4901
SAC Update 3/5: Actor Loss=-0.1625, Q1 Loss=0.6417, Q2 Loss=0.6417, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6280
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0391, Q2 Loss=1.0391, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4579
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.8677, Q2 Loss=1.8677, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4477

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.6%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (20.4%)
Actor update: 0.11s (41.6%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.078550
Q1 loss: 1.375581
Q2 loss: 1.375581
Current threshold: -32.6419
Global Scale Offset: 0.1191
Reward stats: mean=0.0016, std=0.0806, count=143
----------------------------------------------
SAC Update - Actor Loss: -0.0785, Q1 Loss: 1.3756, Q2 Loss: 1.3756, Entropy: 0.0000, Mean TD Error: 1.4347, Threshold: -32.6419
tensor([ 0.0992,  0.5405,  0.5920,  0.6331, -0.1536,  0.6361,  0.9941,  0.8930,
         1.4883,  0.2825,  0.0044,  1.0310,  0.0456, -0.0428, -0.3084, -0.1913],
       device='cuda:0')
Original likelihood: -32.47186279296875
Adjusted likelihood: -32.47186279296875
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.6617)
Solve time for step 2 5.547040802019183
Current ori: tensor([ 0.0456, -0.0428, -0.3084], device='cuda:0')
Middle force: tensor([0.5173, 0.5105, 0.5264, 0.5732, 0.6305, 1.0332, 0.8060, 0.8186, 0.6013,
        0.5849, 0.5967], device='cuda:0')
Thumb force: tensor([1.8155, 1.2281, 0.5833, 1.0617, 0.7175, 1.4076, 0.5739, 0.6729, 0.6549,
        0.5213, 0.5738], device='cuda:0')
Index force: tensor([0.7719, 0.5818, 0.6286, 0.5476, 0.5433, 0.5662, 0.5173, 0.5555, 0.5620,
        0.5019, 0.5941], device='cuda:0')
Storing NORMAL transition: reward=0.0635 (scaled=0.0635), steps=1
Reward stats updated: mean 0.0016 -> 0.0020, std: 0.0805
Collected 144 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=2.0846, Q2 Loss=2.0846, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1987
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.3309, Q2 Loss=2.3309, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2915
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.3205, Q2 Loss=1.3205, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9268
SAC Update 4/5: Actor Loss=-0.0003, Q1 Loss=1.0058, Q2 Loss=1.0058, Entropy=0.1398, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0057
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8265, Q2 Loss=0.8265, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9448

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.1%)
Q1 update: 0.06s (19.8%)
Q2 update: 0.06s (20.2%)
Actor update: 0.12s (42.9%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046121
Q1 loss: 1.513645
Q2 loss: 1.513645
Current threshold: -32.6619
Global Scale Offset: 0.1174
Reward stats: mean=0.0020, std=0.0805, count=144
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.5136, Q2 Loss: 1.5136, Entropy: 0.0280, Mean TD Error: 1.0735, Threshold: -32.6619
tensor([ 0.0803,  0.5661,  0.5281,  0.6523, -0.1712,  0.6554,  0.9219,  0.9819,
         1.4876,  0.3635,  0.0390,  0.9607,  0.0407, -0.0314, -0.3709, -0.1939],
       device='cuda:0')
Original likelihood: -33.415626525878906
Adjusted likelihood: -33.415626525878906
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0309)
State is out of distribution
Projection step: 0, Loss: 33.601806640625
Projection step: 1, Loss: 32.99571228027344
Projection step: 2, Loss: 32.849090576171875
Projection step: 3, Loss: 31.644227981567383
Projection step: 4, Loss: 31.331533432006836
Projection step: 5, Loss: 30.990570068359375
Projection step: 6, Loss: 30.121089935302734
Projection step: 7, Loss: 30.290603637695312
Projection step: 8, Loss: 29.882198333740234
Projection step: 9, Loss: 28.90099334716797
Projection step: 10, Loss: 30.012752532958984
Projection step: 11, Loss: 29.376609802246094
Projection step: 12, Loss: 27.891319274902344
Projection step: 13, Loss: 25.83489990234375
Projection step: 14, Loss: 28.38144302368164
Projection step: 15, Loss: 26.86015510559082
Projection step: 16, Loss: 25.29665184020996
Projection step: 17, Loss: 26.15138053894043
Projection step: 18, Loss: 26.876392364501953
Projection step: 19, Loss: 27.034019470214844
Projection step: 20, Loss: 26.941747665405273
Projection step: 21, Loss: 27.618797302246094
Projection step: 22, Loss: 26.108850479125977
Projection step: 23, Loss: 25.940994262695312
Projection step: 24, Loss: 25.250396728515625
Final likelihood: tensor([-28.7303, -25.0531, -25.3529, -20.9369, -23.4650, -22.6385, -27.8159,
        -22.8849, -17.1093, -22.7269, -21.5238, -24.4617, -25.8850, -27.9684,
        -27.0563, -27.2109])
Final projection likelihood: -24.4262
1 mode projection succeeded
New goal: tensor([ 0.0937,  0.5562,  0.6207,  0.6698, -0.1095,  0.6148,  0.9252,  0.9493,
         1.4716,  0.2981,  0.1040,  1.0611,  0.0370, -0.0262, -1.5095],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -26.90753746032715
Adjusted likelihood: -26.90753746032715
Likelihood residual: 0.0
Original likelihood: -32.573486328125
Adjusted likelihood: -32.573486328125
Likelihood residual: 0.0
{'index': 32.573486328125, 'thumb_middle': 26.90753746032715}
Current yaw: tensor([ 0.0407, -0.0314, -0.3709], device='cuda:0')
9 thumb_middle
tensor([ 0.0803,  0.5661,  0.5281,  0.6523, -0.1712,  0.6554,  0.9219,  0.9819,
         1.4876,  0.3635,  0.0390,  0.9607,  0.0407, -0.0314, -0.3709, -0.1939],
       device='cuda:0')
Solve time for step 1 8.951489773986395
Current ori: tensor([ 0.0407, -0.0314, -0.3709], device='cuda:0')
Index force: tensor([0.5552, 0.5789, 0.5836, 0.5592], device='cuda:0')
tensor([ 0.0799,  0.5485,  0.5633,  0.6287, -0.2125,  0.5912,  0.8761,  0.9139,
         1.4024,  0.2986, -0.0077,  0.9798,  0.0431, -0.0313, -0.3709, -0.1886],
       device='cuda:0')
Solve time for step 2 3.668231885996647
Current ori: tensor([ 0.0431, -0.0313, -0.3709], device='cuda:0')
Index force: tensor([0.5680, 0.5749, 0.5520], device='cuda:0')
tensor([ 0.1006,  0.5230,  0.6034,  0.6587, -0.2043,  0.6029,  0.8833,  0.9247,
         1.3937,  0.2761, -0.0164,  0.9916,  0.0536, -0.0399, -0.3709, -0.1485],
       device='cuda:0')
Solve time for step 3 3.405257985985372
Current ori: tensor([ 0.0536, -0.0399, -0.3709], device='cuda:0')
Index force: tensor([0.5638, 0.5441], device='cuda:0')
tensor([ 0.0850,  0.5021,  0.6145,  0.6630, -0.2111,  0.5969,  0.8754,  0.9162,
         1.3970,  0.2808, -0.0125,  1.0111,  0.0582, -0.0315, -0.3709, -0.1619],
       device='cuda:0')
Solve time for step 4 3.318180317990482
Current ori: tensor([ 0.0582, -0.0315, -0.3709], device='cuda:0')
Index force: tensor([0.5570], device='cuda:0')
Storing RECOVERY transition: reward=0.0106 (scaled=0.0053), steps=2
Reward stats updated: mean 0.0020 -> 0.0020, std: 0.0802
Collected 145 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.9774, Q2 Loss=0.9774, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7216
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.5641, Q2 Loss=2.5641, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4874
SAC Update 3/5: Actor Loss=-0.0002, Q1 Loss=1.4552, Q2 Loss=1.4552, Entropy=0.0698, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8997
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9041, Q2 Loss=0.9041, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9231
SAC Update 5/5: Actor Loss=-0.1062, Q1 Loss=1.1099, Q2 Loss=1.1099, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1642

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.0%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (19.4%)
Actor update: 0.10s (42.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.021277
Q1 loss: 1.402146
Q2 loss: 1.402146
Current threshold: -32.6737
Global Scale Offset: 0.1164
Reward stats: mean=0.0020, std=0.0802, count=145
----------------------------------------------
SAC Update - Actor Loss: -0.0213, Q1 Loss: 1.4021, Q2 Loss: 1.4021, Entropy: 0.0140, Mean TD Error: 1.2392, Threshold: -32.6737
Original likelihood: -32.67720413208008
Adjusted likelihood: -32.67720413208008
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.4965)
State is out of distribution
Projection step: 0, Loss: 32.12936019897461
Projection step: 1, Loss: 31.600326538085938
Projection step: 2, Loss: 31.2098388671875
Projection step: 3, Loss: 30.94745635986328
Projection step: 4, Loss: 29.82234001159668
Projection step: 5, Loss: 28.25099754333496
Projection step: 6, Loss: 29.298866271972656
Projection step: 7, Loss: 28.43830108642578
Projection step: 8, Loss: 28.61825942993164
Projection step: 9, Loss: 29.1015625
Projection step: 10, Loss: 28.28380584716797
Projection step: 11, Loss: 27.99089813232422
Projection step: 12, Loss: 27.5817928314209
Projection step: 13, Loss: 27.211057662963867
Projection step: 14, Loss: 27.582622528076172
Projection step: 15, Loss: 25.996612548828125
Projection step: 16, Loss: 26.117977142333984
Projection step: 17, Loss: 26.479650497436523
Projection step: 18, Loss: 27.535640716552734
Projection step: 19, Loss: 26.361927032470703
Projection step: 20, Loss: 25.398014068603516
Projection step: 21, Loss: 25.233394622802734
Projection step: 22, Loss: 26.21251106262207
Projection step: 23, Loss: 24.921175003051758
Projection step: 24, Loss: 24.365829467773438
Final likelihood: tensor([-24.8023, -26.9275, -23.6232, -22.2156, -24.3581, -25.2289, -25.0585,
        -24.6712, -25.9603, -24.3856, -25.8561, -21.9149, -25.0107, -23.1337,
        -23.2079, -21.3023])
Final projection likelihood: -24.2285
1 mode projection succeeded
New goal: tensor([ 0.0903,  0.5030,  0.6579,  0.7314, -0.0873,  0.6042,  0.9019,  0.9105,
         1.4236,  0.2461,  0.1002,  1.1218,  0.0550, -0.0295, -1.0895],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -27.659523010253906
Adjusted likelihood: -27.659523010253906
Likelihood residual: 0.0
Original likelihood: -32.15743637084961
Adjusted likelihood: -32.15743637084961
Likelihood residual: 0.0
{'index': 32.15743637084961, 'thumb_middle': 27.659523010253906}
Current yaw: tensor([ 0.0584, -0.0342, -0.3842], device='cuda:0')
10 thumb_middle
tensor([ 0.0788,  0.4944,  0.6052,  0.6901, -0.1385,  0.6505,  0.9103,  0.9413,
         1.4671,  0.3059,  0.0422,  1.0356,  0.0584, -0.0342, -0.3842, -0.2057],
       device='cuda:0')
Solve time for step 1 9.10817576001864
Current ori: tensor([ 0.0584, -0.0342, -0.3842], device='cuda:0')
Index force: tensor([0.5601, 0.5919, 0.5863, 0.5871], device='cuda:0')
tensor([ 0.0785,  0.4858,  0.6155,  0.6920, -0.1811,  0.5925,  0.8595,  0.8845,
         1.3712,  0.2447, -0.0043,  1.0759,  0.0673, -0.0243, -0.3842, -0.0878],
       device='cuda:0')
Solve time for step 2 3.591047608992085
Current ori: tensor([ 0.0673, -0.0243, -0.3842], device='cuda:0')
Index force: tensor([0.5805, 0.5783, 0.5778], device='cuda:0')
tensor([ 0.0987,  0.4465,  0.6588,  0.7517, -0.1703,  0.5953,  0.8650,  0.8765,
         1.3734,  0.2372,  0.0087,  1.0629,  0.0878, -0.0238, -0.3842,  0.0547],
       device='cuda:0')
Solve time for step 3 3.423203408019617
Current ori: tensor([ 0.0878, -0.0238, -0.3842], device='cuda:0')
Index force: tensor([0.5673, 0.5679], device='cuda:0')
tensor([ 0.1153,  0.4683,  0.6500,  0.7421, -0.1789,  0.6006,  0.8564,  0.9027,
         1.3797,  0.2260, -0.0048,  1.0759,  0.0803, -0.0371, -0.3842, -0.0024],
       device='cuda:0')
Solve time for step 4 3.197015895973891
Current ori: tensor([ 0.0803, -0.0371, -0.3842], device='cuda:0')
Index force: tensor([0.5505], device='cuda:0')
Storing RECOVERY transition: reward=0.0038 (scaled=0.0019), steps=2
Reward stats updated: mean 0.0020 -> 0.0020, std: 0.0799
Collected 146 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2112, Q2 Loss=1.2112, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1693
SAC Update 2/5: Actor Loss=-0.0010, Q1 Loss=0.7227, Q2 Loss=0.7227, Entropy=0.3078, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6511
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1110, Q2 Loss=1.1110, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9256
SAC Update 4/5: Actor Loss=-0.2304, Q1 Loss=0.8522, Q2 Loss=0.8522, Entropy=0.0718, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4486
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.7141, Q2 Loss=1.7141, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6665

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.3%)
Q1 update: 0.04s (19.5%)
Q2 update: 0.04s (19.2%)
Actor update: 0.09s (40.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046280
Q1 loss: 1.122253
Q2 loss: 1.122253
Current threshold: -32.6804
Global Scale Offset: 0.1158
Reward stats: mean=0.0020, std=0.0799, count=146
----------------------------------------------
SAC Update - Actor Loss: -0.0463, Q1 Loss: 1.1223, Q2 Loss: 1.1223, Entropy: 0.0759, Mean TD Error: 1.1722, Threshold: -32.6804
Original likelihood: -32.714176177978516
Adjusted likelihood: -32.714176177978516
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.4663)
Current yaw: tensor([ 0.0761, -0.0277, -0.3800], device='cuda:0')
11 turn
Sampling time 3.622121969994623
tensor([ 0.0827,  0.4647,  0.6330,  0.7223, -0.1186,  0.6412,  0.9017,  0.8997,
         1.4358,  0.2466,  0.0738,  1.1104,  0.0761, -0.0277, -0.3800, -0.0589],
       device='cuda:0')
Original likelihood: -31.47135353088379
Adjusted likelihood: -31.47135353088379
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9988)
Solve time for step 1 14.203377412981354
Current ori: tensor([ 0.0761, -0.0277, -0.3800], device='cuda:0')
Middle force: tensor([1.3590, 0.8483, 0.8867, 1.4342, 1.3993, 0.7230, 0.5566, 0.7564, 0.5087,
        0.5103, 0.5810, 0.5821], device='cuda:0')
Thumb force: tensor([0.5992, 0.5175, 0.5842, 0.6135, 0.8901, 1.1035, 0.8887, 0.5759, 0.5586,
        0.5566, 0.5565, 0.5903], device='cuda:0')
Index force: tensor([0.8086, 0.7707, 0.6857, 0.5238, 0.9113, 0.7560, 0.5718, 0.5158, 0.5306,
        0.6437, 0.6750, 0.6337], device='cuda:0')
Storing NORMAL transition: reward=-0.0238 (scaled=-0.0238), steps=1
Reward stats updated: mean 0.0020 -> 0.0018, std: 0.0797
Collected 147 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=3.1438, Q2 Loss=3.1438, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7916
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=2.0206, Q2 Loss=2.0206, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6290
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7778, Q2 Loss=0.7778, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5568
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.4455, Q2 Loss=1.4455, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2274
SAC Update 5/5: Actor Loss=-0.0005, Q1 Loss=0.8463, Q2 Loss=0.8463, Entropy=0.1018, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7647

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (17.7%)
Q1 update: 0.05s (20.9%)
Q2 update: 0.05s (18.8%)
Actor update: 0.10s (39.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046158
Q1 loss: 1.646799
Q2 loss: 1.646799
Current threshold: -32.6845
Global Scale Offset: 0.1155
Reward stats: mean=0.0018, std=0.0797, count=147
----------------------------------------------
SAC Update - Actor Loss: -0.0462, Q1 Loss: 1.6468, Q2 Loss: 1.6468, Entropy: 0.0204, Mean TD Error: 1.3939, Threshold: -32.6845
tensor([ 0.1123,  0.4352,  0.6458,  0.8260, -0.2040,  0.7013,  0.9518,  0.8783,
         1.3617,  0.3027,  0.1683,  1.1386,  0.1094, -0.0178, -0.3612,  0.2757],
       device='cuda:0')
Original likelihood: -46.961769104003906
Adjusted likelihood: -46.961769104003906
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 47.79977035522461
Projection step: 1, Loss: 47.68125915527344
Projection step: 2, Loss: 46.626190185546875
Projection step: 3, Loss: 45.14446258544922
Projection step: 4, Loss: 45.333778381347656
Projection step: 5, Loss: 44.60682678222656
Projection step: 6, Loss: 44.42811965942383
Projection step: 7, Loss: 43.242576599121094
Projection step: 8, Loss: 41.826717376708984
Projection step: 9, Loss: 43.0037956237793
Projection step: 10, Loss: 42.93489074707031
Projection step: 11, Loss: 41.79636001586914
Projection step: 12, Loss: 41.975608825683594
Projection step: 13, Loss: 41.20577621459961
Projection step: 14, Loss: 40.23721694946289
Projection step: 15, Loss: 39.41244888305664
Projection step: 16, Loss: 39.1551628112793
Projection step: 17, Loss: 38.67698287963867
Projection step: 18, Loss: 39.228660583496094
Projection step: 19, Loss: 38.418521881103516
Projection step: 20, Loss: 38.52532958984375
Projection step: 21, Loss: 37.12984085083008
Projection step: 22, Loss: 37.220863342285156
Projection step: 23, Loss: 36.763526916503906
Projection step: 24, Loss: 36.52035903930664
Final likelihood: tensor([-38.6039, -33.2972, -32.2212, -34.6299, -34.8279, -38.2008, -33.1357,
        -33.1782, -34.1399, -36.8219, -33.7810, -37.2880, -34.1330, -33.9643,
        -42.0299, -34.0157])
Final projection likelihood: -35.2668
1 mode projection failed, trying anyway
New goal: tensor([ 0.1140,  0.4625,  0.6532,  0.8414, -0.1292,  0.6548,  0.9161,  0.9014,
         1.3517,  0.2163,  0.1600,  1.1892,  0.1031, -0.0202, -0.2765],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -37.095699310302734
Adjusted likelihood: -37.095699310302734
Likelihood residual: 0.0
Original likelihood: -48.77410888671875
Adjusted likelihood: -48.77410888671875
Likelihood residual: 0.0
{'index': 48.77410888671875, 'thumb_middle': 37.095699310302734}
Current yaw: tensor([ 0.1094, -0.0178, -0.3612], device='cuda:0')
12 thumb_middle
tensor([ 0.1123,  0.4352,  0.6458,  0.8260, -0.2040,  0.7013,  0.9518,  0.8783,
         1.3617,  0.3027,  0.1683,  1.1386,  0.1094, -0.0178, -0.3612,  0.2757],
       device='cuda:0')
Solve time for step 1 9.072185415017884
Current ori: tensor([ 0.1094, -0.0178, -0.3612], device='cuda:0')
Index force: tensor([0.5893, 0.5932, 0.5975, 0.6101], device='cuda:0')
tensor([ 0.1254,  0.4316,  0.6537,  0.8435, -0.2199,  0.6454,  0.8728,  0.8655,
         1.3104,  0.2149,  0.0827,  1.1449,  0.1128, -0.0229, -0.3612,  0.3071],
       device='cuda:0')
Solve time for step 2 3.4981122440076433
Current ori: tensor([ 0.1128, -0.0229, -0.3612], device='cuda:0')
Index force: tensor([0.5841, 0.5904, 0.6060], device='cuda:0')
tensor([ 0.1198,  0.4296,  0.6537,  0.8381, -0.2246,  0.6433,  0.8654,  0.8670,
         1.3167,  0.2009,  0.0765,  1.1527,  0.1123, -0.0207, -0.3612,  0.2973],
       device='cuda:0')
Solve time for step 3 3.492384022974875
Current ori: tensor([ 0.1123, -0.0207, -0.3612], device='cuda:0')
Index force: tensor([0.5833, 0.6007], device='cuda:0')
tensor([ 0.1227,  0.4272,  0.6520,  0.8529, -0.2224,  0.6453,  0.8668,  0.8661,
         1.3179,  0.1972,  0.0736,  1.1539,  0.1147, -0.0212, -0.3612,  0.3067],
       device='cuda:0')
Solve time for step 4 3.590573256020434
Current ori: tensor([ 0.1147, -0.0212, -0.3612], device='cuda:0')
Index force: tensor([0.5839], device='cuda:0')
Storing RECOVERY transition: reward=0.0035 (scaled=0.0035), steps=1
Reward stats updated: mean 0.0018 -> 0.0019, std: 0.0794
Collected 148 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.2190, Q2 Loss=1.2190, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1079
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.6613, Q2 Loss=2.6613, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0767
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=2.0776, Q2 Loss=2.0776, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2046
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8160, Q2 Loss=0.8160, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6944
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.4757, Q2 Loss=1.4757, Entropy=0.0044, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0491

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.9%)
Target Q: 0.05s (20.1%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (37.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092105
Q1 loss: 1.649928
Q2 loss: 1.649928
Current threshold: -32.6872
Global Scale Offset: 0.1153
Reward stats: mean=0.0019, std=0.0794, count=148
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.6499, Q2 Loss: 1.6499, Entropy: 0.0009, Mean TD Error: 1.2265, Threshold: -32.6872
Original likelihood: -40.949378967285156
Adjusted likelihood: -40.949378967285156
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 40.986289978027344
Projection step: 1, Loss: 41.44847106933594
Projection step: 2, Loss: 41.088783264160156
Projection step: 3, Loss: 41.91611099243164
Projection step: 4, Loss: 40.25916290283203
Projection step: 5, Loss: 39.52878189086914
Projection step: 6, Loss: 38.79338836669922
Projection step: 7, Loss: 40.12504577636719
Projection step: 8, Loss: 37.246559143066406
Projection step: 9, Loss: 38.87120056152344
Projection step: 10, Loss: 37.80874252319336
Projection step: 11, Loss: 36.22673416137695
Projection step: 12, Loss: 37.06853485107422
Projection step: 13, Loss: 38.380706787109375
Projection step: 14, Loss: 36.273353576660156
Projection step: 15, Loss: 37.088218688964844
Projection step: 16, Loss: 35.199615478515625
Projection step: 17, Loss: 36.14817810058594
Projection step: 18, Loss: 35.995052337646484
Projection step: 19, Loss: 35.39812469482422
Projection step: 20, Loss: 36.034576416015625
Projection step: 21, Loss: 34.84992218017578
Projection step: 22, Loss: 35.41948318481445
Projection step: 23, Loss: 35.336151123046875
Projection step: 24, Loss: 33.34040832519531
Final likelihood: tensor([-33.4699, -32.2356, -31.8214, -33.1246, -30.6495, -30.0179, -32.9598,
        -35.9039, -29.9044, -34.0315, -34.6030, -32.2186, -36.0328, -34.3849,
        -31.0471, -33.2057])
Final projection likelihood: -32.8507
1 mode projection failed, trying anyway
New goal: tensor([ 0.0992,  0.4743,  0.6451,  0.8167, -0.1052,  0.6341,  0.8780,  0.9195,
         1.3574,  0.1662,  0.1411,  1.2280,  0.1021, -0.0222, -0.3642],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -35.41896438598633
Adjusted likelihood: -35.41896438598633
Likelihood residual: 0.0
Original likelihood: -39.64483642578125
Adjusted likelihood: -39.64483642578125
Likelihood residual: 0.0
{'index': 39.64483642578125, 'thumb_middle': 35.41896438598633}
Current yaw: tensor([ 0.1078, -0.0188, -0.3645], device='cuda:0')
13 thumb_middle
tensor([ 0.0993,  0.4406,  0.6277,  0.8239, -0.1616,  0.6923,  0.9106,  0.8937,
         1.3759,  0.2161,  0.1400,  1.1959,  0.1078, -0.0188, -0.3645,  0.2088],
       device='cuda:0')
Solve time for step 1 9.009690865001176
Current ori: tensor([ 0.1078, -0.0188, -0.3645], device='cuda:0')
Index force: tensor([0.5870, 0.5930, 0.5884, 0.6052], device='cuda:0')
tensor([ 0.1026,  0.4481,  0.6286,  0.8093, -0.1978,  0.6286,  0.8366,  0.8837,
         1.3193,  0.1522,  0.0657,  1.1892,  0.1047, -0.0181, -0.3645,  0.2443],
       device='cuda:0')
Solve time for step 2 3.5761950320447795
Current ori: tensor([ 0.1047, -0.0181, -0.3645], device='cuda:0')
Index force: tensor([0.5844, 0.5814, 0.5979], device='cuda:0')
tensor([ 0.1048,  0.4527,  0.6253,  0.8078, -0.1986,  0.6259,  0.8325,  0.8863,
         1.3199,  0.1459,  0.0578,  1.1947,  0.1035, -0.0195, -0.3645,  0.2443],
       device='cuda:0')
Solve time for step 3 3.4163132600369863
Current ori: tensor([ 0.1035, -0.0195, -0.3645], device='cuda:0')
Index force: tensor([0.5711, 0.5889], device='cuda:0')
tensor([ 0.1169,  0.4290,  0.6512,  0.8387, -0.1872,  0.6298,  0.8341,  0.8883,
         1.3205,  0.1435,  0.0585,  1.1938,  0.1136, -0.0227, -0.3645,  0.2704],
       device='cuda:0')
Solve time for step 4 3.29927147901617
Current ori: tensor([ 0.1136, -0.0227, -0.3645], device='cuda:0')
Index force: tensor([0.5706], device='cuda:0')
Storing RECOVERY transition: reward=0.0069 (scaled=0.0069), steps=1
Reward stats updated: mean 0.0019 -> 0.0019, std: 0.0791
Collected 149 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=3.0503, Q2 Loss=3.0503, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6991
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.4897, Q2 Loss=2.4897, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5944
SAC Update 3/5: Actor Loss=-0.1814, Q1 Loss=0.6011, Q2 Loss=0.6011, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1653
SAC Update 4/5: Actor Loss=-0.1774, Q1 Loss=2.3440, Q2 Loss=2.3440, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9725
SAC Update 5/5: Actor Loss=-0.0001, Q1 Loss=2.3169, Q2 Loss=2.3169, Entropy=0.0257, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3860

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.8%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (40.8%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.117832
Q1 loss: 2.160408
Q2 loss: 2.160408
Current threshold: -32.7032
Global Scale Offset: 0.1140
Reward stats: mean=0.0019, std=0.0791, count=149
----------------------------------------------
SAC Update - Actor Loss: -0.1178, Q1 Loss: 2.1604, Q2 Loss: 2.1604, Entropy: 0.0051, Mean TD Error: 1.7635, Threshold: -32.7032
Original likelihood: -37.538795471191406
Adjusted likelihood: -37.538795471191406
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.619503021240234
Projection step: 1, Loss: 38.91576385498047
Projection step: 2, Loss: 36.658172607421875
Projection step: 3, Loss: 37.41632080078125
Projection step: 4, Loss: 36.51130676269531
Projection step: 5, Loss: 36.09557342529297
Projection step: 6, Loss: 35.83311462402344
Projection step: 7, Loss: 38.61502456665039
Projection step: 8, Loss: 36.00962829589844
Projection step: 9, Loss: 34.89870834350586
Projection step: 10, Loss: 36.22690963745117
Projection step: 11, Loss: 35.53725051879883
Projection step: 12, Loss: 34.139007568359375
Projection step: 13, Loss: 34.26665496826172
Projection step: 14, Loss: 34.91344451904297
Projection step: 15, Loss: 34.314056396484375
Projection step: 16, Loss: 34.67046356201172
Projection step: 17, Loss: 34.240440368652344
Projection step: 18, Loss: 33.567474365234375
Projection step: 19, Loss: 33.600128173828125
Projection step: 20, Loss: 32.657073974609375
Projection step: 21, Loss: 34.483829498291016
Projection step: 22, Loss: 33.352081298828125
Projection step: 23, Loss: 33.09446334838867
Projection step: 24, Loss: 33.2310676574707
Final likelihood: tensor([-32.4994, -55.6013, -30.7614, -34.1863, -35.7947, -34.0094, -35.1553,
        -31.2311, -32.5745, -30.3538, -32.5294, -31.6533, -34.1212, -32.0486,
        -38.1290, -31.0683])
Final projection likelihood: -34.4823
1 mode projection failed, trying anyway
New goal: tensor([ 0.0939,  0.4768,  0.6419,  0.7979, -0.0859,  0.6072,  0.8619,  0.9393,
         1.3628,  0.1441,  0.1337,  1.2561,  0.1009, -0.0235, -0.3930],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -31.2434139251709
Adjusted likelihood: -31.2434139251709
Likelihood residual: 0.0
Original likelihood: -40.8746337890625
Adjusted likelihood: -40.8746337890625
Likelihood residual: 0.0
{'index': 40.8746337890625, 'thumb_middle': 31.2434139251709}
Current yaw: tensor([ 0.1064, -0.0197, -0.3678], device='cuda:0')
14 thumb_middle
tensor([ 0.0968,  0.4431,  0.6234,  0.8210, -0.1277,  0.6698,  0.8846,  0.9185,
         1.3812,  0.1644,  0.1253,  1.2376,  0.1064, -0.0197, -0.3678,  0.1919],
       device='cuda:0')
Solve time for step 1 9.048780034005176
Current ori: tensor([ 0.1064, -0.0197, -0.3678], device='cuda:0')
Index force: tensor([0.5726, 0.5418, 0.5845, 0.5839], device='cuda:0')
tensor([ 0.1081,  0.4496,  0.6395,  0.7945, -0.1578,  0.6069,  0.8114,  0.9100,
         1.3211,  0.1228,  0.0389,  1.2282,  0.1031, -0.0212, -0.3678,  0.2501],
       device='cuda:0')
Solve time for step 2 3.5673389389994554
Current ori: tensor([ 0.1031, -0.0212, -0.3678], device='cuda:0')
Index force: tensor([0.5368, 0.5795, 0.5789], device='cuda:0')
tensor([ 0.1154,  0.4271,  0.6537,  0.8357, -0.1712,  0.6131,  0.8258,  0.8971,
         1.3285,  0.1268,  0.0510,  1.2044,  0.1136, -0.0222, -0.3678,  0.2704],
       device='cuda:0')
Solve time for step 3 3.3598102160030976
Current ori: tensor([ 0.1136, -0.0222, -0.3678], device='cuda:0')
Index force: tensor([0.5730, 0.5740], device='cuda:0')
tensor([ 0.1045,  0.4494,  0.6340,  0.7991, -0.1839,  0.5988,  0.8223,  0.9109,
         1.3202,  0.1212,  0.0517,  1.2262,  0.1033, -0.0197, -0.3678,  0.2443],
       device='cuda:0')
Solve time for step 4 3.2178868210176006
Current ori: tensor([ 0.1033, -0.0197, -0.3678], device='cuda:0')
Index force: tensor([0.5675], device='cuda:0')
Storing RECOVERY transition: reward=0.0068 (scaled=0.0068), steps=1
Reward stats updated: mean 0.0019 -> 0.0019, std: 0.0789
Collected 150 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0423, Q2 Loss=1.0423, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8136
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0714, Q2 Loss=1.0714, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6648
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.6343, Q2 Loss=0.6343, Entropy=0.0009, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3993
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=0.9816, Q2 Loss=0.9816, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0274
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8479, Q2 Loss=0.8479, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5810

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.3%)
Q1 update: 0.05s (20.3%)
Q2 update: 0.05s (17.9%)
Actor update: 0.10s (39.1%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 0.915484
Q2 loss: 0.915484
Current threshold: -32.7201
Global Scale Offset: 0.1126
Reward stats: mean=0.0019, std=0.0789, count=150
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 0.9155, Q2 Loss: 0.9155, Entropy: 0.0002, Mean TD Error: 0.6972, Threshold: -32.7201
Original likelihood: -36.63402557373047
Adjusted likelihood: -36.63402557373047
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 36.854087829589844
Projection step: 1, Loss: 36.03971862792969
Projection step: 2, Loss: 36.62865447998047
Projection step: 3, Loss: 35.55345916748047
Projection step: 4, Loss: 35.13770294189453
Projection step: 5, Loss: 35.02854919433594
Projection step: 6, Loss: 34.75335693359375
Projection step: 7, Loss: 33.885169982910156
Projection step: 8, Loss: 33.95820617675781
Projection step: 9, Loss: 33.889225006103516
Projection step: 10, Loss: 33.51780700683594
Projection step: 11, Loss: 32.91313934326172
Projection step: 12, Loss: 32.473411560058594
Projection step: 13, Loss: 33.08308410644531
Projection step: 14, Loss: 32.965492248535156
Projection step: 15, Loss: 31.84462547302246
Projection step: 16, Loss: 31.7435302734375
Projection step: 17, Loss: 32.67616271972656
Projection step: 18, Loss: 30.9975643157959
Projection step: 19, Loss: 31.098161697387695
Projection step: 20, Loss: 31.50824737548828
Projection step: 21, Loss: 30.676612854003906
Projection step: 22, Loss: 30.76367950439453
Projection step: 23, Loss: 31.617889404296875
Projection step: 24, Loss: 30.967159271240234
Final likelihood: tensor([-28.5673, -32.4607, -31.2652, -31.2123, -32.4573, -29.5532, -33.0668,
        -28.5822, -29.7122, -29.4694, -31.8795, -32.0295, -29.8156, -29.5632,
        -29.3527, -30.3221])
Final projection likelihood: -30.5818
1 mode projection succeeded
New goal: tensor([ 0.0826,  0.4766,  0.6432,  0.7653, -0.0856,  0.5988,  0.8304,  0.9590,
         1.3681,  0.1215,  0.1344,  1.2730,  0.0956, -0.0190, -0.6184],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -30.442352294921875
Adjusted likelihood: -30.442352294921875
Likelihood residual: 0.0
Original likelihood: -38.0721435546875
Adjusted likelihood: -38.0721435546875
Likelihood residual: 0.0
{'index': 38.0721435546875, 'thumb_middle': 30.442352294921875}
Current yaw: tensor([ 0.1015, -0.0151, -0.3666], device='cuda:0')
15 thumb_middle
tensor([ 0.0822,  0.4403,  0.6355,  0.7793, -0.1203,  0.6666,  0.8501,  0.9424,
         1.3941,  0.1432,  0.1151,  1.2500,  0.1015, -0.0151, -0.3666,  0.1631],
       device='cuda:0')
Solve time for step 1 8.676484711002558
Current ori: tensor([ 0.1015, -0.0151, -0.3666], device='cuda:0')
Index force: tensor([0.5573, 0.5688, 0.5872, 0.6028], device='cuda:0')
tensor([ 0.0823,  0.4399,  0.6375,  0.7765, -0.1740,  0.5968,  0.7896,  0.9255,
         1.3283,  0.1014,  0.0585,  1.2394,  0.1017, -0.0121, -0.3666,  0.1932],
       device='cuda:0')
Solve time for step 2 3.4862071530078538
Current ori: tensor([ 0.1017, -0.0121, -0.3666], device='cuda:0')
Index force: tensor([0.5608, 0.5805, 0.5972], device='cuda:0')
tensor([ 0.0961,  0.4593,  0.6313,  0.7635, -0.1707,  0.6019,  0.7930,  0.9292,
         1.3249,  0.0928,  0.0446,  1.2351,  0.0963, -0.0190, -0.3666,  0.2073],
       device='cuda:0')
Solve time for step 3 3.4464428359642625
Current ori: tensor([ 0.0963, -0.0190, -0.3666], device='cuda:0')
Index force: tensor([0.5740, 0.5912], device='cuda:0')
tensor([ 0.0863,  0.4632,  0.6214,  0.7545, -0.1802,  0.5970,  0.7908,  0.9290,
         1.3267,  0.0923,  0.0468,  1.2420,  0.0938, -0.0148, -0.3666,  0.1936],
       device='cuda:0')
Solve time for step 4 3.2990827079629526
Current ori: tensor([ 0.0938, -0.0148, -0.3666], device='cuda:0')
Index force: tensor([0.5743], device='cuda:0')
Storing RECOVERY transition: reward=0.0047 (scaled=0.0047), steps=1
Reward stats updated: mean 0.0019 -> 0.0019, std: 0.0786
Collected 151 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3044, Q2 Loss=1.3044, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0668
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5847, Q2 Loss=1.5847, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3236
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.9004, Q2 Loss=1.9004, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5601
SAC Update 4/5: Actor Loss=-0.2306, Q1 Loss=1.0094, Q2 Loss=1.0094, Entropy=0.0997, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7567
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.4067, Q2 Loss=2.4067, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0861

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.8%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.05s (18.9%)
Actor update: 0.09s (38.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046117
Q1 loss: 1.641131
Q2 loss: 1.641131
Current threshold: -32.7300
Global Scale Offset: 0.1118
Reward stats: mean=0.0019, std=0.0786, count=151
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.6411, Q2 Loss: 1.6411, Entropy: 0.0199, Mean TD Error: 1.3587, Threshold: -32.7300
Original likelihood: -34.03303527832031
Adjusted likelihood: -34.03303527832031
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0004)
State is out of distribution
Projection step: 0, Loss: 33.65826416015625
Projection step: 1, Loss: 34.44657897949219
Projection step: 2, Loss: 33.598724365234375
Projection step: 3, Loss: 32.680381774902344
Projection step: 4, Loss: 32.21949005126953
Projection step: 5, Loss: 32.475791931152344
Projection step: 6, Loss: 32.5235481262207
Projection step: 7, Loss: 31.746854782104492
Projection step: 8, Loss: 31.92398452758789
Projection step: 9, Loss: 31.494138717651367
Projection step: 10, Loss: 31.135385513305664
Projection step: 11, Loss: 31.188518524169922
Projection step: 12, Loss: 31.08187484741211
Projection step: 13, Loss: 30.808780670166016
Projection step: 14, Loss: 30.995807647705078
Projection step: 15, Loss: 30.160404205322266
Projection step: 16, Loss: 29.744701385498047
Projection step: 17, Loss: 30.25343894958496
Projection step: 18, Loss: 29.357664108276367
Projection step: 19, Loss: 29.794111251831055
Projection step: 20, Loss: 28.733543395996094
Projection step: 21, Loss: 30.236492156982422
Projection step: 22, Loss: 28.929821014404297
Projection step: 23, Loss: 30.41423797607422
Projection step: 24, Loss: 28.53451156616211
Final likelihood: tensor([-27.2277, -29.2014, -27.3339, -26.6475, -26.8238, -27.3077, -29.3320,
        -27.1139, -31.1494, -28.0720, -26.6683, -27.6419, -25.4006, -31.2702,
        -29.0381, -43.1054])
Final projection likelihood: -28.9584
1 mode projection succeeded
New goal: tensor([ 0.0693,  0.4824,  0.6234,  0.7437, -0.0788,  0.5760,  0.8140,  0.9710,
         1.3728,  0.1079,  0.1386,  1.3007,  0.0900, -0.0148, -0.7159],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -35.11248016357422
Adjusted likelihood: -35.11248016357422
Likelihood residual: 0.0
Original likelihood: -36.39582824707031
Adjusted likelihood: -36.39582824707031
Likelihood residual: 0.0
{'index': 36.39582824707031, 'thumb_middle': 35.11248016357422}
Current yaw: tensor([ 0.0961, -0.0110, -0.3633], device='cuda:0')
16 thumb_middle
tensor([ 0.0682,  0.4508,  0.6183,  0.7602, -0.1079,  0.6448,  0.8341,  0.9554,
         1.3948,  0.1234,  0.1102,  1.2772,  0.0961, -0.0110, -0.3633,  0.1270],
       device='cuda:0')
Solve time for step 1 8.725031312031206
Current ori: tensor([ 0.0961, -0.0110, -0.3633], device='cuda:0')
Index force: tensor([0.5755, 0.5921, 0.5010, 0.5971], device='cuda:0')
tensor([ 0.0652,  0.4477,  0.6302,  0.7387, -0.1737,  0.5769,  0.7784,  0.9476,
         1.3317,  0.0829,  0.0456,  1.2665,  0.0950, -0.0053, -0.3633,  0.1682],
       device='cuda:0')
Solve time for step 2 3.476107125985436
Current ori: tensor([ 0.0950, -0.0053, -0.3633], device='cuda:0')
Index force: tensor([0.5708, 0.5869, 0.4999], device='cuda:0')
tensor([ 0.0823,  0.4177,  0.6682,  0.7738, -0.1520,  0.5692,  0.7896,  0.9609,
         1.3300,  0.0855,  0.0336,  1.2604,  0.1072, -0.0111, -0.3633,  0.1958],
       device='cuda:0')
Solve time for step 3 3.348117920977529
Current ori: tensor([ 0.1072, -0.0111, -0.3633], device='cuda:0')
Index force: tensor([0.5765, 0.5000], device='cuda:0')
tensor([ 0.0540,  0.4268,  0.6603,  0.7155, -0.1882,  0.5625,  0.7742,  0.9357,
         1.3394,  0.0872,  0.0601,  1.2622,  0.0979,  0.0019, -0.3633,  0.1837],
       device='cuda:0')
Solve time for step 4 3.465163309010677
Current ori: tensor([ 0.0979,  0.0019, -0.3633], device='cuda:0')
Index force: tensor([0.5000], device='cuda:0')
Storing RECOVERY transition: reward=0.0041 (scaled=0.0041), steps=1
Reward stats updated: mean 0.0019 -> 0.0020, std: 0.0784
Collected 152 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.3523, Q2 Loss=1.3523, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8429
SAC Update 2/5: Actor Loss=-0.0005, Q1 Loss=1.4046, Q2 Loss=1.4046, Entropy=0.0972, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0103
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.3245, Q2 Loss=2.3245, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5007
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.1279, Q2 Loss=1.1279, Entropy=0.0010, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1498
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7232, Q2 Loss=0.7232, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3354

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (18.9%)
Actor update: 0.10s (38.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000096
Q1 loss: 1.386493
Q2 loss: 1.386493
Current threshold: -32.7354
Global Scale Offset: 0.1113
Reward stats: mean=0.0020, std=0.0784, count=152
----------------------------------------------
SAC Update - Actor Loss: -0.0001, Q1 Loss: 1.3865, Q2 Loss: 1.3865, Entropy: 0.0197, Mean TD Error: 1.3678, Threshold: -32.7354
Original likelihood: -36.659339904785156
Adjusted likelihood: -36.659339904785156
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 35.98332595825195
Projection step: 1, Loss: 35.783966064453125
Projection step: 2, Loss: 35.3663330078125
Projection step: 3, Loss: 34.89361572265625
Projection step: 4, Loss: 34.15686798095703
Projection step: 5, Loss: 32.36417770385742
Projection step: 6, Loss: 31.675243377685547
Projection step: 7, Loss: 34.19571304321289
Projection step: 8, Loss: 33.724430084228516
Projection step: 9, Loss: 32.27814483642578
Projection step: 10, Loss: 33.21257781982422
Projection step: 11, Loss: 33.01279067993164
Projection step: 12, Loss: 32.378623962402344
Projection step: 13, Loss: 32.32373809814453
Projection step: 14, Loss: 31.27730941772461
Projection step: 15, Loss: 32.176910400390625
Projection step: 16, Loss: 30.82343101501465
Projection step: 17, Loss: 31.581201553344727
Projection step: 18, Loss: 31.575393676757812
Projection step: 19, Loss: 30.833120346069336
Projection step: 20, Loss: 30.88332748413086
Projection step: 21, Loss: 29.878658294677734
Projection step: 22, Loss: 29.67425537109375
Projection step: 23, Loss: 29.928537368774414
Projection step: 24, Loss: 30.244396209716797
Final likelihood: tensor([-30.6487, -29.9231, -27.9873, -28.8688, -29.6418, -27.2809, -30.0456,
        -18.9083, -28.6730, -29.2193, -32.0449, -29.9703, -30.9431, -28.2814,
        -28.2928, -30.3693])
Final projection likelihood: -28.8187
1 mode projection succeeded
New goal: tensor([ 0.0521,  0.4747,  0.6374,  0.7001, -0.0830,  0.5588,  0.7931,  0.9961,
         1.3891,  0.0760,  0.1505,  1.3324,  0.0931, -0.0019, -0.7716],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -32.383602142333984
Adjusted likelihood: -32.383602142333984
Likelihood residual: 0.0
Original likelihood: -37.392677307128906
Adjusted likelihood: -37.392677307128906
Likelihood residual: 0.0
{'index': 37.392677307128906, 'thumb_middle': 32.383602142333984}
Current yaw: tensor([ 0.0993,  0.0024, -0.3632], device='cuda:0')
17 thumb_middle
tensor([ 0.0503,  0.4310,  0.6563,  0.7024, -0.1060,  0.6234,  0.8122,  0.9728,
         1.4045,  0.1031,  0.1143,  1.3142,  0.0993,  0.0024, -0.3632,  0.1818],
       device='cuda:0')
Solve time for step 1 9.025071450974792
Current ori: tensor([ 0.0993,  0.0024, -0.3632], device='cuda:0')
Index force: tensor([0.5910, 0.6016, 0.5966, 0.5003], device='cuda:0')
tensor([ 0.0561,  0.4645,  0.6222,  0.6931, -0.1685,  0.5597,  0.7593,  0.9659,
         1.3295,  0.0415,  0.0637,  1.2931,  0.0911,  0.0077, -0.3632,  0.2777],
       device='cuda:0')
Solve time for step 2 3.5130648619960994
Current ori: tensor([ 0.0911,  0.0077, -0.3632], device='cuda:0')
Index force: tensor([0.5893, 0.5859, 0.6087], device='cuda:0')
tensor([ 0.0578,  0.4543,  0.6351,  0.6977, -0.1821,  0.5588,  0.7537,  0.9652,
         1.3375,  0.0373,  0.0631,  1.2978,  0.0939,  0.0057, -0.3632,  0.2559],
       device='cuda:0')
Solve time for step 3 3.5784430209896527
Current ori: tensor([ 0.0939,  0.0057, -0.3632], device='cuda:0')
Index force: tensor([0.5810, 0.6051], device='cuda:0')
tensor([ 0.0594,  0.4612,  0.6297,  0.6921, -0.1835,  0.5607,  0.7545,  0.9661,
         1.3376,  0.0369,  0.0588,  1.2990,  0.0918,  0.0036, -0.3632,  0.2323],
       device='cuda:0')
Solve time for step 4 3.407222958980128
Current ori: tensor([ 0.0918,  0.0036, -0.3632], device='cuda:0')
Index force: tensor([0.5907], device='cuda:0')
Storing RECOVERY transition: reward=0.0050 (scaled=0.0050), steps=1
Reward stats updated: mean 0.0020 -> 0.0020, std: 0.0781
Collected 153 transitions for RL
SAC Update 1/5: Actor Loss=-0.1971, Q1 Loss=0.6749, Q2 Loss=0.6749, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2469
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.1901, Q2 Loss=2.1901, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0182
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.1768, Q2 Loss=2.1768, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5361
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=3.9045, Q2 Loss=3.9045, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6575
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.7124, Q2 Loss=1.7124, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7047

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.4%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (18.9%)
Actor update: 0.10s (38.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.039413
Q1 loss: 2.131743
Q2 loss: 2.131743
Current threshold: -32.7607
Global Scale Offset: 0.1091
Reward stats: mean=0.0020, std=0.0781, count=153
----------------------------------------------
SAC Update - Actor Loss: -0.0394, Q1 Loss: 2.1317, Q2 Loss: 2.1317, Entropy: 0.0000, Mean TD Error: 1.8327, Threshold: -32.7607
Original likelihood: -35.01543426513672
Adjusted likelihood: -35.01543426513672
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 33.338829040527344
Projection step: 1, Loss: 34.19898223876953
Projection step: 2, Loss: 32.49755096435547
Projection step: 3, Loss: 32.162967681884766
Projection step: 4, Loss: 32.519744873046875
Projection step: 5, Loss: 32.382450103759766
Projection step: 6, Loss: 32.36809539794922
Projection step: 7, Loss: 31.80633544921875
Projection step: 8, Loss: 29.97909927368164
Projection step: 9, Loss: 31.581241607666016
Projection step: 10, Loss: 30.57861328125
Projection step: 11, Loss: 31.24085807800293
Projection step: 12, Loss: 30.425376892089844
Projection step: 13, Loss: 30.097919464111328
Projection step: 14, Loss: 30.484127044677734
Projection step: 15, Loss: 29.910383224487305
Projection step: 16, Loss: 30.229198455810547
Projection step: 17, Loss: 30.719432830810547
Projection step: 18, Loss: 28.837528228759766
Projection step: 19, Loss: 29.218463897705078
Projection step: 20, Loss: 28.040096282958984
Projection step: 21, Loss: 28.307964324951172
Projection step: 22, Loss: 28.43777084350586
Projection step: 23, Loss: 28.405376434326172
Projection step: 24, Loss: 28.37527084350586
Final likelihood: tensor([-28.8480, -28.7536, -27.9331, -28.8231, -29.3772, -25.9413, -28.6019,
        -26.3799, -28.1682, -21.2204, -28.7016, -28.5833, -27.9389, -28.7910,
        -27.7503, -28.1678])
Final projection likelihood: -27.7487
1 mode projection succeeded
New goal: tensor([ 0.0373,  0.4833,  0.6176,  0.6808, -0.0859,  0.5516,  0.7824,  1.0100,
         1.3942,  0.0548,  0.1693,  1.3545,  0.0894,  0.0045, -0.7052],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -34.02544403076172
Adjusted likelihood: -34.02544403076172
Likelihood residual: 0.0
Original likelihood: -35.847572326660156
Adjusted likelihood: -35.847572326660156
Likelihood residual: 0.0
{'index': 35.847572326660156, 'thumb_middle': 34.02544403076172}
Current yaw: tensor([ 0.0955,  0.0082, -0.3635], device='cuda:0')
18 thumb_middle
tensor([ 0.0359,  0.4458,  0.6268,  0.6973, -0.1116,  0.6071,  0.8009,  0.9940,
         1.4117,  0.0734,  0.1227,  1.3354,  0.0955,  0.0082, -0.3635,  0.1747],
       device='cuda:0')
Solve time for step 1 9.091155691014137
Current ori: tensor([ 0.0955,  0.0082, -0.3635], device='cuda:0')
Index force: tensor([0.5735, 0.5868, 0.5762, 0.5793], device='cuda:0')
tensor([ 0.0462,  0.4493,  0.6260,  0.7091, -0.1786,  0.5484,  0.7662,  0.9824,
         1.3452,  0.0041,  0.0675,  1.3130,  0.1008,  0.0293, -0.3635,  0.4041],
       device='cuda:0')
Solve time for step 2 3.5064501049928367
Current ori: tensor([ 0.1008,  0.0293, -0.3635], device='cuda:0')
Index force: tensor([0.5765, 0.5672, 0.5697], device='cuda:0')
tensor([ 0.0564,  0.4653,  0.6248,  0.6875, -0.1941,  0.5478,  0.7355,  0.9688,
         1.3338,  0.0118,  0.1038,  1.3201,  0.0922,  0.0346, -0.3635,  0.5082],
       device='cuda:0')
Solve time for step 3 3.5718940760125406
Current ori: tensor([ 0.0922,  0.0346, -0.3635], device='cuda:0')
Index force: tensor([0.5443, 0.5673], device='cuda:0')
tensor([ 0.0654,  0.4872,  0.6156,  0.6625, -0.2141,  0.5487,  0.7421,  0.9627,
         1.3417,  0.0232,  0.0932,  1.3206,  0.0848,  0.0286, -0.3635,  0.4970],
       device='cuda:0')
Solve time for step 4 3.2625561199965887
Current ori: tensor([ 0.0848,  0.0286, -0.3635], device='cuda:0')
Index force: tensor([0.5517], device='cuda:0')
Storing RECOVERY transition: reward=0.0091 (scaled=0.0091), steps=1
Reward stats updated: mean 0.0020 -> 0.0020, std: 0.0778
Collected 154 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2728, Q2 Loss=1.2728, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2404
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1527, Q2 Loss=1.1527, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7223
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0377, Q2 Loss=1.0377, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0043
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6914, Q2 Loss=1.6914, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2950
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0643, Q2 Loss=1.0643, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9715

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (18.4%)
Actor update: 0.09s (38.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 1.243786
Q2 loss: 1.243786
Current threshold: -32.7758
Global Scale Offset: 0.1079
Reward stats: mean=0.0020, std=0.0778, count=154
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.2438, Q2 Loss: 1.2438, Entropy: 0.0000, Mean TD Error: 1.0467, Threshold: -32.7758
Original likelihood: -32.52040100097656
Adjusted likelihood: -32.52040100097656
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.7500)
Current yaw: tensor([ 0.0837,  0.0236, -0.3661], device='cuda:0')
19 turn
Sampling time 3.834644748014398
tensor([ 0.0578,  0.4981,  0.5930,  0.6632, -0.1307,  0.5897,  0.7759,  1.0052,
         1.4150,  0.0544,  0.1426,  1.3576,  0.0837,  0.0236, -0.3661,  0.4034],
       device='cuda:0')
Original likelihood: -31.282726287841797
Adjusted likelihood: -31.282726287841797
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.497396535007283
Current ori: tensor([ 0.0837,  0.0236, -0.3661], device='cuda:0')
Middle force: tensor([1.4489, 0.5167, 0.5061, 0.5765, 0.5539, 0.6203, 1.0842, 0.8894, 0.7622,
        0.5842, 0.5483, 0.5257], device='cuda:0')
Thumb force: tensor([1.9867, 1.9448, 1.3358, 0.5168, 1.0532, 0.7721, 1.4624, 0.5665, 0.6967,
        0.5842, 0.5317, 0.6722], device='cuda:0')
Index force: tensor([0.5596, 0.8131, 0.6953, 0.6151, 0.5773, 0.5590, 0.5740, 0.5310, 0.5164,
        0.5783, 0.7274, 0.5738], device='cuda:0')
Storing NORMAL transition: reward=0.1504 (scaled=0.1504), steps=1
Reward stats updated: mean 0.0020 -> 0.0030, std: 0.0785
Collected 155 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6065, Q2 Loss=1.6065, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5479
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2491, Q2 Loss=1.2491, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8020
SAC Update 3/5: Actor Loss=-0.0003, Q1 Loss=1.4527, Q2 Loss=1.4527, Entropy=0.0730, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1416
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=0.8811, Q2 Loss=0.8811, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8208
SAC Update 5/5: Actor Loss=-0.0465, Q1 Loss=0.7083, Q2 Loss=0.7083, Entropy=0.0046, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7778

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.7%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (19.5%)
Actor update: 0.10s (40.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.009373
Q1 loss: 1.179540
Q2 loss: 1.179540
Current threshold: -32.7846
Global Scale Offset: 0.1071
Reward stats: mean=0.0030, std=0.0785, count=155
----------------------------------------------
SAC Update - Actor Loss: -0.0094, Q1 Loss: 1.1795, Q2 Loss: 1.1795, Entropy: 0.0155, Mean TD Error: 1.0180, Threshold: -32.7846
tensor([ 2.1878e-02,  5.4346e-01,  5.1816e-01,  6.1803e-01, -1.0950e-01,
         5.9715e-01,  7.7513e-01,  1.0471e+00,  1.3606e+00,  1.6506e-01,
         1.7340e-01,  1.1922e+00,  6.8100e-02,  6.4798e-04, -5.1555e-01,
         2.4733e-01], device='cuda:0')
Original likelihood: -26.894920349121094
Adjusted likelihood: -26.894920349121094
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.454881927988026
Current ori: tensor([ 0.0681,  0.0006, -0.5155], device='cuda:0')
Middle force: tensor([0.5189, 0.5077, 0.5791, 0.5541, 0.6367, 1.0670, 0.8991, 0.7677, 0.5834,
        0.5567, 0.5288], device='cuda:0')
Thumb force: tensor([1.8847, 1.2992, 0.5144, 1.0265, 0.7452, 1.4280, 0.5589, 0.6834, 0.5791,
        0.5231, 0.6586], device='cuda:0')
Index force: tensor([0.7884, 0.6659, 0.6107, 0.5738, 0.5476, 0.5740, 0.5280, 0.5144, 0.5738,
        0.7191, 0.5665], device='cuda:0')
Storing NORMAL transition: reward=0.1199 (scaled=0.1199), steps=1
Reward stats updated: mean 0.0030 -> 0.0037, std: 0.0788
Collected 156 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3119, Q2 Loss=1.3119, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8823
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.9025, Q2 Loss=1.9025, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8649
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6539, Q2 Loss=0.6539, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0565
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.2376, Q2 Loss=2.2376, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3334
SAC Update 5/5: Actor Loss=-0.1356, Q1 Loss=0.7804, Q2 Loss=0.7804, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3330

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.8%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.2%)
Actor update: 0.09s (40.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.3%)
Actor loss: -0.073176
Q1 loss: 1.377247
Q2 loss: 1.377247
Current threshold: -32.7945
Global Scale Offset: 0.1064
Reward stats: mean=0.0037, std=0.0788, count=156
----------------------------------------------
SAC Update - Actor Loss: -0.0732, Q1 Loss: 1.3772, Q2 Loss: 1.3772, Entropy: 0.0000, Mean TD Error: 1.0940, Threshold: -32.7945
tensor([ 0.0581,  0.5979,  0.5038,  0.5621, -0.0559,  0.5856,  0.7540,  1.0862,
         1.4358,  0.0996,  0.0817,  1.1528,  0.0520, -0.0199, -0.6353,  0.3638],
       device='cuda:0')
Original likelihood: -24.861644744873047
Adjusted likelihood: -24.861644744873047
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.174784889037255
Current ori: tensor([ 0.0520, -0.0199, -0.6353], device='cuda:0')
Middle force: tensor([0.5087, 0.5639, 0.5531, 0.6347, 1.0242, 0.8756, 0.7579, 0.5791, 0.5529,
        0.5283], device='cuda:0')
Thumb force: tensor([1.2623, 0.5133, 1.0078, 0.7341, 1.4188, 0.5584, 0.6754, 0.5762, 0.5209,
        0.6514], device='cuda:0')
Index force: tensor([0.6436, 0.6138, 0.5673, 0.5445, 0.5724, 0.5277, 0.5133, 0.5704, 0.7160,
        0.5626], device='cuda:0')
Storing NORMAL transition: reward=0.0430 (scaled=0.0430), steps=1
Reward stats updated: mean 0.0037 -> 0.0040, std: 0.0786
Collected 157 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=2.7251, Q2 Loss=2.7251, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2739
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7749, Q2 Loss=0.7749, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5804
SAC Update 3/5: Actor Loss=-0.0003, Q1 Loss=1.4373, Q2 Loss=1.4373, Entropy=0.0627, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0188
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.3467, Q2 Loss=1.3467, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1025
SAC Update 5/5: Actor Loss=-0.2208, Q1 Loss=0.6553, Q2 Loss=0.6553, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6586

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.9%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.7%)
Actor update: 0.08s (39.7%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.136312
Q1 loss: 1.387857
Q2 loss: 1.387857
Current threshold: -32.8185
Global Scale Offset: 0.1047
Reward stats: mean=0.0040, std=0.0786, count=157
----------------------------------------------
SAC Update - Actor Loss: -0.1363, Q1 Loss: 1.3879, Q2 Loss: 1.3879, Entropy: 0.0125, Mean TD Error: 1.1269, Threshold: -32.8185
tensor([ 0.1679,  0.6160,  0.5529,  0.6352, -0.0272,  0.5504,  0.7428,  1.1991,
         1.3787,  0.3282,  0.0906,  1.1923,  0.0960, -0.0243, -0.6962,  1.0645],
       device='cuda:0')
Original likelihood: -37.20152282714844
Adjusted likelihood: -37.20152282714844
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 34.33549499511719
Projection step: 1, Loss: 34.106101989746094
Projection step: 2, Loss: 34.37152862548828
Projection step: 3, Loss: 33.62196350097656
Projection step: 4, Loss: 34.54463195800781
Projection step: 5, Loss: 32.5977783203125
Projection step: 6, Loss: 33.22956085205078
Projection step: 7, Loss: 32.797813415527344
Projection step: 8, Loss: 32.43578338623047
Projection step: 9, Loss: 32.96590042114258
Projection step: 10, Loss: 32.474708557128906
Projection step: 11, Loss: 31.745891571044922
Projection step: 12, Loss: 31.228530883789062
Projection step: 13, Loss: 31.267070770263672
Projection step: 14, Loss: 31.64434242248535
Projection step: 15, Loss: 30.48876190185547
Projection step: 16, Loss: 32.110496520996094
Projection step: 17, Loss: 30.092613220214844
Projection step: 18, Loss: 30.378217697143555
Projection step: 19, Loss: 29.705402374267578
Projection step: 20, Loss: 31.30752944946289
Projection step: 21, Loss: 29.263757705688477
Projection step: 22, Loss: 30.033082962036133
Projection step: 23, Loss: 29.977041244506836
Projection step: 24, Loss: 31.078350067138672
Final likelihood: tensor([-30.9259, -28.0968, -30.3284, -30.5274, -27.3784, -31.9980, -27.1250,
        -26.7305, -27.1268, -32.5375, -30.6338, -32.5027, -27.2515, -29.4110,
        -29.4890, -28.1175])
Final projection likelihood: -29.3863
1 mode projection succeeded
New goal: tensor([ 0.1381,  0.5831,  0.5787,  0.6666, -0.0291,  0.4989,  0.8064,  1.0981,
         1.3386,  0.2699,  0.1204,  1.2095,  0.0905, -0.0244, -0.3893],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -31.897266387939453
Adjusted likelihood: -31.897266387939453
Likelihood residual: 0.0
Original likelihood: -31.06397247314453
Adjusted likelihood: -31.06397247314453
Likelihood residual: 0.0
{'index': 31.06397247314453, 'thumb_middle': 31.897266387939453}
Current yaw: tensor([ 0.0960, -0.0243, -0.6962], device='cuda:0')
20 index
tensor([ 0.1679,  0.6160,  0.5529,  0.6352, -0.0272,  0.5504,  0.7428,  1.1991,
         1.3787,  0.3282,  0.0906,  1.1923,  0.0960, -0.0243, -0.6962,  1.0645],
       device='cuda:0')
Solve time for step 1 10.307616513979156
Current ori: tensor([ 0.0960, -0.0243, -0.6962], device='cuda:0')
Middle force: tensor([0.5031, 0.5327, 0.5341, 0.5602], device='cuda:0')
Thumb force: tensor([0.5603, 0.6388, 0.6299, 0.5433], device='cuda:0')
tensor([ 0.1937,  0.4950,  0.5304,  0.6466, -0.0197,  0.5370,  0.8191,  1.1321,
         1.4051,  0.2964,  0.0906,  1.1903,  0.1243, -0.0336, -0.7093, -0.0141],
       device='cuda:0')
Solve time for step 2 4.081786797964014
Current ori: tensor([ 0.1243, -0.0336, -0.7093], device='cuda:0')
Middle force: tensor([0.5297, 0.5314, 0.5562], device='cuda:0')
Thumb force: tensor([0.6318, 0.6250, 0.5407], device='cuda:0')
tensor([ 0.1854,  0.4698,  0.5389,  0.6596, -0.0100,  0.5793,  0.8397,  1.1010,
         1.4592,  0.3042,  0.1212,  1.1707,  0.2440, -0.0498, -0.7041, -0.8485],
       device='cuda:0')
Solve time for step 3 3.891339199966751
Current ori: tensor([ 0.2440, -0.0498, -0.7041], device='cuda:0')
Middle force: tensor([0.5402, 0.5406], device='cuda:0')
Thumb force: tensor([0.5735, 0.5440], device='cuda:0')
tensor([ 0.1552,  0.3725,  0.5549,  0.6750,  0.0537,  0.6679,  0.8623,  1.0808,
         1.5000,  0.3296,  0.1321,  1.1587,  0.3307, -0.1874, -0.6899,  0.9703],
       device='cuda:0')
Solve time for step 4 3.8750881840242073
Current ori: tensor([ 0.3307, -0.1874, -0.6899], device='cuda:0')
Middle force: tensor([0.5161], device='cuda:0')
Thumb force: tensor([0.5909], device='cuda:0')
Storing RECOVERY transition: reward=-0.3755 (scaled=-0.1252), steps=3
Reward stats updated: mean 0.0040 -> 0.0032, std: 0.0790
Collected 158 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6500, Q2 Loss=1.6500, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6692
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.7334, Q2 Loss=0.7334, Entropy=0.0001, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2549
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.6501, Q2 Loss=0.6501, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9008
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.3921, Q2 Loss=1.3921, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1985
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.9752, Q2 Loss=1.9752, Entropy=0.0001, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6856

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.0%)
Q1 update: 0.06s (20.3%)
Q2 update: 0.05s (18.0%)
Actor update: 0.11s (40.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.280147
Q2 loss: 1.280147
Current threshold: -32.8444
Global Scale Offset: 0.1028
Reward stats: mean=0.0032, std=0.0790, count=158
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.2801, Q2 Loss: 1.2801, Entropy: 0.0000, Mean TD Error: 1.3418, Threshold: -32.8444
Original likelihood: -131.36514282226562
Adjusted likelihood: -131.36514282226562
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 151.9428253173828
Projection step: 1, Loss: 126.54656982421875
Projection step: 2, Loss: 151.72259521484375
Projection step: 3, Loss: 142.0226593017578
Projection step: 4, Loss: 149.0653076171875
Projection step: 5, Loss: 141.7740020751953
Projection step: 6, Loss: 136.60678100585938
Projection step: 7, Loss: 139.8128662109375
Projection step: 8, Loss: 158.78765869140625
Projection step: 9, Loss: 143.042236328125
Projection step: 10, Loss: 144.49655151367188
Projection step: 11, Loss: 144.6631317138672
Projection step: 12, Loss: 154.96063232421875
Projection step: 13, Loss: 135.5318145751953
Projection step: 14, Loss: 148.62599182128906
Projection step: 15, Loss: 161.8097381591797
Projection step: 16, Loss: 161.432373046875
Projection step: 17, Loss: 146.43707275390625
Projection step: 18, Loss: 143.16262817382812
Projection step: 19, Loss: 156.80335998535156
Projection step: 20, Loss: 133.31607055664062
Projection step: 21, Loss: 145.23988342285156
Projection step: 22, Loss: 140.3454132080078
Projection step: 23, Loss: 128.69911193847656
Projection step: 24, Loss: 150.93719482421875
Final likelihood: tensor([ -96.7528, -144.7904,  -88.5935, -145.2744, -161.9290, -174.5314,
         -86.6480, -190.0802, -146.6171,  -95.8597, -152.3504, -120.9665,
        -157.9744, -154.3571,  -92.2749, -116.5649])
Final projection likelihood: -132.8478
1 mode projection failed, trying anyway
New goal: tensor([ 0.2752,  0.5642,  0.6018,  0.6915,  0.0423,  0.7753,  1.0441,  1.2379,
         1.4796,  0.3429,  0.1664,  1.1866,  0.3540, -0.2562, -0.4651],
       device='cuda:0')
Marked last transition as done (final step)
{}

Trial 10
Loaded trajectory sampler
Current yaw: tensor([ 0.0007,  0.0141, -0.0466], device='cuda:0')
Current yaw: tensor([ 0.0007,  0.0141, -0.0466], device='cuda:0')
1 turn
Sampling time 3.65930084802676
tensor([ 1.3819e-01,  5.9933e-01,  5.8389e-01,  5.8133e-01, -1.3018e-01,
         5.4764e-01,  9.3366e-01,  8.5137e-01,  1.2177e+00,  2.8002e-01,
         2.6688e-01,  1.1864e+00,  6.9719e-04,  1.4071e-02, -4.6578e-02,
         1.9336e-01], device='cuda:0')
Original likelihood: -23.213207244873047
Adjusted likelihood: -23.213207244873047
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.433352350955829
Current ori: tensor([ 0.0007,  0.0141, -0.0466], device='cuda:0')
Middle force: tensor([1.1751, 1.7698, 0.8249, 0.5202, 0.5687, 0.8640, 1.1652, 0.5144, 0.7010,
        0.8656, 0.8468, 0.4949], device='cuda:0')
Thumb force: tensor([0.8824, 1.3625, 0.5737, 0.5579, 0.5399, 1.2627, 0.6515, 0.5572, 0.6049,
        1.2081, 1.5901, 0.5606], device='cuda:0')
Index force: tensor([0.9238, 1.8154, 0.5687, 0.6014, 0.5952, 0.8219, 0.5422, 0.5910, 0.5911,
        0.5469, 0.6100, 0.7146], device='cuda:0')
Storing NORMAL transition: reward=-0.0278 (scaled=-0.0278), steps=1
Reward stats updated: mean 0.0032 -> 0.0030, std: 0.0788
Collected 159 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.9576, Q2 Loss=1.9576, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3869
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.8071, Q2 Loss=1.8071, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7850
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.5830, Q2 Loss=1.5830, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5447
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9871, Q2 Loss=0.9871, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0975
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0667, Q2 Loss=1.0667, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1416

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.3%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (20.4%)
Actor update: 0.11s (41.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.480296
Q2 loss: 1.480296
Current threshold: -32.8597
Global Scale Offset: 0.1016
Reward stats: mean=0.0030, std=0.0788, count=159
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.4803, Q2 Loss: 1.4803, Entropy: 0.0000, Mean TD Error: 1.5911, Threshold: -32.8597
tensor([ 0.1122,  0.6277,  0.5261,  0.5650, -0.1501,  0.5856,  0.8547,  0.8683,
         1.2214,  0.2908,  0.3071,  1.1367, -0.0083,  0.0219, -0.0191,  0.2514],
       device='cuda:0')
Original likelihood: -19.15714454650879
Adjusted likelihood: -19.15714454650879
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.53945067897439
Current ori: tensor([-0.0083,  0.0219, -0.0191], device='cuda:0')
Middle force: tensor([1.0119, 0.6091, 1.3198, 0.5612, 0.5020, 0.6105, 1.1013, 0.5007, 0.5734,
        0.5245, 0.5305], device='cuda:0')
Thumb force: tensor([0.5717, 0.6099, 0.6917, 0.8947, 0.5556, 0.7065, 0.6353, 0.5086, 0.5851,
        0.6193, 0.6238], device='cuda:0')
Index force: tensor([0.5227, 0.5247, 0.7593, 0.5399, 0.6615, 0.8501, 0.5024, 0.5070, 0.6063,
        0.6255, 0.6198], device='cuda:0')
Storing NORMAL transition: reward=0.2420 (scaled=0.2420), steps=1
Reward stats updated: mean 0.0030 -> 0.0045, std: 0.0808
Collected 160 transitions for RL
SAC Update 1/5: Actor Loss=-0.0001, Q1 Loss=1.0988, Q2 Loss=1.0988, Entropy=0.0419, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7944
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6501, Q2 Loss=0.6501, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5522
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=2.7072, Q2 Loss=2.7072, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6436
SAC Update 4/5: Actor Loss=-0.2282, Q1 Loss=0.6746, Q2 Loss=0.6746, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6978
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.7368, Q2 Loss=1.7368, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4538

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.7%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (19.2%)
Actor update: 0.09s (39.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.091707
Q1 loss: 1.373502
Q2 loss: 1.373502
Current threshold: -32.8712
Global Scale Offset: 0.1007
Reward stats: mean=0.0045, std=0.0808, count=160
----------------------------------------------
SAC Update - Actor Loss: -0.0917, Q1 Loss: 1.3735, Q2 Loss: 1.3735, Entropy: 0.0084, Mean TD Error: 1.2284, Threshold: -32.8712
tensor([ 0.1699,  0.7268,  0.4598,  0.5398, -0.0948,  0.5716,  0.8994,  0.9350,
         1.2315,  0.3675,  0.2530,  0.9819, -0.0280, -0.0187, -0.2625,  0.5527],
       device='cuda:0')
Original likelihood: -25.469478607177734
Adjusted likelihood: -25.469478607177734
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.0992628739913926
Current ori: tensor([-0.0280, -0.0187, -0.2625], device='cuda:0')
Middle force: tensor([0.5007, 0.5082, 0.6233, 0.5896, 1.0471, 0.7827, 0.8153, 0.6117, 0.5108,
        0.5249], device='cuda:0')
Thumb force: tensor([1.4161, 0.5806, 1.1212, 0.7697, 1.3997, 0.5716, 0.7155, 0.6789, 0.5540,
        0.6452], device='cuda:0')
Index force: tensor([0.9852, 0.6585, 0.5691, 0.5465, 0.5842, 0.5269, 0.5758, 0.5645, 0.5949,
        0.6524], device='cuda:0')
Storing NORMAL transition: reward=0.0146 (scaled=0.0146), steps=1
Reward stats updated: mean 0.0045 -> 0.0045, std: 0.0806
Collected 161 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.7080, Q2 Loss=1.7080, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3111
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.0153, Q2 Loss=1.0153, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6534
SAC Update 3/5: Actor Loss=-0.0274, Q1 Loss=0.6172, Q2 Loss=0.6172, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5940
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5426, Q2 Loss=1.5426, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9730
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.4460, Q2 Loss=1.4460, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1254

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.5%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.04s (18.3%)
Actor update: 0.10s (39.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.051532
Q1 loss: 1.265810
Q2 loss: 1.265810
Current threshold: -32.8808
Global Scale Offset: 0.1000
Reward stats: mean=0.0045, std=0.0806, count=161
----------------------------------------------
SAC Update - Actor Loss: -0.0515, Q1 Loss: 1.2658, Q2 Loss: 1.2658, Entropy: 0.0000, Mean TD Error: 1.1314, Threshold: -32.8808
tensor([ 0.1474,  0.6791,  0.4891,  0.5715, -0.0475,  0.5931,  0.9113,  0.9483,
         1.1083,  0.5363,  0.2786,  1.0123, -0.0213, -0.0559, -0.2814,  0.3649],
       device='cuda:0')
Original likelihood: -27.83694076538086
Adjusted likelihood: -27.83694076538086
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.369268115027808
Current ori: tensor([-0.0213, -0.0559, -0.2814], device='cuda:0')
Middle force: tensor([0.5088, 0.6354, 0.6090, 1.0400, 0.8032, 0.8281, 0.6152, 0.5180, 0.5393],
       device='cuda:0')
Thumb force: tensor([0.5643, 1.0848, 0.7316, 1.3673, 0.5578, 0.6931, 0.6636, 0.5317, 0.6011],
       device='cuda:0')
Index force: tensor([0.6499, 0.5647, 0.5417, 0.5806, 0.5234, 0.5712, 0.5606, 0.5834, 0.6380],
       device='cuda:0')
Storing NORMAL transition: reward=0.1531 (scaled=0.1531), steps=1
Reward stats updated: mean 0.0045 -> 0.0054, std: 0.0811
Collected 162 transitions for RL
SAC Update 1/5: Actor Loss=-0.2298, Q1 Loss=3.2059, Q2 Loss=3.2059, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3631
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0420, Q2 Loss=1.0420, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8395
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1323, Q2 Loss=1.1323, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8850
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.4838, Q2 Loss=1.4838, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1190
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.3887, Q2 Loss=2.3887, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0189

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.1%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (40.7%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.3%)
Actor loss: -0.092018
Q1 loss: 1.850529
Q2 loss: 1.850529
Current threshold: -32.8866
Global Scale Offset: 0.0996
Reward stats: mean=0.0054, std=0.0811, count=162
----------------------------------------------
SAC Update - Actor Loss: -0.0920, Q1 Loss: 1.8505, Q2 Loss: 1.8505, Entropy: 0.0000, Mean TD Error: 1.6451, Threshold: -32.8866
tensor([ 0.1716,  0.6767,  0.5321,  0.5465, -0.0277,  0.6149,  0.8220,  1.2043,
         1.1056,  0.6733,  0.3277,  0.8874, -0.0186, -0.0697, -0.4482,  0.5268],
       device='cuda:0')
Original likelihood: -30.03881072998047
Adjusted likelihood: -30.03881072998047
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.638244406029116
Current ori: tensor([-0.0186, -0.0697, -0.4482], device='cuda:0')
Middle force: tensor([0.6412, 0.5204, 0.5144, 0.5852, 0.5500, 0.5595, 0.5371, 0.5517],
       device='cuda:0')
Thumb force: tensor([0.5877, 0.8292, 0.5137, 1.1021, 0.6495, 0.5414, 0.5465, 0.5536],
       device='cuda:0')
Index force: tensor([0.8323, 0.5331, 0.5692, 0.5008, 0.5088, 0.6097, 0.5952, 0.6474],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0219 (scaled=-0.0219), steps=1
Reward stats updated: mean 0.0054 -> 0.0053, std: 0.0809
Collected 163 transitions for RL
SAC Update 1/5: Actor Loss=-0.1705, Q1 Loss=2.1542, Q2 Loss=2.1542, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9435
SAC Update 2/5: Actor Loss=-0.0960, Q1 Loss=1.0531, Q2 Loss=1.0531, Entropy=0.0001, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1413
SAC Update 3/5: Actor Loss=-0.1407, Q1 Loss=1.4142, Q2 Loss=1.4142, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3926
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4515, Q2 Loss=1.4515, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4549
SAC Update 5/5: Actor Loss=-0.1627, Q1 Loss=0.5834, Q2 Loss=0.5834, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7743

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (19.1%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (18.8%)
Actor update: 0.09s (38.8%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.113995
Q1 loss: 1.331276
Q2 loss: 1.331276
Current threshold: -32.8954
Global Scale Offset: 0.0990
Reward stats: mean=0.0053, std=0.0809, count=163
----------------------------------------------
SAC Update - Actor Loss: -0.1140, Q1 Loss: 1.3313, Q2 Loss: 1.3313, Entropy: 0.0000, Mean TD Error: 1.3413, Threshold: -32.8954
tensor([ 0.2891,  0.7855,  0.4881,  0.5403, -0.0984,  0.6715,  0.8982,  1.1472,
         1.1730,  0.4593,  0.1426,  1.0420, -0.0108, -0.1135, -0.4527,  2.0525],
       device='cuda:0')
Original likelihood: -37.376068115234375
Adjusted likelihood: -37.376068115234375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 36.113651275634766
Projection step: 1, Loss: 37.31950759887695
Projection step: 2, Loss: 37.267452239990234
Projection step: 3, Loss: 37.28214645385742
Projection step: 4, Loss: 37.738773345947266
Projection step: 5, Loss: 38.64700698852539
Projection step: 6, Loss: 36.877220153808594
Projection step: 7, Loss: 36.53683853149414
Projection step: 8, Loss: 34.2141227722168
Projection step: 9, Loss: 36.256473541259766
Projection step: 10, Loss: 34.77232360839844
Projection step: 11, Loss: 36.25108337402344
Projection step: 12, Loss: 36.37576675415039
Projection step: 13, Loss: 36.41913604736328
Projection step: 14, Loss: 36.460693359375
Projection step: 15, Loss: 34.680503845214844
Projection step: 16, Loss: 34.137451171875
Projection step: 17, Loss: 37.149234771728516
Projection step: 18, Loss: 33.412681579589844
Projection step: 19, Loss: 33.17071533203125
Projection step: 20, Loss: 33.25122833251953
Projection step: 21, Loss: 33.11479949951172
Projection step: 22, Loss: 32.42671585083008
Projection step: 23, Loss: 33.27796173095703
Projection step: 24, Loss: 33.52830505371094
Final likelihood: tensor([-38.1591, -27.4208, -32.6918, -37.5594, -37.7164, -32.5181, -36.4223,
        -33.4752, -32.3161, -42.1362, -38.1363, -33.4261, -35.0256, -37.8161,
        -27.2747, -30.6667])
Final projection likelihood: -34.5475
1 mode projection failed, trying anyway
New goal: tensor([ 0.2686,  0.7365,  0.4796,  0.5733, -0.0724,  0.6568,  0.8785,  1.0935,
         1.1943,  0.3944,  0.1087,  1.0692, -0.0122, -0.1080, -0.5397],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0066]], device='cuda:0') tensor([[0.0038]], device='cuda:0')
Original likelihood: -32.04890441894531
Adjusted likelihood: -32.04890441894531
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 32.04890441894531}
Current yaw: tensor([-0.0108, -0.1135, -0.4527], device='cuda:0')
2 thumb_middle
tensor([ 0.2891,  0.7855,  0.4881,  0.5403, -0.0984,  0.6715,  0.8982,  1.1472,
         1.1730,  0.4593,  0.1426,  1.0420, -0.0108, -0.1135, -0.4527,  2.0525],
       device='cuda:0')
Solve time for step 1 8.936834198015276
Current ori: tensor([-0.0108, -0.1135, -0.4527], device='cuda:0')
Index force: tensor([0.5874, 0.6051, 0.5711, 0.6007], device='cuda:0')
tensor([ 0.2727,  0.7806,  0.4995,  0.5672, -0.1444,  0.6490,  0.8514,  1.0898,
         1.1516,  0.3985,  0.0470,  1.0454, -0.0078, -0.1162, -0.4527,  3.4302],
       device='cuda:0')
Solve time for step 2 3.655437038047239
Current ori: tensor([-0.0078, -0.1162, -0.4527], device='cuda:0')
Index force: tensor([0.5958, 0.5648, 0.5933], device='cuda:0')
tensor([ 0.2711,  0.7820,  0.4920,  0.5742, -0.1477,  0.6588,  0.8553,  1.0892,
         1.1648,  0.3911,  0.0322,  1.0313, -0.0077, -0.1149, -0.4527,  3.4302],
       device='cuda:0')
Solve time for step 3 3.471286406973377
Current ori: tensor([-0.0077, -0.1149, -0.4527], device='cuda:0')
Index force: tensor([0.5517, 0.5792], device='cuda:0')
tensor([ 0.2685,  0.7706,  0.4932,  0.5958, -0.1365,  0.6491,  0.8513,  1.0680,
         1.1658,  0.3786,  0.0324,  1.0408, -0.0040, -0.1115, -0.4527,  3.4398],
       device='cuda:0')
Solve time for step 4 3.362576187995728
Current ori: tensor([-0.0040, -0.1115, -0.4527], device='cuda:0')
Index force: tensor([0.5495], device='cuda:0')
Storing RECOVERY transition: reward=0.0102 (scaled=0.0020), steps=5
Reward stats updated: mean 0.0053 -> 0.0053, std: 0.0807
Collected 164 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6593, Q2 Loss=1.6593, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7907
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2703, Q2 Loss=1.2703, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0292
SAC Update 3/5: Actor Loss=-0.0533, Q1 Loss=0.7150, Q2 Loss=0.7150, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6317
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9167, Q2 Loss=0.9167, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6618
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1404, Q2 Loss=1.1404, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5208

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.9%)
Q1 update: 0.05s (19.0%)
Q2 update: 0.05s (18.9%)
Actor update: 0.10s (40.8%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.010660
Q1 loss: 1.140344
Q2 loss: 1.140344
Current threshold: -32.9174
Global Scale Offset: 0.0975
Reward stats: mean=0.0053, std=0.0807, count=164
----------------------------------------------
SAC Update - Actor Loss: -0.0107, Q1 Loss: 1.1403, Q2 Loss: 1.1403, Entropy: 0.0000, Mean TD Error: 1.1268, Threshold: -32.9174
Original likelihood: -32.13434600830078
Adjusted likelihood: -32.13434600830078
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9871)
Current yaw: tensor([-0.0071, -0.0998, -0.4538], device='cuda:0')
3 turn
Sampling time 3.7066046820254996
tensor([ 0.2466,  0.7676,  0.4763,  0.5830, -0.0827,  0.7061,  0.8805,  1.0860,
         1.2214,  0.3925,  0.1042,  1.0799, -0.0071, -0.0998, -0.4538,  3.4082],
       device='cuda:0')
Original likelihood: -33.90803146362305
Adjusted likelihood: -33.90803146362305
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0024)
State is out of distribution
Projection step: 0, Loss: 36.02435302734375
Projection step: 1, Loss: 31.93268394470215
Projection step: 2, Loss: 32.86406707763672
Projection step: 3, Loss: 31.556184768676758
Projection step: 4, Loss: 34.52458190917969
Projection step: 5, Loss: 31.2287540435791
Projection step: 6, Loss: 32.147544860839844
Projection step: 7, Loss: 30.508703231811523
Projection step: 8, Loss: 33.943992614746094
Projection step: 9, Loss: 31.503719329833984
Projection step: 10, Loss: 30.224666595458984
Projection step: 11, Loss: 31.82644271850586
Projection step: 12, Loss: 29.608154296875
Projection step: 13, Loss: 27.24378204345703
Projection step: 14, Loss: 27.80905532836914
Projection step: 15, Loss: 27.273826599121094
Projection step: 16, Loss: 29.754356384277344
Projection step: 17, Loss: 26.86152458190918
Projection step: 18, Loss: 28.255762100219727
Projection step: 19, Loss: 25.717220306396484
Projection step: 20, Loss: 27.96127700805664
Projection step: 21, Loss: 26.70122528076172
Projection step: 22, Loss: 27.40799903869629
Projection step: 23, Loss: 26.38520050048828
Projection step: 24, Loss: 25.776132583618164
Final likelihood: tensor([-23.0834, -30.1159, -23.3866, -23.7962, -22.9640, -23.0431, -22.9115,
        -31.9684, -22.5102, -34.4155, -23.1157, -23.1293, -23.0166, -22.7946,
        -24.0097, -34.5304])
Final projection likelihood: -25.5494
1 mode projection succeeded
New goal: tensor([ 0.2382,  0.7206,  0.4720,  0.6388, -0.0474,  0.6954,  0.8571,  1.0351,
         1.2428,  0.3355,  0.0774,  1.1507, -0.0090, -0.0952, -0.4227],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -31.693531036376953
Adjusted likelihood: -31.693531036376953
Likelihood residual: 0.0
Original likelihood: -28.09378433227539
Adjusted likelihood: -28.09378433227539
Likelihood residual: 0.0
{'index': 28.09378433227539, 'thumb_middle': 31.693531036376953}
Current yaw: tensor([-0.0071, -0.0998, -0.4538], device='cuda:0')
4 index
tensor([ 0.2466,  0.7676,  0.4763,  0.5830, -0.0827,  0.7061,  0.8805,  1.0860,
         1.2214,  0.3925,  0.1042,  1.0799, -0.0071, -0.0998, -0.4538,  3.4082],
       device='cuda:0')
Solve time for step 1 10.428365769970696
Current ori: tensor([-0.0071, -0.0998, -0.4538], device='cuda:0')
Middle force: tensor([0.5906, 0.5023, 0.5510, 0.5710], device='cuda:0')
Thumb force: tensor([0.5803, 0.5072, 0.6116, 0.5381], device='cuda:0')
tensor([ 0.2968,  0.6821,  0.4296,  0.6086, -0.0717,  0.7133,  0.8881,  1.0671,
         1.2985,  0.2560,  0.0356,  1.0843, -0.0155, -0.1081, -0.4854,  2.3581],
       device='cuda:0')
Solve time for step 2 4.206881941005122
Current ori: tensor([-0.0155, -0.1081, -0.4854], device='cuda:0')
Middle force: tensor([0.5020, 0.5481, 0.5682], device='cuda:0')
Thumb force: tensor([0.5062, 0.6069, 0.5362], device='cuda:0')
tensor([ 0.2980,  0.6813,  0.4299,  0.6144, -0.0806,  0.7354,  0.9065,  1.0723,
         1.2454,  0.3502,  0.0416,  1.1116, -0.0091, -0.1202, -0.5046,  2.2949],
       device='cuda:0')
Solve time for step 3 3.8461420020321384
Current ori: tensor([-0.0091, -0.1202, -0.5046], device='cuda:0')
Middle force: tensor([0.5433, 0.5656], device='cuda:0')
Thumb force: tensor([0.6003, 0.5335], device='cuda:0')
tensor([ 0.2984,  0.6811,  0.4303,  0.6167, -0.0878,  0.7432,  0.9110,  1.0689,
         1.2552,  0.3274,  0.0276,  1.1273, -0.0047, -0.1222, -0.5112,  2.8802],
       device='cuda:0')
Solve time for step 4 3.9203051260556094
Current ori: tensor([-0.0047, -0.1222, -0.5112], device='cuda:0')
Middle force: tensor([0.5404], device='cuda:0')
Thumb force: tensor([0.5531], device='cuda:0')
Storing RECOVERY transition: reward=0.0626 (scaled=0.0626), steps=0
Reward stats updated: mean 0.0053 -> 0.0056, std: 0.0806
Collected 165 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5864, Q2 Loss=1.5864, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6311
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=3.9959, Q2 Loss=3.9959, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1137
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=3.5046, Q2 Loss=3.5046, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5765
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.3717, Q2 Loss=2.3717, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1129
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.8663, Q2 Loss=1.8663, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6173

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.1%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.04s (18.6%)
Actor update: 0.09s (38.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.138155
Q1 loss: 2.664997
Q2 loss: 2.664997
Current threshold: -32.9305
Global Scale Offset: 0.0967
Reward stats: mean=0.0056, std=0.0806, count=165
----------------------------------------------
SAC Update - Actor Loss: -0.1382, Q1 Loss: 2.6650, Q2 Loss: 2.6650, Entropy: 0.0000, Mean TD Error: 2.2103, Threshold: -32.9305
Original likelihood: -31.677431106567383
Adjusted likelihood: -31.677431106567383
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9998)
Current yaw: tensor([-0.0105, -0.1250, -0.5240], device='cuda:0')
5 turn
Sampling time 3.7104184060008265
tensor([ 0.2361,  0.7397,  0.4708,  0.6375, -0.0980,  0.7677,  0.9025,  1.0501,
         1.2551,  0.3343,  0.0251,  1.1160, -0.0105, -0.1250, -0.5240,  3.1184],
       device='cuda:0')
Original likelihood: -33.83355712890625
Adjusted likelihood: -33.83355712890625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0048)
State is out of distribution
Projection step: 0, Loss: 33.56855010986328
Projection step: 1, Loss: 31.251178741455078
Projection step: 2, Loss: 32.17082214355469
Projection step: 3, Loss: 31.10240364074707
Projection step: 4, Loss: 31.41965103149414
Projection step: 5, Loss: 31.131858825683594
Projection step: 6, Loss: 30.817411422729492
Projection step: 7, Loss: 33.11004638671875
Projection step: 8, Loss: 30.25534439086914
Projection step: 9, Loss: 30.121627807617188
Projection step: 10, Loss: 29.912635803222656
Projection step: 11, Loss: 30.8749942779541
Projection step: 12, Loss: 31.67208480834961
Projection step: 13, Loss: 28.84364891052246
Projection step: 14, Loss: 31.05860137939453
Projection step: 15, Loss: 30.431156158447266
Projection step: 16, Loss: 29.194974899291992
Projection step: 17, Loss: 28.98282241821289
Projection step: 18, Loss: 30.268932342529297
Projection step: 19, Loss: 30.981307983398438
Projection step: 20, Loss: 29.341228485107422
Projection step: 21, Loss: 28.71413230895996
Projection step: 22, Loss: 28.67203140258789
Projection step: 23, Loss: 29.73247718811035
Projection step: 24, Loss: 29.331195831298828
Final likelihood: tensor([-26.8052, -32.8768, -26.4805, -38.5530, -26.8178, -25.5654, -27.6550,
        -33.7398, -27.0302, -33.9842, -35.2043, -27.7804, -27.2221, -24.6953,
        -26.0316, -25.8435])
Final projection likelihood: -29.1428
1 mode projection succeeded
New goal: tensor([ 0.2356,  0.6980,  0.4781,  0.6896, -0.0570,  0.7438,  0.8763,  1.0049,
         1.2586,  0.3041,  0.0280,  1.1412, -0.0118, -0.1197, -0.5635],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -31.88758087158203
Adjusted likelihood: -31.88758087158203
Likelihood residual: 0.0
Original likelihood: -32.440643310546875
Adjusted likelihood: -32.440643310546875
Likelihood residual: 0.0
{'index': 32.440643310546875, 'thumb_middle': 31.88758087158203}
Current yaw: tensor([-0.0105, -0.1250, -0.5240], device='cuda:0')
6 thumb_middle
tensor([ 0.2361,  0.7397,  0.4708,  0.6375, -0.0980,  0.7677,  0.9025,  1.0501,
         1.2551,  0.3343,  0.0251,  1.1160, -0.0105, -0.1250, -0.5240,  3.1184],
       device='cuda:0')
Solve time for step 1 9.137141144019552
Current ori: tensor([-0.0105, -0.1250, -0.5240], device='cuda:0')
Index force: tensor([0.5567, 0.6508, 0.5001, 0.5980], device='cuda:0')
tensor([ 0.2414,  0.7424,  0.4664,  0.6512, -0.1476,  0.7228,  0.8373,  0.9895,
         1.2111,  0.2955, -0.0543,  1.1038, -0.0109, -0.1288, -0.5239,  3.0556],
       device='cuda:0')
Solve time for step 2 3.502393814967945
Current ori: tensor([-0.0109, -0.1288, -0.5239], device='cuda:0')
Index force: tensor([0.5668, 0.5899, 0.5916], device='cuda:0')
tensor([ 2.5100e-01,  7.2552e-01,  4.7974e-01,  6.9402e-01, -1.4789e-01,
         7.2627e-01,  8.5150e-01,  9.7658e-01,  1.2094e+00,  2.9853e-01,
        -6.6235e-02,  1.1079e+00, -8.4286e-04, -1.3110e-01, -5.2386e-01,
         3.1144e+00], device='cuda:0')
Solve time for step 3 3.400530497019645
Current ori: tensor([-0.0008, -0.1311, -0.5239], device='cuda:0')
Index force: tensor([0.5835, 0.5869], device='cuda:0')
tensor([ 2.4392e-01,  7.1636e-01,  4.8443e-01,  6.9304e-01, -1.5091e-01,
         7.3344e-01,  8.4189e-01,  9.5446e-01,  1.2235e+00,  2.8865e-01,
        -6.8714e-02,  1.1060e+00,  1.9208e-04, -1.2603e-01, -5.2386e-01,
         3.1102e+00], device='cuda:0')
Solve time for step 4 3.3106626869994216
Current ori: tensor([ 1.9208e-04, -1.2603e-01, -5.2386e-01], device='cuda:0')
Index force: tensor([0.5807], device='cuda:0')
Storing RECOVERY transition: reward=0.0094 (scaled=0.0094), steps=0
Reward stats updated: mean 0.0056 -> 0.0056, std: 0.0803
Collected 166 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.7433, Q2 Loss=1.7433, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5571
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.9400, Q2 Loss=2.9400, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7765
SAC Update 3/5: Actor Loss=-0.2300, Q1 Loss=0.6589, Q2 Loss=0.6589, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1093
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.1431, Q2 Loss=1.1431, Entropy=0.0090, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0286
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=2.4871, Q2 Loss=2.4871, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1138

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.5%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (17.8%)
Actor update: 0.10s (40.0%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.138108
Q1 loss: 1.794483
Q2 loss: 1.794483
Current threshold: -32.9388
Global Scale Offset: 0.0962
Reward stats: mean=0.0056, std=0.0803, count=166
----------------------------------------------
SAC Update - Actor Loss: -0.1381, Q1 Loss: 1.7945, Q2 Loss: 1.7945, Entropy: 0.0018, Mean TD Error: 1.9170, Threshold: -32.9388
Original likelihood: -34.16703796386719
Adjusted likelihood: -34.16703796386719
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0002)
State is out of distribution
Projection step: 0, Loss: 33.05748748779297
Projection step: 1, Loss: 32.32383346557617
Projection step: 2, Loss: 31.69943618774414
Projection step: 3, Loss: 32.9389533996582
Projection step: 4, Loss: 31.110219955444336
Projection step: 5, Loss: 32.60584259033203
Projection step: 6, Loss: 31.74833106994629
Projection step: 7, Loss: 32.21070861816406
Projection step: 8, Loss: 30.307178497314453
Projection step: 9, Loss: 31.219449996948242
Projection step: 10, Loss: 29.561613082885742
Projection step: 11, Loss: 29.960378646850586
Projection step: 12, Loss: 31.29169464111328
Projection step: 13, Loss: 30.289371490478516
Projection step: 14, Loss: 31.13623809814453
Projection step: 15, Loss: 30.879932403564453
Projection step: 16, Loss: 28.429779052734375
Projection step: 17, Loss: 29.426015853881836
Projection step: 18, Loss: 28.97758674621582
Projection step: 19, Loss: 27.83397102355957
Projection step: 20, Loss: 28.217594146728516
Projection step: 21, Loss: 29.69394302368164
Projection step: 22, Loss: 27.734447479248047
Projection step: 23, Loss: 26.740280151367188
Projection step: 24, Loss: 29.815845489501953
Final likelihood: tensor([-24.4905, -25.9503, -35.8749, -26.0083, -30.7288, -25.0952, -32.1224,
        -33.5242, -25.2200, -28.2271, -25.7315, -32.4018, -27.5942, -24.6967,
        -31.5015, -24.7984])
Final projection likelihood: -28.3729
1 mode projection succeeded
New goal: tensor([ 0.2313,  0.6729,  0.4946,  0.7281, -0.0571,  0.7617,  0.8496,  0.9781,
         1.2759,  0.2721,  0.0216,  1.1655, -0.0041, -0.1142, -0.6167],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -30.315311431884766
Adjusted likelihood: -30.315311431884766
Likelihood residual: 0.0
Original likelihood: -34.01042175292969
Adjusted likelihood: -34.01042175292969
Likelihood residual: 0.0
{'index': 34.01042175292969, 'thumb_middle': 30.315311431884766}
Current yaw: tensor([-0.0018, -0.1196, -0.5323], device='cuda:0')
7 thumb_middle
tensor([ 2.3254e-01,  7.1738e-01,  4.7050e-01,  6.9019e-01, -9.4468e-02,
         7.8862e-01,  8.5608e-01,  1.0117e+00,  1.2814e+00,  2.9630e-01,
         3.7664e-03,  1.1452e+00, -1.7997e-03, -1.1959e-01, -5.3229e-01,
         3.1019e+00], device='cuda:0')
Solve time for step 1 9.196996637969278
Current ori: tensor([-0.0018, -0.1196, -0.5323], device='cuda:0')
Index force: tensor([0.5675, 0.5760, 0.5603, 0.5945], device='cuda:0')
tensor([ 2.3755e-01,  7.1731e-01,  4.7388e-01,  6.9455e-01, -1.4439e-01,
         7.4126e-01,  8.0419e-01,  9.5774e-01,  1.2386e+00,  2.5948e-01,
        -7.6032e-02,  1.1309e+00, -7.4436e-04, -1.2197e-01, -5.3216e-01,
         3.0955e+00], device='cuda:0')
Solve time for step 2 3.508277587010525
Current ori: tensor([-0.0007, -0.1220, -0.5322], device='cuda:0')
Index force: tensor([0.5667, 0.5536, 0.5871], device='cuda:0')
tensor([ 2.2771e-01,  6.9963e-01,  4.8363e-01,  7.0080e-01, -1.5215e-01,
         7.4256e-01,  8.0378e-01,  9.4333e-01,  1.2423e+00,  2.4863e-01,
        -6.6922e-02,  1.1182e+00,  6.8819e-04, -1.1566e-01, -5.3216e-01,
         3.0694e+00], device='cuda:0')
Solve time for step 3 3.3213832449982874
Current ori: tensor([ 0.0007, -0.1157, -0.5322], device='cuda:0')
Index force: tensor([0.5440, 0.5769], device='cuda:0')
tensor([ 0.2421,  0.6871,  0.4969,  0.7416, -0.1475,  0.7542,  0.7961,  0.9576,
         1.2298,  0.2542, -0.0778,  1.1410,  0.0113, -0.1207, -0.5322,  3.1274],
       device='cuda:0')
Solve time for step 4 3.3137071980163455
Current ori: tensor([ 0.0113, -0.1207, -0.5322], device='cuda:0')
Index force: tensor([0.5604], device='cuda:0')
Storing RECOVERY transition: reward=0.0211 (scaled=0.0211), steps=0
Reward stats updated: mean 0.0056 -> 0.0057, std: 0.0801
Collected 167 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1728, Q2 Loss=1.1728, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9445
SAC Update 2/5: Actor Loss=-0.2301, Q1 Loss=0.8683, Q2 Loss=0.8683, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2230
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0695, Q2 Loss=1.0695, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1626
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.5073, Q2 Loss=2.5073, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6336
SAC Update 5/5: Actor Loss=-0.0004, Q1 Loss=1.2324, Q2 Loss=1.2324, Entropy=0.0896, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4319

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.9%)
Actor update: 0.08s (39.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092155
Q1 loss: 1.370069
Q2 loss: 1.370069
Current threshold: -32.9445
Global Scale Offset: 0.0958
Reward stats: mean=0.0057, std=0.0801, count=167
----------------------------------------------
SAC Update - Actor Loss: -0.0922, Q1 Loss: 1.3701, Q2 Loss: 1.3701, Entropy: 0.0179, Mean TD Error: 1.6791, Threshold: -32.9445
Original likelihood: -32.52838134765625
Adjusted likelihood: -32.52838134765625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.8851)
Current yaw: tensor([ 0.0078, -0.1208, -0.5446], device='cuda:0')
8 turn
Sampling time 3.8684898760402575
tensor([ 0.2397,  0.6896,  0.5035,  0.7169, -0.0880,  0.8025,  0.8497,  0.9748,
         1.2901,  0.2666, -0.0042,  1.1641,  0.0078, -0.1208, -0.5446,  3.1222],
       device='cuda:0')
Original likelihood: -35.128746032714844
Adjusted likelihood: -35.128746032714844
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 34.21623992919922
Projection step: 1, Loss: 32.9249267578125
Projection step: 2, Loss: 32.25505065917969
Projection step: 3, Loss: 32.77100372314453
Projection step: 4, Loss: 34.3709716796875
Projection step: 5, Loss: 32.91196060180664
Projection step: 6, Loss: 31.71898651123047
Projection step: 7, Loss: 31.574777603149414
Projection step: 8, Loss: 30.246572494506836
Projection step: 9, Loss: 31.630508422851562
Projection step: 10, Loss: 31.483383178710938
Projection step: 11, Loss: 31.793123245239258
Projection step: 12, Loss: 31.053184509277344
Projection step: 13, Loss: 33.421512603759766
Projection step: 14, Loss: 30.666776657104492
Projection step: 15, Loss: 33.787044525146484
Projection step: 16, Loss: 30.771270751953125
Projection step: 17, Loss: 30.7197322845459
Projection step: 18, Loss: 32.045040130615234
Projection step: 19, Loss: 30.691871643066406
Projection step: 20, Loss: 30.05933952331543
Projection step: 21, Loss: 30.665206909179688
Projection step: 22, Loss: 29.420391082763672
Projection step: 23, Loss: 29.041067123413086
Projection step: 24, Loss: 29.152122497558594
Final likelihood: tensor([-27.2074, -26.8707, -24.1418, -33.5314, -26.8529, -26.4145, -33.6842,
        -27.2056, -26.1660, -30.8037, -27.3700, -31.3367, -26.5631, -30.2481,
        -33.4888, -29.9478])
Final projection likelihood: -28.8646
1 mode projection succeeded
New goal: tensor([ 0.2332,  0.6391,  0.5452,  0.7422, -0.0607,  0.7721,  0.8573,  0.9413,
         1.2828,  0.2523,  0.0268,  1.1602,  0.0055, -0.1152, -0.7339],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -34.24735641479492
Adjusted likelihood: -34.24735641479492
Likelihood residual: 0.0
Original likelihood: -33.526878356933594
Adjusted likelihood: -33.526878356933594
Likelihood residual: 0.0
{'index': 33.526878356933594, 'thumb_middle': 34.24735641479492}
Current yaw: tensor([ 0.0078, -0.1208, -0.5446], device='cuda:0')
9 index
tensor([ 0.2397,  0.6896,  0.5035,  0.7169, -0.0880,  0.8025,  0.8497,  0.9748,
         1.2901,  0.2666, -0.0042,  1.1641,  0.0078, -0.1208, -0.5446,  3.1222],
       device='cuda:0')
Solve time for step 1 10.28427061100956
Current ori: tensor([ 0.0078, -0.1208, -0.5446], device='cuda:0')
Middle force: tensor([0.5436, 0.5862, 0.5318, 0.5152], device='cuda:0')
Thumb force: tensor([0.6067, 0.5739, 0.5281, 0.5210], device='cuda:0')
tensor([ 0.2930,  0.6065,  0.4913,  0.7133, -0.1029,  0.8239,  0.8804,  0.9576,
         1.2986,  0.2559, -0.0204,  1.1595,  0.0117, -0.1336, -0.6014,  2.9005],
       device='cuda:0')
Solve time for step 2 4.189128390979022
Current ori: tensor([ 0.0117, -0.1336, -0.6014], device='cuda:0')
Middle force: tensor([0.5821, 0.5279, 0.5136], device='cuda:0')
Thumb force: tensor([0.5650, 0.5250, 0.5183], device='cuda:0')
tensor([ 2.9440e-01,  6.0231e-01,  5.0047e-01,  7.1476e-01, -1.1744e-01,
         8.2194e-01,  8.8448e-01,  9.5000e-01,  1.3108e+00,  2.3922e-01,
         2.3178e-03,  1.1569e+00,  2.6308e-02, -1.2195e-01, -6.4191e-01,
         3.4118e+00], device='cuda:0')
Solve time for step 3 4.11152762704296
Current ori: tensor([ 0.0263, -0.1219, -0.6419], device='cuda:0')
Middle force: tensor([0.5237, 0.5120], device='cuda:0')
Thumb force: tensor([0.5215, 0.5160], device='cuda:0')
tensor([ 0.2955,  0.6025,  0.4997,  0.7171, -0.1101,  0.8466,  0.8726,  0.9297,
         1.3295,  0.1903, -0.0240,  1.1844,  0.0358, -0.1304, -0.6742,  4.2128],
       device='cuda:0')
Solve time for step 4 3.954089423001278
Current ori: tensor([ 0.0358, -0.1304, -0.6742], device='cuda:0')
Middle force: tensor([0.5108], device='cuda:0')
Thumb force: tensor([0.5129], device='cuda:0')
Storing RECOVERY transition: reward=0.1384 (scaled=0.1384), steps=0
Reward stats updated: mean 0.0057 -> 0.0065, std: 0.0805
Collected 168 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0165, Q2 Loss=1.0165, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8724
SAC Update 2/5: Actor Loss=-0.1080, Q1 Loss=1.3712, Q2 Loss=1.3712, Entropy=0.1463, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5640
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6509, Q2 Loss=1.6509, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9402
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=3.0043, Q2 Loss=3.0043, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9267
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6857, Q2 Loss=1.6857, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4497

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.5%)
Q1 update: 0.04s (19.4%)
Q2 update: 0.04s (18.8%)
Actor update: 0.09s (40.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.021609
Q1 loss: 1.745711
Q2 loss: 1.745711
Current threshold: -32.9477
Global Scale Offset: 0.0956
Reward stats: mean=0.0065, std=0.0805, count=168
----------------------------------------------
SAC Update - Actor Loss: -0.0216, Q1 Loss: 1.7457, Q2 Loss: 1.7457, Entropy: 0.0293, Mean TD Error: 1.7506, Threshold: -32.9477
Original likelihood: -35.7890625
Adjusted likelihood: -35.7890625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.910369873046875
Projection step: 1, Loss: 36.94184112548828
Projection step: 2, Loss: 35.62459945678711
Projection step: 3, Loss: 36.46299362182617
Projection step: 4, Loss: 35.24430465698242
Projection step: 5, Loss: 35.3892707824707
Projection step: 6, Loss: 36.45903778076172
Projection step: 7, Loss: 36.43416976928711
Projection step: 8, Loss: 36.025482177734375
Projection step: 9, Loss: 34.71823501586914
Projection step: 10, Loss: 35.79529571533203
Projection step: 11, Loss: 34.551177978515625
Projection step: 12, Loss: 35.537784576416016
Projection step: 13, Loss: 35.243465423583984
Projection step: 14, Loss: 36.02106475830078
Projection step: 15, Loss: 35.10582733154297
Projection step: 16, Loss: 35.53422164916992
Projection step: 17, Loss: 34.80925369262695
Projection step: 18, Loss: 35.183990478515625
Projection step: 19, Loss: 33.91696548461914
Projection step: 20, Loss: 34.245079040527344
Projection step: 21, Loss: 33.710960388183594
Projection step: 22, Loss: 31.61808967590332
Projection step: 23, Loss: 34.57572937011719
Projection step: 24, Loss: 33.34234619140625
Final likelihood: tensor([-37.6352, -36.0782, -30.5317, -30.6664, -39.9507, -39.1691, -38.3529,
        -31.0688, -30.0451, -32.6267, -31.0570, -45.0936, -31.0520, -30.4314,
        -36.7123, -33.3011])
Final projection likelihood: -34.6108
1 mode projection failed, trying anyway
New goal: tensor([ 0.2385,  0.6223,  0.5948,  0.7874, -0.0852,  0.8546,  0.8788,  0.8857,
         1.2865,  0.2629,  0.0018,  1.2392,  0.0403, -0.1269, -0.7147],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0021]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -32.79749298095703
Adjusted likelihood: -32.79749298095703
Likelihood residual: 0.0
Original likelihood: -33.88261795043945
Adjusted likelihood: -33.88261795043945
Likelihood residual: 0.0
{'index': 33.88261795043945, 'thumb_middle': 32.79749298095703}
Current yaw: tensor([ 0.0423, -0.1314, -0.6939], device='cuda:0')
10 thumb_middle
tensor([ 0.2379,  0.6473,  0.5404,  0.7396, -0.1247,  0.8727,  0.8657,  0.9106,
         1.2857,  0.2919, -0.0169,  1.2147,  0.0423, -0.1314, -0.6939,  4.4301],
       device='cuda:0')
Solve time for step 1 8.739063624001574
Current ori: tensor([ 0.0423, -0.1314, -0.6939], device='cuda:0')
Index force: tensor([0.5672, 0.5893, 0.6075, 0.5977], device='cuda:0')
tensor([ 0.2451,  0.6426,  0.5559,  0.7374, -0.1698,  0.8259,  0.8195,  0.8542,
         1.2327,  0.2537, -0.0934,  1.1953,  0.0234, -0.1494, -0.6938,  4.3104],
       device='cuda:0')
Solve time for step 2 3.647027990024071
Current ori: tensor([ 0.0234, -0.1494, -0.6938], device='cuda:0')
Index force: tensor([0.5806, 0.6009, 0.5927], device='cuda:0')
tensor([ 0.2566,  0.6026,  0.5983,  0.7859, -0.1630,  0.8407,  0.8264,  0.8529,
         1.2252,  0.2467, -0.1155,  1.1871,  0.0384, -0.1538, -0.6938,  4.3311],
       device='cuda:0')
Solve time for step 3 3.800845540012233
Current ori: tensor([ 0.0384, -0.1538, -0.6938], device='cuda:0')
Index force: tensor([0.5906, 0.5859], device='cuda:0')
tensor([ 0.2285,  0.5887,  0.5790,  0.7954, -0.1718,  0.8325,  0.8175,  0.8475,
         1.2324,  0.2450, -0.1071,  1.1939,  0.0328, -0.1407, -0.6938,  4.2415],
       device='cuda:0')
Solve time for step 4 3.6251542950049043
Current ori: tensor([ 0.0328, -0.1407, -0.6938], device='cuda:0')
Index force: tensor([0.5691], device='cuda:0')
Storing RECOVERY transition: reward=0.1391 (scaled=0.1391), steps=0
Reward stats updated: mean 0.0065 -> 0.0073, std: 0.0809
Collected 169 transitions for RL
SAC Update 1/5: Actor Loss=-0.0449, Q1 Loss=0.7244, Q2 Loss=0.7244, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7542
SAC Update 2/5: Actor Loss=-0.2302, Q1 Loss=0.5901, Q2 Loss=0.5901, Entropy=0.0163, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2362
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4207, Q2 Loss=1.4207, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3810
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=0.5981, Q2 Loss=0.5981, Entropy=0.0081, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3335
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0660, Q2 Loss=1.0660, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0609

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.3%)
Q1 update: 0.04s (19.4%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (40.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.055025
Q1 loss: 0.879836
Q2 loss: 0.879836
Current threshold: -32.9496
Global Scale Offset: 0.0955
Reward stats: mean=0.0073, std=0.0809, count=169
----------------------------------------------
SAC Update - Actor Loss: -0.0550, Q1 Loss: 0.8798, Q2 Loss: 0.8798, Entropy: 0.0049, Mean TD Error: 0.9532, Threshold: -32.9496
Original likelihood: -35.22954559326172
Adjusted likelihood: -35.22954559326172
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 36.46807098388672
Projection step: 1, Loss: 34.769004821777344
Projection step: 2, Loss: 35.117733001708984
Projection step: 3, Loss: 34.405452728271484
Projection step: 4, Loss: 33.82822036743164
Projection step: 5, Loss: 33.8357048034668
Projection step: 6, Loss: 35.329498291015625
Projection step: 7, Loss: 34.31441879272461
Projection step: 8, Loss: 33.948272705078125
Projection step: 9, Loss: 33.37364959716797
Projection step: 10, Loss: 34.74951171875
Projection step: 11, Loss: 34.57837677001953
Projection step: 12, Loss: 34.79106903076172
Projection step: 13, Loss: 33.14105224609375
Projection step: 14, Loss: 34.426414489746094
Projection step: 15, Loss: 34.28044891357422
Projection step: 16, Loss: 34.24009704589844
Projection step: 17, Loss: 35.61021423339844
Projection step: 18, Loss: 33.54430389404297
Projection step: 19, Loss: 34.05592727661133
Projection step: 20, Loss: 34.83189392089844
Projection step: 21, Loss: 34.07257080078125
Projection step: 22, Loss: 34.06266784667969
Projection step: 23, Loss: 33.23564147949219
Projection step: 24, Loss: 34.734954833984375
Final likelihood: tensor([-37.5028, -30.0675, -36.3156, -29.8783, -35.8457, -37.5915, -29.4568,
        -32.9741, -31.7776, -39.0443, -29.4316, -31.4782, -37.0381, -31.3871,
        -31.3750, -30.9152])
Final projection likelihood: -33.2550
1 mode projection failed, trying anyway
New goal: tensor([ 0.2228,  0.5692,  0.6242,  0.8161, -0.0865,  0.8650,  0.8688,  0.8502,
         1.2903,  0.2372, -0.0077,  1.2139,  0.0304, -0.1320, -0.9243],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0020]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -34.55177307128906
Adjusted likelihood: -34.55177307128906
Likelihood residual: 0.0
Original likelihood: -33.64773178100586
Adjusted likelihood: -33.64773178100586
Likelihood residual: 0.0
{'index': 33.64773178100586, 'thumb_middle': 34.55177307128906}
Current yaw: tensor([ 0.0323, -0.1369, -0.6956], device='cuda:0')
11 index
tensor([ 0.2237,  0.5935,  0.5712,  0.7874, -0.1207,  0.8857,  0.8665,  0.8756,
         1.2833,  0.2557, -0.0419,  1.2384,  0.0323, -0.1369, -0.6956,  4.2490],
       device='cuda:0')
Solve time for step 1 10.170715451007709
Current ori: tensor([ 0.0323, -0.1369, -0.6956], device='cuda:0')
Middle force: tensor([0.5873, 0.5929, 0.6035, 0.5344], device='cuda:0')
Thumb force: tensor([0.5100, 0.5673, 0.5689, 0.5875], device='cuda:0')
tensor([ 0.2901,  0.5259,  0.5560,  0.7810, -0.1405,  0.9199,  0.8940,  0.8680,
         1.2988,  0.2195, -0.0700,  1.2455,  0.0483, -0.1613, -0.7803,  4.3050],
       device='cuda:0')
Solve time for step 2 4.146762797958218
Current ori: tensor([ 0.0483, -0.1613, -0.7803], device='cuda:0')
Middle force: tensor([0.5860, 0.5993, 0.5323], device='cuda:0')
Thumb force: tensor([0.5628, 0.5631, 0.5816], device='cuda:0')
tensor([ 0.2986,  0.5248,  0.5662,  0.7777, -0.1595,  0.9591,  0.9076,  0.8542,
         1.2917,  0.2312, -0.0392,  1.2551,  0.0827, -0.1668, -0.8771,  4.5850],
       device='cuda:0')
Solve time for step 3 3.9639170169830322
Current ori: tensor([ 0.0827, -0.1668, -0.8771], device='cuda:0')
Middle force: tensor([0.5763, 0.5799], device='cuda:0')
Thumb force: tensor([0.5289, 0.5208], device='cuda:0')
tensor([ 0.2952,  0.5213,  0.5664,  0.7786, -0.1705,  1.0030,  0.9039,  0.8310,
         1.2799,  0.2342, -0.0372,  1.2915,  0.1128, -0.1804, -0.9527,  4.9574],
       device='cuda:0')
Solve time for step 4 4.074733879999258
Current ori: tensor([ 0.1128, -0.1804, -0.9527], device='cuda:0')
Middle force: tensor([0.5003], device='cuda:0')
Thumb force: tensor([0.5272], device='cuda:0')
Storing RECOVERY transition: reward=0.1494 (scaled=0.1494), steps=0
Reward stats updated: mean 0.0073 -> 0.0081, std: 0.0814
Collected 170 transitions for RL
SAC Update 1/5: Actor Loss=-0.0014, Q1 Loss=0.8045, Q2 Loss=0.8045, Entropy=0.1966, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8348
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.8049, Q2 Loss=1.8049, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8263
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6683, Q2 Loss=1.6683, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6708
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8840, Q2 Loss=0.8840, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4699
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7458, Q2 Loss=0.7458, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4132

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.5%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (18.8%)
Actor update: 0.09s (40.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000287
Q1 loss: 1.181470
Q2 loss: 1.181470
Current threshold: -32.9519
Global Scale Offset: 0.0954
Reward stats: mean=0.0081, std=0.0814, count=170
----------------------------------------------
SAC Update - Actor Loss: -0.0003, Q1 Loss: 1.1815, Q2 Loss: 1.1815, Entropy: 0.0393, Mean TD Error: 1.0430, Threshold: -32.9519
Original likelihood: -163.43829345703125
Adjusted likelihood: -163.43829345703125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 217.19842529296875
Projection step: 1, Loss: 202.64263916015625
Projection step: 2, Loss: 199.34963989257812
Projection step: 3, Loss: 208.13723754882812
Projection step: 4, Loss: 212.67889404296875
Projection step: 5, Loss: 183.035888671875
Projection step: 6, Loss: 179.3992156982422
Projection step: 7, Loss: 202.05392456054688
Projection step: 8, Loss: 185.81295776367188
Projection step: 9, Loss: 215.21002197265625
Projection step: 10, Loss: 192.57020568847656
Projection step: 11, Loss: 217.68405151367188
Projection step: 12, Loss: 217.91920471191406
Projection step: 13, Loss: 203.67547607421875
Projection step: 14, Loss: 219.94091796875
Projection step: 15, Loss: 196.19386291503906
Projection step: 16, Loss: 205.40406799316406
Projection step: 17, Loss: 212.01150512695312
Projection step: 18, Loss: 182.86129760742188
Projection step: 19, Loss: 216.7673797607422
Projection step: 20, Loss: 207.4351043701172
Projection step: 21, Loss: 198.1629638671875
Projection step: 22, Loss: 206.53048706054688
Projection step: 23, Loss: 200.39346313476562
Projection step: 24, Loss: 199.0474853515625
Final likelihood: tensor([-225.2372, -153.6239, -251.2775, -288.6053, -253.7041, -254.0484,
        -129.0234, -217.7877, -143.4676, -258.5481, -197.4084, -236.3425,
        -230.8327, -245.1083, -244.1929, -184.1667])
Final projection likelihood: -219.5859
1 mode projection failed, trying anyway
New goal: tensor([ 0.2275,  0.6201,  0.6610,  0.8215, -0.1730,  1.0375,  0.8984,  0.8483,
         1.2790,  0.2830, -0.0116,  1.3988,  0.2675, -0.2721, -0.9995],
       device='cuda:0')
tensor([[0.0041]], device='cuda:0') tensor([[0.0156]], device='cuda:0') tensor([[0.0163]], device='cuda:0')
Original likelihood: -120.332763671875
Adjusted likelihood: -120.332763671875
Likelihood residual: 0.0
Original likelihood: -98.89061737060547
Adjusted likelihood: -98.89061737060547
Likelihood residual: 0.0
{'index': 98.89061737060547, 'thumb_middle': 120.332763671875}
Current yaw: tensor([ 0.2676, -0.2717, -0.9969], device='cuda:0')
12 index
tensor([ 0.2178,  0.6209,  0.6575,  0.8225, -0.1761,  1.0439,  0.9064,  0.8276,
         1.2965,  0.2764, -0.0135,  1.3948,  0.2676, -0.2717, -0.9969,  5.2689],
       device='cuda:0')
Solve time for step 1 10.509240610001143
Current ori: tensor([ 0.2676, -0.2717, -0.9969], device='cuda:0')
Middle force: tensor([0.5985, 0.5783, 0.5629, 0.5680], device='cuda:0')
Thumb force: tensor([0.5085, 0.5792, 0.5759, 0.5536], device='cuda:0')
tensor([ 0.2640,  0.7036,  0.7069,  0.8346, -0.1440,  1.1629,  0.9438,  0.8459,
         1.3102,  0.2806,  0.0095,  1.3896,  0.3740, -0.3412, -1.1407,  5.5160],
       device='cuda:0')
Solve time for step 2 4.227371499990113
Current ori: tensor([ 0.3740, -0.3412, -1.1407], device='cuda:0')
Middle force: tensor([0.5776, 0.5303, 0.6106], device='cuda:0')
Thumb force: tensor([0.5234, 0.5580, 0.5272], device='cuda:0')
tensor([ 0.2983,  0.5577,  0.7283,  0.8403, -0.1417,  1.2477,  0.9428,  0.8011,
         1.2977,  0.3251,  0.0944,  1.3885,  0.3754, -0.3372, -1.1541,  5.4556],
       device='cuda:0')
Solve time for step 3 4.081869056972209
Current ori: tensor([ 0.3754, -0.3372, -1.1541], device='cuda:0')
Middle force: tensor([0.5304, 0.5785], device='cuda:0')
Thumb force: tensor([0.5053, 0.6084], device='cuda:0')
tensor([ 0.2975,  0.6062,  0.7311,  0.8410, -0.1519,  1.2849,  0.9447,  0.8020,
         1.2927,  0.3117,  0.1512,  1.4110,  0.3724, -0.3297, -1.1964,  5.5079],
       device='cuda:0')
Solve time for step 4 4.208764513023198
Current ori: tensor([ 0.3724, -0.3297, -1.1964], device='cuda:0')
Middle force: tensor([0.7648], device='cuda:0')
Thumb force: tensor([0.6201], device='cuda:0')
Storing RECOVERY transition: reward=0.0237 (scaled=0.0237), steps=0
Reward stats updated: mean 0.0081 -> 0.0082, std: 0.0812
Collected 171 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=4.0717, Q2 Loss=4.0717, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1113
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.5859, Q2 Loss=0.5859, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9271
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.6630, Q2 Loss=0.6630, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9632
SAC Update 4/5: Actor Loss=-0.1221, Q1 Loss=1.5261, Q2 Loss=1.5261, Entropy=0.0186, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6694
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9650, Q2 Loss=0.9650, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9838

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (40.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.024426
Q1 loss: 1.562347
Q2 loss: 1.562347
Current threshold: -32.9532
Global Scale Offset: 0.0954
Reward stats: mean=0.0082, std=0.0812, count=171
----------------------------------------------
SAC Update - Actor Loss: -0.0244, Q1 Loss: 1.5623, Q2 Loss: 1.5623, Entropy: 0.0037, Mean TD Error: 1.3310, Threshold: -32.9532
Original likelihood: -332.87213134765625
Adjusted likelihood: -332.87213134765625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 11
Loaded trajectory sampler
Current yaw: tensor([ 0.0005,  0.0145, -0.0448], device='cuda:0')
Current yaw: tensor([ 0.0005,  0.0145, -0.0448], device='cuda:0')
1 turn
Sampling time 3.6377169060287997
tensor([ 1.2648e-01,  5.8557e-01,  5.8357e-01,  5.9568e-01, -1.1181e-01,
         5.4758e-01,  8.9582e-01,  8.4257e-01,  1.2393e+00,  2.1686e-01,
         2.7784e-01,  1.2155e+00,  5.3477e-04,  1.4474e-02, -4.4780e-02,
         4.9527e-02], device='cuda:0')
Original likelihood: -19.076717376708984
Adjusted likelihood: -19.076717376708984
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.242221685999539
Current ori: tensor([ 0.0005,  0.0145, -0.0448], device='cuda:0')
Middle force: tensor([0.5294, 0.5049, 1.2216, 0.9261, 0.5613, 0.5016, 0.5014, 0.7634, 0.5770,
        0.5856, 0.5239, 0.5809], device='cuda:0')
Thumb force: tensor([0.6392, 3.2584, 1.6481, 1.0061, 0.6208, 0.6154, 0.9725, 0.7650, 0.5821,
        0.6015, 0.5483, 0.6186], device='cuda:0')
Index force: tensor([0.5321, 0.7792, 0.5315, 0.7704, 0.5361, 0.6154, 0.6264, 0.6467, 0.6014,
        0.5960, 0.5530, 0.5947], device='cuda:0')
Storing NORMAL transition: reward=0.0175 (scaled=0.0175), steps=1
Reward stats updated: mean 0.0082 -> 0.0083, std: 0.0809
Collected 172 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.1197, Q2 Loss=1.1197, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7802
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8972, Q2 Loss=0.8972, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3770
SAC Update 3/5: Actor Loss=-0.1719, Q1 Loss=1.6569, Q2 Loss=1.6569, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5277
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2637, Q2 Loss=1.2637, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2110
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5048, Q2 Loss=1.5048, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6506

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (13.6%)
Q1 update: 0.06s (20.6%)
Q2 update: 0.06s (20.6%)
Actor update: 0.12s (42.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.080429
Q1 loss: 1.288452
Q2 loss: 1.288452
Current threshold: -32.9539
Global Scale Offset: 0.0953
Reward stats: mean=0.0083, std=0.0809, count=172
----------------------------------------------
SAC Update - Actor Loss: -0.0804, Q1 Loss: 1.2885, Q2 Loss: 1.2885, Entropy: 0.0000, Mean TD Error: 1.1093, Threshold: -32.9539
tensor([ 0.0647,  0.6005,  0.5543,  0.4735, -0.0327,  0.5198,  0.9031,  0.8069,
         1.3061,  0.2565,  0.1835,  1.0505, -0.0158, -0.0143, -0.0626,  0.3565],
       device='cuda:0')
Original likelihood: -9.826650619506836
Adjusted likelihood: -9.826650619506836
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.5690742680453695
Current ori: tensor([-0.0158, -0.0143, -0.0626], device='cuda:0')
Middle force: tensor([1.7090, 1.8259, 0.5617, 0.6081, 0.5268, 0.5619, 0.5080, 0.5664, 0.5007,
        0.5355, 0.5683], device='cuda:0')
Thumb force: tensor([1.8046, 0.5641, 0.6672, 0.5689, 0.5426, 0.5310, 0.7813, 0.8109, 1.3243,
        0.5387, 0.5607], device='cuda:0')
Index force: tensor([0.7453, 0.5773, 0.5117, 0.5656, 0.6388, 0.5735, 0.5950, 0.5600, 0.6854,
        0.6168, 0.6642], device='cuda:0')
Storing NORMAL transition: reward=0.1212 (scaled=0.1212), steps=1
Reward stats updated: mean 0.0083 -> 0.0089, std: 0.0811
Collected 173 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.1937, Q2 Loss=1.1937, Entropy=0.0168, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7537
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.5520, Q2 Loss=1.5520, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8118
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.7042, Q2 Loss=1.7042, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7415
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5536, Q2 Loss=1.5536, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1573
SAC Update 5/5: Actor Loss=-0.1916, Q1 Loss=2.3726, Q2 Loss=2.3726, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0844

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (13.3%)
Q1 update: 0.06s (20.4%)
Q2 update: 0.06s (20.4%)
Actor update: 0.12s (42.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.176471
Q1 loss: 1.675237
Q2 loss: 1.675237
Current threshold: -32.9544
Global Scale Offset: 0.0953
Reward stats: mean=0.0089, std=0.0811, count=173
----------------------------------------------
SAC Update - Actor Loss: -0.1765, Q1 Loss: 1.6752, Q2 Loss: 1.6752, Entropy: 0.0034, Mean TD Error: 1.5097, Threshold: -32.9544
tensor([ 0.2000,  0.6523,  0.5540,  0.6332,  0.0499,  0.6340,  0.7915,  0.8537,
         1.3289,  0.2045,  0.0894,  1.0155, -0.0454, -0.0707, -0.1926,  0.1605],
       device='cuda:0')
Original likelihood: -29.67792510986328
Adjusted likelihood: -29.67792510986328
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.374673836980946
Current ori: tensor([-0.0454, -0.0707, -0.1926], device='cuda:0')
Middle force: tensor([0.6185, 0.5070, 0.5063, 0.9051, 0.5023, 1.0713, 0.5471, 0.6098, 0.5361,
        0.6169], device='cuda:0')
Thumb force: tensor([0.7833, 0.5397, 0.8001, 0.9157, 1.0941, 0.7591, 0.7283, 0.5893, 0.5572,
        0.5727], device='cuda:0')
Index force: tensor([0.5429, 0.7218, 0.6720, 0.6470, 0.6094, 0.5048, 0.5579, 0.5841, 0.5811,
        0.5659], device='cuda:0')
Storing NORMAL transition: reward=0.1364 (scaled=0.1364), steps=1
Reward stats updated: mean 0.0089 -> 0.0097, std: 0.0815
Collected 174 transitions for RL
SAC Update 1/5: Actor Loss=-0.0005, Q1 Loss=1.1260, Q2 Loss=1.1260, Entropy=0.0932, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8215
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.7979, Q2 Loss=1.7979, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4431
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.8257, Q2 Loss=1.8257, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5824
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1023, Q2 Loss=1.1023, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3696
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.2176, Q2 Loss=2.2176, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1123

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.3%)
Q1 update: 0.06s (20.5%)
Q2 update: 0.06s (20.3%)
Actor update: 0.12s (42.0%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046146
Q1 loss: 1.613893
Q2 loss: 1.613893
Current threshold: -32.9552
Global Scale Offset: 0.0953
Reward stats: mean=0.0097, std=0.0815, count=174
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.6139, Q2 Loss: 1.6139, Entropy: 0.0186, Mean TD Error: 1.4658, Threshold: -32.9552
tensor([ 2.5628e-01,  5.4935e-01,  6.8287e-01,  7.6120e-01,  1.2228e-01,
         6.2568e-01,  8.3138e-01,  9.1550e-01,  1.3666e+00,  9.2378e-02,
        -1.5591e-04,  1.0566e+00, -3.0276e-02, -1.1876e-01, -3.5201e-01,
         1.5504e-01], device='cuda:0')
Original likelihood: -31.266979217529297
Adjusted likelihood: -31.266979217529297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.988153513986617
Current ori: tensor([-0.0303, -0.1188, -0.3520], device='cuda:0')
Middle force: tensor([0.5759, 1.3142, 0.5223, 0.6194, 0.5326, 0.5031, 0.5360, 0.5730, 0.5687],
       device='cuda:0')
Thumb force: tensor([1.7334, 0.9035, 0.5017, 0.5131, 0.8295, 0.5050, 1.4265, 0.6228, 0.5780],
       device='cuda:0')
Index force: tensor([0.5298, 0.6186, 0.6437, 0.6032, 0.5134, 0.7551, 0.6877, 0.5460, 0.5641],
       device='cuda:0')
Storing NORMAL transition: reward=0.0421 (scaled=0.0421), steps=1
Reward stats updated: mean 0.0097 -> 0.0098, std: 0.0813
Collected 175 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=2.0848, Q2 Loss=2.0848, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4915
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7648, Q2 Loss=0.7648, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8768
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=2.1028, Q2 Loss=2.1028, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6291
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.4139, Q2 Loss=1.4139, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3264
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8773, Q2 Loss=0.8773, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7480

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.5%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.6%)
Actor update: 0.08s (40.4%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.448697
Q2 loss: 1.448697
Current threshold: -32.9556
Global Scale Offset: 0.0953
Reward stats: mean=0.0098, std=0.0813, count=175
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.4487, Q2 Loss: 1.4487, Entropy: 0.0000, Mean TD Error: 1.6143, Threshold: -32.9556
tensor([ 0.1990,  0.5238,  0.6617,  0.7503, -0.0530,  0.6700,  0.7783,  0.8897,
         1.3491,  0.2262,  0.1652,  0.8900, -0.0407, -0.0502, -0.3722, -0.0494],
       device='cuda:0')
Original likelihood: -23.662487030029297
Adjusted likelihood: -23.662487030029297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.7734846560051665
Current ori: tensor([-0.0407, -0.0502, -0.3722], device='cuda:0')
Middle force: tensor([0.5034, 0.8593, 0.5018, 1.0265, 0.5447, 0.5949, 0.5319, 0.6164],
       device='cuda:0')
Thumb force: tensor([0.7833, 0.8988, 1.0691, 0.7637, 0.7214, 0.5925, 0.5531, 0.5642],
       device='cuda:0')
Index force: tensor([0.6890, 0.6620, 0.6470, 0.5036, 0.5480, 0.5764, 0.5747, 0.5565],
       device='cuda:0')
Storing NORMAL transition: reward=0.0110 (scaled=0.0110), steps=1
Reward stats updated: mean 0.0098 -> 0.0098, std: 0.0811
Collected 176 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.9570, Q2 Loss=0.9570, Entropy=0.0001, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4931
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.4538, Q2 Loss=1.4538, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9495
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9967, Q2 Loss=0.9967, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9386
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6865, Q2 Loss=0.6865, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5152
SAC Update 5/5: Actor Loss=-0.2302, Q1 Loss=0.6087, Q2 Loss=0.6087, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7847

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (16.8%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (18.8%)
Actor update: 0.11s (42.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092083
Q1 loss: 0.940537
Q2 loss: 0.940537
Current threshold: -32.9559
Global Scale Offset: 0.0953
Reward stats: mean=0.0098, std=0.0811, count=176
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 0.9405, Q2 Loss: 0.9405, Entropy: 0.0000, Mean TD Error: 0.7362, Threshold: -32.9559
tensor([ 0.0528,  0.3313,  0.7858,  0.7208, -0.1204,  0.6309,  0.7372,  0.7696,
         1.3452,  0.1641,  0.2732,  0.8744,  0.0040,  0.0023, -0.3729,  0.0752],
       device='cuda:0')
Original likelihood: -24.671424865722656
Adjusted likelihood: -24.671424865722656
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.567058696004096
Current ori: tensor([ 0.0040,  0.0023, -0.3729], device='cuda:0')
Middle force: tensor([0.8459, 0.5026, 1.0118, 0.5397, 0.5927, 0.5352, 0.6240],
       device='cuda:0')
Thumb force: tensor([0.8759, 1.0284, 0.7574, 0.7135, 0.5849, 0.5445, 0.5581],
       device='cuda:0')
Index force: tensor([0.6463, 0.6165, 0.5026, 0.5461, 0.5710, 0.5704, 0.5502],
       device='cuda:0')
Storing NORMAL transition: reward=0.0474 (scaled=0.0474), steps=1
Reward stats updated: mean 0.0098 -> 0.0101, std: 0.0809
Collected 177 transitions for RL
SAC Update 1/5: Actor Loss=-0.2302, Q1 Loss=0.9526, Q2 Loss=0.9526, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8723
SAC Update 2/5: Actor Loss=-0.0008, Q1 Loss=1.1863, Q2 Loss=1.1863, Entropy=0.1398, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9008
SAC Update 3/5: Actor Loss=-0.0004, Q1 Loss=1.0704, Q2 Loss=1.0704, Entropy=0.0938, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5265
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.4640, Q2 Loss=1.4640, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9922
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9733, Q2 Loss=0.9733, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0181

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.7%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.05s (19.0%)
Actor update: 0.09s (38.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092337
Q1 loss: 1.129330
Q2 loss: 1.129330
Current threshold: -32.9561
Global Scale Offset: 0.0953
Reward stats: mean=0.0101, std=0.0809, count=177
----------------------------------------------
SAC Update - Actor Loss: -0.0923, Q1 Loss: 1.1293, Q2 Loss: 1.1293, Entropy: 0.0467, Mean TD Error: 0.8620, Threshold: -32.9561
tensor([ 0.0640,  0.3254,  0.7803,  0.7698, -0.0798,  0.6382,  0.7630,  0.8396,
         1.3870,  0.1477,  0.2685,  0.8669,  0.0097, -0.0022, -0.4205,  0.1301],
       device='cuda:0')
Original likelihood: -16.82770538330078
Adjusted likelihood: -16.82770538330078
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.368460878962651
Current ori: tensor([ 0.0097, -0.0022, -0.4205], device='cuda:0')
Middle force: tensor([0.5024, 0.9996, 0.5370, 0.5903, 0.5333, 0.6185], device='cuda:0')
Thumb force: tensor([1.0083, 0.7427, 0.7041, 0.5787, 0.5418, 0.5557], device='cuda:0')
Index force: tensor([0.6045, 0.5018, 0.5422, 0.5655, 0.5665, 0.5464], device='cuda:0')
Storing NORMAL transition: reward=-0.1131 (scaled=-0.1131), steps=1
Reward stats updated: mean 0.0101 -> 0.0094, std: 0.0812
Collected 178 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.6873, Q2 Loss=0.6873, Entropy=0.0035, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3158
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1292, Q2 Loss=1.1292, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1391
SAC Update 3/5: Actor Loss=-0.1056, Q1 Loss=0.9969, Q2 Loss=0.9969, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9181
SAC Update 4/5: Actor Loss=-0.0148, Q1 Loss=0.6337, Q2 Loss=0.6337, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9673
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4394, Q2 Loss=1.4394, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3309

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.8%)
Q1 update: 0.05s (21.4%)
Q2 update: 0.05s (18.5%)
Actor update: 0.09s (37.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.024089
Q1 loss: 0.977298
Q2 loss: 0.977298
Current threshold: -32.9562
Global Scale Offset: 0.0953
Reward stats: mean=0.0094, std=0.0812, count=178
----------------------------------------------
SAC Update - Actor Loss: -0.0241, Q1 Loss: 0.9773, Q2 Loss: 0.9773, Entropy: 0.0007, Mean TD Error: 1.1342, Threshold: -32.9562
tensor([-0.1136,  0.3378,  0.6996,  0.6351, -0.1959,  0.5834,  0.7939,  0.8544,
         1.3708,  0.3267,  0.3207,  0.8078,  0.0324,  0.0666, -0.3178,  0.1994],
       device='cuda:0')
Original likelihood: -24.904502868652344
Adjusted likelihood: -24.904502868652344
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.118845674034674
Current ori: tensor([ 0.0324,  0.0666, -0.3178], device='cuda:0')
Middle force: tensor([0.5049, 0.6255, 0.7371, 0.5023, 0.6210], device='cuda:0')
Thumb force: tensor([0.5818, 0.5525, 0.6254, 0.5270, 0.5016], device='cuda:0')
Index force: tensor([0.5951, 0.5000, 0.5009, 0.5086, 0.6479], device='cuda:0')
Storing NORMAL transition: reward=0.0915 (scaled=0.0915), steps=1
Reward stats updated: mean 0.0094 -> 0.0098, std: 0.0812
Collected 179 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.8900, Q2 Loss=1.8900, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6383
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.6448, Q2 Loss=1.6448, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4788
SAC Update 3/5: Actor Loss=-0.2297, Q1 Loss=3.0701, Q2 Loss=3.0701, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4129
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.0933, Q2 Loss=2.0933, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9860
SAC Update 5/5: Actor Loss=-0.0233, Q1 Loss=0.6555, Q2 Loss=0.6555, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7712

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (40.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.142702
Q1 loss: 1.870753
Q2 loss: 1.870753
Current threshold: -32.9563
Global Scale Offset: 0.0953
Reward stats: mean=0.0098, std=0.0812, count=179
----------------------------------------------
SAC Update - Actor Loss: -0.1427, Q1 Loss: 1.8708, Q2 Loss: 1.8708, Entropy: 0.0000, Mean TD Error: 1.6575, Threshold: -32.9563
tensor([-0.0184,  0.3780,  0.6527,  0.7588, -0.1614,  0.5862,  0.8069,  0.8845,
         1.4261,  0.3246,  0.3128,  0.7111,  0.0261,  0.0386, -0.4051,  0.7019],
       device='cuda:0')
Original likelihood: -31.11648941040039
Adjusted likelihood: -31.11648941040039
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 3.9304960619774647
Current ori: tensor([ 0.0261,  0.0386, -0.4051], device='cuda:0')
Middle force: tensor([0.5034, 0.5102, 0.5118, 0.5483], device='cuda:0')
Thumb force: tensor([0.5358, 0.5413, 0.5676, 0.5255], device='cuda:0')
Index force: tensor([0.5572, 0.6982, 0.5527, 0.5807], device='cuda:0')
Storing NORMAL transition: reward=-0.0000 (scaled=-0.0000), steps=1
Reward stats updated: mean 0.0098 -> 0.0098, std: 0.0809
Collected 180 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.7952, Q2 Loss=1.7952, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4891
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1402, Q2 Loss=1.1402, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5860
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.8597, Q2 Loss=0.8597, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9682
SAC Update 4/5: Actor Loss=-0.1020, Q1 Loss=1.0280, Q2 Loss=1.0280, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0235
SAC Update 5/5: Actor Loss=-0.2060, Q1 Loss=1.7458, Q2 Loss=1.7458, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4735

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.2%)
Q2 update: 0.04s (18.0%)
Actor update: 0.09s (40.7%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.061594
Q1 loss: 1.313790
Q2 loss: 1.313790
Current threshold: -32.9563
Global Scale Offset: 0.0953
Reward stats: mean=0.0098, std=0.0809, count=180
----------------------------------------------
SAC Update - Actor Loss: -0.0616, Q1 Loss: 1.3138, Q2 Loss: 1.3138, Entropy: 0.0000, Mean TD Error: 1.1080, Threshold: -32.9563
tensor([-0.0055,  0.4482,  0.5978,  0.6782, -0.0881,  0.5726,  0.9400,  0.9241,
         1.4136,  0.2234,  0.2577,  0.5840,  0.0600, -0.0325, -0.4129,  1.3481],
       device='cuda:0')
Original likelihood: -29.875497817993164
Adjusted likelihood: -29.875497817993164
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 10 3.8164949680212885
Current ori: tensor([ 0.0600, -0.0325, -0.4129], device='cuda:0')
Middle force: tensor([0.5868, 0.5131, 0.5817], device='cuda:0')
Thumb force: tensor([0.5522, 0.5261, 0.5613], device='cuda:0')
Index force: tensor([0.5463, 0.5332, 0.5526], device='cuda:0')
Storing NORMAL transition: reward=-0.0150 (scaled=-0.0150), steps=1
Reward stats updated: mean 0.0098 -> 0.0096, std: 0.0807
Collected 181 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.1590, Q2 Loss=1.1590, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7847
SAC Update 2/5: Actor Loss=-0.0015, Q1 Loss=0.7789, Q2 Loss=0.7789, Entropy=0.2002, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7373
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4828, Q2 Loss=1.4828, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5848
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0469, Q2 Loss=1.0469, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5729
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3486, Q2 Loss=1.3486, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7424

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.1%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.04s (17.8%)
Actor update: 0.10s (40.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046348
Q1 loss: 1.163240
Q2 loss: 1.163240
Current threshold: -32.9574
Global Scale Offset: 0.0953
Reward stats: mean=0.0096, std=0.0807, count=181
----------------------------------------------
SAC Update - Actor Loss: -0.0463, Q1 Loss: 1.1632, Q2 Loss: 1.1632, Entropy: 0.0400, Mean TD Error: 1.2844, Threshold: -32.9574
tensor([ 0.0993,  0.5336,  0.6652,  0.5798,  0.0501,  0.6327,  0.8941,  0.9139,
         1.3549,  0.2244,  0.3537,  0.7088,  0.1069, -0.0870, -0.4453,  1.9408],
       device='cuda:0')
Original likelihood: -27.876888275146484
Adjusted likelihood: -27.876888275146484
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 11 3.6745628039934672
Current ori: tensor([ 0.1069, -0.0870, -0.4453], device='cuda:0')
Middle force: tensor([0.5326, 0.6190], device='cuda:0')
Thumb force: tensor([0.5319, 0.5443], device='cuda:0')
Index force: tensor([0.5454, 0.5276], device='cuda:0')
Storing NORMAL transition: reward=-0.1052 (scaled=-0.1052), steps=1
Reward stats updated: mean 0.0096 -> 0.0090, std: 0.0810
Collected 182 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.9795, Q2 Loss=0.9795, Entropy=0.0001, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6921
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9519, Q2 Loss=0.9519, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5698
SAC Update 3/5: Actor Loss=-0.0001, Q1 Loss=1.3706, Q2 Loss=1.3706, Entropy=0.0179, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0313
SAC Update 4/5: Actor Loss=-0.2012, Q1 Loss=1.9751, Q2 Loss=1.9751, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7250
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8960, Q2 Loss=0.8960, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7056

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (16.7%)
Q1 update: 0.05s (18.7%)
Q2 update: 0.05s (19.7%)
Actor update: 0.11s (41.7%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.040247
Q1 loss: 1.234608
Q2 loss: 1.234608
Current threshold: -32.9581
Global Scale Offset: 0.0953
Reward stats: mean=0.0090, std=0.0810, count=182
----------------------------------------------
SAC Update - Actor Loss: -0.0402, Q1 Loss: 1.2346, Q2 Loss: 1.2346, Entropy: 0.0036, Mean TD Error: 0.9448, Threshold: -32.9581
tensor([ 0.1209,  0.6316,  0.7034,  0.5204,  0.0525,  0.6941,  0.7898,  1.0460,
         1.3847,  0.2605,  0.3924,  0.7412,  0.2234, -0.1355, -0.4428,  3.0766],
       device='cuda:0')
Original likelihood: -46.26753234863281
Adjusted likelihood: -46.26753234863281
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 45.0173454284668
Projection step: 1, Loss: 44.43952560424805
Projection step: 2, Loss: 45.51940155029297
Projection step: 3, Loss: 45.278343200683594
Projection step: 4, Loss: 43.45428466796875
Projection step: 5, Loss: 43.353416442871094
Projection step: 6, Loss: 43.52685546875
Projection step: 7, Loss: 44.435157775878906
Projection step: 8, Loss: 43.23965072631836
Projection step: 9, Loss: 41.69013214111328
Projection step: 10, Loss: 41.77252960205078
Projection step: 11, Loss: 44.17810821533203
Projection step: 12, Loss: 43.823219299316406
Projection step: 13, Loss: 42.75850296020508
Projection step: 14, Loss: 43.850318908691406
Projection step: 15, Loss: 42.681243896484375
Projection step: 16, Loss: 42.71405029296875
Projection step: 17, Loss: 44.07347869873047
Projection step: 18, Loss: 42.06391906738281
Projection step: 19, Loss: 41.28526306152344
Projection step: 20, Loss: 41.498695373535156
Projection step: 21, Loss: 39.948516845703125
Projection step: 22, Loss: 41.58887481689453
Projection step: 23, Loss: 39.74092102050781
Projection step: 24, Loss: 41.29302215576172
Final likelihood: tensor([-45.2095, -42.3002, -38.5532, -42.1920, -38.5340, -38.1748, -37.1352,
        -42.8707, -41.1355, -42.1548, -42.7661, -51.1108, -41.9550, -39.4830,
        -41.1087, -40.7935])
Final projection likelihood: -41.5923
1 mode projection failed, trying anyway
New goal: tensor([ 0.1143,  0.5770,  0.7200,  0.4619,  0.0517,  0.6737,  0.7725,  1.0388,
         1.4057,  0.1865,  0.4016,  0.7617,  0.2208, -0.1306, -0.0976],
       device='cuda:0')
tensor([[0.0040]], device='cuda:0') tensor([[0.0168]], device='cuda:0') tensor([[0.0050]], device='cuda:0')
Original likelihood: -60.302406311035156
Adjusted likelihood: -60.302406311035156
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 60.302406311035156}
Current yaw: tensor([ 0.2234, -0.1355, -0.4428], device='cuda:0')
2 thumb_middle
tensor([ 0.1209,  0.6316,  0.7034,  0.5204,  0.0525,  0.6941,  0.7898,  1.0460,
         1.3847,  0.2605,  0.3924,  0.7412,  0.2234, -0.1355, -0.4428,  3.0766],
       device='cuda:0')
Solve time for step 1 8.812092026986647
Current ori: tensor([ 0.2234, -0.1355, -0.4428], device='cuda:0')
Index force: tensor([0.5866, 0.5635, 0.6011, 0.5943], device='cuda:0')
tensor([ 0.0897,  0.7380,  0.7384,  0.4710,  0.0696,  0.7467,  0.7893,  1.0125,
         1.3563,  0.1817,  0.2973,  0.7096,  0.3554, -0.2642, -0.4195,  4.4031],
       device='cuda:0')
Solve time for step 2 3.6873902469524182
Current ori: tensor([ 0.3554, -0.2642, -0.4195], device='cuda:0')
Index force: tensor([0.5559, 0.5928, 0.5825], device='cuda:0')
tensor([ 0.0722,  0.7749,  0.7115,  0.4589,  0.1152,  0.7738,  0.8491,  1.0676,
         1.2445,  0.2606,  0.3280,  0.7650,  0.3673, -0.3155, -0.2567,  5.4193],
       device='cuda:0')
Solve time for step 3 3.499142392014619
Current ori: tensor([ 0.3673, -0.3155, -0.2567], device='cuda:0')
Index force: tensor([0.5610, 0.5799], device='cuda:0')
tensor([-0.0083,  0.7456,  0.7088,  0.4564,  0.1358,  0.7979,  0.8654,  1.0739,
         1.1953,  0.2343,  0.3627,  0.8071,  0.3719, -0.3287, -0.2186,  4.6484],
       device='cuda:0')
Solve time for step 4 3.074033491022419
Current ori: tensor([ 0.3719, -0.3287, -0.2186], device='cuda:0')
Index force: tensor([0.5722], device='cuda:0')
Storing RECOVERY transition: reward=-0.2587 (scaled=-0.0235), steps=11
Reward stats updated: mean 0.0090 -> 0.0088, std: 0.0808
Collected 183 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8628, Q2 Loss=0.8628, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8770
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.6129, Q2 Loss=0.6129, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4177
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.7695, Q2 Loss=1.7695, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4068
SAC Update 4/5: Actor Loss=-0.0759, Q1 Loss=0.9516, Q2 Loss=0.9516, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0729
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4979, Q2 Loss=1.4979, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6923

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.9%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (19.7%)
Actor update: 0.11s (41.9%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.061225
Q1 loss: 1.138919
Q2 loss: 1.138919
Current threshold: -32.9585
Global Scale Offset: 0.0953
Reward stats: mean=0.0088, std=0.0808, count=183
----------------------------------------------
SAC Update - Actor Loss: -0.0612, Q1 Loss: 1.1389, Q2 Loss: 1.1389, Entropy: 0.0000, Mean TD Error: 1.0933, Threshold: -32.9585
Original likelihood: -226.34661865234375
Adjusted likelihood: -226.34661865234375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 243.9827117919922
Projection step: 1, Loss: 224.09033203125
Projection step: 2, Loss: 242.27407836914062
Projection step: 3, Loss: 242.06982421875
Projection step: 4, Loss: 236.2388458251953
Projection step: 5, Loss: 229.93006896972656
Projection step: 6, Loss: 258.0798645019531
Projection step: 7, Loss: 236.92666625976562
Projection step: 8, Loss: 261.36773681640625
Projection step: 9, Loss: 255.71180725097656
Projection step: 10, Loss: 229.81082153320312
Projection step: 11, Loss: 249.69662475585938
Projection step: 12, Loss: 221.7464599609375
Projection step: 13, Loss: 236.96755981445312
Projection step: 14, Loss: 236.48129272460938
Projection step: 15, Loss: 228.26657104492188
Projection step: 16, Loss: 239.77418518066406
Projection step: 17, Loss: 237.41104125976562
Projection step: 18, Loss: 236.27658081054688
Projection step: 19, Loss: 239.30111694335938
Projection step: 20, Loss: 214.31619262695312
Projection step: 21, Loss: 232.84376525878906
Projection step: 22, Loss: 246.16943359375
Projection step: 23, Loss: 241.48504638671875
Projection step: 24, Loss: 240.95411682128906
Final likelihood: tensor([-262.8913, -229.2647, -255.9037, -262.2058, -300.6650, -225.1237,
        -248.9034, -221.1451, -222.6638, -235.4475, -245.4485, -226.5802,
        -207.7748, -198.0217, -252.3212, -262.8056])
Final projection likelihood: -241.0729
1 mode projection failed, trying anyway
New goal: tensor([-0.0028,  0.6462,  0.7556,  0.4706,  0.3040,  0.9290,  0.9447,  1.0886,
         1.2068,  0.1206,  0.4166,  0.7933,  0.3638, -0.3054, -0.2758],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0130]], device='cuda:0') tensor([[0.0048]], device='cuda:0')
Original likelihood: -231.55874633789062
Adjusted likelihood: -231.55874633789062
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 231.55874633789062}
Current yaw: tensor([ 0.3636, -0.3047, -0.2930], device='cuda:0')
3 thumb_middle
tensor([-0.0127,  0.6467,  0.7497,  0.4647,  0.3068,  0.9306,  0.9495,  1.0765,
         1.2160,  0.1158,  0.4173,  0.7843,  0.3636, -0.3047, -0.2930,  4.1219],
       device='cuda:0')
Solve time for step 1 8.736158827960026
Current ori: tensor([ 0.3636, -0.3047, -0.2930], device='cuda:0')
Index force: tensor([0.5978, 0.5928, 0.5889, 0.5913], device='cuda:0')
tensor([-0.0432,  0.6677,  0.7552,  0.4712,  0.2771,  0.9469,  0.9426,  1.0766,
         1.1304,  0.1499,  0.3292,  0.7917,  0.3821, -0.3528, -0.1219,  4.5317],
       device='cuda:0')
Solve time for step 2 3.788543700007722
Current ori: tensor([ 0.3821, -0.3528, -0.1219], device='cuda:0')
Index force: tensor([0.5861, 0.5794, 0.5942], device='cuda:0')
tensor([-0.0592,  0.6838,  0.7523,  0.4753,  0.2605,  0.9556,  0.9499,  1.0804,
         1.1294,  0.1461,  0.3221,  0.7894,  0.3824, -0.3538, -0.1200,  4.6954],
       device='cuda:0')
Solve time for step 3 3.4351288359612226
Current ori: tensor([ 0.3824, -0.3538, -0.1200], device='cuda:0')
Index force: tensor([0.5685, 0.5834], device='cuda:0')
tensor([-0.1329,  0.6900,  0.7687,  0.4717,  0.2753,  0.9702,  0.9510,  1.0838,
         1.1304,  0.1430,  0.3262,  0.7907,  0.3821, -0.3529, -0.1222,  4.2923],
       device='cuda:0')
Solve time for step 4 3.373685623984784
Current ori: tensor([ 0.3821, -0.3529, -0.1222], device='cuda:0')
Index force: tensor([0.5519], device='cuda:0')
Storing RECOVERY transition: reward=-0.3340 (scaled=-0.0304), steps=11
Reward stats updated: mean 0.0088 -> 0.0086, std: 0.0806
Collected 184 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9720, Q2 Loss=0.9720, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1097
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6513, Q2 Loss=1.6513, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3284
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.8131, Q2 Loss=1.8131, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5824
SAC Update 4/5: Actor Loss=-0.0223, Q1 Loss=0.6527, Q2 Loss=0.6527, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7778
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.9780, Q2 Loss=1.9780, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6797

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.4%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (38.7%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.004468
Q1 loss: 1.413425
Q2 loss: 1.413425
Current threshold: -32.9588
Global Scale Offset: 0.0953
Reward stats: mean=0.0086, std=0.0806, count=184
----------------------------------------------
SAC Update - Actor Loss: -0.0045, Q1 Loss: 1.4134, Q2 Loss: 1.4134, Entropy: 0.0000, Mean TD Error: 1.2956, Threshold: -32.9588
Original likelihood: -251.92434692382812
Adjusted likelihood: -251.92434692382812
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 289.6552429199219
Projection step: 1, Loss: 276.74676513671875
Projection step: 2, Loss: 296.94366455078125
Projection step: 3, Loss: 274.4561767578125
Projection step: 4, Loss: 291.55633544921875
Projection step: 5, Loss: 274.6997985839844
Projection step: 6, Loss: 284.58270263671875
Projection step: 7, Loss: 279.74261474609375
Projection step: 8, Loss: 275.59722900390625
Projection step: 9, Loss: 291.63482666015625
Projection step: 10, Loss: 291.80987548828125
Projection step: 11, Loss: 295.6408386230469
Projection step: 12, Loss: 272.7925720214844
Projection step: 13, Loss: 278.58770751953125
Projection step: 14, Loss: 282.507568359375
Projection step: 15, Loss: 265.9367980957031
Projection step: 16, Loss: 290.3651123046875
Projection step: 17, Loss: 271.17864990234375
Projection step: 18, Loss: 272.6670227050781
Projection step: 19, Loss: 282.10809326171875
Projection step: 20, Loss: 284.166015625
Projection step: 21, Loss: 290.3383483886719
Projection step: 22, Loss: 258.44903564453125
Projection step: 23, Loss: 278.1066589355469
Projection step: 24, Loss: 283.0667419433594
Final likelihood: tensor([-267.4110, -222.3217, -305.5192, -264.1697, -319.4422, -296.9417,
        -279.8349, -302.0417, -276.8834, -200.7563, -293.9813, -216.7590,
        -230.5014, -291.2803, -290.0324, -221.1278])
Final projection likelihood: -267.4377
1 mode projection failed, trying anyway
New goal: tensor([-0.2054,  0.6836,  0.7718,  0.4805,  0.3267,  1.0336,  1.0150,  1.1220,
         1.1652,  0.0787,  0.3855,  0.7953,  0.3723, -0.3277, -0.1678],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0020]], device='cuda:0') tensor([[0.0047]], device='cuda:0')
Original likelihood: -241.21974182128906
Adjusted likelihood: -241.21974182128906
Likelihood residual: 0.0
Original likelihood: -263.0064697265625
Adjusted likelihood: -263.0064697265625
Likelihood residual: 0.0
{'index': 263.0064697265625, 'thumb_middle': 241.21974182128906}
Current yaw: tensor([ 0.3723, -0.3271, -0.1894], device='cuda:0')
4 thumb_middle
tensor([-0.2114,  0.6845,  0.7673,  0.4781,  0.3265,  1.0319,  1.0198,  1.1144,
         1.1724,  0.0766,  0.3868,  0.7916,  0.3723, -0.3271, -0.1894,  4.4087],
       device='cuda:0')
Solve time for step 1 8.956312987953424
Current ori: tensor([ 0.3723, -0.3271, -0.1894], device='cuda:0')
Index force: tensor([0.5852, 0.5959, 0.6017, 0.6038], device='cuda:0')
tensor([-0.2530,  0.6827,  0.7359,  0.4620,  0.2912,  0.9971,  0.9508,  1.0732,
         1.1120,  0.1052,  0.2872,  0.7958,  0.3846, -0.3607, -0.1082,  4.5641],
       device='cuda:0')
Solve time for step 2 3.873266772949137
Current ori: tensor([ 0.3846, -0.3607, -0.1082], device='cuda:0')
Index force: tensor([0.5955, 0.5979, 0.6002], device='cuda:0')
tensor([-0.2713,  0.6951,  0.7166,  0.4527,  0.2711,  0.9928,  0.9565,  1.0741,
         1.1107,  0.1020,  0.2749,  0.7949,  0.3853, -0.3622, -0.1077,  4.4171],
       device='cuda:0')
Solve time for step 3 3.494287833047565
Current ori: tensor([ 0.3853, -0.3622, -0.1077], device='cuda:0')
Index force: tensor([0.5925, 0.5906], device='cuda:0')
tensor([-0.2847,  0.7002,  0.7053,  0.4452,  0.2575,  0.9874,  0.9610,  1.0753,
         1.1119,  0.0976,  0.2745,  0.7939,  0.3851, -0.3616, -0.1099,  4.3291],
       device='cuda:0')
Solve time for step 4 3.332734123978298
Current ori: tensor([ 0.3851, -0.3616, -0.1099], device='cuda:0')
Index force: tensor([0.5615], device='cuda:0')
Storing RECOVERY transition: reward=-0.3456 (scaled=-0.0314), steps=11
Reward stats updated: mean 0.0086 -> 0.0084, std: 0.0805
Collected 185 transitions for RL
SAC Update 1/5: Actor Loss=-0.2302, Q1 Loss=0.5928, Q2 Loss=0.5928, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2654
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.4557, Q2 Loss=1.4557, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0859
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.8776, Q2 Loss=1.8776, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6138
SAC Update 4/5: Actor Loss=-0.0864, Q1 Loss=1.0375, Q2 Loss=1.0375, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1998
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0322, Q2 Loss=1.0322, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3327

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (13.7%)
Q1 update: 0.06s (19.8%)
Q2 update: 0.06s (20.4%)
Actor update: 0.12s (43.1%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.109359
Q1 loss: 1.199159
Q2 loss: 1.199159
Current threshold: -32.9593
Global Scale Offset: 0.0953
Reward stats: mean=0.0084, std=0.0805, count=185
----------------------------------------------
SAC Update - Actor Loss: -0.1094, Q1 Loss: 1.1992, Q2 Loss: 1.1992, Entropy: 0.0000, Mean TD Error: 0.8995, Threshold: -32.9593
Original likelihood: -313.5790710449219
Adjusted likelihood: -313.5790710449219
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 12
Loaded trajectory sampler
Current yaw: tensor([-0.0009,  0.0145, -0.0434], device='cuda:0')
Current yaw: tensor([-0.0009,  0.0145, -0.0434], device='cuda:0')
1 turn
Sampling time 3.576193736982532
tensor([ 1.3919e-01,  5.6932e-01,  6.3533e-01,  5.6263e-01, -8.1650e-02,
         5.2439e-01,  8.8470e-01,  8.4461e-01,  1.2242e+00,  2.8964e-01,
         2.3401e-01,  1.2344e+00, -9.0228e-04,  1.4547e-02, -4.3402e-02,
         1.9825e-01], device='cuda:0')
Original likelihood: -16.783323287963867
Adjusted likelihood: -16.783323287963867
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.98746640200261
Current ori: tensor([-0.0009,  0.0145, -0.0434], device='cuda:0')
Middle force: tensor([0.5685, 0.5174, 1.5708, 0.6071, 0.5909, 0.5624, 0.5343, 0.5194, 0.5216,
        1.9333, 0.7244, 0.6673], device='cuda:0')
Thumb force: tensor([0.6956, 0.9028, 1.8911, 0.7566, 1.0393, 0.5828, 0.6167, 0.8520, 2.0513,
        0.5674, 0.5910, 0.5689], device='cuda:0')
Index force: tensor([0.5612, 0.9232, 0.5703, 0.5515, 0.6068, 0.5791, 0.6494, 0.9743, 0.6525,
        0.7245, 0.5262, 0.5662], device='cuda:0')
Storing NORMAL transition: reward=0.1188 (scaled=0.1188), steps=1
Reward stats updated: mean 0.0084 -> 0.0090, std: 0.0806
Collected 186 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.9909, Q2 Loss=1.9909, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1365
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.1994, Q2 Loss=1.1994, Entropy=0.0177, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7955
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.8027, Q2 Loss=1.8027, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5986
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.2470, Q2 Loss=1.2470, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3724
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0238, Q2 Loss=1.0238, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8265

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (13.8%)
Q1 update: 0.06s (20.5%)
Q2 update: 0.06s (20.4%)
Actor update: 0.12s (42.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092113
Q1 loss: 1.452762
Q2 loss: 1.452762
Current threshold: -32.9595
Global Scale Offset: 0.0953
Reward stats: mean=0.0090, std=0.0806, count=186
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.4528, Q2 Loss: 1.4528, Entropy: 0.0035, Mean TD Error: 1.3459, Threshold: -32.9595
tensor([ 0.1781,  0.7309,  0.4900,  0.4491, -0.1341,  0.4434,  0.9277,  0.9247,
         1.3218,  0.1878,  0.3103,  1.0610,  0.0070,  0.0394, -0.1640, -0.7506],
       device='cuda:0')
Original likelihood: -29.476022720336914
Adjusted likelihood: -29.476022720336914
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.510965466033667
Current ori: tensor([ 0.0070,  0.0394, -0.1640], device='cuda:0')
Middle force: tensor([0.5082, 1.5726, 0.6030, 0.5936, 0.5565, 0.5193, 0.5040, 0.5145, 1.8888,
        0.7091, 0.6655], device='cuda:0')
Thumb force: tensor([0.9690, 1.8459, 0.7571, 1.0207, 0.5789, 0.6429, 0.9837, 2.0316, 0.5725,
        0.6016, 0.5646], device='cuda:0')
Index force: tensor([0.9034, 0.5733, 0.5449, 0.5987, 0.5788, 0.6513, 1.0468, 0.6589, 0.7058,
        0.5223, 0.5632], device='cuda:0')
Storing NORMAL transition: reward=0.0019 (scaled=0.0019), steps=1
Reward stats updated: mean 0.0090 -> 0.0090, std: 0.0804
Collected 187 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0602, Q2 Loss=1.0602, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2454
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3859, Q2 Loss=1.3859, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7408
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9158, Q2 Loss=0.9158, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7012
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=4.3388, Q2 Loss=4.3388, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8060
SAC Update 5/5: Actor Loss=-0.0743, Q1 Loss=0.7522, Q2 Loss=0.7522, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1805

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.6%)
Q1 update: 0.05s (20.3%)
Q2 update: 0.05s (20.2%)
Actor update: 0.11s (41.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.014855
Q1 loss: 1.690572
Q2 loss: 1.690572
Current threshold: -32.9596
Global Scale Offset: 0.0953
Reward stats: mean=0.0090, std=0.0804, count=187
----------------------------------------------
SAC Update - Actor Loss: -0.0149, Q1 Loss: 1.6906, Q2 Loss: 1.6906, Entropy: 0.0000, Mean TD Error: 1.5348, Threshold: -32.9596
tensor([ 0.1324,  0.7236,  0.4651,  0.4193, -0.1166,  0.4217,  0.9926,  0.8787,
         1.2729,  0.2580,  0.3681,  0.8405,  0.0104,  0.0269, -0.1650, -0.9284],
       device='cuda:0')
Original likelihood: -30.19002342224121
Adjusted likelihood: -30.19002342224121
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.064043888996821
Current ori: tensor([ 0.0104,  0.0269, -0.1650], device='cuda:0')
Middle force: tensor([0.6064, 0.5192, 0.5414, 0.5621, 0.5036, 0.5011, 0.5666, 0.5746, 0.5077,
        0.5265], device='cuda:0')
Thumb force: tensor([1.1831, 0.5510, 1.4613, 0.5810, 0.6895, 0.5097, 0.6193, 0.5735, 0.5853,
        0.6494], device='cuda:0')
Index force: tensor([1.0914, 0.5522, 0.5432, 0.5982, 0.5809, 0.8685, 0.5028, 0.5018, 0.6363,
        0.5432], device='cuda:0')
Storing NORMAL transition: reward=-0.0353 (scaled=-0.0353), steps=1
Reward stats updated: mean 0.0090 -> 0.0087, std: 0.0803
Collected 188 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9242, Q2 Loss=0.9242, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7661
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1628, Q2 Loss=1.1628, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2120
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.2561, Q2 Loss=1.2561, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6216
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0425, Q2 Loss=1.0425, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5494
SAC Update 5/5: Actor Loss=-0.0008, Q1 Loss=1.1540, Q2 Loss=1.1540, Entropy=0.1378, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0502

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.9%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (18.9%)
Q2 update: 0.05s (19.1%)
Actor update: 0.10s (38.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000164
Q1 loss: 1.107889
Q2 loss: 1.107889
Current threshold: -32.9596
Global Scale Offset: 0.0953
Reward stats: mean=0.0087, std=0.0803, count=188
----------------------------------------------
SAC Update - Actor Loss: -0.0002, Q1 Loss: 1.1079, Q2 Loss: 1.1079, Entropy: 0.0276, Mean TD Error: 0.8399, Threshold: -32.9596
tensor([ 5.4628e-02,  5.7599e-01,  4.6089e-01,  3.5948e-01, -3.0466e-02,
         4.7565e-01,  9.3690e-01,  8.4826e-01,  1.4588e+00, -5.4220e-02,
         1.8001e-01,  1.0249e+00,  6.3588e-04, -1.1867e-02, -1.2885e-01,
        -6.6688e-01], device='cuda:0')
Original likelihood: -25.519676208496094
Adjusted likelihood: -25.519676208496094
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.880087953992188
Current ori: tensor([ 0.0006, -0.0119, -0.1288], device='cuda:0')
Middle force: tensor([0.5012, 0.5057, 0.9523, 0.5033, 1.1398, 0.5728, 0.5366, 0.5422, 0.7370],
       device='cuda:0')
Thumb force: tensor([0.5638, 0.7296, 0.8724, 0.9834, 0.7531, 0.5039, 0.5439, 0.5334, 0.8565],
       device='cuda:0')
Index force: tensor([0.7320, 0.6542, 0.6344, 0.5757, 0.5074, 0.5213, 0.5980, 0.5751, 0.5004],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0125 (scaled=-0.0125), steps=1
Reward stats updated: mean 0.0087 -> 0.0086, std: 0.0801
Collected 189 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.8720, Q2 Loss=1.8720, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6276
SAC Update 2/5: Actor Loss=-0.2302, Q1 Loss=0.8978, Q2 Loss=0.8978, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2536
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9942, Q2 Loss=0.9942, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4483
SAC Update 4/5: Actor Loss=-0.2204, Q1 Loss=2.1852, Q2 Loss=2.1852, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8352
SAC Update 5/5: Actor Loss=-0.1523, Q1 Loss=1.9087, Q2 Loss=1.9087, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9235

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.6%)
Actor update: 0.08s (40.0%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.120586
Q1 loss: 1.571583
Q2 loss: 1.571583
Current threshold: -32.9590
Global Scale Offset: 0.0953
Reward stats: mean=0.0086, std=0.0801, count=189
----------------------------------------------
SAC Update - Actor Loss: -0.1206, Q1 Loss: 1.5716, Q2 Loss: 1.5716, Entropy: 0.0000, Mean TD Error: 1.4176, Threshold: -32.9590
tensor([-0.0062,  0.5258,  0.5734,  0.5331, -0.1491,  0.5068,  0.8506,  0.9470,
         1.4429,  0.0603,  0.2545,  0.9758, -0.0071,  0.0428, -0.1183, -0.6370],
       device='cuda:0')
Original likelihood: -20.849769592285156
Adjusted likelihood: -20.849769592285156
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.646342898020521
Current ori: tensor([-0.0071,  0.0428, -0.1183], device='cuda:0')
Middle force: tensor([0.5028, 0.9150, 0.5021, 0.5533, 0.5097, 0.5011, 0.5357, 0.6088],
       device='cuda:0')
Thumb force: tensor([0.7403, 0.8521, 0.9836, 0.5465, 0.6467, 0.5336, 0.5825, 0.5423],
       device='cuda:0')
Index force: tensor([0.6567, 0.6027, 0.6282, 0.5769, 0.6159, 0.5282, 0.5803, 0.5450],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0201 (scaled=-0.0201), steps=1
Reward stats updated: mean 0.0086 -> 0.0085, std: 0.0799
Collected 190 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.2236, Q2 Loss=1.2236, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8196
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=3.2597, Q2 Loss=3.2597, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5978
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4713, Q2 Loss=1.4713, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2670
SAC Update 4/5: Actor Loss=-0.1542, Q1 Loss=0.6386, Q2 Loss=0.6386, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2511
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0886, Q2 Loss=1.0886, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7752

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.9%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.04s (19.4%)
Actor update: 0.10s (42.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.076883
Q1 loss: 1.536366
Q2 loss: 1.536366
Current threshold: -32.9703
Global Scale Offset: 0.0946
Reward stats: mean=0.0085, std=0.0799, count=190
----------------------------------------------
SAC Update - Actor Loss: -0.0769, Q1 Loss: 1.5364, Q2 Loss: 1.5364, Entropy: 0.0000, Mean TD Error: 1.1421, Threshold: -32.9703
tensor([-0.0047,  0.5713,  0.5242,  0.5023, -0.1408,  0.4640,  0.9282,  0.8644,
         1.3856,  0.1626,  0.1869,  1.1538, -0.0200,  0.0427, -0.0985, -0.6817],
       device='cuda:0')
Original likelihood: -16.12162971496582
Adjusted likelihood: -16.12162971496582
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.553821614012122
Current ori: tensor([-0.0200,  0.0427, -0.0985], device='cuda:0')
Middle force: tensor([0.9208, 0.5006, 1.1220, 0.5561, 0.5219, 0.5389, 0.6978],
       device='cuda:0')
Thumb force: tensor([0.8405, 0.9650, 0.7192, 0.5022, 0.5431, 0.5277, 0.8492],
       device='cuda:0')
Index force: tensor([0.6195, 0.6451, 0.5051, 0.5201, 0.6022, 0.5646, 0.5001],
       device='cuda:0')
Storing NORMAL transition: reward=0.1252 (scaled=0.1252), steps=1
Reward stats updated: mean 0.0085 -> 0.0091, std: 0.0801
Collected 191 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.9149, Q2 Loss=0.9149, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6727
SAC Update 2/5: Actor Loss=-0.0015, Q1 Loss=1.2914, Q2 Loss=1.2914, Entropy=0.2096, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7676
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.5902, Q2 Loss=1.5902, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9444
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0809, Q2 Loss=1.0809, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0000
SAC Update 5/5: Actor Loss=-0.0017, Q1 Loss=0.6067, Q2 Loss=0.6067, Entropy=0.2161, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8884

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.6%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (19.0%)
Actor update: 0.11s (39.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092745
Q1 loss: 1.096788
Q2 loss: 1.096788
Current threshold: -32.9918
Global Scale Offset: 0.0934
Reward stats: mean=0.0091, std=0.0801, count=191
----------------------------------------------
SAC Update - Actor Loss: -0.0927, Q1 Loss: 1.0968, Q2 Loss: 1.0968, Entropy: 0.0851, Mean TD Error: 1.0546, Threshold: -32.9918
tensor([-0.0559,  0.4851,  0.5957,  0.5177, -0.0827,  0.4107,  0.8353,  0.9434,
         1.3978,  0.3261,  0.2377,  0.8834, -0.0138,  0.0518, -0.2254, -0.6164],
       device='cuda:0')
Original likelihood: -25.502138137817383
Adjusted likelihood: -25.502138137817383
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.499541504017543
Current ori: tensor([-0.0138,  0.0518, -0.2254], device='cuda:0')
Middle force: tensor([0.5014, 0.5460, 0.5105, 0.5005, 0.5353, 0.5910], device='cuda:0')
Thumb force: tensor([0.9370, 0.5345, 0.6125, 0.5265, 0.5688, 0.5376], device='cuda:0')
Index force: tensor([0.6171, 0.5752, 0.6034, 0.5259, 0.5712, 0.5394], device='cuda:0')
Storing NORMAL transition: reward=-0.0445 (scaled=-0.0445), steps=1
Reward stats updated: mean 0.0091 -> 0.0088, std: 0.0800
Collected 192 transitions for RL
SAC Update 1/5: Actor Loss=-0.0007, Q1 Loss=1.2476, Q2 Loss=1.2476, Entropy=0.1177, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5921
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.9405, Q2 Loss=1.9405, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6585
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.6276, Q2 Loss=1.6276, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3201
SAC Update 4/5: Actor Loss=-0.1973, Q1 Loss=0.7431, Q2 Loss=0.7431, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9161
SAC Update 5/5: Actor Loss=-0.0006, Q1 Loss=1.1838, Q2 Loss=1.1838, Entropy=0.1124, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7433

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (20.9%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (18.0%)
Actor update: 0.08s (38.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.131806
Q1 loss: 1.348518
Q2 loss: 1.348518
Current threshold: -33.0177
Global Scale Offset: 0.0920
Reward stats: mean=0.0088, std=0.0800, count=192
----------------------------------------------
SAC Update - Actor Loss: -0.1318, Q1 Loss: 1.3485, Q2 Loss: 1.3485, Entropy: 0.0460, Mean TD Error: 1.6460, Threshold: -33.0177
tensor([-0.0394,  0.5783,  0.5097,  0.4573, -0.0594,  0.5199,  0.7277,  0.8924,
         1.4058,  0.4150,  0.1079,  0.9880, -0.0460,  0.0384, -0.1814, -0.7579],
       device='cuda:0')
Original likelihood: -19.118026733398438
Adjusted likelihood: -19.118026733398438
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.041962283023167
Current ori: tensor([-0.0460,  0.0384, -0.1814], device='cuda:0')
Middle force: tensor([0.5430, 0.5097, 0.5003, 0.5338, 0.5848], device='cuda:0')
Thumb force: tensor([0.5303, 0.6040, 0.5237, 0.5650, 0.5357], device='cuda:0')
Index force: tensor([0.5768, 0.6051, 0.5251, 0.5691, 0.5369], device='cuda:0')
Storing NORMAL transition: reward=-0.1034 (scaled=-0.1034), steps=1
Reward stats updated: mean 0.0088 -> 0.0082, std: 0.0802
Collected 193 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9031, Q2 Loss=0.9031, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5544
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.6087, Q2 Loss=0.6087, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7498
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.3591, Q2 Loss=1.3591, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2409
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9647, Q2 Loss=0.9647, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6990
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.7068, Q2 Loss=1.7068, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9230

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (16.7%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (19.5%)
Actor update: 0.10s (41.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046051
Q1 loss: 1.108475
Q2 loss: 1.108475
Current threshold: -33.0477
Global Scale Offset: 0.0902
Reward stats: mean=0.0082, std=0.0802, count=193
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1085, Q2 Loss: 1.1085, Entropy: 0.0000, Mean TD Error: 1.0334, Threshold: -33.0477
tensor([-0.0185,  0.5061,  0.6056,  0.5028, -0.1079,  0.4105,  0.7866,  1.0284,
         1.3936,  0.4067,  0.0954,  1.1768,  0.0187,  0.0608, -0.0774, -1.1525],
       device='cuda:0')
Original likelihood: -33.10110092163086
Adjusted likelihood: -33.10110092163086
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.4360)
State is out of distribution
Projection step: 0, Loss: 35.668304443359375
Projection step: 1, Loss: 33.39678192138672
Projection step: 2, Loss: 34.23772048950195
Projection step: 3, Loss: 34.76509094238281
Projection step: 4, Loss: 32.97809600830078
Projection step: 5, Loss: 32.513893127441406
Projection step: 6, Loss: 34.572998046875
Projection step: 7, Loss: 34.675010681152344
Projection step: 8, Loss: 32.17322540283203
Projection step: 9, Loss: 30.914352416992188
Projection step: 10, Loss: 31.212892532348633
Projection step: 11, Loss: 30.279949188232422
Projection step: 12, Loss: 30.111312866210938
Projection step: 13, Loss: 29.281776428222656
Projection step: 14, Loss: 29.262863159179688
Projection step: 15, Loss: 28.726856231689453
Projection step: 16, Loss: 27.843908309936523
Projection step: 17, Loss: 26.750051498413086
Projection step: 18, Loss: 27.54941177368164
Projection step: 19, Loss: 25.89911460876465
Projection step: 20, Loss: 26.72647476196289
Projection step: 21, Loss: 25.060867309570312
Projection step: 22, Loss: 25.207565307617188
Projection step: 23, Loss: 25.140634536743164
Projection step: 24, Loss: 24.5738525390625
Final likelihood: tensor([-25.3291, -23.0618, -23.4439, -24.0971, -23.7891, -25.0868, -21.7793,
        -23.6268, -23.7464, -20.7270, -20.1817, -23.4667, -22.1640, -28.3346,
        -23.2536, -23.1800])
Final projection likelihood: -23.4543
1 mode projection succeeded
New goal: tensor([ 0.0024,  0.5235,  0.6178,  0.5944, -0.1256,  0.4473,  0.8069,  0.8948,
         1.3957,  0.4175,  0.1035,  1.1727,  0.0241,  0.0436, -0.9691],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -30.156951904296875
Adjusted likelihood: -30.156951904296875
Likelihood residual: 0.0
Original likelihood: -30.94306755065918
Adjusted likelihood: -30.94306755065918
Likelihood residual: 0.0
{'index': 30.94306755065918, 'thumb_middle': 30.156951904296875}
Current yaw: tensor([ 0.0187,  0.0608, -0.0774], device='cuda:0')
2 thumb_middle
tensor([-0.0185,  0.5061,  0.6056,  0.5028, -0.1079,  0.4105,  0.7866,  1.0284,
         1.3936,  0.4067,  0.0954,  1.1768,  0.0187,  0.0608, -0.0774, -1.1525],
       device='cuda:0')
Solve time for step 1 8.817738684010692
Current ori: tensor([ 0.0187,  0.0608, -0.0774], device='cuda:0')
Index force: tensor([0.5883, 0.5904, 0.5882, 0.6071], device='cuda:0')
tensor([ 9.0374e-06,  4.9920e-01,  5.9827e-01,  5.6758e-01, -1.9642e-01,
         4.2989e-01,  7.8216e-01,  9.0271e-01,  1.3516e+00,  3.9954e-01,
         2.5347e-02,  1.1362e+00,  2.4815e-02,  5.1517e-02, -7.7329e-02,
        -1.1103e+00], device='cuda:0')
Solve time for step 2 3.7220070979674347
Current ori: tensor([ 0.0248,  0.0515, -0.0773], device='cuda:0')
Index force: tensor([0.5853, 0.5849, 0.6023], device='cuda:0')
tensor([ 0.0024,  0.4960,  0.5991,  0.5786, -0.2031,  0.4425,  0.7865,  0.8860,
         1.3567,  0.4034,  0.0158,  1.1315,  0.0264,  0.0504, -0.0773, -1.1031],
       device='cuda:0')
Solve time for step 3 3.4959455900243483
Current ori: tensor([ 0.0264,  0.0504, -0.0773], device='cuda:0')
Index force: tensor([0.5747, 0.5946], device='cuda:0')
tensor([-0.0050,  0.4946,  0.5930,  0.5822, -0.2062,  0.4438,  0.7855,  0.8772,
         1.3588,  0.4059,  0.0172,  1.1330,  0.0273,  0.0542, -0.0773, -1.1119],
       device='cuda:0')
Solve time for step 4 3.4514215189847164
Current ori: tensor([ 0.0273,  0.0542, -0.0773], device='cuda:0')
Index force: tensor([0.5523], device='cuda:0')
Storing RECOVERY transition: reward=0.0034 (scaled=0.0004), steps=8
Reward stats updated: mean 0.0082 -> 0.0082, std: 0.0800
Collected 194 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.6201, Q2 Loss=1.6201, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7993
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1137, Q2 Loss=1.1137, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5977
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.0619, Q2 Loss=1.0619, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4982
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9070, Q2 Loss=0.9070, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9613
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0138, Q2 Loss=1.0138, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1878

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.0%)
Q1 update: 0.05s (20.4%)
Q2 update: 0.05s (18.7%)
Actor update: 0.10s (39.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000000
Q1 loss: 1.143300
Q2 loss: 1.143300
Current threshold: -33.0656
Global Scale Offset: 0.0892
Reward stats: mean=0.0082, std=0.0800, count=194
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.1433, Q2 Loss: 1.1433, Entropy: 0.0000, Mean TD Error: 1.0089, Threshold: -33.0656
Original likelihood: -28.830049514770508
Adjusted likelihood: -28.830049514770508
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0342,  0.0532, -0.0808], device='cuda:0')
3 turn
Sampling time 3.6866824770113453
tensor([-0.0036,  0.4746,  0.6097,  0.6062, -0.1383,  0.4871,  0.8283,  0.9008,
         1.4218,  0.4214,  0.0672,  1.1616,  0.0342,  0.0532, -0.0808, -1.0909],
       device='cuda:0')
Original likelihood: -27.37804412841797
Adjusted likelihood: -27.37804412841797
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.327810131013393
Current ori: tensor([ 0.0342,  0.0532, -0.0808], device='cuda:0')
Middle force: tensor([1.4811, 0.5076, 0.5142, 0.5260, 0.5799, 0.6403, 1.0288, 0.7930, 0.7756,
        0.6207, 0.5216, 0.5689], device='cuda:0')
Thumb force: tensor([2.1943, 1.8044, 1.3628, 0.5663, 1.0689, 0.7627, 1.4838, 0.5804, 0.7034,
        0.6220, 0.5430, 0.5525], device='cuda:0')
Index force: tensor([0.5633, 0.8701, 0.6922, 0.6496, 0.5702, 0.5402, 0.5892, 0.5304, 0.5572,
        0.5616, 0.5690, 0.6025], device='cuda:0')
Storing NORMAL transition: reward=0.1504 (scaled=0.1504), steps=1
Reward stats updated: mean 0.0082 -> 0.0089, std: 0.0804
Collected 195 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.9948, Q2 Loss=1.9948, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0624
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=2.0152, Q2 Loss=2.0152, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6447
SAC Update 3/5: Actor Loss=-0.0010, Q1 Loss=0.6898, Q2 Loss=0.6898, Entropy=0.3452, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2455
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.0211, Q2 Loss=1.0211, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9747
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0409, Q2 Loss=1.0409, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3144

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.9%)
Target Q: 0.05s (18.7%)
Q1 update: 0.05s (18.8%)
Q2 update: 0.05s (18.7%)
Actor update: 0.10s (39.8%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092312
Q1 loss: 1.352369
Q2 loss: 1.352369
Current threshold: -33.0766
Global Scale Offset: 0.0885
Reward stats: mean=0.0089, std=0.0804, count=195
----------------------------------------------
SAC Update - Actor Loss: -0.0923, Q1 Loss: 1.3524, Q2 Loss: 1.3524, Entropy: 0.0690, Mean TD Error: 1.4483, Threshold: -33.0766
tensor([ 0.1567,  0.4681,  0.7161,  0.7041, -0.1120,  0.3583,  0.8412,  1.1984,
         1.4154,  0.5081,  0.1231,  0.9632,  0.0237,  0.0118, -0.2284, -0.4716],
       device='cuda:0')
Original likelihood: -24.360614776611328
Adjusted likelihood: -24.360614776611328
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.616173023998272
Current ori: tensor([ 0.0237,  0.0118, -0.2284], device='cuda:0')
Middle force: tensor([0.5101, 0.5095, 0.5234, 0.5659, 0.6212, 0.9481, 0.7364, 0.5506, 0.5471,
        0.5494, 0.7828], device='cuda:0')
Thumb force: tensor([1.7239, 1.3400, 0.5630, 1.0443, 0.7546, 1.4376, 0.5973, 0.5954, 0.5787,
        0.6079, 0.6056], device='cuda:0')
Index force: tensor([0.8220, 0.6708, 0.6372, 0.5665, 0.5402, 0.5929, 0.5279, 0.6044, 0.6120,
        0.6069, 0.5587], device='cuda:0')
Storing NORMAL transition: reward=0.1092 (scaled=0.1092), steps=1
Reward stats updated: mean 0.0089 -> 0.0094, std: 0.0806
Collected 196 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7934, Q2 Loss=0.7934, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5856
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.7848, Q2 Loss=1.7848, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4685
SAC Update 3/5: Actor Loss=-0.0208, Q1 Loss=0.6514, Q2 Loss=0.6514, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7545
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7294, Q2 Loss=0.7294, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8241
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0964, Q2 Loss=1.0964, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4409

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.2%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (19.0%)
Actor update: 0.09s (38.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.004152
Q1 loss: 1.011088
Q2 loss: 1.011088
Current threshold: -33.0833
Global Scale Offset: 0.0882
Reward stats: mean=0.0094, std=0.0806, count=196
----------------------------------------------
SAC Update - Actor Loss: -0.0042, Q1 Loss: 1.0111, Q2 Loss: 1.0111, Entropy: 0.0000, Mean TD Error: 0.8147, Threshold: -33.0833
tensor([ 0.2245,  0.4946,  0.6687,  0.6733, -0.2418,  0.2523,  1.0224,  1.3226,
         1.5000,  0.4614,  0.1987,  0.8571,  0.0683,  0.0832, -0.3593,  0.1579],
       device='cuda:0')
Original likelihood: -43.21062469482422
Adjusted likelihood: -43.21062469482422
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 38.176795959472656
Projection step: 1, Loss: 36.47077560424805
Projection step: 2, Loss: 37.79002380371094
Projection step: 3, Loss: 33.995506286621094
Projection step: 4, Loss: 37.638187408447266
Projection step: 5, Loss: 35.95069885253906
Projection step: 6, Loss: 36.88607406616211
Projection step: 7, Loss: 31.148096084594727
Projection step: 8, Loss: 40.528846740722656
Projection step: 9, Loss: 35.82212829589844
Projection step: 10, Loss: 38.14796447753906
Projection step: 11, Loss: 35.3148193359375
Projection step: 12, Loss: 35.914939880371094
Projection step: 13, Loss: 33.22080612182617
Projection step: 14, Loss: 34.82832336425781
Projection step: 15, Loss: 41.01884841918945
Projection step: 16, Loss: 36.52570343017578
Projection step: 17, Loss: 32.33653259277344
Projection step: 18, Loss: 37.17559051513672
Projection step: 19, Loss: 35.20586395263672
Projection step: 20, Loss: 35.209529876708984
Projection step: 21, Loss: 35.011260986328125
Projection step: 22, Loss: 37.185428619384766
Projection step: 23, Loss: 36.018157958984375
Projection step: 24, Loss: 34.22559356689453
Final likelihood: tensor([-33.9110, -40.5157, -35.9294, -37.5767, -23.0600, -36.5495, -39.9926,
        -29.8803, -37.9265, -36.4744, -23.8908, -37.9569, -34.3458, -40.3885,
        -41.9387, -37.1406])
Final projection likelihood: -35.4673
1 mode projection failed, trying anyway
New goal: tensor([ 0.1845,  0.4956,  0.5767,  0.6565, -0.2092,  0.2609,  1.0220,  1.3398,
         1.5138,  0.5465,  0.2034,  0.9580,  0.0699,  0.0793, -0.2746],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0021]], device='cuda:0') tensor([[0.0079]], device='cuda:0')
Original likelihood: -38.0013427734375
Adjusted likelihood: -38.0013427734375
Likelihood residual: 0.0
{'index': 38.0013427734375, 'thumb_middle': inf}
Current yaw: tensor([ 0.0683,  0.0832, -0.3593], device='cuda:0')
4 index
tensor([ 0.2245,  0.4946,  0.6687,  0.6733, -0.2418,  0.2523,  1.0224,  1.3226,
         1.5000,  0.4614,  0.1987,  0.8571,  0.0683,  0.0832, -0.3593,  0.1579],
       device='cuda:0')
Solve time for step 1 10.56510284502292
Current ori: tensor([ 0.0683,  0.0832, -0.3593], device='cuda:0')
Middle force: tensor([0.5509, 0.5725, 0.5258, 0.5866], device='cuda:0')
Thumb force: tensor([0.5664, 0.6003, 0.5364, 0.5613], device='cuda:0')
tensor([ 0.2059,  0.4679,  0.5761,  0.6539, -0.2303,  0.2468,  1.0367,  1.3543,
         1.4698,  0.4988,  0.1948,  0.9148,  0.0812,  0.0708, -0.3462,  0.3335],
       device='cuda:0')
Solve time for step 2 4.299177204957232
Current ori: tensor([ 0.0812,  0.0708, -0.3462], device='cuda:0')
Middle force: tensor([0.5651, 0.5226, 0.5827], device='cuda:0')
Thumb force: tensor([0.5927, 0.5326, 0.5574], device='cuda:0')
tensor([ 0.1959,  0.4756,  0.5649,  0.6448, -0.2258,  0.2569,  1.0316,  1.3459,
         1.4795,  0.4823,  0.1822,  0.9127,  0.0773,  0.0686, -0.3495,  0.1872],
       device='cuda:0')
Solve time for step 3 4.185835292970296
Current ori: tensor([ 0.0773,  0.0686, -0.3495], device='cuda:0')
Middle force: tensor([0.5455, 0.5321], device='cuda:0')
Thumb force: tensor([0.5903, 0.5333], device='cuda:0')
tensor([ 0.1905,  0.4756,  0.5625,  0.6473, -0.2229,  0.2611,  1.0323,  1.3456,
         1.4726,  0.4917,  0.1840,  0.9165,  0.0776,  0.0658, -0.3476, -0.0957],
       device='cuda:0')
Solve time for step 4 3.8910102330264635
Current ori: tensor([ 0.0776,  0.0658, -0.3476], device='cuda:0')
Middle force: tensor([0.5036], device='cuda:0')
Thumb force: tensor([0.5068], device='cuda:0')
Storing RECOVERY transition: reward=-0.0175 (scaled=-0.0088), steps=2
Reward stats updated: mean 0.0094 -> 0.0093, std: 0.0804
Collected 197 transitions for RL
SAC Update 1/5: Actor Loss=-0.0009, Q1 Loss=0.7997, Q2 Loss=0.7997, Entropy=0.1419, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9035
SAC Update 2/5: Actor Loss=-0.2190, Q1 Loss=1.8191, Q2 Loss=1.8191, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5005
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.9084, Q2 Loss=1.9084, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5705
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.3595, Q2 Loss=1.3595, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3323
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.0723, Q2 Loss=1.0723, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4933

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.3%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (19.5%)
Actor update: 0.08s (38.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.043977
Q1 loss: 1.391812
Q2 loss: 1.391812
Current threshold: -33.0881
Global Scale Offset: 0.0880
Reward stats: mean=0.0093, std=0.0804, count=197
----------------------------------------------
SAC Update - Actor Loss: -0.0440, Q1 Loss: 1.3918, Q2 Loss: 1.3918, Entropy: 0.0284, Mean TD Error: 1.1600, Threshold: -33.0881
Original likelihood: -38.110107421875
Adjusted likelihood: -38.110107421875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 41.277565002441406
Projection step: 1, Loss: 41.05572509765625
Projection step: 2, Loss: 40.44406509399414
Projection step: 3, Loss: 38.44222640991211
Projection step: 4, Loss: 37.42033767700195
Projection step: 5, Loss: 39.02666091918945
Projection step: 6, Loss: 37.92472457885742
Projection step: 7, Loss: 36.0830078125
Projection step: 8, Loss: 36.584815979003906
Projection step: 9, Loss: 36.292510986328125
Projection step: 10, Loss: 37.732017517089844
Projection step: 11, Loss: 35.93507766723633
Projection step: 12, Loss: 35.81440734863281
Projection step: 13, Loss: 36.24094772338867
Projection step: 14, Loss: 35.84264373779297
Projection step: 15, Loss: 35.348602294921875
Projection step: 16, Loss: 35.49229049682617
Projection step: 17, Loss: 36.4899787902832
Projection step: 18, Loss: 36.2796745300293
Projection step: 19, Loss: 34.246150970458984
Projection step: 20, Loss: 34.744056701660156
Projection step: 21, Loss: 34.738224029541016
Projection step: 22, Loss: 33.894775390625
Projection step: 23, Loss: 35.53578186035156
Projection step: 24, Loss: 33.15223693847656
Final likelihood: tensor([-38.6304, -31.5441, -28.1477, -40.3376, -35.3077, -33.0427, -34.6869,
        -35.2100, -34.4431, -35.8471, -34.7407, -34.7975, -32.0140, -32.0645,
        -30.3858, -37.4535])
Final projection likelihood: -34.2908
1 mode projection failed, trying anyway
New goal: tensor([ 0.1224,  0.5451,  0.5323,  0.6419, -0.1779,  0.2931,  0.9573,  1.2650,
         1.4990,  0.5729,  0.1830,  1.0017,  0.0744,  0.0645, -0.4044],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0021]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -35.67448043823242
Adjusted likelihood: -35.67448043823242
Likelihood residual: 0.0
Original likelihood: -33.00569152832031
Adjusted likelihood: -33.00569152832031
Likelihood residual: 0.0
{'index': 33.00569152832031, 'thumb_middle': 35.67448043823242}
Current yaw: tensor([ 0.0748,  0.0626, -0.3364], device='cuda:0')
5 index
tensor([ 0.1280,  0.5338,  0.6022,  0.6691, -0.2153,  0.2687,  1.0277,  1.3363,
         1.4734,  0.4899,  0.1772,  0.9175,  0.0748,  0.0626, -0.3364, -0.1708],
       device='cuda:0')
Solve time for step 1 10.765165905002505
Current ori: tensor([ 0.0748,  0.0626, -0.3364], device='cuda:0')
Middle force: tensor([0.5671, 0.5289, 0.5463, 0.5052], device='cuda:0')
Thumb force: tensor([0.5981, 0.5944, 0.5830, 0.5018], device='cuda:0')
tensor([ 0.1582,  0.4955,  0.5129,  0.6316, -0.1987,  0.2919,  1.0107,  1.3145,
         1.4647,  0.4993,  0.1687,  0.9225,  0.0666,  0.0554, -0.3138,  0.0746],
       device='cuda:0')
Solve time for step 2 4.241305675008334
Current ori: tensor([ 0.0666,  0.0554, -0.3138], device='cuda:0')
Middle force: tensor([0.5260, 0.5403, 0.5036], device='cuda:0')
Thumb force: tensor([0.5860, 0.5767, 0.5013], device='cuda:0')
tensor([ 0.1557,  0.4946,  0.5023,  0.6287, -0.1942,  0.2944,  1.0096,  1.3188,
         1.4561,  0.5126,  0.1710,  0.9283,  0.0670,  0.0526, -0.3126,  0.0940],
       device='cuda:0')
Solve time for step 3 3.8216076400130987
Current ori: tensor([ 0.0670,  0.0526, -0.3126], device='cuda:0')
Middle force: tensor([0.5095, 0.5302], device='cuda:0')
Thumb force: tensor([0.5313, 0.5371], device='cuda:0')
tensor([ 0.1523,  0.4985,  0.5035,  0.6291, -0.1841,  0.3009,  1.0090,  1.3139,
         1.4568,  0.5104,  0.1636,  0.9276,  0.0638,  0.0477, -0.3187, -0.0444],
       device='cuda:0')
Solve time for step 4 3.9426348219858482
Current ori: tensor([ 0.0638,  0.0477, -0.3187], device='cuda:0')
Middle force: tensor([0.5286], device='cuda:0')
Thumb force: tensor([0.5529], device='cuda:0')
Storing RECOVERY transition: reward=-0.0392 (scaled=-0.0196), steps=2
Reward stats updated: mean 0.0093 -> 0.0092, std: 0.0802
Collected 198 transitions for RL
SAC Update 1/5: Actor Loss=-0.1936, Q1 Loss=0.7031, Q2 Loss=0.7031, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7681
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1628, Q2 Loss=1.1628, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1448
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1658, Q2 Loss=1.1658, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5573
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.8138, Q2 Loss=1.8138, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6682
SAC Update 5/5: Actor Loss=-0.1810, Q1 Loss=2.2825, Q2 Loss=2.2825, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1057

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.5%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (18.9%)
Actor update: 0.10s (40.2%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.074927
Q1 loss: 1.425603
Q2 loss: 1.425603
Current threshold: -33.1185
Global Scale Offset: 0.0862
Reward stats: mean=0.0092, std=0.0802, count=198
----------------------------------------------
SAC Update - Actor Loss: -0.0749, Q1 Loss: 1.4256, Q2 Loss: 1.4256, Entropy: 0.0000, Mean TD Error: 1.2488, Threshold: -33.1185
Original likelihood: -35.8396110534668
Adjusted likelihood: -35.8396110534668
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 35.767818450927734
Projection step: 1, Loss: 35.36199951171875
Projection step: 2, Loss: 35.0255126953125
Projection step: 3, Loss: 34.269535064697266
Projection step: 4, Loss: 35.8052864074707
Projection step: 5, Loss: 34.60260772705078
Projection step: 6, Loss: 33.7117919921875
Projection step: 7, Loss: 32.91754913330078
Projection step: 8, Loss: 32.094329833984375
Projection step: 9, Loss: 32.070377349853516
Projection step: 10, Loss: 32.22920227050781
Projection step: 11, Loss: 31.18477439880371
Projection step: 12, Loss: 30.52596664428711
Projection step: 13, Loss: 31.34101104736328
Projection step: 14, Loss: 32.91246795654297
Projection step: 15, Loss: 30.721193313598633
Projection step: 16, Loss: 32.525115966796875
Projection step: 17, Loss: 31.914806365966797
Projection step: 18, Loss: 28.66888999938965
Projection step: 19, Loss: 28.879322052001953
Projection step: 20, Loss: 31.380603790283203
Projection step: 21, Loss: 31.183216094970703
Projection step: 22, Loss: 28.672822952270508
Projection step: 23, Loss: 28.84988784790039
Projection step: 24, Loss: 28.061901092529297
Final likelihood: tensor([-29.9462, -28.3587, -28.3504, -25.6072, -30.0083, -27.5981, -29.9309,
        -28.1380, -27.4600, -31.2007, -31.2214, -24.4880, -29.2396, -24.7265,
        -31.4269, -48.8508])
Final projection likelihood: -29.7845
1 mode projection succeeded
New goal: tensor([ 0.1061,  0.5703,  0.5202,  0.6121, -0.1336,  0.3363,  0.9306,  1.2532,
         1.4555,  0.5991,  0.1723,  1.0432,  0.0610,  0.0390, -0.5834],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -27.930511474609375
Adjusted likelihood: -27.930511474609375
Likelihood residual: 0.0
Original likelihood: -29.53905487060547
Adjusted likelihood: -29.53905487060547
Likelihood residual: 0.0
{'index': 29.53905487060547, 'thumb_middle': 27.930511474609375}
Current yaw: tensor([ 0.0652,  0.0426, -0.3075], device='cuda:0')
6 thumb_middle
tensor([ 0.1031,  0.5698,  0.5441,  0.6462, -0.1761,  0.3036,  1.0120,  1.3126,
         1.4477,  0.5205,  0.1602,  0.9432,  0.0652,  0.0426, -0.3075, -0.1561],
       device='cuda:0')
Solve time for step 1 8.92890193394851
Current ori: tensor([ 0.0652,  0.0426, -0.3075], device='cuda:0')
Index force: tensor([0.5874, 0.5897, 0.5830, 0.5933], device='cuda:0')
tensor([ 0.0984,  0.5698,  0.5425,  0.6393, -0.2603,  0.2926,  0.8949,  1.2355,
         1.3879,  0.5776,  0.0742,  0.9744,  0.0620,  0.0513, -0.3074, -0.1003],
       device='cuda:0')
Solve time for step 2 3.678014524979517
Current ori: tensor([ 0.0620,  0.0513, -0.3074], device='cuda:0')
Index force: tensor([0.5825, 0.5765, 0.5881], device='cuda:0')
tensor([ 0.1117,  0.5865,  0.5396,  0.6269, -0.2526,  0.3106,  0.8891,  1.2315,
         1.3837,  0.5903,  0.0591,  0.9792,  0.0564,  0.0445, -0.3074, -0.0842],
       device='cuda:0')
Solve time for step 3 3.4016341639799066
Current ori: tensor([ 0.0564,  0.0445, -0.3074], device='cuda:0')
Index force: tensor([0.5809, 0.5929], device='cuda:0')
tensor([ 0.1215,  0.5858,  0.5441,  0.6394, -0.2477,  0.3122,  0.8902,  1.2334,
         1.3851,  0.5968,  0.0536,  0.9787,  0.0572,  0.0402, -0.3074, -0.0706],
       device='cuda:0')
Solve time for step 4 3.3685122970491648
Current ori: tensor([ 0.0572,  0.0402, -0.3074], device='cuda:0')
Index force: tensor([0.5738], device='cuda:0')
Storing RECOVERY transition: reward=-0.0368 (scaled=-0.0184), steps=2
Reward stats updated: mean 0.0092 -> 0.0090, std: 0.0800
Collected 199 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.1482, Q2 Loss=2.1482, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2241
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0971, Q2 Loss=1.0971, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7507
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=2.0780, Q2 Loss=2.0780, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1092
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.2105, Q2 Loss=1.2105, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9332
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.8158, Q2 Loss=1.8158, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6151

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.5%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.0%)
Actor update: 0.10s (42.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 1.669919
Q2 loss: 1.669919
Current threshold: -33.1365
Global Scale Offset: 0.0852
Reward stats: mean=0.0090, std=0.0800, count=199
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.6699, Q2 Loss: 1.6699, Entropy: 0.0000, Mean TD Error: 1.5264, Threshold: -33.1365
Original likelihood: -29.5759334564209
Adjusted likelihood: -29.5759334564209
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0606,  0.0493, -0.3100], device='cuda:0')
7 turn
Sampling time 3.860968067019712
tensor([ 0.1030,  0.5746,  0.5407,  0.6396, -0.1866,  0.3601,  0.9255,  1.2466,
         1.4505,  0.6015,  0.1080,  1.0158,  0.0606,  0.0493, -0.3100, -0.1164],
       device='cuda:0')
Original likelihood: -31.45261573791504
Adjusted likelihood: -31.45261573791504
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.158184072002769
Current ori: tensor([ 0.0606,  0.0493, -0.3100], device='cuda:0')
Middle force: tensor([0.8568, 0.5093, 1.2413, 0.7095, 0.9687, 0.5027, 0.4703, 0.5485, 0.4259,
        0.5653, 0.6624, 0.4888], device='cuda:0')
Thumb force: tensor([0.7405, 0.7458, 1.4380, 0.5255, 1.2249, 0.5391, 1.1414, 0.9269, 1.0007,
        0.6153, 0.6921, 0.7267], device='cuda:0')
Index force: tensor([0.5636, 0.5198, 0.6024, 0.4729, 0.5204, 0.5591, 0.5275, 0.4563, 0.4442,
        0.5630, 0.5032, 0.4836], device='cuda:0')
Storing NORMAL transition: reward=-0.0733 (scaled=-0.0733), steps=1
Reward stats updated: mean 0.0090 -> 0.0086, std: 0.0800
Collected 200 transitions for RL
SAC Update 1/5: Actor Loss=-0.0765, Q1 Loss=0.9005, Q2 Loss=0.9005, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9279
SAC Update 2/5: Actor Loss=-0.1965, Q1 Loss=1.9203, Q2 Loss=1.9203, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7076
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2159, Q2 Loss=1.2159, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1216
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1061, Q2 Loss=1.1061, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4464
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.8426, Q2 Loss=0.8426, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9759

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.5%)
Actor update: 0.10s (38.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.100652
Q1 loss: 1.197073
Q2 loss: 1.197073
Current threshold: -33.1472
Global Scale Offset: 0.0846
Reward stats: mean=0.0086, std=0.0800, count=200
----------------------------------------------
SAC Update - Actor Loss: -0.1007, Q1 Loss: 1.1971, Q2 Loss: 1.1971, Entropy: 0.0000, Mean TD Error: 1.0359, Threshold: -33.1472
tensor([ 0.1426,  0.6224,  0.4916,  0.6012, -0.0752,  0.3650,  1.1145,  0.9637,
         1.4688,  0.5375, -0.0708,  1.1665,  0.0435, -0.0167, -0.2328, -1.2966],
       device='cuda:0')
Original likelihood: -25.666080474853516
Adjusted likelihood: -25.666080474853516
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.579883293947205
Current ori: tensor([ 0.0435, -0.0167, -0.2328], device='cuda:0')
Middle force: tensor([0.5045, 0.5115, 0.5178, 0.5874, 0.5793, 0.9336, 0.7819, 0.7067, 0.6037,
        0.5070, 0.5003], device='cuda:0')
Thumb force: tensor([1.6356, 1.2426, 0.5175, 0.9961, 0.7418, 1.3082, 0.5737, 0.6970, 0.5630,
        0.5312, 0.6003], device='cuda:0')
Index force: tensor([0.7782, 0.8785, 0.5867, 0.5328, 0.5220, 0.5393, 0.5194, 0.5164, 0.5476,
        0.6850, 0.6155], device='cuda:0')
Storing NORMAL transition: reward=-0.0001 (scaled=-0.0001), steps=1
Reward stats updated: mean 0.0086 -> 0.0086, std: 0.0798
Collected 201 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.1825, Q2 Loss=2.1825, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8627
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0953, Q2 Loss=1.0953, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1358
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.8449, Q2 Loss=1.8449, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5699
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0364, Q2 Loss=1.0364, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8350
SAC Update 5/5: Actor Loss=-0.1202, Q1 Loss=1.5445, Q2 Loss=1.5445, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7463

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.8%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (19.6%)
Actor update: 0.10s (38.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.024034
Q1 loss: 1.540735
Q2 loss: 1.540735
Current threshold: -33.1536
Global Scale Offset: 0.0842
Reward stats: mean=0.0086, std=0.0798, count=201
----------------------------------------------
SAC Update - Actor Loss: -0.0240, Q1 Loss: 1.5407, Q2 Loss: 1.5407, Entropy: 0.0000, Mean TD Error: 1.4299, Threshold: -33.1536
tensor([ 0.1064,  0.6248,  0.4713,  0.6568, -0.0630,  0.4572,  0.9947,  1.0346,
         1.4683,  0.5643, -0.0994,  1.1757,  0.0308, -0.0303, -0.2324, -1.7417],
       device='cuda:0')
Original likelihood: -26.678688049316406
Adjusted likelihood: -26.678688049316406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.243946477014106
Current ori: tensor([ 0.0308, -0.0303, -0.2324], device='cuda:0')
Middle force: tensor([0.5125, 0.5149, 0.5887, 0.5880, 0.8970, 0.7777, 0.7067, 0.6008, 0.5088,
        0.5007], device='cuda:0')
Thumb force: tensor([1.1570, 0.5156, 0.9443, 0.7004, 1.2388, 0.5594, 0.6737, 0.5566, 0.5176,
        0.5619], device='cuda:0')
Index force: tensor([0.8275, 0.5856, 0.5291, 0.5180, 0.5367, 0.5171, 0.5137, 0.5413, 0.6806,
        0.5989], device='cuda:0')
Storing NORMAL transition: reward=-0.0014 (scaled=-0.0014), steps=1
Reward stats updated: mean 0.0086 -> 0.0085, std: 0.0796
Collected 202 transitions for RL
SAC Update 1/5: Actor Loss=-0.0033, Q1 Loss=0.7390, Q2 Loss=0.7390, Entropy=0.2984, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9534
SAC Update 2/5: Actor Loss=-0.0560, Q1 Loss=0.9674, Q2 Loss=0.9674, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4128
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3389, Q2 Loss=1.3389, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2055
SAC Update 4/5: Actor Loss=-0.0001, Q1 Loss=1.2920, Q2 Loss=1.2920, Entropy=0.0244, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7127
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9257, Q2 Loss=0.9257, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6114

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.0%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (19.0%)
Actor update: 0.11s (41.1%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.011889
Q1 loss: 1.052607
Q2 loss: 1.052607
Current threshold: -33.1598
Global Scale Offset: 0.0840
Reward stats: mean=0.0085, std=0.0796, count=202
----------------------------------------------
SAC Update - Actor Loss: -0.0119, Q1 Loss: 1.0526, Q2 Loss: 1.0526, Entropy: 0.0646, Mean TD Error: 1.1792, Threshold: -33.1598
tensor([ 0.1352,  0.6173,  0.4804,  0.7202, -0.0354,  0.3719,  1.0005,  1.0958,
         1.4999,  0.4648, -0.1890,  1.1917,  0.0410, -0.0431, -0.2324, -1.6793],
       device='cuda:0')
Original likelihood: -33.98680114746094
Adjusted likelihood: -33.98680114746094
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0042)
State is out of distribution
Projection step: 0, Loss: 34.01719665527344
Projection step: 1, Loss: 33.39351272583008
Projection step: 2, Loss: 32.094383239746094
Projection step: 3, Loss: 31.524974822998047
Projection step: 4, Loss: 31.553848266601562
Projection step: 5, Loss: 32.654998779296875
Projection step: 6, Loss: 31.524934768676758
Projection step: 7, Loss: 33.696739196777344
Projection step: 8, Loss: 29.90618133544922
Projection step: 9, Loss: 30.412403106689453
Projection step: 10, Loss: 31.269454956054688
Projection step: 11, Loss: 31.08877944946289
Projection step: 12, Loss: 31.48031997680664
Projection step: 13, Loss: 31.912551879882812
Projection step: 14, Loss: 29.38723373413086
Projection step: 15, Loss: 33.0059814453125
Projection step: 16, Loss: 31.27406883239746
Projection step: 17, Loss: 31.405906677246094
Projection step: 18, Loss: 28.684415817260742
Projection step: 19, Loss: 29.92313003540039
Projection step: 20, Loss: 29.842674255371094
Projection step: 21, Loss: 29.78736114501953
Projection step: 22, Loss: 36.215606689453125
Projection step: 23, Loss: 32.404090881347656
Projection step: 24, Loss: 30.189456939697266
Final likelihood: tensor([-29.3765, -25.5507, -25.6044, -29.6628, -27.6935, -54.3056, -24.7341,
        -27.9558, -26.2561, -27.8629, -29.6935, -26.1428, -28.2236, -49.1052,
        -35.5264, -27.4511])
Final projection likelihood: -30.9466
1 mode projection succeeded
New goal: tensor([ 0.1185,  0.5866,  0.5453,  0.6759, -0.0355,  0.3990,  0.9610,  1.0472,
         1.4616,  0.4281, -0.1160,  1.2218,  0.0380, -0.0363, -0.1746],
       device='cuda:0')
tensor([[0.0058]], device='cuda:0') tensor([[0.0064]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -27.229839324951172
Adjusted likelihood: -27.229839324951172
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 27.229839324951172}
Current yaw: tensor([ 0.0410, -0.0431, -0.2324], device='cuda:0')
8 thumb_middle
tensor([ 0.1352,  0.6173,  0.4804,  0.7202, -0.0354,  0.3719,  1.0005,  1.0958,
         1.4999,  0.4648, -0.1890,  1.1917,  0.0410, -0.0431, -0.2324, -1.6793],
       device='cuda:0')
Solve time for step 1 9.052154045028146
Current ori: tensor([ 0.0410, -0.0431, -0.2324], device='cuda:0')
Index force: tensor([0.5828, 0.5967, 0.5849, 0.6091], device='cuda:0')
tensor([ 0.1312,  0.5987,  0.5269,  0.6710, -0.0965,  0.3866,  0.9449,  1.0412,
         1.4321,  0.4253, -0.1890,  1.1813,  0.0400, -0.0416, -0.2324, -1.6948],
       device='cuda:0')
Solve time for step 2 3.6957074420060962
Current ori: tensor([ 0.0400, -0.0416, -0.2324], device='cuda:0')
Index force: tensor([0.5870, 0.5777, 0.6016], device='cuda:0')
tensor([ 0.1275,  0.6029,  0.5292,  0.6476, -0.1041,  0.3968,  0.9418,  1.0344,
         1.4292,  0.4194, -0.1890,  1.1783,  0.0364, -0.0406, -0.2324, -1.7091],
       device='cuda:0')
Solve time for step 3 3.550102084991522
Current ori: tensor([ 0.0364, -0.0406, -0.2324], device='cuda:0')
Index force: tensor([0.5690, 0.5940], device='cuda:0')
tensor([ 0.1162,  0.5941,  0.5243,  0.6581, -0.1124,  0.3958,  0.9348,  1.0298,
         1.4310,  0.4194, -0.1890,  1.1824,  0.0386, -0.0342, -0.2324, -1.7192],
       device='cuda:0')
Solve time for step 4 3.490062501979992
Current ori: tensor([ 0.0386, -0.0342, -0.2324], device='cuda:0')
Index force: tensor([0.5761], device='cuda:0')
Storing RECOVERY transition: reward=-0.0032 (scaled=-0.0011), steps=3
Reward stats updated: mean 0.0085 -> 0.0085, std: 0.0794
Collected 203 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.0512, Q2 Loss=1.0512, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8008
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.7619, Q2 Loss=1.7619, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4141
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.7454, Q2 Loss=0.7454, Entropy=0.0091, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7626
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.2324, Q2 Loss=1.2324, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2672
SAC Update 5/5: Actor Loss=-0.1757, Q1 Loss=1.1990, Q2 Loss=1.1990, Entropy=0.0252, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8327

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.5%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.1%)
Actor update: 0.10s (39.6%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.127240
Q1 loss: 1.197977
Q2 loss: 1.197977
Current threshold: -33.1636
Global Scale Offset: 0.0839
Reward stats: mean=0.0085, std=0.0794, count=203
----------------------------------------------
SAC Update - Actor Loss: -0.1272, Q1 Loss: 1.1980, Q2 Loss: 1.1980, Entropy: 0.0069, Mean TD Error: 1.0155, Threshold: -33.1636
Original likelihood: -27.724517822265625
Adjusted likelihood: -27.724517822265625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0446, -0.0225, -0.2283], device='cuda:0')
9 turn
Sampling time 3.683802924992051
tensor([ 0.0943,  0.5702,  0.5294,  0.6692, -0.0562,  0.4396,  0.9717,  1.0458,
         1.4965,  0.4278, -0.1327,  1.2283,  0.0446, -0.0225, -0.2283, -1.7097],
       device='cuda:0')
Original likelihood: -26.737886428833008
Adjusted likelihood: -26.737886428833008
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.927567096950952
Current ori: tensor([ 0.0446, -0.0225, -0.2283], device='cuda:0')
Middle force: tensor([1.1729, 0.5209, 0.5199, 0.5314, 0.6031, 0.6348, 1.0356, 0.7846, 0.8300,
        0.5748, 0.5096, 0.5525], device='cuda:0')
Thumb force: tensor([1.8186, 2.0186, 1.3119, 0.5793, 1.0516, 0.7908, 1.4457, 0.5903, 0.6519,
        0.5176, 0.5747, 0.6747], device='cuda:0')
Index force: tensor([0.5906, 0.7925, 0.6181, 0.6374, 0.5537, 0.5519, 0.6008, 0.5302, 0.5976,
        0.6962, 0.7620, 0.5888], device='cuda:0')
Storing NORMAL transition: reward=0.0020 (scaled=0.0020), steps=1
Reward stats updated: mean 0.0085 -> 0.0084, std: 0.0792
Collected 204 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.0346, Q2 Loss=1.0346, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4263
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.8111, Q2 Loss=1.8111, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5212
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.6344, Q2 Loss=1.6344, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3422
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.1685, Q2 Loss=1.1685, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1041
SAC Update 5/5: Actor Loss=-0.0306, Q1 Loss=0.7270, Q2 Loss=0.7270, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0121

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.2%)
Q1 update: 0.05s (20.5%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (38.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.098226
Q1 loss: 1.275123
Q2 loss: 1.275123
Current threshold: -33.1659
Global Scale Offset: 0.0839
Reward stats: mean=0.0084, std=0.0792, count=204
----------------------------------------------
SAC Update - Actor Loss: -0.0982, Q1 Loss: 1.2751, Q2 Loss: 1.2751, Entropy: 0.0000, Mean TD Error: 1.4812, Threshold: -33.1659
tensor([ 0.0829,  0.5840,  0.5489,  0.5739,  0.0077,  0.4040,  0.8879,  1.0310,
         1.5000,  0.4085, -0.1018,  1.1728,  0.0342, -0.0151, -0.2292, -1.7782],
       device='cuda:0')
Original likelihood: -25.06960105895996
Adjusted likelihood: -25.06960105895996
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.715858187992126
Current ori: tensor([ 0.0342, -0.0151, -0.2292], device='cuda:0')
Middle force: tensor([1.3126, 0.5400, 0.5129, 0.9527, 0.8026, 0.5581, 0.5539, 0.6086, 0.5030,
        0.6314, 0.5853], device='cuda:0')
Thumb force: tensor([0.9201, 0.9802, 0.9026, 0.6315, 0.5371, 0.5496, 1.2476, 0.5194, 0.6770,
        0.5723, 0.5675], device='cuda:0')
Index force: tensor([1.4453, 0.5075, 0.5372, 0.5378, 0.5589, 0.6089, 0.5831, 0.6449, 0.7018,
        0.5572, 0.6512], device='cuda:0')
Storing NORMAL transition: reward=0.0025 (scaled=0.0025), steps=1
Reward stats updated: mean 0.0084 -> 0.0084, std: 0.0790
Collected 205 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.9931, Q2 Loss=1.9931, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6597
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.0006, Q2 Loss=2.0006, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7091
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.9729, Q2 Loss=1.9729, Entropy=0.0143, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3254
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0714, Q2 Loss=1.0714, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6930
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.9018, Q2 Loss=1.9018, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4788

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.6%)
Q1 update: 0.05s (18.8%)
Q2 update: 0.06s (19.9%)
Actor update: 0.12s (43.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000006
Q1 loss: 1.787967
Q2 loss: 1.787967
Current threshold: -33.1674
Global Scale Offset: 0.0838
Reward stats: mean=0.0084, std=0.0790, count=205
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.7880, Q2 Loss: 1.7880, Entropy: 0.0029, Mean TD Error: 1.5732, Threshold: -33.1674
tensor([-0.0589,  0.5966,  0.4958,  0.3496, -0.0610,  0.3927,  0.8757,  0.9777,
         1.3836,  0.4797, -0.1077,  1.1328,  0.0161,  0.0322, -0.2317, -1.4216],
       device='cuda:0')
Original likelihood: -29.554241180419922
Adjusted likelihood: -29.554241180419922
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.441627400985453
Current ori: tensor([ 0.0161,  0.0322, -0.2317], device='cuda:0')
Middle force: tensor([0.5396, 0.5142, 0.9447, 0.7977, 0.5563, 0.5533, 0.6238, 0.5017, 0.6416,
        0.6042], device='cuda:0')
Thumb force: tensor([0.9355, 0.8782, 0.6211, 0.5328, 0.5463, 1.2130, 0.5150, 0.6696, 0.5637,
        0.5552], device='cuda:0')
Index force: tensor([0.5068, 0.5317, 0.5364, 0.5553, 0.6027, 0.5823, 0.6306, 0.7312, 0.5519,
        0.6325], device='cuda:0')
Storing NORMAL transition: reward=0.0356 (scaled=0.0356), steps=1
Reward stats updated: mean 0.0084 -> 0.0085, std: 0.0789
Collected 206 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8394, Q2 Loss=0.8394, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4690
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.6048, Q2 Loss=0.6048, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8164
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.7583, Q2 Loss=1.7583, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4223
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.0750, Q2 Loss=1.0750, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0191
SAC Update 5/5: Actor Loss=-0.0001, Q1 Loss=1.2246, Q2 Loss=1.2246, Entropy=0.0260, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8111

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.4%)
Q1 update: 0.05s (20.3%)
Q2 update: 0.05s (18.8%)
Actor update: 0.10s (37.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046070
Q1 loss: 1.100436
Q2 loss: 1.100436
Current threshold: -33.1683
Global Scale Offset: 0.0838
Reward stats: mean=0.0085, std=0.0789, count=206
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1004, Q2 Loss: 1.1004, Entropy: 0.0052, Mean TD Error: 1.1076, Threshold: -33.1683
tensor([-0.0361,  0.5590,  0.5007,  0.5177,  0.0359,  0.3910,  0.8534,  1.1950,
         1.4992,  0.4864,  0.0571,  0.9133,  0.0603, -0.0276, -0.2706, -1.0101],
       device='cuda:0')
Original likelihood: -33.30509948730469
Adjusted likelihood: -33.30509948730469
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.3312)
State is out of distribution
Projection step: 0, Loss: 34.419227600097656
Projection step: 1, Loss: 34.075660705566406
Projection step: 2, Loss: 33.77177429199219
Projection step: 3, Loss: 34.528076171875
Projection step: 4, Loss: 33.431087493896484
Projection step: 5, Loss: 33.49590301513672
Projection step: 6, Loss: 34.80297088623047
Projection step: 7, Loss: 31.18517303466797
Projection step: 8, Loss: 32.81669616699219
Projection step: 9, Loss: 30.835128784179688
Projection step: 10, Loss: 31.956567764282227
Projection step: 11, Loss: 34.73147201538086
Projection step: 12, Loss: 29.515304565429688
Projection step: 13, Loss: 29.778945922851562
Projection step: 14, Loss: 29.089569091796875
Projection step: 15, Loss: 28.797409057617188
Projection step: 16, Loss: 29.43056869506836
Projection step: 17, Loss: 28.044401168823242
Projection step: 18, Loss: 28.900421142578125
Projection step: 19, Loss: 27.44451332092285
Projection step: 20, Loss: 27.679723739624023
Projection step: 21, Loss: 27.673442840576172
Projection step: 22, Loss: 27.20826530456543
Projection step: 23, Loss: 26.88814926147461
Projection step: 24, Loss: 28.519485473632812
Final likelihood: tensor([-25.5150, -28.4492, -24.4778, -28.1844, -29.3119, -24.2070, -35.8936,
        -26.1573, -24.3484, -23.6109, -24.5774, -25.7548, -25.3364, -23.7817,
        -27.1879, -25.5056])
Final projection likelihood: -26.3937
1 mode projection succeeded
New goal: tensor([ 0.0095,  0.5697,  0.5736,  0.5047, -0.0064,  0.4034,  0.8309,  1.0442,
         1.4754,  0.4698,  0.1095,  1.0270,  0.0565, -0.0229,  0.4549],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0024]], device='cuda:0')
Original likelihood: -30.499353408813477
Adjusted likelihood: -30.499353408813477
Likelihood residual: 0.0
Original likelihood: -32.96236801147461
Adjusted likelihood: -32.96236801147461
Likelihood residual: 0.0
{'index': 32.96236801147461, 'thumb_middle': 30.499353408813477}
Current yaw: tensor([ 0.0603, -0.0276, -0.2706], device='cuda:0')
10 thumb_middle
tensor([-0.0361,  0.5590,  0.5007,  0.5177,  0.0359,  0.3910,  0.8534,  1.1950,
         1.4992,  0.4864,  0.0571,  0.9133,  0.0603, -0.0276, -0.2706, -1.0101],
       device='cuda:0')
Solve time for step 1 8.648133446986321
Current ori: tensor([ 0.0603, -0.0276, -0.2706], device='cuda:0')
Index force: tensor([0.5835, 0.6014, 0.5875, 0.6102], device='cuda:0')
tensor([-0.0126,  0.5568,  0.5535,  0.5080, -0.0476,  0.4077,  0.8245,  1.0584,
         1.4056,  0.4645, -0.0106,  0.9464,  0.0996, -0.0555, -0.2676, -0.6852],
       device='cuda:0')
Solve time for step 2 3.6732341540046036
Current ori: tensor([ 0.0996, -0.0555, -0.2676], device='cuda:0')
Index force: tensor([0.5904, 0.5786, 0.6001], device='cuda:0')
tensor([-0.0577,  0.6149,  0.5832,  0.5011, -0.0465,  0.4215,  0.8238,  1.0411,
         1.4120,  0.4589, -0.0065,  0.9652,  0.2440, -0.1411, -0.2661, -0.0818],
       device='cuda:0')
Solve time for step 3 3.4246747069992125
Current ori: tensor([ 0.2440, -0.1411, -0.2661], device='cuda:0')
Index force: tensor([0.5006, 0.5004], device='cuda:0')
tensor([ 0.0193,  0.7277,  0.8277,  0.6611,  0.0733,  0.4992,  0.8661,  1.0429,
         1.4238,  0.4649,  0.0159,  0.9870,  0.3700, -0.3197, -0.1639,  0.4717],
       device='cuda:0')
Solve time for step 4 3.350860022008419
Current ori: tensor([ 0.3700, -0.3197, -0.1639], device='cuda:0')
Index force: tensor([0.5004], device='cuda:0')
Storing RECOVERY transition: reward=-0.1678 (scaled=-0.0559), steps=3
Reward stats updated: mean 0.0085 -> 0.0082, std: 0.0788
Collected 207 transitions for RL
SAC Update 1/5: Actor Loss=-0.0088, Q1 Loss=0.7709, Q2 Loss=0.7709, Entropy=0.3393, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3048
SAC Update 2/5: Actor Loss=-0.0824, Q1 Loss=1.0268, Q2 Loss=1.0268, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2076
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9736, Q2 Loss=0.9736, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0708
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1309, Q2 Loss=1.1309, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5904
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.7577, Q2 Loss=0.7577, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7460

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.9%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (18.6%)
Actor update: 0.09s (38.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.018247
Q1 loss: 0.931965
Q2 loss: 0.931965
Current threshold: -33.1737
Global Scale Offset: 0.0838
Reward stats: mean=0.0082, std=0.0788, count=207
----------------------------------------------
SAC Update - Actor Loss: -0.0182, Q1 Loss: 0.9320, Q2 Loss: 0.9320, Entropy: 0.0679, Mean TD Error: 0.9839, Threshold: -33.1737
Original likelihood: -207.8705596923828
Adjusted likelihood: -207.8705596923828
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 206.93270874023438
Projection step: 1, Loss: 207.01104736328125
Projection step: 2, Loss: 196.82395935058594
Projection step: 3, Loss: 186.68325805664062
Projection step: 4, Loss: 203.17254638671875
Projection step: 5, Loss: 209.8714599609375
Projection step: 6, Loss: 191.08045959472656
Projection step: 7, Loss: 193.7012939453125
Projection step: 8, Loss: 209.19537353515625
Projection step: 9, Loss: 189.05416870117188
Projection step: 10, Loss: 195.8106231689453
Projection step: 11, Loss: 194.38665771484375
Projection step: 12, Loss: 193.415283203125
Projection step: 13, Loss: 207.22291564941406
Projection step: 14, Loss: 196.2121124267578
Projection step: 15, Loss: 212.11297607421875
Projection step: 16, Loss: 212.92527770996094
Projection step: 17, Loss: 197.289794921875
Projection step: 18, Loss: 190.80577087402344
Projection step: 19, Loss: 201.3968505859375
Projection step: 20, Loss: 200.72137451171875
Projection step: 21, Loss: 220.83944702148438
Projection step: 22, Loss: 201.5673828125
Projection step: 23, Loss: 205.3135223388672
Projection step: 24, Loss: 197.82566833496094
Final likelihood: tensor([-223.7052, -139.2457, -190.6510, -142.3432, -217.2865, -248.9163,
        -204.5804, -168.7501, -198.3435, -207.2110, -174.1709, -199.6417,
        -243.3289, -221.7379, -213.6556, -202.4347])
Final projection likelihood: -199.7502
1 mode projection failed, trying anyway
New goal: tensor([ 0.0504,  0.9939,  0.6912,  0.5069,  0.2368,  0.7489,  1.1222,  1.1689,
         1.3990,  0.4619,  0.1987,  1.2594,  0.3587, -0.2850, -0.2928],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0115]], device='cuda:0') tensor([[0.0048]], device='cuda:0')
Original likelihood: -168.16131591796875
Adjusted likelihood: -168.16131591796875
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 168.16131591796875}
Current yaw: tensor([ 0.3584, -0.2843, -0.2850], device='cuda:0')
11 thumb_middle
tensor([ 0.0359,  1.0026,  0.6972,  0.5015,  0.2425,  0.7539,  1.1248,  1.1508,
         1.4168,  0.4521,  0.2006,  1.2518,  0.3584, -0.2843, -0.2850, -0.9749],
       device='cuda:0')
Solve time for step 1 8.89837081800215
Current ori: tensor([ 0.3584, -0.2843, -0.2850], device='cuda:0')
Index force: tensor([0.5973, 0.5931, 0.5755, 0.5984], device='cuda:0')
tensor([ 0.0223,  1.0434,  0.6843,  0.5022,  0.2250,  0.7786,  1.1267,  1.1539,
         1.2916,  0.4920,  0.0436,  1.2345,  0.3857, -0.3632, -0.1363, -2.8729],
       device='cuda:0')
Solve time for step 2 3.6813666700036265
Current ori: tensor([ 0.3857, -0.3632, -0.1363], device='cuda:0')
Index force: tensor([0.5573, 0.5447, 0.5802], device='cuda:0')
tensor([-0.0115,  1.0595,  0.6918,  0.4995,  0.1939,  0.7718,  1.1639,  1.1725,
         1.2899,  0.4969,  0.0501,  1.2541,  0.3858, -0.3635, -0.1358, -2.0873],
       device='cuda:0')
Solve time for step 3 3.679569978034124
Current ori: tensor([ 0.3858, -0.3635, -0.1358], device='cuda:0')
Index force: tensor([0.5659, 0.5853], device='cuda:0')
tensor([-0.0119,  1.0405,  0.6894,  0.4949,  0.1955,  0.7547,  1.1619,  1.1703,
         1.2910,  0.4947,  0.0289,  1.2588,  0.3868, -0.3663, -0.1435, -1.1709],
       device='cuda:0')
Solve time for step 4 3.546146264008712
Current ori: tensor([ 0.3868, -0.3663, -0.1435], device='cuda:0')
Index force: tensor([0.5460], device='cuda:0')
Storing RECOVERY transition: reward=-0.1604 (scaled=-0.0535), steps=3
Reward stats updated: mean 0.0082 -> 0.0079, std: 0.0787
Collected 208 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8398, Q2 Loss=0.8398, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7168
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3268, Q2 Loss=1.3268, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9864
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8668, Q2 Loss=0.8668, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9803
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.6408, Q2 Loss=1.6408, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4592
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.6551, Q2 Loss=0.6551, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8788

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.6%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (18.8%)
Actor update: 0.11s (40.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.065872
Q2 loss: 1.065872
Current threshold: -33.1770
Global Scale Offset: 0.0838
Reward stats: mean=0.0079, std=0.0787, count=208
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0659, Q2 Loss: 1.0659, Entropy: 0.0000, Mean TD Error: 1.0043, Threshold: -33.1770
Original likelihood: -293.00189208984375
Adjusted likelihood: -293.00189208984375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 292.6676940917969
Projection step: 1, Loss: 281.27703857421875
Projection step: 2, Loss: 287.17474365234375
Projection step: 3, Loss: 289.2864074707031
Projection step: 4, Loss: 290.7078857421875
Projection step: 5, Loss: 284.4917907714844
Projection step: 6, Loss: 290.72967529296875
Projection step: 7, Loss: 289.88897705078125
Projection step: 8, Loss: 270.2328796386719
Projection step: 9, Loss: 282.5633544921875
Projection step: 10, Loss: 283.0294189453125
Projection step: 11, Loss: 282.0916748046875
Projection step: 12, Loss: 286.9006042480469
Projection step: 13, Loss: 297.7626953125
Projection step: 14, Loss: 290.447265625
Projection step: 15, Loss: 304.9831237792969
Projection step: 16, Loss: 292.81292724609375
Projection step: 17, Loss: 279.8197937011719
Projection step: 18, Loss: 284.0704345703125
Projection step: 19, Loss: 287.8091125488281
Projection step: 20, Loss: 295.856689453125
Projection step: 21, Loss: 307.726806640625
Projection step: 22, Loss: 289.0938720703125
Projection step: 23, Loss: 283.1973876953125
Projection step: 24, Loss: 288.47186279296875
Final likelihood: tensor([-314.9068, -304.9158, -232.3569, -252.2007, -285.9336, -255.4534,
        -330.9169, -316.1726, -261.7053, -258.2747, -301.6646, -334.5897,
        -314.9183, -311.2684, -297.6523, -294.4055])
Final projection likelihood: -291.7085
1 mode projection failed, trying anyway
New goal: tensor([-0.0237,  1.1291,  0.7296,  0.5079,  0.1726,  0.8308,  1.1906,  1.2092,
         1.3296,  0.4739,  0.1570,  1.2670,  0.3712, -0.3271, -0.3228],
       device='cuda:0')
tensor([[0.0020]], device='cuda:0') tensor([[0.0084]], device='cuda:0') tensor([[0.0056]], device='cuda:0')
Original likelihood: -233.90667724609375
Adjusted likelihood: -233.90667724609375
Likelihood residual: 0.0
Original likelihood: -270.63116455078125
Adjusted likelihood: -270.63116455078125
Likelihood residual: 0.0
{'index': 270.63116455078125, 'thumb_middle': 233.90667724609375}
Current yaw: tensor([ 0.3712, -0.3266, -0.3357], device='cuda:0')
12 thumb_middle
tensor([-0.0288,  1.1299,  0.7240,  0.5055,  0.1725,  0.8294,  1.1945,  1.2043,
         1.3375,  0.4707,  0.1578,  1.2637,  0.3712, -0.3266, -0.3357, -1.6758],
       device='cuda:0')
Solve time for step 1 8.960871382965706
Current ori: tensor([ 0.3712, -0.3266, -0.3357], device='cuda:0')
Index force: tensor([0.6149, 0.6010, 0.5805, 0.6061], device='cuda:0')
tensor([-0.0487,  1.1577,  0.6979,  0.5001,  0.1501,  0.8417,  1.2022,  1.2008,
         1.2878,  0.5034,  0.0122,  1.2719,  0.3872, -0.3716, -0.2072, -2.1134],
       device='cuda:0')
Solve time for step 2 3.745046942960471
Current ori: tensor([ 0.3872, -0.3716, -0.2072], device='cuda:0')
Index force: tensor([0.5975, 0.5749, 0.6002], device='cuda:0')
tensor([-0.0669,  1.1658,  0.6935,  0.4935,  0.1326,  0.8384,  1.2185,  1.2061,
         1.2888,  0.5133,  0.0111,  1.2721,  0.3878, -0.3728, -0.1986, -1.6443],
       device='cuda:0')
Solve time for step 3 3.5308473740005866
Current ori: tensor([ 0.3878, -0.3728, -0.1986], device='cuda:0')
Index force: tensor([0.5742, 0.5716], device='cuda:0')
tensor([-0.0721,  1.1544,  0.6991,  0.5040,  0.1289,  0.8253,  1.2244,  1.2130,
         1.2878,  0.5156,  0.0220,  1.2706,  0.3876, -0.3724, -0.1999, -1.6364],
       device='cuda:0')
Solve time for step 4 3.61421089299256
Current ori: tensor([ 0.3876, -0.3724, -0.1999], device='cuda:0')
Index force: tensor([0.5598], device='cuda:0')
Storing RECOVERY transition: reward=-0.2167 (scaled=-0.0722), steps=3
Reward stats updated: mean 0.0079 -> 0.0076, std: 0.0787
Collected 209 transitions for RL
SAC Update 1/5: Actor Loss=-0.2330, Q1 Loss=2.0205, Q2 Loss=2.0205, Entropy=0.3210, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7374
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.9428, Q2 Loss=1.9428, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6903
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8631, Q2 Loss=0.8631, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5935
SAC Update 4/5: Actor Loss=-0.0028, Q1 Loss=2.0483, Q2 Loss=2.0483, Entropy=0.3219, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6570
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.6030, Q2 Loss=0.6030, Entropy=0.0013, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3992

------ SAC Update Summary (5 iterations) ------
Total time: 0.30s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (16.1%)
Q1 update: 0.06s (20.5%)
Q2 update: 0.06s (20.0%)
Actor update: 0.12s (40.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.047154
Q1 loss: 1.495542
Q2 loss: 1.495542
Current threshold: -33.1817
Global Scale Offset: 0.0838
Reward stats: mean=0.0076, std=0.0787, count=209
----------------------------------------------
SAC Update - Actor Loss: -0.0472, Q1 Loss: 1.4955, Q2 Loss: 1.4955, Entropy: 0.1288, Mean TD Error: 1.4155, Threshold: -33.1817
Original likelihood: -333.64971923828125
Adjusted likelihood: -333.64971923828125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 13
Loaded trajectory sampler
Current yaw: tensor([-0.0012,  0.0145, -0.0305], device='cuda:0')
Current yaw: tensor([-0.0012,  0.0145, -0.0305], device='cuda:0')
1 turn
Sampling time 3.6959669750067405
tensor([ 1.1494e-01,  6.2601e-01,  5.5762e-01,  5.1433e-01, -1.0586e-01,
         5.1717e-01,  9.0509e-01,  9.2359e-01,  1.2015e+00,  3.2485e-01,
         2.4820e-01,  1.2244e+00, -1.1562e-03,  1.4488e-02, -3.0475e-02,
         3.8849e-02], device='cuda:0')
Original likelihood: -15.388669967651367
Adjusted likelihood: -15.388669967651367
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.825368358986452
Current ori: tensor([-0.0012,  0.0145, -0.0305], device='cuda:0')
Middle force: tensor([0.5631, 1.6455, 0.5165, 0.5074, 0.6286, 1.5845, 0.5730, 0.5659, 0.5414,
        0.5189, 0.5461, 0.5541], device='cuda:0')
Thumb force: tensor([0.6837, 0.9172, 0.4942, 1.0012, 0.8991, 1.0473, 0.6945, 0.7368, 0.5830,
        0.8742, 0.6350, 1.0341], device='cuda:0')
Index force: tensor([0.5085, 0.7157, 0.9362, 0.6281, 0.5056, 0.5102, 0.5005, 0.5519, 0.5766,
        0.5374, 0.5755, 0.5532], device='cuda:0')
Storing NORMAL transition: reward=-0.0425 (scaled=-0.0425), steps=1
Reward stats updated: mean 0.0076 -> 0.0073, std: 0.0786
Collected 210 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.8709, Q2 Loss=1.8709, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8368
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9534, Q2 Loss=0.9534, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4039
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0160, Q2 Loss=1.0160, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2199
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.5022, Q2 Loss=1.5022, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2666
SAC Update 5/5: Actor Loss=-0.0019, Q1 Loss=1.2100, Q2 Loss=1.2100, Entropy=0.3239, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1336

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.6%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (20.3%)
Actor update: 0.10s (41.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092478
Q1 loss: 1.310492
Q2 loss: 1.310492
Current threshold: -33.1859
Global Scale Offset: 0.0838
Reward stats: mean=0.0073, std=0.0786, count=210
----------------------------------------------
SAC Update - Actor Loss: -0.0925, Q1 Loss: 1.3105, Q2 Loss: 1.3105, Entropy: 0.0648, Mean TD Error: 1.3721, Threshold: -33.1859
tensor([ 0.0767,  0.5616,  0.5633,  0.6023, -0.0649,  0.5033,  0.8132,  0.9065,
         1.2665,  0.2416,  0.2114,  1.2906,  0.0125,  0.0180,  0.0118,  0.2762],
       device='cuda:0')
Original likelihood: -12.68225383758545
Adjusted likelihood: -12.68225383758545
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.553276353981346
Current ori: tensor([0.0125, 0.0180, 0.0118], device='cuda:0')
Middle force: tensor([1.6000, 0.5165, 0.5071, 0.6221, 1.5404, 0.5713, 0.5616, 0.5404, 0.5180,
        0.5435, 0.5534], device='cuda:0')
Thumb force: tensor([0.9041, 0.5016, 0.9671, 0.8839, 1.0307, 0.6781, 0.7282, 0.5731, 0.8511,
        0.6286, 1.0094], device='cuda:0')
Index force: tensor([0.7047, 0.9561, 0.6242, 0.5050, 0.5095, 0.5003, 0.5496, 0.5735, 0.5352,
        0.5729, 0.5512], device='cuda:0')
Storing NORMAL transition: reward=0.1142 (scaled=0.1142), steps=1
Reward stats updated: mean 0.0073 -> 0.0078, std: 0.0788
Collected 211 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2111, Q2 Loss=1.2111, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7700
SAC Update 2/5: Actor Loss=-0.0962, Q1 Loss=0.9876, Q2 Loss=0.9876, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9572
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7102, Q2 Loss=0.7102, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5299
SAC Update 4/5: Actor Loss=-0.2185, Q1 Loss=2.1892, Q2 Loss=2.1892, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8652
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6527, Q2 Loss=1.6527, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3431

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.0%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (20.1%)
Actor update: 0.11s (42.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.062938
Q1 loss: 1.350163
Q2 loss: 1.350163
Current threshold: -33.1894
Global Scale Offset: 0.0838
Reward stats: mean=0.0078, std=0.0788, count=211
----------------------------------------------
SAC Update - Actor Loss: -0.0629, Q1 Loss: 1.3502, Q2 Loss: 1.3502, Entropy: 0.0000, Mean TD Error: 1.2931, Threshold: -33.1894
tensor([ 0.0604,  0.5308,  0.5259,  0.5625, -0.0668,  0.4815,  0.8092,  1.0107,
         1.3770,  0.0801,  0.1820,  1.2320,  0.0165,  0.0158, -0.1026,  0.5324],
       device='cuda:0')
Original likelihood: -15.67675495147705
Adjusted likelihood: -15.67675495147705
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.1302069470402785
Current ori: tensor([ 0.0165,  0.0158, -0.1026], device='cuda:0')
Middle force: tensor([1.3786, 0.5040, 0.5422, 0.6169, 0.5466, 0.5149, 0.5185, 0.5440, 0.5923,
        0.5647], device='cuda:0')
Thumb force: tensor([1.5630, 0.5322, 1.6817, 0.8193, 0.5301, 0.6218, 0.5819, 0.5897, 0.5852,
        0.5867], device='cuda:0')
Index force: tensor([0.5023, 0.7190, 0.8078, 0.5335, 0.5591, 0.7249, 0.5262, 0.5940, 0.6120,
        0.5951], device='cuda:0')
Storing NORMAL transition: reward=0.1773 (scaled=0.1773), steps=1
Reward stats updated: mean 0.0078 -> 0.0086, std: 0.0795
Collected 212 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9910, Q2 Loss=0.9910, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9637
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2161, Q2 Loss=1.2161, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1060
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7233, Q2 Loss=0.7233, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4075
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2405, Q2 Loss=1.2405, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2681
SAC Update 5/5: Actor Loss=-0.0042, Q1 Loss=1.3082, Q2 Loss=1.3082, Entropy=0.3261, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7723

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.0%)
Q1 update: 0.04s (18.3%)
Q2 update: 0.04s (18.6%)
Actor update: 0.09s (41.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000848
Q1 loss: 1.095820
Q2 loss: 1.095820
Current threshold: -33.1921
Global Scale Offset: 0.0838
Reward stats: mean=0.0086, std=0.0795, count=212
----------------------------------------------
SAC Update - Actor Loss: -0.0008, Q1 Loss: 1.0958, Q2 Loss: 1.0958, Entropy: 0.0652, Mean TD Error: 0.9035, Threshold: -33.1921
tensor([ 0.0768,  0.4951,  0.6297,  0.6513, -0.0213,  0.4170,  0.9367,  1.0024,
         1.3673,  0.0805,  0.1571,  1.1838,  0.0163, -0.0125, -0.2801,  0.9338],
       device='cuda:0')
Original likelihood: -16.736125946044922
Adjusted likelihood: -16.736125946044922
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.679597633017693
Current ori: tensor([ 0.0163, -0.0125, -0.2801], device='cuda:0')
Middle force: tensor([0.5038, 0.5403, 0.6159, 0.5447, 0.5188, 0.5196, 0.5496, 0.5934, 0.5633],
       device='cuda:0')
Thumb force: tensor([0.5282, 1.6363, 0.7971, 0.5278, 0.6028, 0.5699, 0.5760, 0.5754, 0.5794],
       device='cuda:0')
Index force: tensor([0.6992, 0.7961, 0.5300, 0.5556, 0.6975, 0.5226, 0.5828, 0.6066, 0.5908],
       device='cuda:0')
Storing NORMAL transition: reward=0.0247 (scaled=0.0247), steps=1
Reward stats updated: mean 0.0086 -> 0.0087, std: 0.0793
Collected 213 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0224, Q2 Loss=1.0224, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5421
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.1152, Q2 Loss=2.1152, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1350
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8973, Q2 Loss=0.8973, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8942
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.8017, Q2 Loss=1.8017, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6091
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9282, Q2 Loss=0.9282, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6060

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.8%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.4%)
Actor update: 0.08s (40.1%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.352945
Q2 loss: 1.352945
Current threshold: -33.1960
Global Scale Offset: 0.0838
Reward stats: mean=0.0087, std=0.0793, count=213
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.3529, Q2 Loss: 1.3529, Entropy: 0.0000, Mean TD Error: 1.1573, Threshold: -33.1960
tensor([ 0.0681,  0.5200,  0.5941,  0.6377, -0.0317,  0.4350,  0.9013,  1.0210,
         1.3016,  0.2223,  0.2097,  1.1051,  0.0101, -0.0076, -0.3044,  0.9037],
       device='cuda:0')
Original likelihood: -13.199682235717773
Adjusted likelihood: -13.199682235717773
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.406754996976815
Current ori: tensor([ 0.0101, -0.0076, -0.3044], device='cuda:0')
Middle force: tensor([0.5340, 0.6093, 0.5424, 0.5178, 0.5178, 0.5469, 0.5881, 0.5596],
       device='cuda:0')
Thumb force: tensor([1.6010, 0.7825, 0.5257, 0.5955, 0.5644, 0.5705, 0.5714, 0.5749],
       device='cuda:0')
Index force: tensor([0.7824, 0.5276, 0.5521, 0.6880, 0.5204, 0.5773, 0.6018, 0.5867],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0020 (scaled=-0.0020), steps=1
Reward stats updated: mean 0.0087 -> 0.0087, std: 0.0791
Collected 214 transitions for RL
SAC Update 1/5: Actor Loss=-0.0033, Q1 Loss=1.1164, Q2 Loss=1.1164, Entropy=0.3278, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6634
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.1827, Q2 Loss=1.1827, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2394
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.8396, Q2 Loss=1.8396, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4882
SAC Update 4/5: Actor Loss=-0.2071, Q1 Loss=2.0464, Q2 Loss=2.0464, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7865
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0751, Q2 Loss=1.0751, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7174

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (21.7%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (18.1%)
Actor update: 0.08s (37.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.042090
Q1 loss: 1.452064
Q2 loss: 1.452064
Current threshold: -33.2006
Global Scale Offset: 0.0838
Reward stats: mean=0.0087, std=0.0791, count=214
----------------------------------------------
SAC Update - Actor Loss: -0.0421, Q1 Loss: 1.4521, Q2 Loss: 1.4521, Entropy: 0.0656, Mean TD Error: 1.1790, Threshold: -33.2006
tensor([ 0.0620,  0.4852,  0.6222,  0.5715, -0.0483,  0.4885,  0.8538,  0.9769,
         1.3048,  0.1328,  0.1899,  1.0481,  0.0283, -0.0187, -0.3044, -0.3862],
       device='cuda:0')
Original likelihood: -19.785429000854492
Adjusted likelihood: -19.785429000854492
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.386682442040183
Current ori: tensor([ 0.0283, -0.0187, -0.3044], device='cuda:0')
Middle force: tensor([0.5022, 0.5726, 0.5129, 0.5560, 0.6434, 1.2282, 0.5420],
       device='cuda:0')
Thumb force: tensor([0.8535, 0.9909, 0.5278, 0.5357, 0.9915, 0.5305, 0.5644],
       device='cuda:0')
Index force: tensor([0.5762, 0.6127, 0.6181, 0.6082, 0.5261, 0.5107, 0.5717],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0170 (scaled=-0.0170), steps=1
Reward stats updated: mean 0.0087 -> 0.0085, std: 0.0789
Collected 215 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1268, Q2 Loss=1.1268, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7276
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.6785, Q2 Loss=0.6785, Entropy=0.0125, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1380
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.0536, Q2 Loss=1.0536, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0862
SAC Update 4/5: Actor Loss=-0.0046, Q1 Loss=1.2495, Q2 Loss=1.2495, Entropy=0.3299, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3716
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0731, Q2 Loss=1.0731, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0096

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.04s (17.7%)
Actor update: 0.10s (39.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000937
Q1 loss: 1.036304
Q2 loss: 1.036304
Current threshold: -33.2049
Global Scale Offset: 0.0838
Reward stats: mean=0.0085, std=0.0789, count=215
----------------------------------------------
SAC Update - Actor Loss: -0.0009, Q1 Loss: 1.0363, Q2 Loss: 1.0363, Entropy: 0.0685, Mean TD Error: 1.2666, Threshold: -33.2049
tensor([ 0.0147,  0.5437,  0.5551,  0.5375, -0.0285,  0.5045,  0.8523,  0.9347,
         1.3332,  0.0999,  0.2713,  0.7906,  0.0276, -0.0120, -0.2868, -0.4548],
       device='cuda:0')
Original likelihood: -20.253604888916016
Adjusted likelihood: -20.253604888916016
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.366025245049968
Current ori: tensor([ 0.0276, -0.0120, -0.2868], device='cuda:0')
Middle force: tensor([0.5371, 0.5202, 0.5166, 0.5508, 0.5931, 0.5569], device='cuda:0')
Thumb force: tensor([0.5201, 0.5741, 0.5486, 0.5544, 0.5557, 0.5603], device='cuda:0')
Index force: tensor([0.5438, 0.6503, 0.5141, 0.5577, 0.5842, 0.5758], device='cuda:0')
Storing NORMAL transition: reward=-0.0340 (scaled=-0.0340), steps=1
Reward stats updated: mean 0.0085 -> 0.0083, std: 0.0788
Collected 216 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8917, Q2 Loss=0.8917, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2253
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=4.6270, Q2 Loss=4.6270, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6188
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2485, Q2 Loss=1.2485, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0530
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2818, Q2 Loss=1.2818, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9811
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3029, Q2 Loss=1.3029, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3802

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.7%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (17.9%)
Actor update: 0.09s (41.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.870357
Q2 loss: 1.870357
Current threshold: -33.2092
Global Scale Offset: 0.0839
Reward stats: mean=0.0083, std=0.0788, count=216
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.8704, Q2 Loss: 1.8704, Entropy: 0.0000, Mean TD Error: 1.8517, Threshold: -33.2092
tensor([ 0.0087,  0.6217,  0.4666,  0.4698, -0.0179,  0.5459,  0.7992,  0.9454,
         1.2771,  0.2048,  0.3019,  0.8456,  0.0203, -0.0188, -0.2523, -0.2989],
       device='cuda:0')
Original likelihood: -20.79834747314453
Adjusted likelihood: -20.79834747314453
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.186122275015805
Current ori: tensor([ 0.0203, -0.0188, -0.2523], device='cuda:0')
Middle force: tensor([0.5212, 0.5165, 0.5515, 0.5903, 0.5534], device='cuda:0')
Thumb force: tensor([0.5653, 0.5427, 0.5490, 0.5514, 0.5552], device='cuda:0')
Index force: tensor([0.6310, 0.5107, 0.5490, 0.5779, 0.5712], device='cuda:0')
Storing NORMAL transition: reward=-0.0050 (scaled=-0.0050), steps=1
Reward stats updated: mean 0.0083 -> 0.0083, std: 0.0786
Collected 217 transitions for RL
SAC Update 1/5: Actor Loss=-0.0001, Q1 Loss=1.1765, Q2 Loss=1.1765, Entropy=0.0265, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0593
SAC Update 2/5: Actor Loss=-0.0119, Q1 Loss=0.6378, Q2 Loss=0.6378, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9918
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=3.4681, Q2 Loss=3.4681, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0311
SAC Update 4/5: Actor Loss=-0.1493, Q1 Loss=1.8922, Q2 Loss=1.8922, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9567
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2165, Q2 Loss=1.2165, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1059

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (16.6%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (18.9%)
Actor update: 0.11s (38.1%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.032257
Q1 loss: 1.678218
Q2 loss: 1.678218
Current threshold: -33.2116
Global Scale Offset: 0.0839
Reward stats: mean=0.0083, std=0.0786, count=217
----------------------------------------------
SAC Update - Actor Loss: -0.0323, Q1 Loss: 1.6782, Q2 Loss: 1.6782, Entropy: 0.0053, Mean TD Error: 1.4290, Threshold: -33.2116
tensor([ 0.0916,  0.6221,  0.5388,  0.4964, -0.1684,  0.6082,  0.8481,  0.8795,
         1.3082,  0.2206,  0.2401,  0.8166,  0.0169, -0.0596, -0.2523, -0.3211],
       device='cuda:0')
Original likelihood: -33.87262725830078
Adjusted likelihood: -33.87262725830078
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0175)
State is out of distribution
Projection step: 0, Loss: 35.914344787597656
Projection step: 1, Loss: 34.87725830078125
Projection step: 2, Loss: 34.13542175292969
Projection step: 3, Loss: 32.276100158691406
Projection step: 4, Loss: 32.23708724975586
Projection step: 5, Loss: 31.005237579345703
Projection step: 6, Loss: 32.02424621582031
Projection step: 7, Loss: 30.07554054260254
Projection step: 8, Loss: 28.876371383666992
Projection step: 9, Loss: 29.280689239501953
Projection step: 10, Loss: 27.790571212768555
Projection step: 11, Loss: 28.032691955566406
Projection step: 12, Loss: 27.719154357910156
Projection step: 13, Loss: 28.43305206298828
Projection step: 14, Loss: 26.87110137939453
Projection step: 15, Loss: 27.17994499206543
Projection step: 16, Loss: 26.078800201416016
Projection step: 17, Loss: 25.37650489807129
Projection step: 18, Loss: 24.434202194213867
Projection step: 19, Loss: 24.363414764404297
Projection step: 20, Loss: 24.61618423461914
Projection step: 21, Loss: 26.194324493408203
Projection step: 22, Loss: 23.625534057617188
Projection step: 23, Loss: 24.33997344970703
Projection step: 24, Loss: 23.88882064819336
Final likelihood: tensor([-20.5492, -21.3032, -22.0461, -23.1775, -21.4267, -24.8099, -25.3820,
        -24.8309, -23.9647, -23.9542, -23.5817, -24.0798, -24.5205, -23.3327,
        -20.2360, -23.7397])
Final projection likelihood: -23.1834
1 mode projection succeeded
New goal: tensor([ 0.0671,  0.5677,  0.5954,  0.5114, -0.0942,  0.5900,  0.8850,  0.9042,
         1.3583,  0.1878,  0.2329,  0.9281,  0.0143, -0.0514, -0.5904],
       device='cuda:0')
tensor([[0.0058]], device='cuda:0') tensor([[0.0146]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -22.825363159179688
Adjusted likelihood: -22.825363159179688
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 22.825363159179688}
Current yaw: tensor([ 0.0169, -0.0596, -0.2523], device='cuda:0')
2 thumb_middle
tensor([ 0.0916,  0.6221,  0.5388,  0.4964, -0.1684,  0.6082,  0.8481,  0.8795,
         1.3082,  0.2206,  0.2401,  0.8166,  0.0169, -0.0596, -0.2523, -0.3211],
       device='cuda:0')
Solve time for step 1 8.624451495008543
Current ori: tensor([ 0.0169, -0.0596, -0.2523], device='cuda:0')
Index force: tensor([0.5601, 0.5039, 0.5722, 0.5786], device='cuda:0')
tensor([ 0.0848,  0.5413,  0.6048,  0.5691, -0.1701,  0.6031,  0.8691,  0.8877,
         1.3050,  0.1833,  0.1509,  0.8643,  0.0381, -0.0535, -0.2523, -0.3379],
       device='cuda:0')
Solve time for step 2 3.566981554031372
Current ori: tensor([ 0.0381, -0.0535, -0.2523], device='cuda:0')
Index force: tensor([0.5030, 0.5604, 0.5670], device='cuda:0')
tensor([ 0.0830,  0.5450,  0.6088,  0.5467, -0.1545,  0.5904,  0.8594,  0.8978,
         1.3150,  0.1784,  0.1277,  0.8884,  0.0359, -0.0534, -0.2523, -0.3327],
       device='cuda:0')
Solve time for step 3 3.225821171014104
Current ori: tensor([ 0.0359, -0.0534, -0.2523], device='cuda:0')
Index force: tensor([0.5494, 0.5575], device='cuda:0')
tensor([ 0.0678,  0.5710,  0.5815,  0.5041, -0.1594,  0.5981,  0.8514,  0.8854,
         1.3098,  0.1577,  0.1298,  0.8950,  0.0223, -0.0471, -0.2523, -0.3925],
       device='cuda:0')
Solve time for step 4 3.284909170004539
Current ori: tensor([ 0.0223, -0.0471, -0.2523], device='cuda:0')
Index force: tensor([0.5714], device='cuda:0')
Storing RECOVERY transition: reward=0.0025 (scaled=0.0003), steps=8
Reward stats updated: mean 0.0083 -> 0.0082, std: 0.0784
Collected 218 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.8022, Q2 Loss=1.8022, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6981
SAC Update 2/5: Actor Loss=-0.2182, Q1 Loss=1.7901, Q2 Loss=1.7901, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4835
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3720, Q2 Loss=1.3720, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1770
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=2.8570, Q2 Loss=2.8570, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0514
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.8389, Q2 Loss=1.8389, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5005

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.2%)
Q1 update: 0.06s (20.7%)
Q2 update: 0.06s (20.7%)
Actor update: 0.11s (41.0%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.043637
Q1 loss: 1.932022
Q2 loss: 1.932022
Current threshold: -33.2131
Global Scale Offset: 0.0839
Reward stats: mean=0.0082, std=0.0784, count=218
----------------------------------------------
SAC Update - Actor Loss: -0.0436, Q1 Loss: 1.9320, Q2 Loss: 1.9320, Entropy: 0.0000, Mean TD Error: 1.7821, Threshold: -33.2131
Original likelihood: -25.55051040649414
Adjusted likelihood: -25.55051040649414
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0247, -0.0478, -0.2533], device='cuda:0')
3 turn
Sampling time 3.661594237026293
tensor([ 0.0668,  0.5708,  0.5821,  0.5005, -0.0990,  0.6497,  0.8988,  0.9087,
         1.3680,  0.1823,  0.1937,  0.9245,  0.0247, -0.0478, -0.2533, -0.3682],
       device='cuda:0')
Original likelihood: -25.97491455078125
Adjusted likelihood: -25.97491455078125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.391823361977004
Current ori: tensor([ 0.0247, -0.0478, -0.2533], device='cuda:0')
Middle force: tensor([0.8724, 2.2717, 0.5490, 1.3938, 0.7242, 1.0449, 0.5868, 0.8794, 0.5577,
        0.7459, 0.6435, 0.7322], device='cuda:0')
Thumb force: tensor([0.6153, 0.9327, 1.5116, 0.5202, 0.5348, 0.6231, 0.5070, 0.6019, 1.1449,
        0.6001, 0.5550, 0.5749], device='cuda:0')
Index force: tensor([0.6133, 1.5355, 0.6305, 0.6763, 0.6653, 0.6936, 0.5855, 0.7528, 0.5190,
        0.6970, 0.5463, 0.5458], device='cuda:0')
Storing NORMAL transition: reward=-0.0490 (scaled=-0.0490), steps=1
Reward stats updated: mean 0.0082 -> 0.0080, std: 0.0784
Collected 219 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1695, Q2 Loss=1.1695, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0463
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=2.0155, Q2 Loss=2.0155, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9336
SAC Update 3/5: Actor Loss=-0.0030, Q1 Loss=0.7107, Q2 Loss=0.7107, Entropy=0.2863, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1572
SAC Update 4/5: Actor Loss=-0.0038, Q1 Loss=1.1633, Q2 Loss=1.1633, Entropy=0.3335, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6873
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=3.5371, Q2 Loss=3.5371, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1849

------ SAC Update Summary (5 iterations) ------
Total time: 0.32s, Avg iteration: 0.06s
Sampling: 0.00s (0.4%)
Target Q: 0.09s (27.2%)
Q1 update: 0.05s (16.6%)
Q2 update: 0.05s (17.3%)
Actor update: 0.11s (36.2%)
Target update: 0.00s (1.1%)
Priority update: 0.00s (0.1%)
Actor loss: -0.001355
Q1 loss: 1.719195
Q2 loss: 1.719195
Current threshold: -33.2167
Global Scale Offset: 0.0839
Reward stats: mean=0.0080, std=0.0784, count=219
----------------------------------------------
SAC Update - Actor Loss: -0.0014, Q1 Loss: 1.7192, Q2 Loss: 1.7192, Entropy: 0.1240, Mean TD Error: 1.8019, Threshold: -33.2167
tensor([ 0.0255,  0.6105,  0.4652,  0.5307, -0.1382,  0.7199,  0.8714,  0.8787,
         1.3512,  0.1786,  0.2976,  0.7406,  0.0120, -0.0471, -0.2037, -0.4895],
       device='cuda:0')
Original likelihood: -33.40049362182617
Adjusted likelihood: -33.40049362182617
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.2790)
State is out of distribution
Projection step: 0, Loss: 29.743064880371094
Projection step: 1, Loss: 29.10323715209961
Projection step: 2, Loss: 27.54808807373047
Projection step: 3, Loss: 31.32027816772461
Projection step: 4, Loss: 28.724994659423828
Projection step: 5, Loss: 29.398069381713867
Projection step: 6, Loss: 29.011131286621094
Projection step: 7, Loss: 28.976974487304688
Projection step: 8, Loss: 26.771835327148438
Projection step: 9, Loss: 26.63362693786621
Projection step: 10, Loss: 27.34920883178711
Projection step: 11, Loss: 25.000499725341797
Projection step: 12, Loss: 26.954254150390625
Projection step: 13, Loss: 24.43665313720703
Projection step: 14, Loss: 25.512493133544922
Projection step: 15, Loss: 23.86501693725586
Projection step: 16, Loss: 24.851104736328125
Projection step: 17, Loss: 23.4388484954834
Projection step: 18, Loss: 23.679832458496094
Projection step: 19, Loss: 23.78236961364746
Projection step: 20, Loss: 23.36227035522461
Projection step: 21, Loss: 22.582996368408203
Projection step: 22, Loss: 22.35796356201172
Projection step: 23, Loss: 21.88461685180664
Projection step: 24, Loss: 20.817752838134766
Final likelihood: tensor([-21.9263, -19.8896, -22.3218, -20.6999, -19.8822, -27.2620, -21.2458,
        -21.6876, -20.2172, -23.7425, -21.8666, -19.2310, -20.3048, -21.9572,
        -22.5776, -20.3301])
Final projection likelihood: -21.5714
1 mode projection succeeded
New goal: tensor([ 0.0337,  0.5569,  0.5635,  0.5460, -0.0837,  0.6295,  0.8436,  0.8622,
         1.3745,  0.1569,  0.2769,  0.8530,  0.0096, -0.0408, -0.4511],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -23.18899154663086
Adjusted likelihood: -23.18899154663086
Likelihood residual: 0.0
Original likelihood: -27.316940307617188
Adjusted likelihood: -27.316940307617188
Likelihood residual: 0.0
{'index': 27.316940307617188, 'thumb_middle': 23.18899154663086}
Current yaw: tensor([ 0.0120, -0.0471, -0.2037], device='cuda:0')
4 thumb_middle
tensor([ 0.0255,  0.6105,  0.4652,  0.5307, -0.1382,  0.7199,  0.8714,  0.8787,
         1.3512,  0.1786,  0.2976,  0.7406,  0.0120, -0.0471, -0.2037, -0.4895],
       device='cuda:0')
Solve time for step 1 9.07497630896978
Current ori: tensor([ 0.0120, -0.0471, -0.2037], device='cuda:0')
Index force: tensor([0.5547, 0.6000, 0.5995, 0.6060], device='cuda:0')
tensor([ 0.0261,  0.5908,  0.5121,  0.4984, -0.1611,  0.6332,  0.8198,  0.8485,
         1.3187,  0.1390,  0.1846,  0.7881,  0.0198, -0.0504, -0.2037, -0.4084],
       device='cuda:0')
Solve time for step 2 3.6873684329912066
Current ori: tensor([ 0.0198, -0.0504, -0.2037], device='cuda:0')
Index force: tensor([0.5867, 0.5886, 0.5919], device='cuda:0')
tensor([ 8.4734e-04,  5.8456e-01,  5.8008e-01,  5.5552e-01, -1.5518e-01,
         6.3292e-01,  8.2129e-01,  8.4240e-01,  1.3284e+00,  1.3648e-01,
         1.7424e-01,  8.0332e-01,  5.3134e-02, -1.1959e-01, -2.0301e-01,
        -5.4247e-01], device='cuda:0')
Solve time for step 3 3.5090270930086263
Current ori: tensor([ 0.0531, -0.1196, -0.2030], device='cuda:0')
Index force: tensor([0.5757, 0.5808], device='cuda:0')
tensor([-0.0366,  0.6341,  0.6112,  0.5663, -0.1027,  0.6643,  0.8493,  0.8565,
         1.3092,  0.1327,  0.1398,  0.7876,  0.1339, -0.2972, -0.2057, -1.0775],
       device='cuda:0')
Solve time for step 4 3.3269633399904706
Current ori: tensor([ 0.1339, -0.2972, -0.2057], device='cuda:0')
Index force: tensor([0.5002], device='cuda:0')
Storing RECOVERY transition: reward=0.0159 (scaled=0.0159), steps=1
Reward stats updated: mean 0.0080 -> 0.0080, std: 0.0782
Collected 220 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4949, Q2 Loss=1.4949, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4867
SAC Update 2/5: Actor Loss=-0.0044, Q1 Loss=0.8482, Q2 Loss=0.8482, Entropy=0.3257, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4504
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8678, Q2 Loss=0.8678, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2110
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.8100, Q2 Loss=1.8100, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6380
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2114, Q2 Loss=1.2114, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7922

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.7%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (18.8%)
Actor update: 0.09s (40.9%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000882
Q1 loss: 1.246466
Q2 loss: 1.246466
Current threshold: -33.2238
Global Scale Offset: 0.0839
Reward stats: mean=0.0080, std=0.0782, count=220
----------------------------------------------
SAC Update - Actor Loss: -0.0009, Q1 Loss: 1.2465, Q2 Loss: 1.2465, Entropy: 0.0651, Mean TD Error: 1.5157, Threshold: -33.2238
Original likelihood: -218.1351318359375
Adjusted likelihood: -218.1351318359375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 233.50326538085938
Projection step: 1, Loss: 215.2683563232422
Projection step: 2, Loss: 231.5601043701172
Projection step: 3, Loss: 205.36703491210938
Projection step: 4, Loss: 210.80825805664062
Projection step: 5, Loss: 214.53802490234375
Projection step: 6, Loss: 221.81881713867188
Projection step: 7, Loss: 217.16470336914062
Projection step: 8, Loss: 226.8118896484375
Projection step: 9, Loss: 215.87533569335938
Projection step: 10, Loss: 245.77783203125
Projection step: 11, Loss: 213.88504028320312
Projection step: 12, Loss: 211.44284057617188
Projection step: 13, Loss: 240.30996704101562
Projection step: 14, Loss: 228.20663452148438
Projection step: 15, Loss: 229.64248657226562
Projection step: 16, Loss: 218.67214965820312
Projection step: 17, Loss: 215.6981201171875
Projection step: 18, Loss: 234.80535888671875
Projection step: 19, Loss: 218.48574829101562
Projection step: 20, Loss: 223.2506103515625
Projection step: 21, Loss: 229.11045837402344
Projection step: 22, Loss: 214.00511169433594
Projection step: 23, Loss: 214.95367431640625
Projection step: 24, Loss: 235.65298461914062
Final likelihood: tensor([-241.2524, -235.1343, -104.9802, -183.6662, -247.5024, -209.1725,
        -192.6633, -278.4679, -187.1850, -109.0918, -306.5583, -230.3658,
        -147.7992, -178.3428, -266.5307, -157.8109])
Final projection likelihood: -204.7827
1 mode projection failed, trying anyway
New goal: tensor([-0.0476,  0.7777,  0.6468,  0.5555,  0.1116,  0.8441,  0.8580,  0.8168,
         1.3183,  0.1507,  0.2118,  0.8404,  0.2003, -0.2951, -0.3101],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0236]], device='cuda:0') tensor([[0.0239]], device='cuda:0')
Original likelihood: -66.74990844726562
Adjusted likelihood: -66.74990844726562
Likelihood residual: 0.0
Original likelihood: -76.1636734008789
Adjusted likelihood: -76.1636734008789
Likelihood residual: 0.0
{'index': 76.1636734008789, 'thumb_middle': 66.74990844726562}
Current yaw: tensor([ 0.2008, -0.2946, -0.3458], device='cuda:0')
5 thumb_middle
tensor([-0.0551,  0.7865,  0.6477,  0.5495,  0.1099,  0.8438,  0.8630,  0.8066,
         1.3355,  0.1506,  0.2143,  0.8362,  0.2008, -0.2946, -0.3458, -2.3193],
       device='cuda:0')
Solve time for step 1 9.220638175029308
Current ori: tensor([ 0.2008, -0.2946, -0.3458], device='cuda:0')
Index force: tensor([0.5720, 0.5539, 0.5428, 0.5664], device='cuda:0')
tensor([-0.0680,  0.9372,  0.6530,  0.5525,  0.0965,  0.9374,  0.8949,  0.8211,
         1.3102,  0.1721,  0.1890,  0.8385,  0.2466, -0.3148, -0.4827, -2.3581],
       device='cuda:0')
Solve time for step 2 3.696909873979166
Current ori: tensor([ 0.2466, -0.3148, -0.4827], device='cuda:0')
Index force: tensor([0.5812, 0.5330, 0.5882], device='cuda:0')
tensor([-0.0691,  1.0303,  0.6400,  0.5655,  0.0864,  1.0001,  0.9167,  0.8326,
         1.2601,  0.1907,  0.1868,  0.8556,  0.3595, -0.3294, -0.6111, -1.1808],
       device='cuda:0')
Solve time for step 3 3.5034465119824745
Current ori: tensor([ 0.3595, -0.3294, -0.6111], device='cuda:0')
Index force: tensor([0.5317, 0.5543], device='cuda:0')
tensor([-0.0726,  1.0209,  0.6560,  0.5546,  0.0842,  0.9916,  0.9281,  0.8327,
         1.2240,  0.1872,  0.1495,  0.8270,  0.3780, -0.3448, -0.5364, -1.0033],
       device='cuda:0')
Solve time for step 4 3.2867465860326774
Current ori: tensor([ 0.3780, -0.3448, -0.5364], device='cuda:0')
Index force: tensor([0.5482], device='cuda:0')
Storing RECOVERY transition: reward=0.0627 (scaled=0.0627), steps=1
Reward stats updated: mean 0.0080 -> 0.0083, std: 0.0781
Collected 221 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.1168, Q2 Loss=1.1168, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4208
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.3410, Q2 Loss=2.3410, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1082
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.8392, Q2 Loss=1.8392, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6572
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1180, Q2 Loss=1.1180, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1271
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.9701, Q2 Loss=1.9701, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2062

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.4%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (41.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.138155
Q1 loss: 1.677008
Q2 loss: 1.677008
Current threshold: -33.2285
Global Scale Offset: 0.0839
Reward stats: mean=0.0083, std=0.0781, count=221
----------------------------------------------
SAC Update - Actor Loss: -0.1382, Q1 Loss: 1.6770, Q2 Loss: 1.6770, Entropy: 0.0000, Mean TD Error: 1.7039, Threshold: -33.2285
Original likelihood: -290.19830322265625
Adjusted likelihood: -290.19830322265625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 293.98175048828125
Projection step: 1, Loss: 290.7900390625
Projection step: 2, Loss: 299.5066223144531
Projection step: 3, Loss: 299.5904541015625
Projection step: 4, Loss: 298.22857666015625
Projection step: 5, Loss: 298.13848876953125
Projection step: 6, Loss: 287.2196960449219
Projection step: 7, Loss: 320.08404541015625
Projection step: 8, Loss: 299.38323974609375
Projection step: 9, Loss: 305.6882629394531
Projection step: 10, Loss: 309.41644287109375
Projection step: 11, Loss: 285.51275634765625
Projection step: 12, Loss: 310.486328125
Projection step: 13, Loss: 289.89337158203125
Projection step: 14, Loss: 302.0838317871094
Projection step: 15, Loss: 307.67041015625
Projection step: 16, Loss: 315.896240234375
Projection step: 17, Loss: 298.2607727050781
Projection step: 18, Loss: 302.30853271484375
Projection step: 19, Loss: 290.73541259765625
Projection step: 20, Loss: 309.6996765136719
Projection step: 21, Loss: 296.3750915527344
Projection step: 22, Loss: 291.12506103515625
Projection step: 23, Loss: 305.1341552734375
Projection step: 24, Loss: 296.34576416015625
Final likelihood: tensor([-275.0290, -272.5590, -261.8278, -264.4970, -376.1551, -348.5724,
        -311.5438, -326.9155, -300.0721, -288.3064, -326.9105, -362.5783,
        -331.9751, -270.2167, -303.2897, -324.4139])
Final projection likelihood: -309.0539
1 mode projection failed, trying anyway
New goal: tensor([ 0.0103,  0.9947,  0.7521,  0.5769,  0.1855,  1.0168,  1.0055,  0.8733,
         1.2200,  0.1628,  0.2672,  0.7274,  0.3714, -0.3273, -0.6319],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0147]], device='cuda:0') tensor([[0.0058]], device='cuda:0')
Original likelihood: -254.59866333007812
Adjusted likelihood: -254.59866333007812
Likelihood residual: 0.0
Original likelihood: -292.40631103515625
Adjusted likelihood: -292.40631103515625
Likelihood residual: 0.0
{'index': 292.40631103515625, 'thumb_middle': 254.59866333007812}
Current yaw: tensor([ 0.3713, -0.3269, -0.6426], device='cuda:0')
6 thumb_middle
tensor([ 0.0057,  0.9954,  0.7471,  0.5747,  0.1851,  1.0154,  1.0100,  0.8685,
         1.2272,  0.1608,  0.2679,  0.7238,  0.3713, -0.3269, -0.6426, -1.5381],
       device='cuda:0')
Solve time for step 1 8.956267888017464
Current ori: tensor([ 0.3713, -0.3269, -0.6426], device='cuda:0')
Index force: tensor([0.5868, 0.5889, 0.5932, 0.5838], device='cuda:0')
tensor([-0.0130,  1.0424,  0.7391,  0.5818,  0.1618,  1.0577,  1.0127,  0.8685,
         1.2012,  0.1914,  0.1688,  0.7116,  0.3799, -0.3503, -0.5458, -1.5883],
       device='cuda:0')
Solve time for step 2 3.5891271659638733
Current ori: tensor([ 0.3799, -0.3503, -0.5458], device='cuda:0')
Index force: tensor([0.5805, 0.5854, 0.5767], device='cuda:0')
tensor([-0.0284,  1.0403,  0.7356,  0.5568,  0.1466,  1.0531,  1.0164,  0.8621,
         1.1973,  0.1747,  0.1670,  0.7092,  0.3802, -0.3511, -0.5412, -1.2409],
       device='cuda:0')
Solve time for step 3 3.4426103519508615
Current ori: tensor([ 0.3802, -0.3511, -0.5412], device='cuda:0')
Index force: tensor([0.5906, 0.5819], device='cuda:0')
tensor([-0.0407,  1.0453,  0.7348,  0.5606,  0.1346,  1.0485,  1.0331,  0.8734,
         1.1983,  0.1696,  0.1646,  0.7119,  0.3800, -0.3507, -0.5430, -1.3867],
       device='cuda:0')
Solve time for step 4 3.3259001019760035
Current ori: tensor([ 0.3800, -0.3507, -0.5430], device='cuda:0')
Index force: tensor([0.5686], device='cuda:0')
Storing RECOVERY transition: reward=0.0763 (scaled=0.0763), steps=1
Reward stats updated: mean 0.0083 -> 0.0086, std: 0.0781
Collected 222 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1590, Q2 Loss=1.1590, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7642
SAC Update 2/5: Actor Loss=-0.0034, Q1 Loss=3.9795, Q2 Loss=3.9795, Entropy=0.3022, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5107
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.8177, Q2 Loss=0.8177, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8336
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5278, Q2 Loss=1.5278, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2731
SAC Update 5/5: Actor Loss=-0.2078, Q1 Loss=1.2107, Q2 Loss=1.2107, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5310

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.8%)
Q1 update: 0.04s (19.8%)
Q2 update: 0.04s (19.3%)
Actor update: 0.09s (39.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.042244
Q1 loss: 1.738921
Q2 loss: 1.738921
Current threshold: -33.2335
Global Scale Offset: 0.0840
Reward stats: mean=0.0086, std=0.0781, count=222
----------------------------------------------
SAC Update - Actor Loss: -0.0422, Q1 Loss: 1.7389, Q2 Loss: 1.7389, Entropy: 0.0604, Mean TD Error: 1.1825, Threshold: -33.2335
Original likelihood: -300.3893127441406
Adjusted likelihood: -300.3893127441406
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 14
Loaded trajectory sampler
Current yaw: tensor([-0.0019,  0.0146, -0.0309], device='cuda:0')
Current yaw: tensor([-0.0019,  0.0146, -0.0309], device='cuda:0')
1 turn
Sampling time 3.6748568580369465
tensor([ 0.1378,  0.6140,  0.5734,  0.5613, -0.1183,  0.5454,  0.8851,  0.9209,
         1.2120,  0.3173,  0.2583,  1.1852, -0.0019,  0.0146, -0.0309,  0.2367],
       device='cuda:0')
Original likelihood: -19.943092346191406
Adjusted likelihood: -19.943092346191406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.023463669000193
Current ori: tensor([-0.0019,  0.0146, -0.0309], device='cuda:0')
Middle force: tensor([0.5142, 0.4961, 1.3161, 1.4009, 0.5665, 0.5362, 0.5173, 0.5544, 0.5970,
        0.5928, 0.6045, 0.5803], device='cuda:0')
Thumb force: tensor([0.7419, 0.5998, 1.2043, 1.0673, 0.7427, 0.8884, 0.5404, 1.1893, 0.6048,
        0.5287, 0.5984, 0.5431], device='cuda:0')
Index force: tensor([0.5102, 0.6726, 0.6187, 0.5351, 0.9256, 0.5451, 0.5940, 0.4981, 0.6227,
        0.5919, 0.6183, 0.5844], device='cuda:0')
Storing NORMAL transition: reward=0.1043 (scaled=0.1043), steps=1
Reward stats updated: mean 0.0086 -> 0.0090, std: 0.0781
Collected 223 transitions for RL
SAC Update 1/5: Actor Loss=-0.1554, Q1 Loss=1.3786, Q2 Loss=1.3786, Entropy=0.0006, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2738
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.0875, Q2 Loss=2.0875, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0918
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.1518, Q2 Loss=2.1518, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2026
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9255, Q2 Loss=0.9255, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9029
SAC Update 5/5: Actor Loss=-0.1337, Q1 Loss=1.2002, Q2 Loss=1.2002, Entropy=0.0219, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0940

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.9%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (18.5%)
Actor update: 0.10s (39.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.057835
Q1 loss: 1.548697
Q2 loss: 1.548697
Current threshold: -33.2368
Global Scale Offset: 0.0840
Reward stats: mean=0.0090, std=0.0781, count=223
----------------------------------------------
SAC Update - Actor Loss: -0.0578, Q1 Loss: 1.5487, Q2 Loss: 1.5487, Entropy: 0.0045, Mean TD Error: 1.5130, Threshold: -33.2368
tensor([ 1.3145e-01,  5.5524e-01,  6.3356e-01,  5.8866e-01, -1.1134e-01,
         5.0606e-01,  8.6539e-01,  1.0021e+00,  1.4455e+00,  6.0512e-02,
         1.4010e-01,  1.0933e+00, -1.2082e-04,  1.4038e-02, -1.3527e-01,
         8.9459e-01], device='cuda:0')
Original likelihood: -20.17312240600586
Adjusted likelihood: -20.17312240600586
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.596935660985764
Current ori: tensor([-1.2082e-04,  1.4038e-02, -1.3527e-01], device='cuda:0')
Middle force: tensor([0.5025, 1.2904, 1.3707, 0.5544, 0.5336, 0.5157, 0.5531, 0.5923, 0.5882,
        0.5981, 0.5757], device='cuda:0')
Thumb force: tensor([0.5909, 1.1870, 1.0485, 0.7406, 0.8712, 0.5370, 1.1680, 0.5995, 0.5259,
        0.5933, 0.5403], device='cuda:0')
Index force: tensor([0.6622, 0.6200, 0.5335, 0.9186, 0.5420, 0.5901, 0.5008, 0.6171, 0.5869,
        0.6132, 0.5816], device='cuda:0')
Storing NORMAL transition: reward=0.0018 (scaled=0.0018), steps=1
Reward stats updated: mean 0.0090 -> 0.0090, std: 0.0780
Collected 224 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.4559, Q2 Loss=1.4559, Entropy=0.0006, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9836
SAC Update 2/5: Actor Loss=-0.2304, Q1 Loss=1.2495, Q2 Loss=1.2495, Entropy=0.0519, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8558
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4511, Q2 Loss=1.4511, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4928
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8898, Q2 Loss=0.8898, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7849
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.4639, Q2 Loss=1.4639, Entropy=0.0006, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7549

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.4%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (18.3%)
Actor update: 0.10s (39.6%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046073
Q1 loss: 1.302044
Q2 loss: 1.302044
Current threshold: -33.2389
Global Scale Offset: 0.0840
Reward stats: mean=0.0090, std=0.0780, count=224
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.3020, Q2 Loss: 1.3020, Entropy: 0.0106, Mean TD Error: 1.3744, Threshold: -33.2389
tensor([ 0.1492,  0.5924,  0.5938,  0.5495, -0.0682,  0.4565,  0.8434,  1.1271,
         1.3107,  0.2591,  0.2035,  1.1092,  0.0201,  0.0022, -0.1373, -0.7172],
       device='cuda:0')
Original likelihood: -17.820388793945312
Adjusted likelihood: -17.820388793945312
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.226014292973559
Current ori: tensor([ 0.0201,  0.0022, -0.1373], device='cuda:0')
Middle force: tensor([0.5055, 0.5277, 0.5211, 0.5442, 0.5085, 0.6059, 0.7019, 1.1465, 0.7820,
        0.5256], device='cuda:0')
Thumb force: tensor([0.5648, 0.6874, 0.5456, 0.5151, 1.0231, 0.5906, 0.7142, 0.6463, 0.7519,
        0.5695], device='cuda:0')
Index force: tensor([0.6561, 0.5263, 0.5990, 0.5674, 0.5500, 0.5817, 0.5483, 0.5479, 0.5420,
        0.5006], device='cuda:0')
Storing NORMAL transition: reward=0.0028 (scaled=0.0028), steps=1
Reward stats updated: mean 0.0090 -> 0.0089, std: 0.0778
Collected 225 transitions for RL
SAC Update 1/5: Actor Loss=-0.0016, Q1 Loss=1.1860, Q2 Loss=1.1860, Entropy=0.3068, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7962
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.6410, Q2 Loss=1.6410, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4593
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5697, Q2 Loss=1.5697, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3445
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2465, Q2 Loss=1.2465, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2912
SAC Update 5/5: Actor Loss=-0.2112, Q1 Loss=1.2313, Q2 Loss=1.2313, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5456

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (17.9%)
Q1 update: 0.05s (19.0%)
Q2 update: 0.05s (18.3%)
Actor update: 0.11s (41.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.088615
Q1 loss: 1.374887
Q2 loss: 1.374887
Current threshold: -33.2413
Global Scale Offset: 0.0840
Reward stats: mean=0.0089, std=0.0778, count=225
----------------------------------------------
SAC Update - Actor Loss: -0.0886, Q1 Loss: 1.3749, Q2 Loss: 1.3749, Entropy: 0.0614, Mean TD Error: 1.0874, Threshold: -33.2413
tensor([ 0.1137,  0.6260,  0.5696,  0.4858,  0.0642,  0.2642,  0.9098,  1.0605,
         1.1793,  0.3706,  0.2477,  1.0160,  0.0115,  0.0123, -0.1399, -0.8069],
       device='cuda:0')
Original likelihood: -32.43416976928711
Adjusted likelihood: -32.43416976928711
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9949)
Solve time for step 4 5.020313117012847
Current ori: tensor([ 0.0115,  0.0123, -0.1399], device='cuda:0')
Middle force: tensor([0.5232, 0.5170, 0.5406, 0.5071, 0.5947, 0.6892, 1.1022, 0.7628, 0.5219],
       device='cuda:0')
Thumb force: tensor([0.6638, 0.5410, 0.5126, 1.0194, 0.5871, 0.7030, 0.6462, 0.7445, 0.5620],
       device='cuda:0')
Index force: tensor([0.5224, 0.5934, 0.5647, 0.5436, 0.5789, 0.5476, 0.5479, 0.5404, 0.5004],
       device='cuda:0')
Storing NORMAL transition: reward=0.0961 (scaled=0.0961), steps=1
Reward stats updated: mean 0.0089 -> 0.0093, std: 0.0778
Collected 226 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5862, Q2 Loss=1.5862, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3310
SAC Update 2/5: Actor Loss=-0.0054, Q1 Loss=1.2863, Q2 Loss=1.2863, Entropy=0.3401, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9545
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=2.1017, Q2 Loss=2.1017, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1133
SAC Update 4/5: Actor Loss=-0.1122, Q1 Loss=1.4661, Q2 Loss=1.4661, Entropy=0.0006, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7402
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.9014, Q2 Loss=1.9014, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3115

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.4%)
Q1 update: 0.05s (19.0%)
Q2 update: 0.05s (19.0%)
Actor update: 0.10s (41.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.069587
Q1 loss: 1.668335
Q2 loss: 1.668335
Current threshold: -33.2460
Global Scale Offset: 0.0840
Reward stats: mean=0.0093, std=0.0778, count=226
----------------------------------------------
SAC Update - Actor Loss: -0.0696, Q1 Loss: 1.6683, Q2 Loss: 1.6683, Entropy: 0.0681, Mean TD Error: 1.4901, Threshold: -33.2460
tensor([ 2.3143e-01,  7.3423e-01,  4.7473e-01,  4.2030e-01,  7.1963e-02,
         2.9365e-01,  9.2488e-01,  1.1569e+00,  1.2936e+00,  3.0466e-01,
         2.9165e-01,  8.9054e-01,  3.6209e-04, -2.2832e-03, -2.3565e-01,
        -1.8099e+00], device='cuda:0')
Original likelihood: -42.54735565185547
Adjusted likelihood: -42.54735565185547
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 44.658592224121094
Projection step: 1, Loss: 38.458072662353516
Projection step: 2, Loss: 35.06065368652344
Projection step: 3, Loss: 34.41843032836914
Projection step: 4, Loss: 33.16568374633789
Projection step: 5, Loss: 31.826953887939453
Projection step: 6, Loss: 30.00600814819336
Projection step: 7, Loss: 29.519638061523438
Projection step: 8, Loss: 26.975671768188477
Projection step: 9, Loss: 29.16582489013672
Projection step: 10, Loss: 27.696840286254883
Projection step: 11, Loss: 26.471725463867188
Projection step: 12, Loss: 25.849628448486328
Projection step: 13, Loss: 25.41510009765625
Projection step: 14, Loss: 26.184734344482422
Projection step: 15, Loss: 23.92023468017578
Projection step: 16, Loss: 22.500503540039062
Projection step: 17, Loss: 22.107738494873047
Projection step: 18, Loss: 23.244674682617188
Projection step: 19, Loss: 20.969165802001953
Projection step: 20, Loss: 22.155302047729492
Projection step: 21, Loss: 21.00271224975586
Projection step: 22, Loss: 20.184844970703125
Projection step: 23, Loss: 19.897241592407227
Projection step: 24, Loss: 19.283050537109375
Final likelihood: tensor([-21.0704, -20.4073, -19.8292, -20.4787, -19.1882, -20.1274, -18.8132,
        -20.3138, -19.7595, -16.1126, -25.5287, -19.8678, -23.8975, -18.4567,
        -17.1375, -20.1977])
Final projection likelihood: -20.0741
1 mode projection succeeded
New goal: tensor([ 1.8896e-01,  6.7748e-01,  5.5681e-01,  5.2957e-01,  2.0917e-03,
         3.3460e-01,  7.6626e-01,  1.2296e+00,  1.2113e+00,  3.3437e-01,
         1.5986e-01,  1.0077e+00,  3.2700e-05,  1.6848e-03, -4.6457e-01],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0063]], device='cuda:0')
Original likelihood: -34.6175537109375
Adjusted likelihood: -34.6175537109375
Likelihood residual: 0.0
{'index': 34.6175537109375, 'thumb_middle': inf}
Current yaw: tensor([ 0.0004, -0.0023, -0.2356], device='cuda:0')
2 index
tensor([ 2.3143e-01,  7.3423e-01,  4.7473e-01,  4.2030e-01,  7.1963e-02,
         2.9365e-01,  9.2488e-01,  1.1569e+00,  1.2936e+00,  3.0466e-01,
         2.9165e-01,  8.9054e-01,  3.6209e-04, -2.2832e-03, -2.3565e-01,
        -1.8099e+00], device='cuda:0')
Solve time for step 1 10.512664260983001
Current ori: tensor([ 0.0004, -0.0023, -0.2356], device='cuda:0')
Middle force: tensor([0.5545, 0.5570, 0.5346, 0.5283], device='cuda:0')
Thumb force: tensor([0.5400, 0.5248, 0.5170, 0.5348], device='cuda:0')
tensor([ 2.5294e-01,  6.2516e-01,  4.9304e-01,  4.9504e-01,  6.6949e-02,
         3.4724e-01,  8.3670e-01,  1.2528e+00,  1.3019e+00,  2.9645e-01,
         2.6251e-01,  9.1826e-01, -1.4471e-03, -7.5728e-03, -2.5661e-01,
        -2.4471e+00], device='cuda:0')
Solve time for step 2 4.224734981020447
Current ori: tensor([-0.0014, -0.0076, -0.2566], device='cuda:0')
Middle force: tensor([0.5573, 0.5419, 0.5272], device='cuda:0')
Thumb force: tensor([0.6023, 0.6043, 0.5942], device='cuda:0')
tensor([ 0.2554,  0.6188,  0.5035,  0.5083,  0.0860,  0.3499,  0.8350,  1.2830,
         1.2711,  0.3226,  0.2396,  0.9890,  0.0103, -0.0203, -0.2339, -2.8572],
       device='cuda:0')
Solve time for step 3 4.177036433015019
Current ori: tensor([ 0.0103, -0.0203, -0.2339], device='cuda:0')
Middle force: tensor([0.5401, 0.5246], device='cuda:0')
Thumb force: tensor([0.5933, 0.5889], device='cuda:0')
tensor([ 2.5784e-01,  6.1733e-01,  5.0887e-01,  5.1342e-01,  9.4875e-02,
         3.7187e-01,  8.2282e-01,  1.2647e+00,  1.2836e+00,  3.0547e-01,
         2.2238e-01,  9.7884e-01,  7.7380e-04, -2.5883e-02, -2.3859e-01,
        -3.0203e+00], device='cuda:0')
Solve time for step 4 3.830577018961776
Current ori: tensor([ 0.0008, -0.0259, -0.2386], device='cuda:0')
Middle force: tensor([0.5266], device='cuda:0')
Thumb force: tensor([0.5946], device='cuda:0')
Storing RECOVERY transition: reward=-0.0115 (scaled=-0.0029), steps=4
Reward stats updated: mean 0.0093 -> 0.0093, std: 0.0777
Collected 227 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0023, Q2 Loss=1.0023, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8258
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8281, Q2 Loss=0.8281, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4382
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2101, Q2 Loss=1.2101, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7879
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.4241, Q2 Loss=1.4241, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3390
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8299, Q2 Loss=0.8299, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4566

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.5%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (38.4%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.3%)
Actor loss: -0.046052
Q1 loss: 1.058901
Q2 loss: 1.058901
Current threshold: -33.2494
Global Scale Offset: 0.0840
Reward stats: mean=0.0093, std=0.0777, count=227
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0589, Q2 Loss: 1.0589, Entropy: 0.0000, Mean TD Error: 0.9695, Threshold: -33.2494
Original likelihood: -35.186683654785156
Adjusted likelihood: -35.186683654785156
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 32.84039306640625
Projection step: 1, Loss: 35.86882019042969
Projection step: 2, Loss: 32.62427520751953
Projection step: 3, Loss: 29.928348541259766
Projection step: 4, Loss: 29.212215423583984
Projection step: 5, Loss: 28.674541473388672
Projection step: 6, Loss: 29.633747100830078
Projection step: 7, Loss: 31.444435119628906
Projection step: 8, Loss: 26.86437225341797
Projection step: 9, Loss: 31.342016220092773
Projection step: 10, Loss: 28.35809326171875
Projection step: 11, Loss: 27.20228385925293
Projection step: 12, Loss: 25.845626831054688
Projection step: 13, Loss: 24.73086929321289
Projection step: 14, Loss: 26.262184143066406
Projection step: 15, Loss: 27.19551658630371
Projection step: 16, Loss: 23.44198989868164
Projection step: 17, Loss: 23.419689178466797
Projection step: 18, Loss: 23.31899642944336
Projection step: 19, Loss: 22.197254180908203
Projection step: 20, Loss: 23.381309509277344
Projection step: 21, Loss: 23.45292854309082
Projection step: 22, Loss: 25.39987564086914
Projection step: 23, Loss: 24.149070739746094
Projection step: 24, Loss: 23.168455123901367
Final likelihood: tensor([-36.4956, -21.4919, -19.8790, -25.0226, -23.6668, -20.0889, -23.5295,
        -20.6600, -40.8675, -21.6212, -20.9580, -26.8841, -22.2475, -22.2552,
        -24.3702, -24.4533])
Final projection likelihood: -24.6557
1 mode projection succeeded
New goal: tensor([ 1.5941e-01,  6.3061e-01,  5.8102e-01,  6.2378e-01,  2.1765e-02,
         4.1428e-01,  7.1829e-01,  1.2403e+00,  1.2257e+00,  2.9285e-01,
         1.4200e-01,  9.8501e-01, -1.4302e-04, -1.8706e-02, -3.0128e-02],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -32.15105438232422
Adjusted likelihood: -32.15105438232422
Likelihood residual: 0.0
Original likelihood: -32.533470153808594
Adjusted likelihood: -32.533470153808594
Likelihood residual: 0.0
{'index': 32.533470153808594, 'thumb_middle': 32.15105438232422}
Current yaw: tensor([ 0.0010, -0.0220, -0.2249], device='cuda:0')
3 thumb_middle
tensor([ 1.9377e-01,  6.8437e-01,  5.5330e-01,  5.2760e-01,  8.8121e-02,
         3.7122e-01,  8.1842e-01,  1.2634e+00,  1.2790e+00,  3.1384e-01,
         2.3091e-01,  9.7788e-01,  9.8257e-04, -2.2041e-02, -2.2486e-01,
        -3.0643e+00], device='cuda:0')
Solve time for step 1 9.078412054979708
Current ori: tensor([ 0.0010, -0.0220, -0.2249], device='cuda:0')
Index force: tensor([0.5909, 0.5009, 0.5628, 0.5725], device='cuda:0')
tensor([ 0.2066,  0.6241,  0.5860,  0.6505, -0.0371,  0.3810,  0.7164,  1.2341,
         1.2363,  0.2833,  0.1528,  1.0050,  0.0227, -0.0212, -0.2249, -2.9769],
       device='cuda:0')
Solve time for step 2 3.6389548560255207
Current ori: tensor([ 0.0227, -0.0212, -0.2249], device='cuda:0')
Index force: tensor([0.5006, 0.5530, 0.5646], device='cuda:0')
tensor([ 0.2087,  0.6324,  0.5866,  0.6313, -0.0483,  0.4040,  0.7003,  1.2144,
         1.2377,  0.2900,  0.1463,  1.0033,  0.0197, -0.0235, -0.2249, -2.9831],
       device='cuda:0')
Solve time for step 3 3.543687288009096
Current ori: tensor([ 0.0197, -0.0235, -0.2249], device='cuda:0')
Index force: tensor([0.5423, 0.5555], device='cuda:0')
tensor([ 0.2063,  0.6484,  0.5772,  0.6035, -0.0508,  0.4108,  0.6993,  1.2517,
         1.2324,  0.2752,  0.1336,  1.0267,  0.0142, -0.0244, -0.2249, -2.9957],
       device='cuda:0')
Solve time for step 4 3.332243119017221
Current ori: tensor([ 0.0142, -0.0244, -0.2249], device='cuda:0')
Index force: tensor([0.5402], device='cuda:0')
Storing RECOVERY transition: reward=-0.0441 (scaled=-0.0110), steps=4
Reward stats updated: mean 0.0093 -> 0.0092, std: 0.0775
Collected 228 transitions for RL
SAC Update 1/5: Actor Loss=-0.2320, Q1 Loss=1.2397, Q2 Loss=1.2397, Entropy=0.3116, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1911
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8997, Q2 Loss=0.8997, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9922
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.9353, Q2 Loss=1.9353, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6583
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2239, Q2 Loss=1.2239, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5787
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9531, Q2 Loss=0.9531, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9446

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (40.5%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046390
Q1 loss: 1.250347
Q2 loss: 1.250347
Current threshold: -33.2527
Global Scale Offset: 0.0840
Reward stats: mean=0.0092, std=0.0775, count=228
----------------------------------------------
SAC Update - Actor Loss: -0.0464, Q1 Loss: 1.2503, Q2 Loss: 1.2503, Entropy: 0.0623, Mean TD Error: 1.0730, Threshold: -33.2527
Original likelihood: -23.578311920166016
Adjusted likelihood: -23.578311920166016
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0229, -0.0053, -0.1924], device='cuda:0')
4 turn
Sampling time 3.7442540759802796
tensor([ 0.1743,  0.6088,  0.5878,  0.6177, -0.0065,  0.4551,  0.7324,  1.2281,
         1.2611,  0.2472,  0.2575,  1.1048,  0.0229, -0.0053, -0.1924, -3.0333],
       device='cuda:0')
Original likelihood: -21.714326858520508
Adjusted likelihood: -21.714326858520508
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.254514995962381
Current ori: tensor([ 0.0229, -0.0053, -0.1924], device='cuda:0')
Middle force: tensor([1.7868, 0.6289, 0.5217, 1.0407, 0.5248, 0.5285, 0.5565, 0.5439, 0.5439,
        0.5186, 0.5045, 0.5249], device='cuda:0')
Thumb force: tensor([1.0099, 0.7013, 1.4799, 1.1200, 0.7157, 1.0437, 0.5880, 0.5745, 1.6650,
        0.5426, 0.6659, 0.6473], device='cuda:0')
Index force: tensor([0.6598, 0.5254, 0.5322, 0.5006, 0.5538, 0.5405, 0.6025, 0.6920, 0.8399,
        0.7033, 0.7896, 0.6286], device='cuda:0')
Storing NORMAL transition: reward=-0.0959 (scaled=-0.0959), steps=1
Reward stats updated: mean 0.0092 -> 0.0087, std: 0.0776
Collected 229 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.4532, Q2 Loss=2.4532, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9544
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.9920, Q2 Loss=0.9920, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8091
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0695, Q2 Loss=1.0695, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9821
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=0.8167, Q2 Loss=0.8167, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9360
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.7672, Q2 Loss=1.7672, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5021

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (15.0%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (20.5%)
Actor update: 0.11s (41.6%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.419718
Q2 loss: 1.419718
Current threshold: -33.2546
Global Scale Offset: 0.0841
Reward stats: mean=0.0087, std=0.0776, count=229
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.4197, Q2 Loss: 1.4197, Entropy: 0.0000, Mean TD Error: 1.4367, Threshold: -33.2546
tensor([ 0.1893,  0.5580,  0.6491,  0.6647, -0.0500,  0.4669,  0.9219,  0.9714,
         1.1668,  0.2929,  0.2817,  1.1017,  0.0375, -0.0114, -0.0972, -3.0982],
       device='cuda:0')
Original likelihood: -34.06019592285156
Adjusted likelihood: -34.06019592285156
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0052)
State is out of distribution
Projection step: 0, Loss: 25.028446197509766
Projection step: 1, Loss: 22.349796295166016
Projection step: 2, Loss: 21.82033920288086
Projection step: 3, Loss: 26.037099838256836
Projection step: 4, Loss: 21.209896087646484
Projection step: 5, Loss: 20.345237731933594
Projection step: 6, Loss: 20.721332550048828
Projection step: 7, Loss: 21.37751007080078
Projection step: 8, Loss: 19.813335418701172
Projection step: 9, Loss: 20.34525489807129
Projection step: 10, Loss: 20.638195037841797
Projection step: 11, Loss: 18.957901000976562
Final likelihood: tensor([-17.6734, -18.3991, -18.8495, -18.1502, -19.3280, -18.5280, -18.5074,
        -18.6097, -17.9505, -18.3009, -15.6435, -18.4795, -21.1610, -23.1485,
        -23.0872, -17.5101])
Final projection likelihood: -18.9579
1 mode projection succeeded
New goal: tensor([ 0.1598,  0.5466,  0.6664,  0.6851, -0.0419,  0.4744,  0.8548,  0.9494,
         1.2142,  0.3113,  0.2494,  1.0784,  0.0365, -0.0082,  0.0115],
       device='cuda:0')
tensor([[0.0069]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -22.74186897277832
Adjusted likelihood: -22.74186897277832
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 22.74186897277832}
Current yaw: tensor([ 0.0375, -0.0114, -0.0972], device='cuda:0')
5 thumb_middle
tensor([ 0.1893,  0.5580,  0.6491,  0.6647, -0.0500,  0.4669,  0.9219,  0.9714,
         1.1668,  0.2929,  0.2817,  1.1017,  0.0375, -0.0114, -0.0972, -3.0982],
       device='cuda:0')
Solve time for step 1 8.892939857032616
Current ori: tensor([ 0.0375, -0.0114, -0.0972], device='cuda:0')
Index force: tensor([0.6008, 0.5778, 0.6077, 0.5944], device='cuda:0')
tensor([ 0.1886,  0.5541,  0.6512,  0.6662, -0.1250,  0.4528,  0.8358,  0.9329,
         1.1886,  0.2992,  0.2000,  1.0574,  0.0385, -0.0088, -0.0972, -3.0536],
       device='cuda:0')
Solve time for step 2 3.534079053963069
Current ori: tensor([ 0.0385, -0.0088, -0.0972], device='cuda:0')
Index force: tensor([0.5733, 0.6046, 0.5913], device='cuda:0')
tensor([ 0.1824,  0.5399,  0.6596,  0.6752, -0.1331,  0.4621,  0.8275,  0.9292,
         1.2039,  0.3033,  0.1873,  1.0553,  0.0422, -0.0046, -0.0972, -3.0565],
       device='cuda:0')
Solve time for step 3 3.4965858230134472
Current ori: tensor([ 0.0422, -0.0046, -0.0972], device='cuda:0')
Index force: tensor([0.5978, 0.5857], device='cuda:0')
tensor([ 0.1826,  0.5396,  0.6572,  0.6814, -0.1321,  0.4640,  0.8266,  0.9282,
         1.2046,  0.3059,  0.1852,  1.0539,  0.0427, -0.0045, -0.0972, -3.0539],
       device='cuda:0')
Solve time for step 4 3.402982406958472
Current ori: tensor([ 0.0427, -0.0045, -0.0972], device='cuda:0')
Index force: tensor([0.5644], device='cuda:0')
Storing RECOVERY transition: reward=0.0110 (scaled=0.0110), steps=1
Reward stats updated: mean 0.0087 -> 0.0087, std: 0.0775
Collected 230 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3238, Q2 Loss=1.3238, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4428
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3059, Q2 Loss=1.3059, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0764
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0922, Q2 Loss=1.0922, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5414
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.7890, Q2 Loss=1.7890, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0316
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.1906, Q2 Loss=1.1906, Entropy=0.0163, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7649

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.4%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.5%)
Actor update: 0.10s (43.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046056
Q1 loss: 1.340287
Q2 loss: 1.340287
Current threshold: -33.2558
Global Scale Offset: 0.0841
Reward stats: mean=0.0087, std=0.0775, count=230
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.3403, Q2 Loss: 1.3403, Entropy: 0.0033, Mean TD Error: 1.3714, Threshold: -33.2558
Original likelihood: -21.016071319580078
Adjusted likelihood: -21.016071319580078
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0424, -0.0073, -0.1085], device='cuda:0')
6 turn
Sampling time 3.607841713994276
tensor([ 0.1858,  0.5424,  0.6566,  0.6820, -0.0587,  0.5121,  0.8689,  0.9525,
         1.2598,  0.3233,  0.2425,  1.0849,  0.0424, -0.0073, -0.1085, -3.0655],
       device='cuda:0')
Original likelihood: -23.796911239624023
Adjusted likelihood: -23.796911239624023
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.126376578002237
Current ori: tensor([ 0.0424, -0.0073, -0.1085], device='cuda:0')
Middle force: tensor([0.5774, 0.5112, 0.5819, 0.6488, 1.9855, 0.5732, 0.4990, 0.5254, 0.7831,
        0.5935, 0.5601, 0.5011], device='cuda:0')
Thumb force: tensor([0.5341, 1.7446, 1.4817, 1.8640, 0.7744, 0.6096, 0.6963, 0.5308, 0.5643,
        0.6132, 0.5891, 0.8630], device='cuda:0')
Index force: tensor([0.5851, 0.8981, 0.6548, 0.8839, 0.7397, 0.5808, 0.7606, 0.5708, 0.5380,
        0.6102, 0.5904, 0.8384], device='cuda:0')
Storing NORMAL transition: reward=0.0083 (scaled=0.0083), steps=1
Reward stats updated: mean 0.0087 -> 0.0087, std: 0.0773
Collected 231 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8640, Q2 Loss=0.8640, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6196
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.3426, Q2 Loss=2.3426, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1328
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2456, Q2 Loss=1.2456, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2986
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1707, Q2 Loss=1.1707, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2410
SAC Update 5/5: Actor Loss=-0.0977, Q1 Loss=0.9811, Q2 Loss=0.9811, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8513

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.4%)
Actor update: 0.08s (40.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.065598
Q1 loss: 1.320800
Q2 loss: 1.320800
Current threshold: -33.2565
Global Scale Offset: 0.0841
Reward stats: mean=0.0087, std=0.0773, count=231
----------------------------------------------
SAC Update - Actor Loss: -0.0656, Q1 Loss: 1.3208, Q2 Loss: 1.3208, Entropy: 0.0000, Mean TD Error: 1.4287, Threshold: -33.2565
tensor([ 0.3107,  0.5905,  0.6562,  0.6226, -0.1131,  0.4926,  0.8506,  0.9475,
         1.3316,  0.2594,  0.2775,  1.0118,  0.0391,  0.0284, -0.1173, -2.6787],
       device='cuda:0')
Original likelihood: -25.744173049926758
Adjusted likelihood: -25.744173049926758
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.590725629008375
Current ori: tensor([ 0.0391,  0.0284, -0.1173], device='cuda:0')
Middle force: tensor([0.5043, 0.5040, 1.4918, 0.6712, 0.5010, 0.5461, 0.5393, 0.5345, 0.6119,
        0.5304, 0.5033], device='cuda:0')
Thumb force: tensor([0.9754, 1.3780, 0.7373, 0.5372, 1.0638, 1.2873, 0.5053, 0.9438, 0.6133,
        0.6737, 0.7656], device='cuda:0')
Index force: tensor([0.8305, 0.6909, 0.7480, 0.5473, 0.6612, 0.6131, 0.5667, 0.5911, 0.5893,
        0.5352, 0.6883], device='cuda:0')
Storing NORMAL transition: reward=-0.1157 (scaled=-0.1157), steps=1
Reward stats updated: mean 0.0087 -> 0.0082, std: 0.0776
Collected 232 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6970, Q2 Loss=0.6970, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8767
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1752, Q2 Loss=1.1752, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0674
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1209, Q2 Loss=1.1209, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9164
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3576, Q2 Loss=1.3576, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6544
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4592, Q2 Loss=1.4592, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6326

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.6%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.0%)
Actor update: 0.08s (39.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.161960
Q2 loss: 1.161960
Current threshold: -33.2570
Global Scale Offset: 0.0841
Reward stats: mean=0.0082, std=0.0776, count=232
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.1620, Q2 Loss: 1.1620, Entropy: 0.0000, Mean TD Error: 1.2295, Threshold: -33.2570
tensor([ 0.2432,  0.6941,  0.7837,  0.5900, -0.1841,  0.5301,  0.6844,  1.0703,
         1.3945,  0.1886,  0.3347,  0.9926,  0.0573,  0.0766, -0.0066, -3.0923],
       device='cuda:0')
Original likelihood: -36.38950729370117
Adjusted likelihood: -36.38950729370117
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 36.4224853515625
Projection step: 1, Loss: 35.02401351928711
Projection step: 2, Loss: 34.589332580566406
Projection step: 3, Loss: 35.43047332763672
Projection step: 4, Loss: 33.88824462890625
Projection step: 5, Loss: 32.7474365234375
Projection step: 6, Loss: 32.401405334472656
Projection step: 7, Loss: 31.945640563964844
Projection step: 8, Loss: 32.08682632446289
Projection step: 9, Loss: 31.578678131103516
Projection step: 10, Loss: 30.393259048461914
Projection step: 11, Loss: 29.528240203857422
Projection step: 12, Loss: 30.15090560913086
Projection step: 13, Loss: 30.644508361816406
Projection step: 14, Loss: 30.079864501953125
Projection step: 15, Loss: 29.223276138305664
Projection step: 16, Loss: 29.9332275390625
Projection step: 17, Loss: 29.298324584960938
Projection step: 18, Loss: 28.28428840637207
Projection step: 19, Loss: 29.045711517333984
Projection step: 20, Loss: 27.673038482666016
Projection step: 21, Loss: 28.358341217041016
Projection step: 22, Loss: 28.186321258544922
Projection step: 23, Loss: 27.775367736816406
Projection step: 24, Loss: 27.331140518188477
Final likelihood: tensor([-26.0071, -25.3353, -26.2816, -28.6153, -27.8477, -28.2609, -26.4523,
        -25.9945, -30.4674, -28.6396, -26.3569, -31.3828, -29.8134, -29.4942,
        -25.8314, -28.1112])
Final projection likelihood: -27.8057
1 mode projection succeeded
New goal: tensor([ 0.2382,  0.6698,  0.6775,  0.4840, -0.1534,  0.5144,  0.6379,  0.9843,
         1.3508,  0.1712,  0.3533,  1.0171,  0.0583,  0.0675,  0.6153],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0020]], device='cuda:0')
Original likelihood: -33.19004821777344
Adjusted likelihood: -33.19004821777344
Likelihood residual: 0.0
Original likelihood: -28.246179580688477
Adjusted likelihood: -28.246179580688477
Likelihood residual: 0.0
{'index': 28.246179580688477, 'thumb_middle': 33.19004821777344}
Current yaw: tensor([ 0.0573,  0.0766, -0.0066], device='cuda:0')
7 index
tensor([ 0.2432,  0.6941,  0.7837,  0.5900, -0.1841,  0.5301,  0.6844,  1.0703,
         1.3945,  0.1886,  0.3347,  0.9926,  0.0573,  0.0766, -0.0066, -3.0923],
       device='cuda:0')
Solve time for step 1 10.446915307024028
Current ori: tensor([ 0.0573,  0.0766, -0.0066], device='cuda:0')
Middle force: tensor([0.5890, 0.5490, 0.5863, 0.5543], device='cuda:0')
Thumb force: tensor([0.5173, 0.6003, 0.5507, 0.5293], device='cuda:0')
tensor([ 0.3475,  0.6163,  0.6529,  0.4900, -0.1826,  0.5369,  0.6849,  1.0347,
         1.4048,  0.1789,  0.3327,  0.9702,  0.0488,  0.0782, -0.0109, -3.5993],
       device='cuda:0')
Solve time for step 2 4.17114703200059
Current ori: tensor([ 0.0488,  0.0782, -0.0109], device='cuda:0')
Middle force: tensor([0.5457, 0.5825, 0.5516], device='cuda:0')
Thumb force: tensor([0.5946, 0.5479, 0.5275], device='cuda:0')
tensor([ 3.4522e-01,  6.1639e-01,  6.3625e-01,  4.7634e-01, -1.7877e-01,
         5.3971e-01,  6.8554e-01,  1.0275e+00,  1.4025e+00,  1.7952e-01,
         3.2975e-01,  9.7518e-01,  4.8068e-02,  7.6550e-02, -3.5160e-03,
        -4.0537e+00], device='cuda:0')
Solve time for step 3 4.097036349994596
Current ori: tensor([ 0.0481,  0.0765, -0.0035], device='cuda:0')
Middle force: tensor([0.5998, 0.5224], device='cuda:0')
Thumb force: tensor([0.5472, 0.5239], device='cuda:0')
tensor([ 0.3477,  0.6260,  0.6402,  0.4765, -0.1878,  0.5384,  0.6827,  1.0226,
         1.4065,  0.1811,  0.3401,  0.9589,  0.0471,  0.0822, -0.0059, -4.6322],
       device='cuda:0')
Solve time for step 4 4.008762587036472
Current ori: tensor([ 0.0471,  0.0822, -0.0059], device='cuda:0')
Middle force: tensor([0.5188], device='cuda:0')
Thumb force: tensor([0.5200], device='cuda:0')
Storing RECOVERY transition: reward=-0.0013 (scaled=-0.0007), steps=2
Reward stats updated: mean 0.0082 -> 0.0082, std: 0.0774
Collected 233 transitions for RL
SAC Update 1/5: Actor Loss=-0.0036, Q1 Loss=1.2754, Q2 Loss=1.2754, Entropy=0.3149, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6670
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.3383, Q2 Loss=1.3383, Entropy=0.0141, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3748
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6326, Q2 Loss=0.6326, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7873
SAC Update 4/5: Actor Loss=-0.0730, Q1 Loss=1.1202, Q2 Loss=1.1202, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5348
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2543, Q2 Loss=1.2543, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1216

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.4%)
Q1 update: 0.04s (19.7%)
Q2 update: 0.04s (18.1%)
Actor update: 0.09s (38.8%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.015315
Q1 loss: 1.124185
Q2 loss: 1.124185
Current threshold: -33.2600
Global Scale Offset: 0.0841
Reward stats: mean=0.0082, std=0.0774, count=233
----------------------------------------------
SAC Update - Actor Loss: -0.0153, Q1 Loss: 1.1242, Q2 Loss: 1.1242, Entropy: 0.0658, Mean TD Error: 1.0971, Threshold: -33.2600
Original likelihood: -34.8288688659668
Adjusted likelihood: -34.8288688659668
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 34.72971725463867
Projection step: 1, Loss: 34.59486770629883
Projection step: 2, Loss: 33.861488342285156
Projection step: 3, Loss: 33.897544860839844
Projection step: 4, Loss: 33.658729553222656
Projection step: 5, Loss: 33.46493911743164
Projection step: 6, Loss: 32.54658889770508
Projection step: 7, Loss: 32.41802978515625
Projection step: 8, Loss: 32.325958251953125
Projection step: 9, Loss: 31.952913284301758
Projection step: 10, Loss: 31.718360900878906
Projection step: 11, Loss: 30.77825164794922
Projection step: 12, Loss: 30.645858764648438
Projection step: 13, Loss: 30.41485595703125
Projection step: 14, Loss: 30.488231658935547
Projection step: 15, Loss: 30.13275146484375
Projection step: 16, Loss: 29.723556518554688
Projection step: 17, Loss: 29.518417358398438
Projection step: 18, Loss: 30.386518478393555
Projection step: 19, Loss: 29.333744049072266
Projection step: 20, Loss: 29.159564971923828
Projection step: 21, Loss: 28.63368797302246
Projection step: 22, Loss: 28.021278381347656
Projection step: 23, Loss: 28.056537628173828
Projection step: 24, Loss: 28.145904541015625
Final likelihood: tensor([-27.3228, -30.9276, -25.7695, -30.6761, -29.3870, -27.3033, -29.9902,
        -26.4100, -30.5016, -26.8436, -29.9112, -28.7711, -26.5228, -30.9136,
        -28.8323, -27.4698])
Final projection likelihood: -28.5970
1 mode projection succeeded
New goal: tensor([ 0.2254,  0.6846,  0.6472,  0.4345, -0.1565,  0.5289,  0.6305,  0.9625,
         1.3624,  0.1780,  0.3512,  1.0105,  0.0496,  0.0680,  0.7685],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0022]], device='cuda:0')
Original likelihood: -29.662389755249023
Adjusted likelihood: -29.662389755249023
Likelihood residual: 0.0
Original likelihood: -33.478599548339844
Adjusted likelihood: -33.478599548339844
Likelihood residual: 0.0
{'index': 33.478599548339844, 'thumb_middle': 29.662389755249023}
Current yaw: tensor([ 0.0465,  0.0793, -0.0047], device='cuda:0')
8 thumb_middle
tensor([ 2.3311e-01,  6.8881e-01,  6.8194e-01,  4.8423e-01, -1.8250e-01,
         5.4080e-01,  6.8176e-01,  1.0231e+00,  1.4074e+00,  1.8218e-01,
         3.2573e-01,  9.7460e-01,  4.6532e-02,  7.9332e-02, -4.7440e-03,
        -4.9314e+00], device='cuda:0')
Solve time for step 1 8.92289832996903
Current ori: tensor([ 0.0465,  0.0793, -0.0047], device='cuda:0')
Index force: tensor([0.6014, 0.5976, 0.5951, 0.5891], device='cuda:0')
tensor([ 0.1919,  0.7548,  0.6845,  0.4400, -0.2346,  0.5327,  0.6312,  0.9617,
         1.3385,  0.1549,  0.2815,  0.9774,  0.0604,  0.1224,  0.0238, -6.2631],
       device='cuda:0')
Solve time for step 2 3.622669793025125
Current ori: tensor([0.0604, 0.1224, 0.0238], device='cuda:0')
Index force: tensor([0.5893, 0.5877, 0.5823], device='cuda:0')
tensor([ 0.1353,  0.8179,  0.7054,  0.4496, -0.2548,  0.5295,  0.6153,  0.9453,
         1.3605,  0.1661,  0.2989,  0.9896,  0.0648,  0.1368,  0.0634,  5.3368],
       device='cuda:0')
Solve time for step 3 3.212183383991942
Current ori: tensor([0.0648, 0.1368, 0.0634], device='cuda:0')
Index force: tensor([0.5759, 0.5725], device='cuda:0')
tensor([ 0.0902,  0.8700,  0.7271,  0.4454, -0.2712,  0.5270,  0.5987,  0.9339,
         1.3750,  0.1704,  0.3171,  1.0010,  0.0722,  0.1493,  0.1078,  5.2482],
       device='cuda:0')
Solve time for step 4 3.3602893960196525
Current ori: tensor([0.0722, 0.1493, 0.1078], device='cuda:0')
Index force: tensor([0.5557], device='cuda:0')
Storing RECOVERY transition: reward=-0.1178 (scaled=-0.0589), steps=2
Reward stats updated: mean 0.0082 -> 0.0079, std: 0.0774
Collected 234 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9677, Q2 Loss=0.9677, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9376
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7466, Q2 Loss=0.7466, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2744
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=2.0897, Q2 Loss=2.0897, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7870
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.5899, Q2 Loss=1.5899, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8091
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1953, Q2 Loss=1.1953, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8116

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.8%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.05s (20.0%)
Actor update: 0.10s (42.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.317860
Q2 loss: 1.317860
Current threshold: -33.2618
Global Scale Offset: 0.0841
Reward stats: mean=0.0079, std=0.0774, count=234
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.3179, Q2 Loss: 1.3179, Entropy: 0.0000, Mean TD Error: 1.1239, Threshold: -33.2618
Original likelihood: -36.58900833129883
Adjusted likelihood: -36.58900833129883
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 32.55975341796875
Projection step: 1, Loss: 33.36467742919922
Projection step: 2, Loss: 29.460433959960938
Projection step: 3, Loss: 31.54545783996582
Projection step: 4, Loss: 32.269378662109375
Projection step: 5, Loss: 34.14479064941406
Projection step: 6, Loss: 30.16531753540039
Projection step: 7, Loss: 28.88507843017578
Projection step: 8, Loss: 29.883846282958984
Projection step: 9, Loss: 25.66594696044922
Projection step: 10, Loss: 26.526784896850586
Projection step: 11, Loss: 24.58981704711914
Projection step: 12, Loss: 23.6223201751709
Projection step: 13, Loss: 21.490283966064453
Projection step: 14, Loss: 26.19893455505371
Projection step: 15, Loss: 25.272968292236328
Projection step: 16, Loss: 25.66598892211914
Projection step: 17, Loss: 21.654558181762695
Projection step: 18, Loss: 22.186485290527344
Projection step: 19, Loss: 23.692686080932617
Projection step: 20, Loss: 22.61141586303711
Projection step: 21, Loss: 22.963855743408203
Projection step: 22, Loss: 22.00524139404297
Projection step: 23, Loss: 22.969036102294922
Projection step: 24, Loss: 19.306060791015625
Final likelihood: tensor([-19.0860, -20.5283, -17.7347, -18.8449, -19.1122, -33.7106, -38.9097,
        -19.7342, -18.0590, -18.2562, -21.5251, -19.3712, -20.0583, -21.3731,
        -38.0574, -19.1293])
Final projection likelihood: -22.7181
1 mode projection succeeded
New goal: tensor([ 0.0940,  0.8207,  0.5407,  0.3775, -0.2216,  0.5016,  0.5783,  0.9173,
         1.4369,  0.1278,  0.3571,  0.9727,  0.0745,  0.1299,  0.2138],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[-0.0011]], device='cuda:0')
Original likelihood: -27.292633056640625
Adjusted likelihood: -27.292633056640625
Likelihood residual: 0.0
Original likelihood: -23.94369888305664
Adjusted likelihood: -23.94369888305664
Likelihood residual: 0.0
{'index': 23.94369888305664, 'thumb_middle': 27.292633056640625}
Current yaw: tensor([0.0723, 0.1342, 0.1028], device='cuda:0')
9 index
tensor([ 0.0861,  0.8992,  0.7373,  0.4502, -0.2454,  0.5284,  0.6100,  0.9523,
         1.4413,  0.2018,  0.3627,  1.0261,  0.0723,  0.1342,  0.1028,  5.7755],
       device='cuda:0')
Solve time for step 1 10.903954721987247
Current ori: tensor([0.0723, 0.1342, 0.1028], device='cuda:0')
Middle force: tensor([0.5369, 0.5014, 0.5700, 0.5056], device='cuda:0')
Thumb force: tensor([0.5174, 0.5072, 0.6117, 0.6791], device='cuda:0')
tensor([ 0.2672,  0.8570,  0.5801,  0.3922, -0.2378,  0.5268,  0.6243,  0.9515,
         1.4669,  0.1638,  0.3418,  1.0011,  0.0644,  0.1278,  0.0446, -6.2490],
       device='cuda:0')
Solve time for step 2 4.068552600976545
Current ori: tensor([0.0644, 0.1278, 0.0446], device='cuda:0')
Middle force: tensor([0.5008, 0.5647, 0.5047], device='cuda:0')
Thumb force: tensor([0.5069, 0.6102, 0.6747], device='cuda:0')
tensor([ 0.2705,  0.8574,  0.5573,  0.3824, -0.2362,  0.5260,  0.6258,  0.9541,
         1.4791,  0.1455,  0.3344,  1.0013,  0.0640,  0.1267,  0.0379, -6.1110],
       device='cuda:0')
Solve time for step 3 3.996828825038392
Current ori: tensor([0.0640, 0.1267, 0.0379], device='cuda:0')
Middle force: tensor([0.5216, 0.5586], device='cuda:0')
Thumb force: tensor([0.5122, 0.5773], device='cuda:0')
tensor([ 0.2804,  0.8615,  0.5544,  0.3810, -0.2291,  0.5345,  0.6205,  0.9437,
         1.4755,  0.1477,  0.3309,  1.0006,  0.0610,  0.1239,  0.0387, -6.2771],
       device='cuda:0')
Solve time for step 4 3.9084603099618107
Current ori: tensor([0.0610, 0.1239, 0.0387], device='cuda:0')
Middle force: tensor([0.5081], device='cuda:0')
Thumb force: tensor([0.5463], device='cuda:0')
Storing RECOVERY transition: reward=-0.0667 (scaled=-0.0334), steps=2
Reward stats updated: mean 0.0079 -> 0.0077, std: 0.0773
Collected 235 transitions for RL
SAC Update 1/5: Actor Loss=-0.0002, Q1 Loss=1.2772, Q2 Loss=1.2772, Entropy=0.0488, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6635
SAC Update 2/5: Actor Loss=-0.0021, Q1 Loss=1.1063, Q2 Loss=1.1063, Entropy=0.2442, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1563
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1835, Q2 Loss=1.1835, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1026
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2255, Q2 Loss=1.2255, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4035
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3163, Q2 Loss=1.3163, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6411

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.4%)
Actor update: 0.08s (39.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000462
Q1 loss: 1.221772
Q2 loss: 1.221772
Current threshold: -33.2648
Global Scale Offset: 0.0841
Reward stats: mean=0.0077, std=0.0773, count=235
----------------------------------------------
SAC Update - Actor Loss: -0.0005, Q1 Loss: 1.2218, Q2 Loss: 1.2218, Entropy: 0.0586, Mean TD Error: 1.3934, Threshold: -33.2648
Original likelihood: -23.878746032714844
Adjusted likelihood: -23.878746032714844
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([0.0592, 0.1278, 0.0530], device='cuda:0')
10 turn
Sampling time 3.6307771139545366
tensor([ 0.1372,  0.8592,  0.5540,  0.3808, -0.2327,  0.5441,  0.6074,  0.9280,
         1.4777,  0.1489,  0.3321,  1.0023,  0.0592,  0.1278,  0.0530,  6.1772],
       device='cuda:0')
Original likelihood: -29.994171142578125
Adjusted likelihood: -29.994171142578125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.178723156044725
Current ori: tensor([0.0592, 0.1278, 0.0530], device='cuda:0')
Middle force: tensor([1.3833, 0.5431, 0.4948, 0.9503, 0.5237, 1.1049, 0.5692, 0.6565, 0.5365,
        0.7026, 0.5497, 0.6309], device='cuda:0')
Thumb force: tensor([2.1212, 1.2210, 1.1012, 1.1035, 0.5608, 1.2089, 0.6486, 1.1459, 0.5805,
        1.1644, 0.7378, 0.8796], device='cuda:0')
Index force: tensor([0.8119, 0.5479, 0.6400, 0.6594, 0.5652, 0.5043, 0.5173, 0.5740, 0.5324,
        0.7915, 0.5264, 0.5713], device='cuda:0')
Storing NORMAL transition: reward=-0.0087 (scaled=-0.0087), steps=1
Reward stats updated: mean 0.0077 -> 0.0076, std: 0.0771
Collected 236 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.0721, Q2 Loss=1.0721, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1283
SAC Update 2/5: Actor Loss=-0.2180, Q1 Loss=2.1679, Q2 Loss=2.1679, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8995
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.4296, Q2 Loss=1.4296, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2835
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2038, Q2 Loss=1.2038, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0722
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.8846, Q2 Loss=1.8846, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3539

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.1%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.05s (19.0%)
Actor update: 0.09s (38.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.089652
Q1 loss: 1.551620
Q2 loss: 1.551620
Current threshold: -33.2669
Global Scale Offset: 0.0841
Reward stats: mean=0.0076, std=0.0771, count=236
----------------------------------------------
SAC Update - Actor Loss: -0.0897, Q1 Loss: 1.5516, Q2 Loss: 1.5516, Entropy: 0.0000, Mean TD Error: 1.3475, Threshold: -33.2669
tensor([ 0.1348,  0.9085,  0.4310,  0.0709, -0.2275,  0.5610,  0.6139,  0.8456,
         1.5000,  0.1187,  0.3235,  0.9854,  0.0492,  0.1288,  0.0628,  6.2576],
       device='cuda:0')
Original likelihood: -23.20311737060547
Adjusted likelihood: -23.20311737060547
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.571793192997575
Current ori: tensor([0.0492, 0.1288, 0.0628], device='cuda:0')
Middle force: tensor([0.5029, 0.5041, 0.9824, 0.5047, 1.0426, 0.9110, 0.5019, 0.5026, 0.7616,
        0.5232, 0.5068], device='cuda:0')
Thumb force: tensor([1.3793, 0.7470, 1.0236, 0.6222, 1.0506, 0.5071, 0.5335, 0.9197, 1.1231,
        0.6104, 0.9268], device='cuda:0')
Index force: tensor([0.9626, 0.7700, 0.5374, 0.5319, 0.5277, 0.5323, 0.5450, 0.5911, 0.6624,
        0.5083, 0.5121], device='cuda:0')
Storing NORMAL transition: reward=-0.0490 (scaled=-0.0490), steps=1
Reward stats updated: mean 0.0076 -> 0.0074, std: 0.0770
Collected 237 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.1171, Q2 Loss=1.1171, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5081
SAC Update 2/5: Actor Loss=-0.0494, Q1 Loss=0.7613, Q2 Loss=0.7613, Entropy=0.0303, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5539
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=3.6622, Q2 Loss=3.6622, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6863
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.9369, Q2 Loss=1.9369, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6783
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.9991, Q2 Loss=0.9991, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7640

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.6%)
Q1 update: 0.05s (20.8%)
Q2 update: 0.05s (20.5%)
Actor update: 0.10s (38.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.009885
Q1 loss: 1.695332
Q2 loss: 1.695332
Current threshold: -33.2683
Global Scale Offset: 0.0841
Reward stats: mean=0.0074, std=0.0770, count=237
----------------------------------------------
SAC Update - Actor Loss: -0.0099, Q1 Loss: 1.6953, Q2 Loss: 1.6953, Entropy: 0.0061, Mean TD Error: 1.2381, Threshold: -33.2683
tensor([ 0.2557,  0.8404,  0.5903,  0.2592, -0.1021,  0.5428,  0.5235,  0.6280,
         1.4968,  0.1021,  0.2580,  1.1765,  0.0886,  0.1135,  0.1119,  6.2020],
       device='cuda:0')
Original likelihood: -20.253780364990234
Adjusted likelihood: -20.253780364990234
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.146527921955567
Current ori: tensor([0.0886, 0.1135, 0.1119], device='cuda:0')
Middle force: tensor([0.5057, 0.5182, 0.5037, 0.5371, 0.5009, 0.5296, 0.5296, 0.5027, 0.5022,
        0.8937], device='cuda:0')
Thumb force: tensor([0.5712, 0.5425, 0.6099, 1.1627, 0.5487, 1.2949, 0.5464, 0.5960, 0.9032,
        0.9967], device='cuda:0')
Index force: tensor([0.5392, 0.5801, 0.5149, 0.5174, 0.5014, 0.5016, 0.5494, 0.8086, 0.5636,
        0.5060], device='cuda:0')
Storing NORMAL transition: reward=0.0080 (scaled=0.0080), steps=1
Reward stats updated: mean 0.0074 -> 0.0074, std: 0.0769
Collected 238 transitions for RL
SAC Update 1/5: Actor Loss=-0.1795, Q1 Loss=1.7661, Q2 Loss=1.7661, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6729
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9885, Q2 Loss=0.9885, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6960
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.4830, Q2 Loss=1.4830, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2128
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.7178, Q2 Loss=1.7178, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3900
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.6726, Q2 Loss=0.6726, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6748

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.8%)
Q1 update: 0.05s (20.6%)
Q2 update: 0.05s (18.6%)
Actor update: 0.11s (42.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.128011
Q1 loss: 1.325589
Q2 loss: 1.325589
Current threshold: -33.2691
Global Scale Offset: 0.0841
Reward stats: mean=0.0074, std=0.0769, count=238
----------------------------------------------
SAC Update - Actor Loss: -0.1280, Q1 Loss: 1.3256, Q2 Loss: 1.3256, Entropy: 0.0000, Mean TD Error: 1.1293, Threshold: -33.2691
tensor([ 0.2067,  0.8655,  0.6301,  0.1322, -0.1076,  0.5493,  0.4840,  0.7132,
         1.3607,  0.3035,  0.3394,  1.1355,  0.0883,  0.1148,  0.1034,  5.9505],
       device='cuda:0')
Original likelihood: -19.968469619750977
Adjusted likelihood: -19.968469619750977
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.1456643139827065
Current ori: tensor([0.0883, 0.1148, 0.1034], device='cuda:0')
Middle force: tensor([1.0344, 0.9963, 1.8357, 0.5414, 1.2075, 1.1240, 0.5529, 0.5030, 0.9601],
       device='cuda:0')
Thumb force: tensor([0.8190, 0.5299, 0.9155, 1.3497, 1.6485, 1.0330, 1.2188, 1.8009, 0.8879],
       device='cuda:0')
Index force: tensor([0.5374, 1.0912, 0.6354, 0.5164, 0.7400, 0.6546, 0.9051, 0.5302, 0.8892],
       device='cuda:0')
Storing NORMAL transition: reward=0.0332 (scaled=0.0332), steps=1
Reward stats updated: mean 0.0074 -> 0.0075, std: 0.0767
Collected 239 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1691, Q2 Loss=1.1691, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7366
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.0913, Q2 Loss=1.0913, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9060
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4929, Q2 Loss=1.4929, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6795
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=0.8759, Q2 Loss=0.8759, Entropy=0.0004, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6027
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0672, Q2 Loss=1.0672, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9578

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.6%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (18.5%)
Actor update: 0.10s (39.7%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.139266
Q2 loss: 1.139266
Current threshold: -33.2696
Global Scale Offset: 0.0842
Reward stats: mean=0.0075, std=0.0767, count=239
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1393, Q2 Loss: 1.1393, Entropy: 0.0001, Mean TD Error: 0.9765, Threshold: -33.2696
tensor([ 0.2074,  0.7961,  0.5764,  0.1635, -0.0565,  0.5435,  0.5182,  0.6517,
         1.2936,  0.3741,  0.3761,  1.0862,  0.0797,  0.0966,  0.0739,  5.9565],
       device='cuda:0')
Original likelihood: -23.8162841796875
Adjusted likelihood: -23.8162841796875
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.772011313994881
Current ori: tensor([0.0797, 0.0966, 0.0739], device='cuda:0')
Middle force: tensor([0.8836, 0.6383, 0.7714, 0.5205, 1.0203, 0.6649, 0.5012, 0.5105],
       device='cuda:0')
Thumb force: tensor([0.5184, 0.9301, 0.7953, 0.6038, 1.1293, 1.2545, 1.9107, 1.4207],
       device='cuda:0')
Index force: tensor([0.9222, 0.5359, 0.5556, 0.5425, 0.5524, 0.8294, 0.5155, 0.8101],
       device='cuda:0')
Storing NORMAL transition: reward=0.0075 (scaled=0.0075), steps=1
Reward stats updated: mean 0.0075 -> 0.0075, std: 0.0766
Collected 240 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.7069, Q2 Loss=1.7069, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4442
SAC Update 2/5: Actor Loss=-0.0868, Q1 Loss=0.9739, Q2 Loss=0.9739, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9531
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7479, Q2 Loss=0.7479, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5263
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1611, Q2 Loss=1.1611, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5399
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0949, Q2 Loss=1.0949, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1811

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (17.1%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (18.9%)
Actor update: 0.11s (40.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.017366
Q1 loss: 1.136946
Q2 loss: 1.136946
Current threshold: -33.2699
Global Scale Offset: 0.0842
Reward stats: mean=0.0075, std=0.0766, count=240
----------------------------------------------
SAC Update - Actor Loss: -0.0174, Q1 Loss: 1.1369, Q2 Loss: 1.1369, Entropy: 0.0000, Mean TD Error: 1.1289, Threshold: -33.2699
tensor([ 0.2644,  0.7853,  0.8246, -0.1340, -0.1166,  0.4704,  0.5637,  0.8362,
         1.4021,  0.2981,  0.2532,  1.2178,  0.0966,  0.1131,  0.0602,  6.0616],
       device='cuda:0')
Original likelihood: -23.49073600769043
Adjusted likelihood: -23.49073600769043
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.486686013988219
Current ori: tensor([0.0966, 0.1131, 0.0602], device='cuda:0')
Middle force: tensor([1.8148, 0.5468, 1.2080, 1.0990, 0.5620, 0.5033, 0.9349],
       device='cuda:0')
Thumb force: tensor([0.8803, 1.3080, 1.5645, 0.9881, 1.1445, 1.7471, 0.8805],
       device='cuda:0')
Index force: tensor([0.6527, 0.5163, 0.7280, 0.6343, 0.8825, 0.5267, 0.8593],
       device='cuda:0')
Storing NORMAL transition: reward=0.0667 (scaled=0.0667), steps=1
Reward stats updated: mean 0.0075 -> 0.0077, std: 0.0765
Collected 241 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.9599, Q2 Loss=1.9599, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7416
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.9391, Q2 Loss=0.9391, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9923
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9718, Q2 Loss=0.9718, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6585
SAC Update 4/5: Actor Loss=-0.2269, Q1 Loss=1.7582, Q2 Loss=1.7582, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4168
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.4109, Q2 Loss=2.4109, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8111

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.2%)
Q1 update: 0.05s (18.8%)
Q2 update: 0.05s (17.7%)
Actor update: 0.11s (42.8%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.091426
Q1 loss: 1.607982
Q2 loss: 1.607982
Current threshold: -33.2701
Global Scale Offset: 0.0842
Reward stats: mean=0.0077, std=0.0765, count=241
----------------------------------------------
SAC Update - Actor Loss: -0.0914, Q1 Loss: 1.6080, Q2 Loss: 1.6080, Entropy: 0.0000, Mean TD Error: 1.7240, Threshold: -33.2701
tensor([ 2.2256e-01,  8.9734e-01,  7.6397e-01, -2.7563e-02, -5.9627e-02,
         4.6846e-01,  6.2545e-01,  8.7070e-01,  1.3628e+00,  3.3015e-01,
         2.3440e-01,  1.2087e+00,  9.0060e-02,  8.2286e-02, -1.6502e-03,
         6.1261e+00], device='cuda:0')
Original likelihood: -22.683185577392578
Adjusted likelihood: -22.683185577392578
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.340318061993457
Current ori: tensor([ 0.0901,  0.0823, -0.0017], device='cuda:0')
Middle force: tensor([0.5033, 1.3591, 1.2856, 0.7957, 0.5973, 0.5007], device='cuda:0')
Thumb force: tensor([0.5897, 1.3928, 1.0249, 1.5925, 1.5699, 0.5433], device='cuda:0')
Index force: tensor([0.6292, 0.5740, 0.7341, 0.5762, 0.5026, 0.6790], device='cuda:0')
Storing NORMAL transition: reward=0.0160 (scaled=0.0160), steps=1
Reward stats updated: mean 0.0077 -> 0.0078, std: 0.0763
Collected 242 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.9219, Q2 Loss=1.9219, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3312
SAC Update 2/5: Actor Loss=-0.0041, Q1 Loss=1.2182, Q2 Loss=1.2182, Entropy=0.3203, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2166
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3645, Q2 Loss=1.3645, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4831
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4929, Q2 Loss=1.4929, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6401
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8277, Q2 Loss=0.8277, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5135

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.9%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.7%)
Actor update: 0.10s (42.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046880
Q1 loss: 1.365080
Q2 loss: 1.365080
Current threshold: -33.2730
Global Scale Offset: 0.0842
Reward stats: mean=0.0078, std=0.0763, count=242
----------------------------------------------
SAC Update - Actor Loss: -0.0469, Q1 Loss: 1.3651, Q2 Loss: 1.3651, Entropy: 0.0641, Mean TD Error: 1.4369, Threshold: -33.2730
tensor([ 0.2912,  0.9694,  0.7443, -0.0811,  0.0078,  0.5261,  0.5783,  0.9036,
         1.3138,  0.2958,  0.1922,  1.2495,  0.0823,  0.0481, -0.0118, -6.2426],
       device='cuda:0')
Original likelihood: -25.002918243408203
Adjusted likelihood: -25.002918243408203
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 3.948391725018155
Current ori: tensor([ 0.0823,  0.0481, -0.0118], device='cuda:0')
Middle force: tensor([0.5767, 0.5302, 1.1385, 0.5006, 0.5152], device='cuda:0')
Thumb force: tensor([0.5881, 0.5537, 1.6148, 0.8396, 0.8413], device='cuda:0')
Index force: tensor([0.5165, 0.5477, 0.6689, 0.5420, 0.5344], device='cuda:0')
Storing NORMAL transition: reward=-0.0863 (scaled=-0.0863), steps=1
Reward stats updated: mean 0.0078 -> 0.0074, std: 0.0764
Collected 243 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0262, Q2 Loss=1.0262, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8897
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.4768, Q2 Loss=1.4768, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9845
SAC Update 3/5: Actor Loss=-0.0055, Q1 Loss=0.6646, Q2 Loss=0.6646, Entropy=0.3410, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5132
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9908, Q2 Loss=0.9908, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5644
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7473, Q2 Loss=0.7473, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1468

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (22.1%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (17.3%)
Actor update: 0.08s (37.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.047161
Q1 loss: 0.981126
Q2 loss: 0.981126
Current threshold: -33.2780
Global Scale Offset: 0.0842
Reward stats: mean=0.0074, std=0.0764, count=243
----------------------------------------------
SAC Update - Actor Loss: -0.0472, Q1 Loss: 0.9811, Q2 Loss: 0.9811, Entropy: 0.0682, Mean TD Error: 0.6197, Threshold: -33.2780
tensor([ 3.7138e-01,  9.9186e-01,  6.9799e-01,  6.2961e-02, -9.3163e-04,
         5.5445e-01,  5.4212e-01,  9.0718e-01,  1.2430e+00,  3.3323e-01,
         3.2623e-01,  1.2394e+00,  9.7905e-02,  5.4843e-02,  7.2847e-02,
         5.6672e+00], device='cuda:0')
Original likelihood: -25.72099494934082
Adjusted likelihood: -25.72099494934082
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 4.067211708985269
Current ori: tensor([0.0979, 0.0548, 0.0728], device='cuda:0')
Middle force: tensor([0.5269, 1.1132, 0.5005, 0.5142], device='cuda:0')
Thumb force: tensor([0.5501, 1.5916, 0.8163, 0.8306], device='cuda:0')
Index force: tensor([0.5440, 0.6486, 0.5440, 0.5321], device='cuda:0')
Storing NORMAL transition: reward=0.0251 (scaled=0.0251), steps=1
Reward stats updated: mean 0.0074 -> 0.0075, std: 0.0763
Collected 244 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0714, Q2 Loss=1.0714, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2010
SAC Update 2/5: Actor Loss=-0.0016, Q1 Loss=1.0697, Q2 Loss=1.0697, Entropy=0.2548, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4920
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0007, Q2 Loss=1.0007, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1573
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.3601, Q2 Loss=2.3601, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1621
SAC Update 5/5: Actor Loss=-0.0057, Q1 Loss=1.2755, Q2 Loss=1.2755, Entropy=0.3425, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1431

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.0%)
Q1 update: 0.04s (18.3%)
Q2 update: 0.05s (20.2%)
Actor update: 0.10s (41.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.047512
Q1 loss: 1.355473
Q2 loss: 1.355473
Current threshold: -33.2846
Global Scale Offset: 0.0842
Reward stats: mean=0.0075, std=0.0763, count=244
----------------------------------------------
SAC Update - Actor Loss: -0.0475, Q1 Loss: 1.3555, Q2 Loss: 1.3555, Entropy: 0.1195, Mean TD Error: 1.2311, Threshold: -33.2846
tensor([ 0.2836,  1.0834,  0.5118,  0.1993, -0.0131,  0.5669,  0.5621,  0.7610,
         1.2866,  0.3026,  0.2872,  1.1740,  0.0863,  0.0628,  0.0483,  6.0871],
       device='cuda:0')
Original likelihood: -25.92561149597168
Adjusted likelihood: -25.92561149597168
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 10 3.62434514000779
Current ori: tensor([0.0863, 0.0628, 0.0483], device='cuda:0')
Middle force: tensor([0.5971, 1.3956, 0.6752], device='cuda:0')
Thumb force: tensor([0.7963, 1.3171, 0.6343], device='cuda:0')
Index force: tensor([0.5002, 0.5185, 0.5762], device='cuda:0')
Storing NORMAL transition: reward=0.0188 (scaled=0.0188), steps=1
Reward stats updated: mean 0.0075 -> 0.0075, std: 0.0761
Collected 245 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.0319, Q2 Loss=1.0319, Entropy=0.0114, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8193
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.4841, Q2 Loss=2.4841, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3983
SAC Update 3/5: Actor Loss=-0.1039, Q1 Loss=1.1910, Q2 Loss=1.1910, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3336
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.4864, Q2 Loss=1.4864, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1043
SAC Update 5/5: Actor Loss=-0.0059, Q1 Loss=1.2824, Q2 Loss=1.2824, Entropy=0.3437, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1981

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (20.8%)
Q1 update: 0.04s (19.4%)
Q2 update: 0.04s (17.6%)
Actor update: 0.09s (38.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.114062
Q1 loss: 1.495180
Q2 loss: 1.495180
Current threshold: -33.2930
Global Scale Offset: 0.0842
Reward stats: mean=0.0075, std=0.0761, count=245
----------------------------------------------
SAC Update - Actor Loss: -0.1141, Q1 Loss: 1.4952, Q2 Loss: 1.4952, Entropy: 0.0710, Mean TD Error: 1.3707, Threshold: -33.2930
tensor([ 0.3069,  1.0982,  0.3926,  0.1317, -0.0400,  0.5634,  0.5153,  0.8980,
         1.4166,  0.1919,  0.2231,  1.2048,  0.0928,  0.0716,  0.0268,  5.6327],
       device='cuda:0')
Original likelihood: -24.197673797607422
Adjusted likelihood: -24.197673797607422
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 11 3.509625669044908
Current ori: tensor([0.0928, 0.0716, 0.0268], device='cuda:0')
Middle force: tensor([0.5016, 0.5051], device='cuda:0')
Thumb force: tensor([1.7155, 1.2870], device='cuda:0')
Index force: tensor([0.5120, 0.7711], device='cuda:0')
Storing NORMAL transition: reward=0.0005 (scaled=0.0005), steps=1
Reward stats updated: mean 0.0075 -> 0.0075, std: 0.0760
Collected 246 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6744, Q2 Loss=0.6744, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1856
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8945, Q2 Loss=0.8945, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6242
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8433, Q2 Loss=0.8433, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3774
SAC Update 4/5: Actor Loss=-0.0068, Q1 Loss=1.5369, Q2 Loss=1.5369, Entropy=0.3465, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8410
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.9593, Q2 Loss=1.9593, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7537

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (18.6%)
Actor update: 0.08s (40.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.047405
Q1 loss: 1.181701
Q2 loss: 1.181701
Current threshold: -33.3035
Global Scale Offset: 0.0842
Reward stats: mean=0.0075, std=0.0760, count=246
----------------------------------------------
SAC Update - Actor Loss: -0.0474, Q1 Loss: 1.1817, Q2 Loss: 1.1817, Entropy: 0.0693, Mean TD Error: 0.9564, Threshold: -33.3035
tensor([ 0.2642,  1.1436,  0.4457,  0.1393, -0.0241,  0.5492,  0.5709,  0.8403,
         1.3939,  0.2375,  0.2106,  1.2164,  0.0932,  0.0640,  0.0272,  5.3440],
       device='cuda:0')
Original likelihood: -26.260311126708984
Adjusted likelihood: -26.260311126708984
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 12 3.5936141929705627
Current ori: tensor([0.0932, 0.0640, 0.0272], device='cuda:0')
Middle force: tensor([0.5049], device='cuda:0')
Thumb force: tensor([1.2676], device='cuda:0')
Index force: tensor([0.7579], device='cuda:0')
Storing NORMAL transition: reward=0.0040 (scaled=0.0040), steps=1
Reward stats updated: mean 0.0075 -> 0.0075, std: 0.0758
Collected 247 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9372, Q2 Loss=0.9372, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6984
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.4410, Q2 Loss=1.4410, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2583
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2542, Q2 Loss=1.2542, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5950
SAC Update 4/5: Actor Loss=-0.0125, Q1 Loss=0.6676, Q2 Loss=0.6676, Entropy=0.0245, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0163
SAC Update 5/5: Actor Loss=-0.1612, Q1 Loss=2.0275, Q2 Loss=2.0275, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0795

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (19.0%)
Q2 update: 0.05s (19.0%)
Actor update: 0.10s (38.9%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.080791
Q1 loss: 1.265511
Q2 loss: 1.265511
Current threshold: -33.3123
Global Scale Offset: 0.0843
Reward stats: mean=0.0075, std=0.0758, count=247
----------------------------------------------
SAC Update - Actor Loss: -0.0808, Q1 Loss: 1.2655, Q2 Loss: 1.2655, Entropy: 0.0049, Mean TD Error: 1.3295, Threshold: -33.3123
Original likelihood: -28.110023498535156
Adjusted likelihood: -28.110023498535156
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([0.0943, 0.0643, 0.0228], device='cuda:0')
11 turn
Sampling time 3.6944984319852665
tensor([ 0.2355,  1.1800,  0.4895,  0.1467, -0.0253,  0.5383,  0.5912,  0.8291,
         1.3834,  0.2570,  0.2166,  1.2156,  0.0943,  0.0643,  0.0228,  5.3163],
       device='cuda:0')
Original likelihood: -28.686321258544922
Adjusted likelihood: -28.686321258544922
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.8377607690054
Current ori: tensor([0.0943, 0.0643, 0.0228], device='cuda:0')
Middle force: tensor([0.8902, 0.6311, 0.3883, 1.1187, 0.4417, 1.4079, 0.9686, 0.4463, 0.5960,
        0.8697, 0.6464, 0.7193], device='cuda:0')
Thumb force: tensor([0.5021, 1.5780, 0.5086, 0.8226, 0.8466, 1.9639, 1.2393, 2.8241, 0.5398,
        2.0138, 1.8628, 0.9216], device='cuda:0')
Index force: tensor([0.5669, 0.4999, 0.5116, 0.5821, 0.4808, 0.5416, 0.4960, 0.7047, 0.7668,
        0.4972, 1.6209, 0.7058], device='cuda:0')
Storing NORMAL transition: reward=-0.0005 (scaled=-0.0005), steps=1
Reward stats updated: mean 0.0075 -> 0.0074, std: 0.0757
Collected 248 transitions for RL
SAC Update 1/5: Actor Loss=-0.0856, Q1 Loss=0.9842, Q2 Loss=0.9842, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9994
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2194, Q2 Loss=1.2194, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8302
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.8495, Q2 Loss=0.8495, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1989
SAC Update 4/5: Actor Loss=-0.0001, Q1 Loss=0.7693, Q2 Loss=0.7693, Entropy=0.0300, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9405
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.6808, Q2 Loss=0.6808, Entropy=0.0088, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6029

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.04s (18.0%)
Actor update: 0.10s (39.7%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.063187
Q1 loss: 0.900646
Q2 loss: 0.900646
Current threshold: -33.3177
Global Scale Offset: 0.0843
Reward stats: mean=0.0074, std=0.0757, count=248
----------------------------------------------
SAC Update - Actor Loss: -0.0632, Q1 Loss: 0.9006, Q2 Loss: 0.9006, Entropy: 0.0078, Mean TD Error: 0.7144, Threshold: -33.3177
tensor([ 0.1488,  1.4728,  0.1901, -0.1672,  0.0150,  0.6300,  0.5480,  0.6667,
         1.3838,  0.2375,  0.1962,  1.1856,  0.0775,  0.0459,  0.0280,  5.4357],
       device='cuda:0')
Original likelihood: -40.09577178955078
Adjusted likelihood: -40.09577178955078
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 39.73897933959961
Projection step: 1, Loss: 39.2739143371582
Projection step: 2, Loss: 38.942298889160156
Projection step: 3, Loss: 38.87102508544922
Projection step: 4, Loss: 37.828330993652344
Projection step: 5, Loss: 37.541709899902344
Projection step: 6, Loss: 36.58106994628906
Projection step: 7, Loss: 36.54285430908203
Projection step: 8, Loss: 35.97110366821289
Projection step: 9, Loss: 35.478668212890625
Projection step: 10, Loss: 35.246185302734375
Projection step: 11, Loss: 34.90606689453125
Projection step: 12, Loss: 33.993194580078125
Projection step: 13, Loss: 33.84806823730469
Projection step: 14, Loss: 33.922386169433594
Projection step: 15, Loss: 32.68560028076172
Projection step: 16, Loss: 31.941722869873047
Projection step: 17, Loss: 31.905784606933594
Projection step: 18, Loss: 31.46794319152832
Projection step: 19, Loss: 30.743881225585938
Projection step: 20, Loss: 30.807518005371094
Projection step: 21, Loss: 30.00670623779297
Projection step: 22, Loss: 30.371593475341797
Projection step: 23, Loss: 29.28448486328125
Projection step: 24, Loss: 29.08312225341797
Final likelihood: tensor([-28.1689, -29.6758, -29.1607, -28.8021, -29.6021, -28.3713, -29.0973,
        -29.6099, -28.5328, -29.1177, -31.7165, -27.5155, -26.8883, -30.9574,
        -29.2843, -29.9048])
Final projection likelihood: -29.1503
1 mode projection succeeded
New goal: tensor([ 0.1742,  1.3617,  0.0651, -0.1624, -0.0181,  0.5914,  0.4651,  0.6985,
         1.3700,  0.1339,  0.2044,  1.1634,  0.0771,  0.0478,  0.0062],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[-0.0011]], device='cuda:0')
Original likelihood: -34.00401306152344
Adjusted likelihood: -34.00401306152344
Likelihood residual: 0.0
Original likelihood: -34.00617218017578
Adjusted likelihood: -34.00617218017578
Likelihood residual: 0.0
{'index': 34.00617218017578, 'thumb_middle': 34.00401306152344}
Current yaw: tensor([0.0775, 0.0459, 0.0280], device='cuda:0')
12 thumb_middle
tensor([ 0.1488,  1.4728,  0.1901, -0.1672,  0.0150,  0.6300,  0.5480,  0.6667,
         1.3838,  0.2375,  0.1962,  1.1856,  0.0775,  0.0459,  0.0280,  5.4357],
       device='cuda:0')
Solve time for step 1 8.87730765901506
Current ori: tensor([0.0775, 0.0459, 0.0280], device='cuda:0')
Index force: tensor([0.5650, 0.5701, 0.5876, 0.5992], device='cuda:0')
tensor([ 0.1211,  1.4961,  0.1481, -0.1615, -0.1163,  0.5622,  0.4503,  0.6803,
         1.3711,  0.1433,  0.1623,  1.1498,  0.1726,  0.1025,  0.0376,  5.5423],
       device='cuda:0')
Solve time for step 2 3.55451914697187
Current ori: tensor([0.1726, 0.1025, 0.0376], device='cuda:0')
Index force: tensor([0.5505, 0.5464, 0.5771], device='cuda:0')
tensor([ 0.0929,  1.5368,  0.0935, -0.1604, -0.1404,  0.4980,  0.4409,  0.6936,
         1.3931,  0.1388,  0.1944,  1.1688,  0.2869,  0.0984,  0.0634,  5.5148],
       device='cuda:0')
Solve time for step 3 3.4008308660122566
Current ori: tensor([0.2869, 0.0984, 0.0634], device='cuda:0')
Index force: tensor([0.5643, 0.5883], device='cuda:0')
tensor([ 0.0749,  1.5708,  0.0829, -0.1595, -0.1714,  0.3583,  0.4311,  0.7006,
         1.4261,  0.1417,  0.2679,  1.2141,  0.3497,  0.1314,  0.0699,  5.3698],
       device='cuda:0')
Solve time for step 4 3.324004755995702
Current ori: tensor([0.3497, 0.1314, 0.0699], device='cuda:0')
Index force: tensor([0.5739], device='cuda:0')
Storing RECOVERY transition: reward=-0.1715 (scaled=-0.1715), steps=1
Reward stats updated: mean 0.0074 -> 0.0067, std: 0.0763
Collected 249 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.1304, Q2 Loss=2.1304, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4145
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.4243, Q2 Loss=1.4243, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2842
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.7365, Q2 Loss=0.7365, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6748
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0917, Q2 Loss=1.0917, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1794
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.3696, Q2 Loss=1.3696, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3396

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.8%)
Q1 update: 0.05s (18.7%)
Q2 update: 0.05s (17.4%)
Actor update: 0.11s (41.5%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.350506
Q2 loss: 1.350506
Current threshold: -33.3210
Global Scale Offset: 0.0843
Reward stats: mean=0.0067, std=0.0763, count=249
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.3505, Q2 Loss: 1.3505, Entropy: 0.0000, Mean TD Error: 1.3785, Threshold: -33.3210
Original likelihood: -233.50949096679688
Adjusted likelihood: -233.50949096679688
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 254.4893798828125
Projection step: 1, Loss: 242.46356201171875
Projection step: 2, Loss: 251.94180297851562
Projection step: 3, Loss: 245.79995727539062
Projection step: 4, Loss: 251.892333984375
Projection step: 5, Loss: 249.24752807617188
Projection step: 6, Loss: 244.70379638671875
Projection step: 7, Loss: 243.27188110351562
Projection step: 8, Loss: 243.33486938476562
Projection step: 9, Loss: 261.2989501953125
Projection step: 10, Loss: 252.0127410888672
Projection step: 11, Loss: 255.3316650390625
Projection step: 12, Loss: 248.89585876464844
Projection step: 13, Loss: 260.74322509765625
Projection step: 14, Loss: 254.23110961914062
Projection step: 15, Loss: 247.83218383789062
Projection step: 16, Loss: 250.23699951171875
Projection step: 17, Loss: 249.73294067382812
Projection step: 18, Loss: 245.41517639160156
Projection step: 19, Loss: 259.3990173339844
Projection step: 20, Loss: 249.675048828125
Projection step: 21, Loss: 250.10177612304688
Projection step: 22, Loss: 242.49325561523438
Projection step: 23, Loss: 249.5286407470703
Projection step: 24, Loss: 247.85177612304688
Final likelihood: tensor([-235.2346, -238.6032, -245.5829, -265.8575, -245.7482, -227.0881,
        -234.7720, -236.3238, -245.0017, -250.3890, -273.7481, -253.4878,
        -272.1907, -233.6886, -274.4934, -212.3917])
Final projection likelihood: -246.5376
1 mode projection failed, trying anyway
New goal: tensor([ 0.0295,  1.5926,  0.0827, -0.1528, -0.0600,  0.1975,  0.4782,  0.7047,
         1.4588,  0.1141,  0.3741,  1.3241,  0.3802,  0.1882,  0.0948],
       device='cuda:0')
tensor([[0.0218]], device='cuda:0') tensor([[-0.0100]], device='cuda:0') tensor([[0.0184]], device='cuda:0')
Original likelihood: -176.9298553466797
Adjusted likelihood: -176.9298553466797
Likelihood residual: 0.0
Original likelihood: -219.80410766601562
Adjusted likelihood: -219.80410766601562
Likelihood residual: 0.0
{'index': 219.80410766601562, 'thumb_middle': 176.9298553466797}
Current yaw: tensor([0.3804, 0.1881, 0.0658], device='cuda:0')
13 thumb_middle
tensor([ 0.0324,  1.5900,  0.0883, -0.1612, -0.0570,  0.1954,  0.4695,  0.7034,
         1.4679,  0.1172,  0.3717,  1.3137,  0.3804,  0.1881,  0.0658,  4.2049],
       device='cuda:0')
Solve time for step 1 8.958688393991906
Current ori: tensor([0.3804, 0.1881, 0.0658], device='cuda:0')
Index force: tensor([0.5720, 0.5617, 0.5752, 0.5901], device='cuda:0')
tensor([-0.0670,  1.5992,  0.0501, -0.1594,  0.0558, -0.1112,  0.5159,  0.7165,
         1.5000,  0.1715,  0.3837,  1.3428,  0.4036,  0.2164,  0.1193,  3.8628],
       device='cuda:0')
Solve time for step 2 3.606605426000897
Current ori: tensor([0.4036, 0.2164, 0.1193], device='cuda:0')
Index force: tensor([0.5854, 0.5679, 0.5867], device='cuda:0')
tensor([-0.1114,  1.6040,  0.0464, -0.1568,  0.0629, -0.1425,  0.5230,  0.7181,
         1.5000,  0.2124,  0.3873,  1.3611,  0.4053,  0.2155,  0.1333,  5.5604],
       device='cuda:0')
Solve time for step 3 3.3845534659922123
Current ori: tensor([0.4053, 0.2155, 0.1333], device='cuda:0')
Index force: tensor([0.5681, 0.5768], device='cuda:0')
tensor([-0.1418,  1.6065,  0.0556, -0.1521,  0.0546, -0.1150,  0.5240,  0.7202,
         1.5000,  0.2392,  0.3869,  1.3772,  0.4038,  0.2163,  0.1216,  6.2504],
       device='cuda:0')
Solve time for step 4 3.23688859003596
Current ori: tensor([0.4038, 0.2163, 0.1216], device='cuda:0')
Index force: tensor([0.5394], device='cuda:0')
Storing RECOVERY transition: reward=-0.2430 (scaled=-0.2430), steps=1
Reward stats updated: mean 0.0067 -> 0.0057, std: 0.0778
Collected 250 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8610, Q2 Loss=0.8610, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5100
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.9373, Q2 Loss=1.9373, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0914
SAC Update 3/5: Actor Loss=-0.0596, Q1 Loss=0.8460, Q2 Loss=0.8460, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8519
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6832, Q2 Loss=0.6832, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4489
SAC Update 5/5: Actor Loss=-0.2243, Q1 Loss=1.8535, Q2 Loss=1.8535, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5455

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.9%)
Target Q: 0.04s (18.9%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.04s (18.8%)
Actor update: 0.09s (38.6%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.102835
Q1 loss: 1.236214
Q2 loss: 1.236214
Current threshold: -33.3229
Global Scale Offset: 0.0843
Reward stats: mean=0.0057, std=0.0778, count=250
----------------------------------------------
SAC Update - Actor Loss: -0.1028, Q1 Loss: 1.2362, Q2 Loss: 1.2362, Entropy: 0.0000, Mean TD Error: 1.0896, Threshold: -33.3229
Original likelihood: -297.3809814453125
Adjusted likelihood: -297.3809814453125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 293.003173828125
Projection step: 1, Loss: 292.3408508300781
Projection step: 2, Loss: 300.5013427734375
Projection step: 3, Loss: 288.11639404296875
Projection step: 4, Loss: 295.6978759765625
Projection step: 5, Loss: 290.719482421875
Projection step: 6, Loss: 294.2386474609375
Projection step: 7, Loss: 283.7587890625
Projection step: 8, Loss: 282.0112609863281
Projection step: 9, Loss: 285.6465148925781
Projection step: 10, Loss: 297.7073059082031
Projection step: 11, Loss: 290.74749755859375
Projection step: 12, Loss: 295.4755859375
Projection step: 13, Loss: 296.5152893066406
Projection step: 14, Loss: 303.8661804199219
Projection step: 15, Loss: 295.6616516113281
Projection step: 16, Loss: 293.4638671875
Projection step: 17, Loss: 295.2704162597656
Projection step: 18, Loss: 297.8088073730469
Projection step: 19, Loss: 292.7284851074219
Projection step: 20, Loss: 297.9187927246094
Projection step: 21, Loss: 303.563232421875
Projection step: 22, Loss: 298.47900390625
Projection step: 23, Loss: 290.12274169921875
Projection step: 24, Loss: 296.48992919921875
Final likelihood: tensor([-293.9538, -296.0363, -331.1041, -279.1218, -306.5136, -302.7988,
        -261.0497, -297.9879, -281.0250, -310.8860, -333.0056, -274.6192,
        -280.3760, -261.4398, -310.9703, -286.3112])
Final projection likelihood: -294.2000
1 mode projection failed, trying anyway
New goal: tensor([-0.2000,  1.6103,  0.0397, -0.1412,  0.0342, -0.1145,  0.5316,  0.7210,
         1.4907,  0.2852,  0.4092,  1.4018,  0.4047,  0.2156,  0.1742],
       device='cuda:0')
tensor([[0.0102]], device='cuda:0') tensor([[-0.0037]], device='cuda:0') tensor([[0.0175]], device='cuda:0')
Original likelihood: -228.007568359375
Adjusted likelihood: -228.007568359375
Likelihood residual: 0.0
Original likelihood: -315.004638671875
Adjusted likelihood: -315.004638671875
Likelihood residual: 0.0
{'index': 315.004638671875, 'thumb_middle': 228.007568359375}
Current yaw: tensor([0.4048, 0.2155, 0.1294], device='cuda:0')
14 thumb_middle
tensor([-0.1977,  1.6076,  0.0476, -0.1475,  0.0370, -0.1155,  0.5243,  0.7157,
         1.5000,  0.2856,  0.4070,  1.3899,  0.4048,  0.2155,  0.1294,  5.1835],
       device='cuda:0')
Solve time for step 1 8.663143319019582
Current ori: tensor([0.4048, 0.2155, 0.1294], device='cuda:0')
Index force: tensor([0.6191, 0.5805, 0.5916, 0.6054], device='cuda:0')
tensor([-2.3117e-01,  1.6099e+00,  7.1276e-04, -1.6199e-01,  7.2710e-02,
        -1.9600e-01,  5.5763e-01,  7.2926e-01,  1.5000e+00,  3.1254e-01,
         4.0623e-01,  1.3981e+00,  4.0851e-01,  2.1243e-01,  1.5074e-01,
         4.8281e+00], device='cuda:0')
Solve time for step 2 3.5060814120224677
Current ori: tensor([0.4085, 0.2124, 0.1507], device='cuda:0')
Index force: tensor([0.5734, 0.5849, 0.5995], device='cuda:0')
tensor([-0.2564,  1.6037,  0.0152, -0.1481,  0.0815, -0.1960,  0.5659,  0.7319,
         1.5000,  0.3348,  0.4170,  1.4062,  0.4080,  0.2129,  0.1462,  5.5154],
       device='cuda:0')
Solve time for step 3 3.3482240739976987
Current ori: tensor([0.4080, 0.2129, 0.1462], device='cuda:0')
Index force: tensor([0.5909, 0.6010], device='cuda:0')
tensor([-0.2608,  1.6004,  0.0208, -0.1405,  0.0890, -0.1960,  0.5626,  0.7303,
         1.5000,  0.3375,  0.4203,  1.4067,  0.4077,  0.2132,  0.1424,  5.6090],
       device='cuda:0')
Solve time for step 4 3.2354617869714275
Current ori: tensor([0.4077, 0.2132, 0.1424], device='cuda:0')
Index force: tensor([0.5328], device='cuda:0')
Storing RECOVERY transition: reward=-0.2546 (scaled=-0.2546), steps=1
Reward stats updated: mean 0.0057 -> 0.0047, std: 0.0794
Collected 251 transitions for RL
SAC Update 1/5: Actor Loss=-0.2191, Q1 Loss=2.1345, Q2 Loss=2.1345, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8686
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9372, Q2 Loss=0.9372, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8072
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8479, Q2 Loss=0.8479, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9234
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6909, Q2 Loss=0.6909, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3944
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.3670, Q2 Loss=1.3670, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3143

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (19.1%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (39.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.043823
Q1 loss: 1.195514
Q2 loss: 1.195514
Current threshold: -33.3241
Global Scale Offset: 0.0843
Reward stats: mean=0.0047, std=0.0794, count=251
----------------------------------------------
SAC Update - Actor Loss: -0.0438, Q1 Loss: 1.1955, Q2 Loss: 1.1955, Entropy: 0.0000, Mean TD Error: 1.0616, Threshold: -33.3241
Original likelihood: -287.4925842285156
Adjusted likelihood: -287.4925842285156
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 291.0455017089844
Projection step: 1, Loss: 291.9774475097656
Projection step: 2, Loss: 296.01910400390625
Projection step: 3, Loss: 287.5733947753906
Projection step: 4, Loss: 296.7440490722656
Projection step: 5, Loss: 290.15228271484375
Projection step: 6, Loss: 301.8550109863281
Projection step: 7, Loss: 300.32183837890625
Projection step: 8, Loss: 295.186767578125
Projection step: 9, Loss: 302.3789367675781
Projection step: 10, Loss: 283.4043273925781
Projection step: 11, Loss: 296.61981201171875
Projection step: 12, Loss: 291.575927734375
Projection step: 13, Loss: 307.2681884765625
Projection step: 14, Loss: 297.03863525390625
Projection step: 15, Loss: 302.64093017578125
Projection step: 16, Loss: 303.65252685546875
Projection step: 17, Loss: 294.4042053222656
Projection step: 18, Loss: 302.5345153808594
Projection step: 19, Loss: 296.57952880859375
Projection step: 20, Loss: 293.6610107421875
Projection step: 21, Loss: 287.048583984375
Projection step: 22, Loss: 312.4456787109375
Projection step: 23, Loss: 296.61358642578125
Projection step: 24, Loss: 306.7029113769531
Final likelihood: tensor([-365.0149, -234.1034, -307.0905, -280.5845, -323.8955, -296.6245,
        -324.3481, -292.8126, -314.7780, -299.5822, -311.2238, -279.0657,
        -380.3110, -304.6034, -244.5887, -270.6100])
Final projection likelihood: -301.8273
1 mode projection failed, trying anyway
New goal: tensor([-0.3112,  1.6116,  0.0112, -0.1300,  0.0856, -0.1963,  0.5594,  0.7271,
         1.4894,  0.3723,  0.4402,  1.4277,  0.4077,  0.2134,  0.1955],
       device='cuda:0')
tensor([[0.0035]], device='cuda:0') tensor([[-0.0019]], device='cuda:0') tensor([[0.0125]], device='cuda:0')
Original likelihood: -285.22259521484375
Adjusted likelihood: -285.22259521484375
Likelihood residual: 0.0
{'index': 285.22259521484375, 'thumb_middle': inf}
Current yaw: tensor([0.4077, 0.2132, 0.1427], device='cuda:0')
15 index
tensor([-0.3087,  1.6097,  0.0202, -0.1369,  0.0885, -0.1960,  0.5517,  0.7186,
         1.5000,  0.3703,  0.4379,  1.4144,  0.4077,  0.2132,  0.1427,  5.1794],
       device='cuda:0')
Solve time for step 1 10.410773139970843
Current ori: tensor([0.4077, 0.2132, 0.1427], device='cuda:0')
Middle force: tensor([0.5166, 0.5789, 0.5728, 0.5397], device='cuda:0')
Thumb force: tensor([0.6371, 0.5820, 0.5279, 0.5729], device='cuda:0')
tensor([-0.2631,  1.6100,  0.0257, -0.1272,  0.1649, -0.1960,  0.5715,  0.7243,
         1.5000,  0.3782,  0.4467,  1.4130,  0.4035,  0.2162,  0.1113,  5.2459],
       device='cuda:0')
Solve time for step 2 4.242601138015743
Current ori: tensor([0.4035, 0.2162, 0.1113], device='cuda:0')
Middle force: tensor([0.5785, 0.5692, 0.5371], device='cuda:0')
Thumb force: tensor([0.5787, 0.5262, 0.5709], device='cuda:0')
tensor([-0.2910,  1.6100,  0.0243, -0.1276,  0.1565, -0.1960,  0.5610,  0.7279,
         1.5000,  0.3780,  0.4462,  1.4155,  0.4040,  0.2162,  0.1150,  5.5048],
       device='cuda:0')
Solve time for step 3 4.090006862999871
Current ori: tensor([0.4040, 0.2162, 0.1150], device='cuda:0')
Middle force: tensor([0.5705, 0.5345], device='cuda:0')
Thumb force: tensor([0.5247, 0.5684], device='cuda:0')
tensor([-0.2729,  1.6100,  0.0267, -0.1263,  0.1656, -0.1959,  0.5626,  0.7278,
         1.5000,  0.3786,  0.4468,  1.4126,  0.4035,  0.2164,  0.1109,  5.4369],
       device='cuda:0')
Solve time for step 4 3.8712240930180997
Current ori: tensor([0.4035, 0.2164, 0.1109], device='cuda:0')
Middle force: tensor([0.5568], device='cuda:0')
Thumb force: tensor([0.5165], device='cuda:0')
Storing RECOVERY transition: reward=-0.2294 (scaled=-0.2294), steps=1
Reward stats updated: mean 0.0047 -> 0.0037, std: 0.0806
Collected 252 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=3.2275, Q2 Loss=3.2275, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7844
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.1743, Q2 Loss=1.1743, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1394
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0560, Q2 Loss=1.0560, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9120
SAC Update 4/5: Actor Loss=-0.1858, Q1 Loss=1.5255, Q2 Loss=1.5255, Entropy=0.0083, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3164
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.1995, Q2 Loss=2.1995, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3097

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.3%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.6%)
Actor update: 0.08s (40.3%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.037153
Q1 loss: 1.836547
Q2 loss: 1.836547
Current threshold: -33.3248
Global Scale Offset: 0.0843
Reward stats: mean=0.0037, std=0.0806, count=252
----------------------------------------------
SAC Update - Actor Loss: -0.0372, Q1 Loss: 1.8365, Q2 Loss: 1.8365, Entropy: 0.0017, Mean TD Error: 1.6924, Threshold: -33.3248
Original likelihood: -303.18792724609375
Adjusted likelihood: -303.18792724609375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 15
Loaded trajectory sampler
Current yaw: tensor([-0.0014,  0.0145, -0.0308], device='cuda:0')
Current yaw: tensor([-0.0014,  0.0145, -0.0308], device='cuda:0')
1 turn
Sampling time 3.8409032380441204
tensor([ 0.1373,  0.5755,  0.6160,  0.5806, -0.1211,  0.4982,  0.9641,  0.8938,
         1.2631,  0.2638,  0.2117,  1.2058, -0.0014,  0.0145, -0.0308,  0.0336],
       device='cuda:0')
Original likelihood: -19.567323684692383
Adjusted likelihood: -19.567323684692383
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.882146888005082
Current ori: tensor([-0.0014,  0.0145, -0.0308], device='cuda:0')
Middle force: tensor([0.5431, 0.5273, 1.5076, 0.6006, 0.5887, 0.5579, 0.5371, 0.5288, 0.5290,
        0.5968, 1.0207, 0.6219], device='cuda:0')
Thumb force: tensor([0.6759, 0.8797, 1.8525, 0.7339, 1.0629, 0.5852, 0.6118, 0.8335, 2.0448,
        0.5828, 0.5480, 0.5944], device='cuda:0')
Index force: tensor([0.5476, 0.8625, 0.5677, 0.5550, 0.6163, 0.5859, 0.6556, 0.9541, 0.6428,
        0.6275, 0.5636, 0.5846], device='cuda:0')
Storing NORMAL transition: reward=-0.0041 (scaled=-0.0041), steps=1
Reward stats updated: mean 0.0037 -> 0.0037, std: 0.0804
Collected 253 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2562, Q2 Loss=1.2562, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0549
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.5999, Q2 Loss=1.5999, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4993
SAC Update 3/5: Actor Loss=-0.1820, Q1 Loss=2.1687, Q2 Loss=2.1687, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1000
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=4.5898, Q2 Loss=4.5898, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.5382
SAC Update 5/5: Actor Loss=-0.1705, Q1 Loss=2.0306, Q2 Loss=2.0306, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0332

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.0%)
Q1 update: 0.05s (18.9%)
Q2 update: 0.05s (19.3%)
Actor update: 0.11s (41.6%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.116558
Q1 loss: 2.329028
Q2 loss: 2.329028
Current threshold: -33.3251
Global Scale Offset: 0.0843
Reward stats: mean=0.0037, std=0.0804, count=253
----------------------------------------------
SAC Update - Actor Loss: -0.1166, Q1 Loss: 2.3290, Q2 Loss: 2.3290, Entropy: 0.0000, Mean TD Error: 2.0451, Threshold: -33.3251
tensor([ 0.0690,  0.6265,  0.5122,  0.5077, -0.1426,  0.4827,  1.0610,  0.8228,
         1.3345,  0.2814,  0.1699,  0.9255, -0.0111,  0.0057, -0.0267,  0.7318],
       device='cuda:0')
Original likelihood: -22.19127655029297
Adjusted likelihood: -22.19127655029297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.473134246014524
Current ori: tensor([-0.0111,  0.0057, -0.0267], device='cuda:0')
Middle force: tensor([0.5642, 1.5306, 0.6124, 0.5947, 0.5550, 0.5423, 0.5630, 0.5348, 0.6417,
        1.0099, 0.6190], device='cuda:0')
Thumb force: tensor([0.7813, 1.7943, 0.7007, 1.0324, 0.5824, 0.5826, 0.7508, 1.9931, 0.5410,
        0.5440, 0.5891], device='cuda:0')
Index force: tensor([0.7767, 0.5713, 0.5511, 0.6070, 0.5879, 0.6572, 0.8854, 0.6201, 0.6334,
        0.5594, 0.5802], device='cuda:0')
Storing NORMAL transition: reward=-0.0338 (scaled=-0.0338), steps=1
Reward stats updated: mean 0.0037 -> 0.0036, std: 0.0803
Collected 254 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.8126, Q2 Loss=1.8126, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6001
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0789, Q2 Loss=1.0789, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8713
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1706, Q2 Loss=1.1706, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4987
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9308, Q2 Loss=0.9308, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4790
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.2431, Q2 Loss=1.2431, Entropy=0.0112, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6497

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (18.4%)
Q2 update: 0.04s (19.0%)
Actor update: 0.08s (39.8%)
Target update: 0.00s (1.8%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000007
Q1 loss: 1.247195
Q2 loss: 1.247195
Current threshold: -33.3253
Global Scale Offset: 0.0843
Reward stats: mean=0.0036, std=0.0803, count=254
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.2472, Q2 Loss: 1.2472, Entropy: 0.0022, Mean TD Error: 1.0197, Threshold: -33.3253
tensor([ 0.0695,  0.6328,  0.4923,  0.5355, -0.1425,  0.5045,  1.0174,  0.8519,
         1.3823,  0.2647,  0.1772,  0.9597, -0.0104,  0.0059,  0.0071,  0.6850],
       device='cuda:0')
Original likelihood: -16.214006423950195
Adjusted likelihood: -16.214006423950195
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.173291267012246
Current ori: tensor([-0.0104,  0.0059,  0.0071], device='cuda:0')
Middle force: tensor([1.4911, 0.6082, 0.5927, 0.5515, 0.5389, 0.5582, 0.5310, 0.6393, 0.9969,
        0.6135], device='cuda:0')
Thumb force: tensor([1.7587, 0.6858, 1.0100, 0.5792, 0.5756, 0.7490, 1.9551, 0.5379, 0.5406,
        0.5854], device='cuda:0')
Index force: tensor([0.5655, 0.5472, 0.6021, 0.5841, 0.6526, 0.8687, 0.6143, 0.6258, 0.5563,
        0.5768], device='cuda:0')
Storing NORMAL transition: reward=0.1370 (scaled=0.1370), steps=1
Reward stats updated: mean 0.0036 -> 0.0041, std: 0.0806
Collected 255 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6892, Q2 Loss=1.6892, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8180
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.5028, Q2 Loss=2.5028, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6216
SAC Update 3/5: Actor Loss=-0.0199, Q1 Loss=0.7051, Q2 Loss=0.7051, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9210
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6317, Q2 Loss=1.6317, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7516
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.8238, Q2 Loss=1.8238, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5345

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.9%)
Q1 update: 0.06s (19.7%)
Q2 update: 0.06s (19.6%)
Actor update: 0.12s (42.8%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.003972
Q1 loss: 1.670505
Q2 loss: 1.670505
Current threshold: -33.3254
Global Scale Offset: 0.0843
Reward stats: mean=0.0041, std=0.0806, count=255
----------------------------------------------
SAC Update - Actor Loss: -0.0040, Q1 Loss: 1.6705, Q2 Loss: 1.6705, Entropy: 0.0000, Mean TD Error: 1.7293, Threshold: -33.3254
tensor([ 0.0865,  0.6279,  0.5001,  0.5662, -0.1316,  0.4781,  1.0248,  0.9558,
         1.4282,  0.2116,  0.2146,  0.8113, -0.0066, -0.0029, -0.1298,  0.8489],
       device='cuda:0')
Original likelihood: -20.788330078125
Adjusted likelihood: -20.788330078125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.255096288048662
Current ori: tensor([-0.0066, -0.0029, -0.1298], device='cuda:0')
Middle force: tensor([0.6019, 0.5915, 0.5483, 0.5381, 0.5604, 0.5285, 0.6414, 0.9846, 0.6089],
       device='cuda:0')
Thumb force: tensor([0.6700, 0.9872, 0.5769, 0.5660, 0.7306, 1.9146, 0.5326, 0.5372, 0.5813],
       device='cuda:0')
Index force: tensor([0.5422, 0.5956, 0.5801, 0.6474, 0.8506, 0.6085, 0.6196, 0.5532, 0.5729],
       device='cuda:0')
Storing NORMAL transition: reward=0.0877 (scaled=0.0877), steps=1
Reward stats updated: mean 0.0041 -> 0.0044, std: 0.0806
Collected 256 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4289, Q2 Loss=1.4289, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0437
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0171, Q2 Loss=1.0171, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0716
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6777, Q2 Loss=1.6777, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3255
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=2.0212, Q2 Loss=2.0212, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9605
SAC Update 5/5: Actor Loss=-0.0029, Q1 Loss=1.1498, Q2 Loss=1.1498, Entropy=0.2820, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0242

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (13.7%)
Q1 update: 0.06s (20.4%)
Q2 update: 0.06s (20.7%)
Actor update: 0.12s (42.4%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000579
Q1 loss: 1.458956
Q2 loss: 1.458956
Current threshold: -33.3261
Global Scale Offset: 0.0843
Reward stats: mean=0.0044, std=0.0806, count=256
----------------------------------------------
SAC Update - Actor Loss: -0.0006, Q1 Loss: 1.4590, Q2 Loss: 1.4590, Entropy: 0.0564, Mean TD Error: 1.4851, Threshold: -33.3261
tensor([ 0.0763,  0.6009,  0.5061,  0.5927, -0.0677,  0.4100,  1.1866,  0.9435,
         1.5000,  0.0799,  0.1229,  0.7203,  0.0102, -0.0442, -0.2205,  1.5062],
       device='cuda:0')
Original likelihood: -23.567378997802734
Adjusted likelihood: -23.567378997802734
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 5.057945633016061
Current ori: tensor([ 0.0102, -0.0442, -0.2205], device='cuda:0')
Middle force: tensor([0.5868, 0.5469, 0.5566, 0.6074, 0.5441, 0.7031, 0.9700, 0.6082],
       device='cuda:0')
Thumb force: tensor([0.9634, 0.5736, 0.5484, 0.6791, 1.8597, 0.5184, 0.5352, 0.5765],
       device='cuda:0')
Index force: tensor([0.5914, 0.5756, 0.6188, 0.7831, 0.5756, 0.6046, 0.5511, 0.5681],
       device='cuda:0')
Storing NORMAL transition: reward=0.1150 (scaled=0.1150), steps=1
Reward stats updated: mean 0.0044 -> 0.0048, std: 0.0807
Collected 257 transitions for RL
SAC Update 1/5: Actor Loss=-0.0959, Q1 Loss=0.8804, Q2 Loss=0.8804, Entropy=0.0449, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2118
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7558, Q2 Loss=0.7558, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7472
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0226, Q2 Loss=1.0226, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1622
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.7492, Q2 Loss=1.7492, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5609
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.9450, Q2 Loss=1.9450, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2482

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (16.9%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (19.1%)
Actor update: 0.12s (41.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.019183
Q1 loss: 1.270613
Q2 loss: 1.270613
Current threshold: -33.3289
Global Scale Offset: 0.0843
Reward stats: mean=0.0048, std=0.0807, count=257
----------------------------------------------
SAC Update - Actor Loss: -0.0192, Q1 Loss: 1.2706, Q2 Loss: 1.2706, Entropy: 0.0090, Mean TD Error: 1.1861, Threshold: -33.3289
tensor([ 0.0922,  0.5762,  0.5562,  0.5876, -0.0678,  0.3610,  1.1048,  0.9085,
         1.5000,  0.1917,  0.2583,  0.7582,  0.0934, -0.0108, -0.3532,  2.3713],
       device='cuda:0')
Original likelihood: -36.46735382080078
Adjusted likelihood: -36.46735382080078
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.664222717285156
Projection step: 1, Loss: 38.13324737548828
Projection step: 2, Loss: 36.41333770751953
Projection step: 3, Loss: 39.01580810546875
Projection step: 4, Loss: 36.01863098144531
Projection step: 5, Loss: 36.039024353027344
Projection step: 6, Loss: 34.867645263671875
Projection step: 7, Loss: 33.77317810058594
Projection step: 8, Loss: 37.154178619384766
Projection step: 9, Loss: 34.510589599609375
Projection step: 10, Loss: 33.31482696533203
Projection step: 11, Loss: 34.39133834838867
Projection step: 12, Loss: 32.56621551513672
Projection step: 13, Loss: 33.3126335144043
Projection step: 14, Loss: 33.785072326660156
Projection step: 15, Loss: 31.48614501953125
Projection step: 16, Loss: 31.942832946777344
Projection step: 17, Loss: 30.990631103515625
Projection step: 18, Loss: 32.32289123535156
Projection step: 19, Loss: 31.271459579467773
Projection step: 20, Loss: 30.97240447998047
Projection step: 21, Loss: 30.04492950439453
Projection step: 22, Loss: 29.842803955078125
Projection step: 23, Loss: 30.687284469604492
Projection step: 24, Loss: 29.689212799072266
Final likelihood: tensor([-31.3623, -28.2166, -29.6706, -27.6701, -31.3147, -32.3173, -30.7953,
        -28.7851, -27.5376, -30.0922, -28.6076, -29.5764, -28.5214, -29.9771,
        -28.1219, -30.1127])
Final projection likelihood: -29.5424
1 mode projection succeeded
New goal: tensor([ 0.0893,  0.5468,  0.5558,  0.6007, -0.0600,  0.4094,  0.9901,  0.9656,
         1.4245,  0.1470,  0.2538,  1.0203,  0.0885, -0.0120, -0.5435],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0024]], device='cuda:0')
Original likelihood: -29.972583770751953
Adjusted likelihood: -29.972583770751953
Likelihood residual: 0.0
Original likelihood: -33.37394714355469
Adjusted likelihood: -33.37394714355469
Likelihood residual: 0.0
{'index': 33.37394714355469, 'thumb_middle': 29.972583770751953}
Current yaw: tensor([ 0.0934, -0.0108, -0.3532], device='cuda:0')
2 thumb_middle
tensor([ 0.0922,  0.5762,  0.5562,  0.5876, -0.0678,  0.3610,  1.1048,  0.9085,
         1.5000,  0.1917,  0.2583,  0.7582,  0.0934, -0.0108, -0.3532,  2.3713],
       device='cuda:0')
Solve time for step 1 9.14948928001104
Current ori: tensor([ 0.0934, -0.0108, -0.3532], device='cuda:0')
Index force: tensor([0.5917, 0.5919, 0.6066, 0.6083], device='cuda:0')
tensor([ 0.0907,  0.5721,  0.5575,  0.5901, -0.1500,  0.3805,  0.9662,  0.9266,
         1.3824,  0.1298,  0.1671,  0.9306,  0.1045, -0.0070, -0.3532,  2.4249],
       device='cuda:0')
Solve time for step 2 3.614629191986751
Current ori: tensor([ 0.1045, -0.0070, -0.3532], device='cuda:0')
Index force: tensor([0.5808, 0.5967, 0.6097], device='cuda:0')
tensor([ 0.0959,  0.5525,  0.5767,  0.6209, -0.1584,  0.4063,  0.9549,  0.9338,
         1.3774,  0.1210,  0.1594,  0.9671,  0.1120, -0.0078, -0.3532,  2.4367],
       device='cuda:0')
Solve time for step 3 3.444023981981445
Current ori: tensor([ 0.1120, -0.0078, -0.3532], device='cuda:0')
Index force: tensor([0.5865, 0.6006], device='cuda:0')
tensor([ 0.0928,  0.5486,  0.5772,  0.6252, -0.1650,  0.4125,  0.9497,  0.9374,
         1.3769,  0.1221,  0.1573,  0.9784,  0.1132, -0.0062, -0.3532,  2.4342],
       device='cuda:0')
Solve time for step 4 3.420269305992406
Current ori: tensor([ 0.1132, -0.0062, -0.3532], device='cuda:0')
Index force: tensor([0.5803], device='cuda:0')
Storing RECOVERY transition: reward=-0.0089 (scaled=-0.0018), steps=5
Reward stats updated: mean 0.0048 -> 0.0048, std: 0.0805
Collected 258 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0250, Q2 Loss=1.0250, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6573
SAC Update 2/5: Actor Loss=-0.2294, Q1 Loss=0.7531, Q2 Loss=0.7531, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7751
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9083, Q2 Loss=0.9083, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8888
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.0242, Q2 Loss=1.0242, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1085
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.7364, Q2 Loss=1.7364, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5703

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.1%)
Q2 update: 0.04s (18.2%)
Actor update: 0.09s (41.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.091939
Q1 loss: 1.089402
Q2 loss: 1.089402
Current threshold: -33.3334
Global Scale Offset: 0.0841
Reward stats: mean=0.0048, std=0.0805, count=258
----------------------------------------------
SAC Update - Actor Loss: -0.0919, Q1 Loss: 1.0894, Q2 Loss: 1.0894, Entropy: 0.0000, Mean TD Error: 1.0000, Threshold: -33.3334
Original likelihood: -40.210365295410156
Adjusted likelihood: -40.210365295410156
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 39.042720794677734
Projection step: 1, Loss: 38.46234893798828
Projection step: 2, Loss: 40.623512268066406
Projection step: 3, Loss: 37.61625671386719
Projection step: 4, Loss: 36.2041015625
Projection step: 5, Loss: 40.09900665283203
Projection step: 6, Loss: 37.44415283203125
Projection step: 7, Loss: 39.487037658691406
Projection step: 8, Loss: 38.830726623535156
Projection step: 9, Loss: 38.20265197753906
Projection step: 10, Loss: 39.126953125
Projection step: 11, Loss: 38.272132873535156
Projection step: 12, Loss: 37.21757507324219
Projection step: 13, Loss: 36.40364456176758
Projection step: 14, Loss: 34.210044860839844
Projection step: 15, Loss: 37.08000183105469
Projection step: 16, Loss: 34.3033561706543
Projection step: 17, Loss: 36.58240509033203
Projection step: 18, Loss: 36.25697326660156
Projection step: 19, Loss: 34.22754669189453
Projection step: 20, Loss: 36.16938018798828
Projection step: 21, Loss: 33.61864471435547
Projection step: 22, Loss: 35.813636779785156
Projection step: 23, Loss: 35.50825500488281
Projection step: 24, Loss: 33.000221252441406
Final likelihood: tensor([-41.4850, -34.0429, -33.9756, -52.4441, -23.2111, -35.4522, -32.2026,
        -39.0743, -36.9450, -37.7982, -35.1243, -30.2751, -33.1821, -32.8680,
        -25.7917, -22.2788])
Final projection likelihood: -34.1344
1 mode projection failed, trying anyway
New goal: tensor([ 0.0531,  0.5585,  0.5486,  0.6053, -0.0831,  0.4489,  0.9444,  0.9550,
         1.4190,  0.0913,  0.2403,  1.1134,  0.1098,  0.0041, -0.1227],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0030]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -43.70811080932617
Adjusted likelihood: -43.70811080932617
Likelihood residual: 0.0
Original likelihood: -39.164794921875
Adjusted likelihood: -39.164794921875
Likelihood residual: 0.0
{'index': 39.164794921875, 'thumb_middle': 43.70811080932617}
Current yaw: tensor([ 0.1135,  0.0065, -0.3534], device='cuda:0')
3 index
tensor([ 0.0510,  0.5468,  0.5524,  0.5994, -0.1170,  0.4597,  0.9927,  0.9525,
         1.4482,  0.1489,  0.2294,  1.0236,  0.1135,  0.0065, -0.3534,  2.3437],
       device='cuda:0')
Solve time for step 1 10.419579292996787
Current ori: tensor([ 0.1135,  0.0065, -0.3534], device='cuda:0')
Middle force: tensor([0.5845, 0.5752, 0.5723, 0.5552], device='cuda:0')
Thumb force: tensor([0.5823, 0.5580, 0.5597, 0.6179], device='cuda:0')
tensor([ 0.0853,  0.4509,  0.5095,  0.5947, -0.0923,  0.4742,  0.9876,  0.9783,
         1.4613,  0.1076,  0.1894,  1.0624,  0.1155, -0.0106, -0.3671,  1.2088],
       device='cuda:0')
Solve time for step 2 4.14686741901096
Current ori: tensor([ 0.1155, -0.0106, -0.3671], device='cuda:0')
Middle force: tensor([0.5669, 0.5644, 0.5513], device='cuda:0')
Thumb force: tensor([0.5528, 0.5572, 0.6129], device='cuda:0')
tensor([ 0.0900,  0.4513,  0.5050,  0.5970, -0.0988,  0.4806,  0.9867,  0.9770,
         1.4616,  0.1088,  0.1930,  1.0565,  0.1155, -0.0098, -0.3726,  0.0193],
       device='cuda:0')
Solve time for step 3 4.06150270299986
Current ori: tensor([ 0.1155, -0.0098, -0.3726], device='cuda:0')
Middle force: tensor([0.5625, 0.5407], device='cuda:0')
Thumb force: tensor([0.5759, 0.5712], device='cuda:0')
tensor([ 0.0947,  0.4632,  0.5049,  0.5939, -0.0836,  0.4972,  0.9779,  0.9636,
         1.4534,  0.1129,  0.1819,  1.0586,  0.1102, -0.0179, -0.3617, -1.1585],
       device='cuda:0')
Solve time for step 4 3.9499655590043403
Current ori: tensor([ 0.1102, -0.0179, -0.3617], device='cuda:0')
Middle force: tensor([0.5498], device='cuda:0')
Thumb force: tensor([0.5436], device='cuda:0')
Storing RECOVERY transition: reward=-0.0021 (scaled=-0.0004), steps=5
Reward stats updated: mean 0.0048 -> 0.0048, std: 0.0804
Collected 259 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1551, Q2 Loss=1.1551, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5945
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1922, Q2 Loss=1.1922, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7795
SAC Update 3/5: Actor Loss=-0.0941, Q1 Loss=0.8772, Q2 Loss=0.8772, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2244
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6755, Q2 Loss=0.6755, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3386
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.6653, Q2 Loss=0.6653, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1529

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (19.4%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.8%)
Actor update: 0.08s (39.1%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.018818
Q1 loss: 0.913056
Q2 loss: 0.913056
Current threshold: -33.3367
Global Scale Offset: 0.0840
Reward stats: mean=0.0048, std=0.0804, count=259
----------------------------------------------
SAC Update - Actor Loss: -0.0188, Q1 Loss: 0.9131, Q2 Loss: 0.9131, Entropy: 0.0000, Mean TD Error: 0.6180, Threshold: -33.3367
Original likelihood: -40.12836837768555
Adjusted likelihood: -40.12836837768555
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 35.717926025390625
Projection step: 1, Loss: 36.135074615478516
Projection step: 2, Loss: 35.431663513183594
Projection step: 3, Loss: 36.1370849609375
Projection step: 4, Loss: 37.2132682800293
Projection step: 5, Loss: 35.23697280883789
Projection step: 6, Loss: 35.29710388183594
Projection step: 7, Loss: 37.20301818847656
Projection step: 8, Loss: 35.266632080078125
Projection step: 9, Loss: 33.43956756591797
Projection step: 10, Loss: 34.338523864746094
Projection step: 11, Loss: 34.153114318847656
Projection step: 12, Loss: 34.556793212890625
Projection step: 13, Loss: 37.125186920166016
Projection step: 14, Loss: 34.27619934082031
Projection step: 15, Loss: 34.57859802246094
Projection step: 16, Loss: 32.21385192871094
Projection step: 17, Loss: 30.54104995727539
Projection step: 18, Loss: 34.06733703613281
Projection step: 19, Loss: 34.29328536987305
Projection step: 20, Loss: 32.60111999511719
Projection step: 21, Loss: 32.641075134277344
Projection step: 22, Loss: 33.25988006591797
Projection step: 23, Loss: 33.98005294799805
Projection step: 24, Loss: 33.25337219238281
Final likelihood: tensor([-36.4453, -33.5426, -34.4274, -35.0002, -35.5137, -41.7605, -33.2566,
        -29.9855, -20.3206, -33.6112, -31.8286, -34.3095, -25.4498, -36.2231,
        -18.8664, -34.4488])
Final projection likelihood: -32.1869
1 mode projection succeeded
New goal: tensor([ 0.0647,  0.5415,  0.5770,  0.6189, -0.0636,  0.4899,  0.9288,  0.9700,
         1.4282,  0.0675,  0.1972,  1.1495,  0.1125, -0.0205, -0.2372],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0032]], device='cuda:0') tensor([[0.0024]], device='cuda:0')
Original likelihood: -34.57594299316406
Adjusted likelihood: -34.57594299316406
Likelihood residual: 0.0
Original likelihood: -36.72224426269531
Adjusted likelihood: -36.72224426269531
Likelihood residual: 0.0
{'index': 36.72224426269531, 'thumb_middle': 34.57594299316406}
Current yaw: tensor([ 0.1174, -0.0188, -0.3644], device='cuda:0')
4 thumb_middle
tensor([ 0.0601,  0.5546,  0.5497,  0.6097, -0.0928,  0.5013,  0.9883,  0.9719,
         1.4620,  0.1116,  0.1821,  1.0538,  0.1174, -0.0188, -0.3644, -1.3972],
       device='cuda:0')
Solve time for step 1 8.87526468100259
Current ori: tensor([ 0.1174, -0.0188, -0.3644], device='cuda:0')
Index force: tensor([0.5632, 0.6049, 0.5984, 0.5695], device='cuda:0')
tensor([ 0.0547,  0.5414,  0.5872,  0.6173, -0.1524,  0.4776,  0.8951,  0.9397,
         1.3839,  0.0455,  0.1134,  1.0972,  0.1741,  0.0047, -0.3644, -0.9042],
       device='cuda:0')
Solve time for step 2 3.483497984998394
Current ori: tensor([ 0.1741,  0.0047, -0.3644], device='cuda:0')
Index force: tensor([0.5997, 0.5959, 0.5672], device='cuda:0')
tensor([ 0.0807,  0.5180,  0.6469,  0.6665, -0.1469,  0.4919,  0.8921,  0.9396,
         1.4104,  0.0495,  0.1279,  1.1214,  0.2109,  0.0028, -0.3644, -1.2100],
       device='cuda:0')
Solve time for step 3 3.638425633951556
Current ori: tensor([ 0.2109,  0.0028, -0.3644], device='cuda:0')
Index force: tensor([0.5828, 0.5002], device='cuda:0')
tensor([ 9.5145e-02,  5.1196e-01,  6.5580e-01,  9.0465e-01, -1.4361e-01,
         4.9970e-01,  8.9237e-01,  9.3925e-01,  1.4285e+00,  5.7647e-02,
         1.6407e-01,  1.1386e+00,  2.3057e-01,  4.3174e-04, -3.6319e-01,
        -1.1883e+00], device='cuda:0')
Solve time for step 4 3.480305934965145
Current ori: tensor([ 0.2306,  0.0004, -0.3632], device='cuda:0')
Index force: tensor([0.5003], device='cuda:0')
Storing RECOVERY transition: reward=-0.0955 (scaled=-0.0191), steps=5
Reward stats updated: mean 0.0048 -> 0.0047, std: 0.0803
Collected 260 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3990, Q2 Loss=1.3990, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0887
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2553, Q2 Loss=1.2553, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4197
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3104, Q2 Loss=1.3104, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2568
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.0769, Q2 Loss=2.0769, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1407
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9210, Q2 Loss=0.9210, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8372

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.4%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.05s (19.2%)
Actor update: 0.10s (41.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 1.392537
Q2 loss: 1.392537
Current threshold: -33.3386
Global Scale Offset: 0.0839
Reward stats: mean=0.0047, std=0.0803, count=260
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.3925, Q2 Loss: 1.3925, Entropy: 0.0000, Mean TD Error: 1.1486, Threshold: -33.3386
Original likelihood: -45.912025451660156
Adjusted likelihood: -45.912025451660156
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 45.74694061279297
Projection step: 1, Loss: 44.024864196777344
Projection step: 2, Loss: 44.962249755859375
Projection step: 3, Loss: 44.54583740234375
Projection step: 4, Loss: 44.468204498291016
Projection step: 5, Loss: 44.548404693603516
Projection step: 6, Loss: 45.13280487060547
Projection step: 7, Loss: 45.08953094482422
Projection step: 8, Loss: 44.1282958984375
Projection step: 9, Loss: 43.55205535888672
Projection step: 10, Loss: 44.76688003540039
Projection step: 11, Loss: 43.67186737060547
Projection step: 12, Loss: 45.182342529296875
Projection step: 13, Loss: 42.75909423828125
Projection step: 14, Loss: 42.7027587890625
Projection step: 15, Loss: 42.97706604003906
Projection step: 16, Loss: 43.869346618652344
Projection step: 17, Loss: 45.5018310546875
Projection step: 18, Loss: 42.91720199584961
Projection step: 19, Loss: 42.345558166503906
Projection step: 20, Loss: 41.5559196472168
Projection step: 21, Loss: 42.80426025390625
Projection step: 22, Loss: 41.85103988647461
Projection step: 23, Loss: 43.006103515625
Projection step: 24, Loss: 44.08679962158203
Final likelihood: tensor([-39.9085, -40.9172, -42.2405, -42.2078, -39.9897, -40.4734, -44.0853,
        -52.5351, -43.4680, -44.4635, -42.8923, -41.1946, -52.0026, -43.0617,
        -45.6173, -37.6029])
Final projection likelihood: -43.2913
1 mode projection failed, trying anyway
New goal: tensor([ 0.0691,  0.4820,  0.6314,  0.7719, -0.0425,  0.5279,  0.9479,  1.0432,
         1.4804,  0.0504,  0.2565,  1.1111,  0.2391,  0.0118, -0.0503],
       device='cuda:0')
tensor([[0.0040]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0005]], device='cuda:0')
Original likelihood: -45.46131896972656
Adjusted likelihood: -45.46131896972656
Likelihood residual: 0.0
Original likelihood: -44.840904235839844
Adjusted likelihood: -44.840904235839844
Likelihood residual: 0.0
{'index': 44.840904235839844, 'thumb_middle': 45.46131896972656}
Current yaw: tensor([ 0.2434,  0.0111, -0.3509], device='cuda:0')
5 index
tensor([ 0.0684,  0.4979,  0.6222,  0.7307, -0.0438,  0.5653,  0.9921,  1.0056,
         1.5000,  0.1013,  0.2438,  1.2074,  0.2434,  0.0111, -0.3509, -1.3662],
       device='cuda:0')
Solve time for step 1 10.271637141995598
Current ori: tensor([ 0.2434,  0.0111, -0.3509], device='cuda:0')
Middle force: tensor([0.5330, 0.5315, 0.5348, 0.5739], device='cuda:0')
Thumb force: tensor([0.5399, 0.5826, 0.6051, 0.5870], device='cuda:0')
tensor([ 0.0133,  0.2009,  0.6674,  0.7768, -0.0499,  0.6236,  0.9980,  1.0212,
         1.5000,  0.1071,  0.2386,  1.1918,  0.2914,  0.0278, -0.3385, -0.0447],
       device='cuda:0')
Solve time for step 2 4.238737445033621
Current ori: tensor([ 0.2914,  0.0278, -0.3385], device='cuda:0')
Middle force: tensor([0.5284, 0.5254, 0.5682], device='cuda:0')
Thumb force: tensor([0.5811, 0.6067, 0.5871], device='cuda:0')
tensor([ 0.0057,  0.1386,  0.6816,  0.7853,  0.0112,  0.6896,  0.9596,  1.1223,
         1.5000,  0.1191,  0.2423,  1.1656,  0.3042,  0.0149, -0.3076,  0.2329],
       device='cuda:0')
Solve time for step 3 4.293454975995701
Current ori: tensor([ 0.3042,  0.0149, -0.3076], device='cuda:0')
Middle force: tensor([0.5678, 0.6027], device='cuda:0')
Thumb force: tensor([0.5138, 0.5571], device='cuda:0')
tensor([ 0.0391,  0.1988,  0.6831,  0.7880,  0.0298,  0.6882,  0.9769,  1.0781,
         1.5000,  0.1238,  0.2149,  1.1809,  0.2940, -0.0024, -0.3554,  0.3449],
       device='cuda:0')
Solve time for step 4 4.009155146952253
Current ori: tensor([ 0.2940, -0.0024, -0.3554], device='cuda:0')
Middle force: tensor([0.5414], device='cuda:0')
Thumb force: tensor([0.6446], device='cuda:0')
Storing RECOVERY transition: reward=-0.1189 (scaled=-0.0238), steps=5
Reward stats updated: mean 0.0047 -> 0.0046, std: 0.0801
Collected 261 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0143, Q2 Loss=1.0143, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9798
SAC Update 2/5: Actor Loss=-0.2015, Q1 Loss=1.8418, Q2 Loss=1.8418, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6554
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.7283, Q2 Loss=1.7283, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3815
SAC Update 4/5: Actor Loss=-0.0027, Q1 Loss=0.8799, Q2 Loss=0.8799, Entropy=0.3465, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4505
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=2.1491, Q2 Loss=2.1491, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8510

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (18.3%)
Actor update: 0.10s (39.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.132954
Q1 loss: 1.522686
Q2 loss: 1.522686
Current threshold: -33.3406
Global Scale Offset: 0.0839
Reward stats: mean=0.0046, std=0.0801, count=261
----------------------------------------------
SAC Update - Actor Loss: -0.1330, Q1 Loss: 1.5227, Q2 Loss: 1.5227, Entropy: 0.0693, Mean TD Error: 1.2636, Threshold: -33.3406
Original likelihood: -55.590476989746094
Adjusted likelihood: -55.590476989746094
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 55.025611877441406
Projection step: 1, Loss: 54.83652877807617
Projection step: 2, Loss: 55.420963287353516
Projection step: 3, Loss: 55.78034973144531
Projection step: 4, Loss: 54.88707733154297
Projection step: 5, Loss: 53.309349060058594
Projection step: 6, Loss: 55.44970703125
Projection step: 7, Loss: 56.47107696533203
Projection step: 8, Loss: 53.77030563354492
Projection step: 9, Loss: 52.40899658203125
Projection step: 10, Loss: 54.91950225830078
Projection step: 11, Loss: 54.257354736328125
Projection step: 12, Loss: 53.58864974975586
Projection step: 13, Loss: 54.04063415527344
Projection step: 14, Loss: 53.16059112548828
Projection step: 15, Loss: 56.33476257324219
Projection step: 16, Loss: 52.132713317871094
Projection step: 17, Loss: 52.467098236083984
Projection step: 18, Loss: 54.31438446044922
Projection step: 19, Loss: 52.397300720214844
Projection step: 20, Loss: 50.89141845703125
Projection step: 21, Loss: 51.421695709228516
Projection step: 22, Loss: 51.28351593017578
Projection step: 23, Loss: 51.77790069580078
Projection step: 24, Loss: 50.882328033447266
Final likelihood: tensor([-51.6889, -54.7229, -57.7674, -56.4098, -55.2463, -55.7059, -47.0876,
        -50.5792, -51.1270, -47.9267, -50.9000, -48.9115, -55.9900, -49.9826,
        -57.5536, -51.6511])
Final projection likelihood: -52.7032
1 mode projection failed, trying anyway
New goal: tensor([ 0.0504,  0.3408,  0.7105,  0.8522,  0.0141,  0.6151,  0.9353,  1.0803,
         1.4838,  0.0996,  0.2677,  1.1767,  0.2678, -0.0208, -0.3019],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0022]], device='cuda:0')
Original likelihood: -54.439048767089844
Adjusted likelihood: -54.439048767089844
Likelihood residual: 0.0
Original likelihood: -69.39227294921875
Adjusted likelihood: -69.39227294921875
Likelihood residual: 0.0
{'index': 69.39227294921875, 'thumb_middle': 54.439048767089844}
Current yaw: tensor([ 0.2729, -0.0215, -0.3513], device='cuda:0')
6 thumb_middle
tensor([ 0.0375,  0.3240,  0.6676,  0.7830,  0.0207,  0.6746,  0.9899,  1.0343,
         1.5000,  0.1320,  0.2624,  1.2111,  0.2729, -0.0215, -0.3513,  0.2232],
       device='cuda:0')
Solve time for step 1 8.61805784801254
Current ori: tensor([ 0.2729, -0.0215, -0.3513], device='cuda:0')
Index force: tensor([0.5826, 0.6026, 0.5875, 0.5690], device='cuda:0')
tensor([ 0.1109,  0.4125,  0.7144,  0.8404, -0.0641,  0.5334,  0.9152,  1.0463,
         1.4441,  0.0813,  0.1894,  1.1467,  0.2566, -0.0440, -0.3046,  0.2396],
       device='cuda:0')
Solve time for step 2 3.6891237280215137
Current ori: tensor([ 0.2566, -0.0440, -0.3046], device='cuda:0')
Index force: tensor([0.5943, 0.5785, 0.5610], device='cuda:0')
tensor([ 0.0654,  0.4601,  0.7260,  0.8522, -0.0625,  0.5547,  0.8994,  1.0463,
         1.4466,  0.0760,  0.1820,  1.1404,  0.2537, -0.0608, -0.2441,  0.3495],
       device='cuda:0')
Solve time for step 3 3.5624726520036347
Current ori: tensor([ 0.2537, -0.0608, -0.2441], device='cuda:0')
Index force: tensor([0.5722, 0.5536], device='cuda:0')
tensor([ 0.0266,  0.4973,  0.7294,  0.8551, -0.0515,  0.5995,  0.8863,  1.0402,
         1.4391,  0.0743,  0.1679,  1.1312,  0.2636, -0.1021, -0.1860,  0.8894],
       device='cuda:0')
Solve time for step 4 3.2996645480161533
Current ori: tensor([ 0.2636, -0.1021, -0.1860], device='cuda:0')
Index force: tensor([0.5462], device='cuda:0')
Storing RECOVERY transition: reward=-0.2698 (scaled=-0.0540), steps=5
Reward stats updated: mean 0.0046 -> 0.0044, std: 0.0800
Collected 262 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.9088, Q2 Loss=1.9088, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4508
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4990, Q2 Loss=1.4990, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7020
SAC Update 3/5: Actor Loss=-0.2169, Q1 Loss=2.5673, Q2 Loss=2.5673, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2958
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0035, Q2 Loss=1.0035, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8955
SAC Update 5/5: Actor Loss=-0.1236, Q1 Loss=1.5166, Q2 Loss=1.5166, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7414

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (18.6%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (18.0%)
Actor update: 0.10s (40.5%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.068109
Q1 loss: 1.699053
Q2 loss: 1.699053
Current threshold: -33.3429
Global Scale Offset: 0.0838
Reward stats: mean=0.0044, std=0.0800, count=262
----------------------------------------------
SAC Update - Actor Loss: -0.0681, Q1 Loss: 1.6991, Q2 Loss: 1.6991, Entropy: 0.0000, Mean TD Error: 1.6171, Threshold: -33.3429
Original likelihood: -49.32008361816406
Adjusted likelihood: -49.32008361816406
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 47.277400970458984
Projection step: 1, Loss: 44.67430877685547
Projection step: 2, Loss: 46.92906951904297
Projection step: 3, Loss: 47.78844451904297
Projection step: 4, Loss: 43.93123245239258
Projection step: 5, Loss: 46.86761474609375
Projection step: 6, Loss: 46.81399917602539
Projection step: 7, Loss: 45.399967193603516
Projection step: 8, Loss: 45.160945892333984
Projection step: 9, Loss: 46.80900573730469
Projection step: 10, Loss: 50.103336334228516
Projection step: 11, Loss: 47.726356506347656
Projection step: 12, Loss: 46.09689712524414
Projection step: 13, Loss: 44.79608154296875
Projection step: 14, Loss: 44.73365783691406
Projection step: 15, Loss: 46.432674407958984
Projection step: 16, Loss: 45.203941345214844
Projection step: 17, Loss: 47.27031707763672
Projection step: 18, Loss: 45.57545471191406
Projection step: 19, Loss: 45.787200927734375
Projection step: 20, Loss: 46.36315155029297
Projection step: 21, Loss: 44.888389587402344
Projection step: 22, Loss: 44.67913055419922
Projection step: 23, Loss: 43.880706787109375
Projection step: 24, Loss: 44.264259338378906
Final likelihood: tensor([-53.0507, -39.8705, -36.3631, -44.4034, -45.0413, -50.7543, -48.4381,
        -42.9031, -51.1459, -42.8379, -49.7770, -43.3749, -44.0147, -44.0135,
        -48.7075, -51.1304])
Final projection likelihood: -45.9891
1 mode projection failed, trying anyway
New goal: tensor([ 1.1643e-03,  4.7155e-01,  7.2938e-01,  8.8289e-01,  3.7325e-02,
         6.6015e-01,  9.8201e-01,  1.1146e+00,  1.4609e+00,  9.1691e-02,
         2.4203e-01,  1.1229e+00,  2.5647e-01, -9.9163e-02, -2.5342e-01],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0029]], device='cuda:0') tensor([[0.0016]], device='cuda:0')
Original likelihood: -48.280479431152344
Adjusted likelihood: -48.280479431152344
Likelihood residual: 0.0
Original likelihood: -53.73124694824219
Adjusted likelihood: -53.73124694824219
Likelihood residual: 0.0
{'index': 53.73124694824219, 'thumb_middle': 48.280479431152344}
Current yaw: tensor([ 0.2608, -0.1015, -0.1499], device='cuda:0')
7 thumb_middle
tensor([-0.0057,  0.5069,  0.7531,  0.8743,  0.0233,  0.7029,  1.0081,  1.1155,
         1.4862,  0.0983,  0.2150,  1.1753,  0.2608, -0.1015, -0.1499,  0.8709],
       device='cuda:0')
Solve time for step 1 8.656136902049184
Current ori: tensor([ 0.2608, -0.1015, -0.1499], device='cuda:0')
Index force: tensor([0.5988, 0.5914, 0.5579, 0.5881], device='cuda:0')
tensor([-0.0526,  0.5539,  0.7479,  0.8829, -0.0209,  0.6475,  0.9356,  1.0780,
         1.4291,  0.0758,  0.1567,  1.0945,  0.3052, -0.2034, -0.1006,  2.2537],
       device='cuda:0')
Solve time for step 2 3.540282727975864
Current ori: tensor([ 0.3052, -0.2034, -0.1006], device='cuda:0')
Index force: tensor([0.5834, 0.5522, 0.5821], device='cuda:0')
tensor([-0.0974,  0.6316,  0.7231,  0.8717,  0.0302,  0.7195,  1.0029,  1.1182,
         1.3958,  0.0879,  0.1294,  1.0785,  0.3563, -0.2730, -0.0094,  4.5644],
       device='cuda:0')
Solve time for step 3 3.41278879402671
Current ori: tensor([ 0.3563, -0.2730, -0.0094], device='cuda:0')
Index force: tensor([0.5518, 0.5899], device='cuda:0')
tensor([-0.1237,  0.6603,  0.7146,  0.8794,  0.0908,  0.7901,  1.0311,  1.1150,
         1.2893,  0.1349,  0.1316,  1.1490,  0.3699, -0.3195,  0.1519,  5.4443],
       device='cuda:0')
Solve time for step 4 3.249347067961935
Current ori: tensor([ 0.3699, -0.3195,  0.1519], device='cuda:0')
Index force: tensor([0.5790], device='cuda:0')
Storing RECOVERY transition: reward=-0.5085 (scaled=-0.1017), steps=5
Reward stats updated: mean 0.0044 -> 0.0040, std: 0.0802
Collected 263 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2143, Q2 Loss=1.2143, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4576
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2853, Q2 Loss=1.2853, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0422
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.7875, Q2 Loss=1.7875, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5388
SAC Update 4/5: Actor Loss=-0.0518, Q1 Loss=0.9353, Q2 Loss=0.9353, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3238
SAC Update 5/5: Actor Loss=-0.1137, Q1 Loss=1.2881, Q2 Loss=1.2881, Entropy=0.3441, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5043

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.8%)
Q1 update: 0.05s (18.6%)
Q2 update: 0.05s (18.4%)
Actor update: 0.10s (40.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.033095
Q1 loss: 1.302100
Q2 loss: 1.302100
Current threshold: -33.3457
Global Scale Offset: 0.0838
Reward stats: mean=0.0040, std=0.0802, count=263
----------------------------------------------
SAC Update - Actor Loss: -0.0331, Q1 Loss: 1.3021, Q2 Loss: 1.3021, Entropy: 0.0688, Mean TD Error: 1.1734, Threshold: -33.3457
Original likelihood: -213.77484130859375
Adjusted likelihood: -213.77484130859375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 188.62533569335938
Projection step: 1, Loss: 209.35107421875
Projection step: 2, Loss: 211.09524536132812
Projection step: 3, Loss: 218.10025024414062
Projection step: 4, Loss: 201.516357421875
Projection step: 5, Loss: 190.92822265625
Projection step: 6, Loss: 192.24337768554688
Projection step: 7, Loss: 188.69775390625
Projection step: 8, Loss: 213.73756408691406
Projection step: 9, Loss: 219.17520141601562
Projection step: 10, Loss: 208.86288452148438
Projection step: 11, Loss: 190.2777099609375
Projection step: 12, Loss: 199.24392700195312
Projection step: 13, Loss: 216.50836181640625
Projection step: 14, Loss: 204.5994873046875
Projection step: 15, Loss: 194.90597534179688
Projection step: 16, Loss: 210.922119140625
Projection step: 17, Loss: 212.57171630859375
Projection step: 18, Loss: 224.90457153320312
Projection step: 19, Loss: 218.44305419921875
Projection step: 20, Loss: 177.8685302734375
Projection step: 21, Loss: 210.86656188964844
Projection step: 22, Loss: 211.7237548828125
Projection step: 23, Loss: 199.59259033203125
Projection step: 24, Loss: 206.594970703125
Final likelihood: tensor([-238.2249, -238.9875, -162.4850, -174.6796, -161.7778, -222.3795,
        -191.0447, -166.8079, -117.7172, -180.9904, -156.6026, -160.4822,
        -265.8489, -118.0594, -225.8000, -158.7733])
Final projection likelihood: -183.7913
1 mode projection failed, trying anyway
New goal: tensor([-0.0654,  0.6160,  0.7765,  0.8929,  0.1489,  0.8033,  1.0736,  1.1242,
         1.2886,  0.1021,  0.2812,  1.3050,  0.3590, -0.2920,  0.0384],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0154]], device='cuda:0') tensor([[0.0040]], device='cuda:0')
Original likelihood: -141.53582763671875
Adjusted likelihood: -141.53582763671875
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 141.53582763671875}
Current yaw: tensor([ 0.3590, -0.2913,  0.0112], device='cuda:0')
8 thumb_middle
tensor([-0.0798,  0.6162,  0.7755,  0.8962,  0.1531,  0.8084,  1.0728,  1.0984,
         1.2984,  0.0901,  0.2842,  1.3013,  0.3590, -0.2913,  0.0112,  4.2463],
       device='cuda:0')
Solve time for step 1 8.933621185016818
Current ori: tensor([ 0.3590, -0.2913,  0.0112], device='cuda:0')
Index force: tensor([0.5965, 0.5603, 0.5931, 0.6051], device='cuda:0')
tensor([-0.0931,  0.6570,  0.7658,  0.8834,  0.1337,  0.8394,  1.0711,  1.0982,
         1.2129,  0.1240,  0.1557,  1.2869,  0.3779, -0.3430,  0.1382,  3.5323],
       device='cuda:0')
Solve time for step 2 3.5575847079744563
Current ori: tensor([ 0.3779, -0.3430,  0.1382], device='cuda:0')
Index force: tensor([0.5830, 0.5668, 0.5683], device='cuda:0')
tensor([-0.1115,  0.6603,  0.7729,  0.8839,  0.1159,  0.8260,  1.1076,  1.1257,
         1.2025,  0.1490,  0.1670,  1.2992,  0.3797, -0.3468,  0.1619,  4.3640],
       device='cuda:0')
Solve time for step 3 3.414459924970288
Current ori: tensor([ 0.3797, -0.3468,  0.1619], device='cuda:0')
Index force: tensor([0.5792, 0.5809], device='cuda:0')
tensor([-0.1173,  0.6727,  0.7680,  0.8825,  0.1082,  0.8321,  1.1138,  1.1354,
         1.2007,  0.1554,  0.1733,  1.3252,  0.3798, -0.3471,  0.1639,  4.4866],
       device='cuda:0')
Solve time for step 4 3.407438342983369
Current ori: tensor([ 0.3798, -0.3471,  0.1639], device='cuda:0')
Index force: tensor([0.5757], device='cuda:0')
Storing RECOVERY transition: reward=-0.5167 (scaled=-0.1033), steps=5
Reward stats updated: mean 0.0040 -> 0.0036, std: 0.0803
Collected 264 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4186, Q2 Loss=1.4186, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1100
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1967, Q2 Loss=1.1967, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7891
SAC Update 3/5: Actor Loss=-0.0842, Q1 Loss=0.9636, Q2 Loss=0.9636, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9033
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1339, Q2 Loss=1.1339, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5766
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5370, Q2 Loss=1.5370, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3929

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.8%)
Q1 update: 0.04s (17.9%)
Q2 update: 0.04s (18.3%)
Actor update: 0.10s (43.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.016834
Q1 loss: 1.249963
Q2 loss: 1.249963
Current threshold: -33.3517
Global Scale Offset: 0.0838
Reward stats: mean=0.0036, std=0.0803, count=264
----------------------------------------------
SAC Update - Actor Loss: -0.0168, Q1 Loss: 1.2500, Q2 Loss: 1.2500, Entropy: 0.0000, Mean TD Error: 0.9544, Threshold: -33.3517
Original likelihood: -229.05813598632812
Adjusted likelihood: -229.05813598632812
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 239.500732421875
Projection step: 1, Loss: 243.71446228027344
Projection step: 2, Loss: 243.14048767089844
Projection step: 3, Loss: 250.15365600585938
Projection step: 4, Loss: 246.73287963867188
Projection step: 5, Loss: 250.97760009765625
Projection step: 6, Loss: 224.4556884765625
Projection step: 7, Loss: 250.38555908203125
Projection step: 8, Loss: 234.2716522216797
Projection step: 9, Loss: 256.1955261230469
Projection step: 10, Loss: 260.7510681152344
Projection step: 11, Loss: 252.367919921875
Projection step: 12, Loss: 257.0335388183594
Projection step: 13, Loss: 234.75108337402344
Projection step: 14, Loss: 230.6636962890625
Projection step: 15, Loss: 252.65101623535156
Projection step: 16, Loss: 245.475341796875
Projection step: 17, Loss: 251.87277221679688
Projection step: 18, Loss: 260.9837951660156
Projection step: 19, Loss: 248.2353515625
Projection step: 20, Loss: 244.73866271972656
Projection step: 21, Loss: 239.06866455078125
Projection step: 22, Loss: 253.25039672851562
Projection step: 23, Loss: 251.56893920898438
Projection step: 24, Loss: 250.54405212402344
Final likelihood: tensor([-301.0981, -272.8297, -195.0461, -238.1908, -272.2970, -187.6363,
        -264.5321, -320.1764, -248.7615, -279.8694, -185.6412, -298.6745,
        -241.5225, -220.9130, -297.3059, -211.1601])
Final projection likelihood: -252.2284
1 mode projection failed, trying anyway
New goal: tensor([-0.0789,  0.7065,  0.8111,  0.8974,  0.1366,  0.8778,  1.1619,  1.1675,
         1.2624,  0.1303,  0.2743,  1.3832,  0.3641, -0.3068,  0.0318],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0058]], device='cuda:0') tensor([[0.0042]], device='cuda:0')
Original likelihood: -236.70274353027344
Adjusted likelihood: -236.70274353027344
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 236.70274353027344}
Current yaw: tensor([ 0.3641, -0.3062,  0.0108], device='cuda:0')
9 thumb_middle
tensor([-0.0882,  0.7073,  0.8049,  0.8953,  0.1380,  0.8783,  1.1657,  1.1564,
         1.2717,  0.1243,  0.2753,  1.3794,  0.3641, -0.3062,  0.0108,  4.0998],
       device='cuda:0')
Solve time for step 1 8.597854964027647
Current ori: tensor([ 0.3641, -0.3062,  0.0108], device='cuda:0')
Index force: tensor([0.5929, 0.5872, 0.5933, 0.6002], device='cuda:0')
tensor([-0.1228,  0.6998,  0.7814,  0.8866,  0.0935,  0.8637,  1.1300,  1.1374,
         1.2046,  0.1556,  0.1189,  1.3393,  0.3814, -0.3514,  0.1660,  4.0555],
       device='cuda:0')
Solve time for step 2 3.5755119859823026
Current ori: tensor([ 0.3814, -0.3514,  0.1660], device='cuda:0')
Index force: tensor([0.5836, 0.5877, 0.5939], device='cuda:0')
tensor([-0.1388,  0.7117,  0.7687,  0.8761,  0.0765,  0.8636,  1.1354,  1.1397,
         1.2029,  0.1593,  0.1257,  1.3578,  0.3815, -0.3515,  0.1672,  4.3065],
       device='cuda:0')
Solve time for step 3 3.4678716150228865
Current ori: tensor([ 0.3815, -0.3515,  0.1672], device='cuda:0')
Index force: tensor([0.6018, 0.5896], device='cuda:0')
tensor([-0.1494,  0.7153,  0.7581,  0.8718,  0.0656,  0.8586,  1.1383,  1.1388,
         1.2013,  0.1615,  0.1163,  1.3618,  0.3820, -0.3530,  0.1680,  4.1516],
       device='cuda:0')
Solve time for step 4 3.362252988968976
Current ori: tensor([ 0.3820, -0.3530,  0.1680], device='cuda:0')
Index force: tensor([0.5765], device='cuda:0')
Storing RECOVERY transition: reward=-0.5269 (scaled=-0.1054), steps=5
Reward stats updated: mean 0.0036 -> 0.0032, std: 0.0804
Collected 265 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.7934, Q2 Loss=1.7934, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9504
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1062, Q2 Loss=1.1062, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3271
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7548, Q2 Loss=0.7548, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7501
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2561, Q2 Loss=1.2561, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1665
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7876, Q2 Loss=0.7876, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8759

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.2%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (18.2%)
Actor update: 0.11s (40.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.139613
Q2 loss: 1.139613
Current threshold: -33.3553
Global Scale Offset: 0.0838
Reward stats: mean=0.0032, std=0.0804, count=265
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1396, Q2 Loss: 1.1396, Entropy: 0.0000, Mean TD Error: 1.4140, Threshold: -33.3553
Original likelihood: -288.0062255859375
Adjusted likelihood: -288.0062255859375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 304.88671875
Projection step: 1, Loss: 290.183837890625
Projection step: 2, Loss: 315.4692077636719
Projection step: 3, Loss: 301.38641357421875
Projection step: 4, Loss: 288.97100830078125
Projection step: 5, Loss: 298.9068603515625
Projection step: 6, Loss: 300.143798828125
Projection step: 7, Loss: 279.31982421875
Projection step: 8, Loss: 288.7344970703125
Projection step: 9, Loss: 275.8135681152344
Projection step: 10, Loss: 308.9766845703125
Projection step: 11, Loss: 308.9645080566406
Projection step: 12, Loss: 286.5654296875
Projection step: 13, Loss: 300.62841796875
Projection step: 14, Loss: 299.2923583984375
Projection step: 15, Loss: 303.53582763671875
Projection step: 16, Loss: 288.82147216796875
Projection step: 17, Loss: 297.44427490234375
Projection step: 18, Loss: 284.35003662109375
Projection step: 19, Loss: 312.2613525390625
Projection step: 20, Loss: 287.98779296875
Projection step: 21, Loss: 294.14031982421875
Projection step: 22, Loss: 304.80120849609375
Projection step: 23, Loss: 302.073974609375
Projection step: 24, Loss: 303.05999755859375
Final likelihood: tensor([-301.0846, -263.0890, -341.0723, -261.3011, -312.6214, -285.9336,
        -272.7250, -282.9693, -258.1531, -283.1987, -297.8166, -290.4244,
        -338.0032, -346.7974, -339.6798, -320.3269])
Final projection likelihood: -299.6998
1 mode projection failed, trying anyway
New goal: tensor([-0.1380,  0.7809,  0.8103,  0.8870,  0.0635,  0.9208,  1.2107,  1.1829,
         1.2057,  0.1446,  0.2532,  1.4419,  0.3736, -0.3341,  0.0230],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0049]], device='cuda:0') tensor([[0.0055]], device='cuda:0')
Original likelihood: -294.3601379394531
Adjusted likelihood: -294.3601379394531
Likelihood residual: 0.0
{'index': 294.3601379394531, 'thumb_middle': inf}
Current yaw: tensor([ 0.3736, -0.3336,  0.0024], device='cuda:0')
10 index
tensor([-1.4147e-01,  7.8140e-01,  8.0653e-01,  8.8579e-01,  6.3118e-02,
         9.2028e-01,  1.2138e+00,  1.1789e+00,  1.2121e+00,  1.4174e-01,
         2.5462e-01,  1.4397e+00,  3.7359e-01, -3.3362e-01,  2.4052e-03,
         4.2241e+00], device='cuda:0')
Solve time for step 1 10.005495157965925
Current ori: tensor([ 0.3736, -0.3336,  0.0024], device='cuda:0')
Middle force: tensor([0.5499, 0.5875, 0.6456, 0.5440], device='cuda:0')
Thumb force: tensor([0.5123, 0.5153, 0.5010, 0.5883], device='cuda:0')
tensor([-0.0977,  0.8163,  0.8231,  0.8906,  0.0710,  0.9736,  1.2213,  1.1631,
         1.2000,  0.1859,  0.2138,  1.4374,  0.3776, -0.3470, -0.0280,  4.3909],
       device='cuda:0')
Solve time for step 2 4.0506991719594225
Current ori: tensor([ 0.3776, -0.3470, -0.0280], device='cuda:0')
Middle force: tensor([0.5830, 0.6370, 0.5485], device='cuda:0')
Thumb force: tensor([0.5132, 0.5019, 0.5921], device='cuda:0')
tensor([-0.1110,  0.8276,  0.8284,  0.8902,  0.0811,  0.9783,  1.2277,  1.1402,
         1.1900,  0.2244,  0.2093,  1.3882,  0.3758, -0.3544, -0.0170,  4.3644],
       device='cuda:0')
Solve time for step 3 4.016623999050353
Current ori: tensor([ 0.3758, -0.3544, -0.0170], device='cuda:0')
Middle force: tensor([0.5305, 0.5532], device='cuda:0')
Thumb force: tensor([0.5187, 0.5625], device='cuda:0')
tensor([-0.1050,  0.8275,  0.8296,  0.8890,  0.0687,  0.9637,  1.2352,  1.1479,
         1.1930,  0.2130,  0.2621,  1.3676,  0.3773, -0.3442, -0.0262,  4.2952],
       device='cuda:0')
Solve time for step 4 3.9407904029940255
Current ori: tensor([ 0.3773, -0.3442, -0.0262], device='cuda:0')
Middle force: tensor([0.5279], device='cuda:0')
Thumb force: tensor([0.5844], device='cuda:0')
Storing RECOVERY transition: reward=-0.5226 (scaled=-0.1045), steps=5
Reward stats updated: mean 0.0032 -> 0.0028, std: 0.0805
Collected 266 transitions for RL
SAC Update 1/5: Actor Loss=-0.1884, Q1 Loss=1.7580, Q2 Loss=1.7580, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6285
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3617, Q2 Loss=1.3617, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8512
SAC Update 3/5: Actor Loss=-0.0471, Q1 Loss=0.7965, Q2 Loss=0.7965, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7556
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2222, Q2 Loss=1.2222, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7818
SAC Update 5/5: Actor Loss=-0.2008, Q1 Loss=1.6194, Q2 Loss=1.6194, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3680

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (21.2%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.04s (17.5%)
Actor update: 0.09s (37.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.087269
Q1 loss: 1.351565
Q2 loss: 1.351565
Current threshold: -33.3574
Global Scale Offset: 0.0838
Reward stats: mean=0.0028, std=0.0805, count=266
----------------------------------------------
SAC Update - Actor Loss: -0.0873, Q1 Loss: 1.3516, Q2 Loss: 1.3516, Entropy: 0.0000, Mean TD Error: 1.2770, Threshold: -33.3574
Original likelihood: -317.2447814941406
Adjusted likelihood: -317.2447814941406
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 16
Loaded trajectory sampler
Current yaw: tensor([-0.0001,  0.0140, -0.0389], device='cuda:0')
Current yaw: tensor([-0.0001,  0.0140, -0.0389], device='cuda:0')
1 turn
Sampling time 3.5979291180265136
tensor([ 1.3448e-01,  5.8192e-01,  6.0749e-01,  5.7420e-01, -1.2679e-01,
         5.5855e-01,  8.7806e-01,  9.1640e-01,  1.2124e+00,  2.8321e-01,
         2.6299e-01,  1.2182e+00, -1.1226e-04,  1.3974e-02, -3.8938e-02,
         2.7925e-01], device='cuda:0')
Original likelihood: -21.461658477783203
Adjusted likelihood: -21.461658477783203
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.11152094998397
Current ori: tensor([-0.0001,  0.0140, -0.0389], device='cuda:0')
Middle force: tensor([0.5518, 1.7165, 0.5325, 0.5137, 0.6303, 1.6021, 0.5622, 0.4992, 0.5404,
        0.5429, 0.4947, 0.5164], device='cuda:0')
Thumb force: tensor([0.6810, 0.9740, 0.4985, 0.9534, 0.8576, 1.0528, 0.7407, 0.5400, 1.0396,
        0.8917, 0.7304, 0.6688], device='cuda:0')
Index force: tensor([0.5169, 0.7027, 0.8105, 0.6290, 0.5017, 0.5225, 0.5001, 0.5571, 0.5353,
        0.5578, 0.7452, 0.6807], device='cuda:0')
Storing NORMAL transition: reward=-0.0227 (scaled=-0.0227), steps=1
Reward stats updated: mean 0.0028 -> 0.0027, std: 0.0804
Collected 267 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1346, Q2 Loss=1.1346, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0432
SAC Update 2/5: Actor Loss=-0.1732, Q1 Loss=1.6124, Q2 Loss=1.6124, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5218
SAC Update 3/5: Actor Loss=-0.0007, Q1 Loss=0.6602, Q2 Loss=0.6602, Entropy=0.1197, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3772
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=0.7559, Q2 Loss=0.7559, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9637
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.6554, Q2 Loss=0.6554, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7193

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.3%)
Q1 update: 0.05s (18.2%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (40.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.034768
Q1 loss: 0.963700
Q2 loss: 0.963700
Current threshold: -33.3592
Global Scale Offset: 0.0838
Reward stats: mean=0.0027, std=0.0804, count=267
----------------------------------------------
SAC Update - Actor Loss: -0.0348, Q1 Loss: 0.9637, Q2 Loss: 0.9637, Entropy: 0.0239, Mean TD Error: 0.9251, Threshold: -33.3592
tensor([ 0.1264,  0.6588,  0.4842,  0.5932, -0.1255,  0.6141,  0.7620,  0.9637,
         1.3410,  0.1186,  0.1800,  1.2744, -0.0129,  0.0132, -0.0164,  0.2335],
       device='cuda:0')
Original likelihood: -22.636083602905273
Adjusted likelihood: -22.636083602905273
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.464374465984292
Current ori: tensor([-0.0129,  0.0132, -0.0164], device='cuda:0')
Middle force: tensor([0.6103, 0.5001, 0.7222, 1.9790, 0.5454, 0.6557, 0.5009, 0.7736, 1.1331,
        0.5030, 0.5530], device='cuda:0')
Thumb force: tensor([0.9205, 0.5121, 0.5323, 1.7792, 0.5135, 0.7259, 0.5906, 0.5267, 1.0883,
        0.6701, 0.6266], device='cuda:0')
Index force: tensor([0.6688, 0.8517, 0.5033, 0.5537, 0.5714, 0.5908, 0.5785, 0.5040, 0.5395,
        0.9079, 0.5577], device='cuda:0')
Storing NORMAL transition: reward=0.0369 (scaled=0.0369), steps=1
Reward stats updated: mean 0.0027 -> 0.0028, std: 0.0803
Collected 268 transitions for RL
SAC Update 1/5: Actor Loss=-0.1041, Q1 Loss=1.2527, Q2 Loss=1.2527, Entropy=0.3419, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5428
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.6926, Q2 Loss=1.6926, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5224
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7438, Q2 Loss=0.7438, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8289
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.2742, Q2 Loss=1.2742, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2501
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6172, Q2 Loss=1.6172, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2921

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.8%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.8%)
Actor update: 0.10s (40.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.020829
Q1 loss: 1.316100
Q2 loss: 1.316100
Current threshold: -33.3667
Global Scale Offset: 0.0838
Reward stats: mean=0.0028, std=0.0803, count=268
----------------------------------------------
SAC Update - Actor Loss: -0.0208, Q1 Loss: 1.3161, Q2 Loss: 1.3161, Entropy: 0.0684, Mean TD Error: 1.2873, Threshold: -33.3667
tensor([ 0.3284,  0.6686,  0.6054,  0.5914, -0.2162,  0.5829,  0.8102,  0.9948,
         1.4397,  0.2042,  0.0773,  1.1956, -0.0434,  0.0583, -0.0582,  0.9792],
       device='cuda:0')
Original likelihood: -32.88795852661133
Adjusted likelihood: -32.88795852661133
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9367)
Solve time for step 3 5.246942938014399
Current ori: tensor([-0.0434,  0.0583, -0.0582], device='cuda:0')
Middle force: tensor([0.5003, 0.7013, 1.9599, 0.5417, 0.6465, 0.5026, 0.7611, 1.0850, 0.5019,
        0.5417], device='cuda:0')
Thumb force: tensor([0.5112, 0.5319, 1.7490, 0.5127, 0.7274, 0.7200, 0.5278, 1.0994, 0.6977,
        0.6986], device='cuda:0')
Index force: tensor([0.8240, 0.5033, 0.5476, 0.5706, 0.5860, 0.5317, 0.5036, 0.5416, 0.9009,
        0.5364], device='cuda:0')
Storing NORMAL transition: reward=0.0527 (scaled=0.0527), steps=1
Reward stats updated: mean 0.0028 -> 0.0030, std: 0.0802
Collected 269 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9886, Q2 Loss=0.9886, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6304
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.7197, Q2 Loss=1.7197, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4123
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4840, Q2 Loss=1.4840, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4706
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.2834, Q2 Loss=1.2834, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3946
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7545, Q2 Loss=0.7545, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1085

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.5%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (19.4%)
Actor update: 0.09s (40.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 1.246030
Q2 loss: 1.246030
Current threshold: -33.3711
Global Scale Offset: 0.0838
Reward stats: mean=0.0030, std=0.0802, count=269
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.2460, Q2 Loss: 1.2460, Entropy: 0.0000, Mean TD Error: 1.2033, Threshold: -33.3711
tensor([ 0.3165,  0.6878,  0.7136,  0.6726, -0.1936,  0.5758,  0.7751,  1.0366,
         1.4036,  0.2723,  0.1114,  1.1511, -0.0466,  0.0544, -0.1114,  1.5040],
       device='cuda:0')
Original likelihood: -31.432037353515625
Adjusted likelihood: -31.432037353515625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.983250993012916
Current ori: tensor([-0.0466,  0.0544, -0.1114], device='cuda:0')
Middle force: tensor([0.5035, 0.5052, 0.9019, 0.5029, 1.1036, 0.5440, 0.5627, 0.5446, 0.9652],
       device='cuda:0')
Thumb force: tensor([0.5834, 0.8748, 0.9391, 1.3244, 0.7402, 0.5470, 0.7552, 0.6539, 0.5476],
       device='cuda:0')
Index force: tensor([0.6969, 0.6348, 0.6326, 0.7060, 0.5067, 0.5343, 0.5302, 0.6005, 0.5226],
       device='cuda:0')
Storing NORMAL transition: reward=0.0144 (scaled=0.0144), steps=1
Reward stats updated: mean 0.0030 -> 0.0030, std: 0.0800
Collected 270 transitions for RL
SAC Update 1/5: Actor Loss=-0.0003, Q1 Loss=1.1315, Q2 Loss=1.1315, Entropy=0.1157, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9859
SAC Update 2/5: Actor Loss=-0.1799, Q1 Loss=1.6775, Q2 Loss=1.6775, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5694
SAC Update 3/5: Actor Loss=-0.0068, Q1 Loss=1.1735, Q2 Loss=1.1735, Entropy=0.3394, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6107
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5714, Q2 Loss=1.5714, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0481
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9916, Q2 Loss=0.9916, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5781

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (16.7%)
Q1 update: 0.05s (18.4%)
Q2 update: 0.06s (19.9%)
Actor update: 0.12s (42.0%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.037387
Q1 loss: 1.309085
Q2 loss: 1.309085
Current threshold: -33.3765
Global Scale Offset: 0.0838
Reward stats: mean=0.0030, std=0.0800, count=270
----------------------------------------------
SAC Update - Actor Loss: -0.0374, Q1 Loss: 1.3091, Q2 Loss: 1.3091, Entropy: 0.0910, Mean TD Error: 1.1584, Threshold: -33.3765
tensor([ 0.3630,  0.8518,  0.6359,  0.5494, -0.1448,  0.6465,  0.7340,  1.1167,
         1.4116,  0.0732, -0.0582,  0.9745, -0.0965,  0.0212, -0.1313,  1.7413],
       device='cuda:0')
Original likelihood: -38.95709228515625
Adjusted likelihood: -38.95709228515625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 41.006832122802734
Projection step: 1, Loss: 37.78266143798828
Projection step: 2, Loss: 36.982444763183594
Projection step: 3, Loss: 37.13731002807617
Projection step: 4, Loss: 39.142112731933594
Projection step: 5, Loss: 35.47086715698242
Projection step: 6, Loss: 36.869529724121094
Projection step: 7, Loss: 35.55550003051758
Projection step: 8, Loss: 34.763084411621094
Projection step: 9, Loss: 35.275447845458984
Projection step: 10, Loss: 34.81112289428711
Projection step: 11, Loss: 35.82734680175781
Projection step: 12, Loss: 33.63304138183594
Projection step: 13, Loss: 35.331966400146484
Projection step: 14, Loss: 34.01004409790039
Projection step: 15, Loss: 33.004371643066406
Projection step: 16, Loss: 32.32269287109375
Projection step: 17, Loss: 31.149627685546875
Projection step: 18, Loss: 31.352352142333984
Projection step: 19, Loss: 33.45629119873047
Projection step: 20, Loss: 31.160259246826172
Projection step: 21, Loss: 30.863689422607422
Projection step: 22, Loss: 33.26799774169922
Projection step: 23, Loss: 34.48344421386719
Projection step: 24, Loss: 31.223278045654297
Final likelihood: tensor([-25.9171, -31.2194, -29.8423, -30.0840, -29.0667, -30.4248, -35.2912,
        -31.3009, -33.6696, -33.1655, -30.2715, -33.0881, -30.1488, -28.9842,
        -35.5094, -32.7044])
Final projection likelihood: -31.2930
1 mode projection succeeded
New goal: tensor([ 0.3445,  0.7886,  0.5084,  0.5929, -0.1256,  0.6037,  0.7145,  1.1157,
         1.4088,  0.0612, -0.0109,  0.9382, -0.0962,  0.0206,  1.5248],
       device='cuda:0')
tensor([[0.0197]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0111]], device='cuda:0')
Original likelihood: -32.97180938720703
Adjusted likelihood: -32.97180938720703
Likelihood residual: 0.0
Original likelihood: -35.03508758544922
Adjusted likelihood: -35.03508758544922
Likelihood residual: 0.0
{'index': 35.03508758544922, 'thumb_middle': 32.97180938720703}
Current yaw: tensor([-0.0965,  0.0212, -0.1313], device='cuda:0')
2 thumb_middle
tensor([ 0.3630,  0.8518,  0.6359,  0.5494, -0.1448,  0.6465,  0.7340,  1.1167,
         1.4116,  0.0732, -0.0582,  0.9745, -0.0965,  0.0212, -0.1313,  1.7413],
       device='cuda:0')
Solve time for step 1 8.565825487021357
Current ori: tensor([-0.0965,  0.0212, -0.1313], device='cuda:0')
Index force: tensor([0.5917, 0.6012, 0.5678, 0.5092], device='cuda:0')
tensor([ 0.2809,  0.9147,  0.6487,  0.6343, -0.1931,  0.6005,  0.7051,  1.1074,
         1.4215,  0.0600, -0.0247,  0.9785, -0.2440,  0.0533, -0.1313,  1.8851],
       device='cuda:0')
Solve time for step 2 3.551114775997121
Current ori: tensor([-0.2440,  0.0533, -0.1313], device='cuda:0')
Index force: tensor([0.5910, 0.5603, 0.5072], device='cuda:0')
tensor([ 0.1751,  0.9281,  0.6791,  0.7002, -0.1908,  0.6130,  0.7049,  1.1103,
         1.4248,  0.0542, -0.0240,  0.9847, -0.6031,  0.1257, -0.1313,  1.8909],
       device='cuda:0')
Solve time for step 3 3.461437377962284
Current ori: tensor([-0.6031,  0.1257, -0.1313], device='cuda:0')
Index force: tensor([0.5126, 0.5439], device='cuda:0')
tensor([-0.0920,  1.0477,  0.7491,  0.7624, -0.1438,  0.6949,  0.7161,  1.0903,
         1.4393,  0.0573, -0.0342,  0.9393, -1.3177,  0.2175, -0.1312,  1.9123],
       device='cuda:0')
Solve time for step 4 3.2877792509971187
Current ori: tensor([-1.3177,  0.2175, -0.1312], device='cuda:0')
Index force: tensor([0.5526], device='cuda:0')
Storing RECOVERY transition: reward=-1.5802 (scaled=-0.3950), steps=4
Reward stats updated: mean 0.0030 -> 0.0015, std: 0.0834
Collected 271 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9425, Q2 Loss=0.9425, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5974
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.2907, Q2 Loss=2.2907, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6823
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.7677, Q2 Loss=0.7677, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8530
SAC Update 4/5: Actor Loss=-0.1473, Q1 Loss=1.6579, Q2 Loss=1.6579, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7532
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.9883, Q2 Loss=0.9883, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0580

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.5%)
Q1 update: 0.04s (18.4%)
Q2 update: 0.04s (18.2%)
Actor update: 0.09s (41.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.075506
Q1 loss: 1.329427
Q2 loss: 1.329427
Current threshold: -33.3814
Global Scale Offset: 0.0838
Reward stats: mean=0.0015, std=0.0834, count=271
----------------------------------------------
SAC Update - Actor Loss: -0.0755, Q1 Loss: 1.3294, Q2 Loss: 1.3294, Entropy: 0.0000, Mean TD Error: 1.3888, Threshold: -33.3814
Original likelihood: -1242.5439453125
Adjusted likelihood: -1242.5439453125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 17
Loaded trajectory sampler
Current yaw: tensor([ 0.0004,  0.0142, -0.0474], device='cuda:0')
Current yaw: tensor([ 0.0004,  0.0142, -0.0474], device='cuda:0')
1 turn
Sampling time 3.7144906299654394
tensor([ 1.3508e-01,  5.9828e-01,  5.8847e-01,  5.6843e-01, -1.2760e-01,
         5.2594e-01,  8.9536e-01,  9.8670e-01,  1.2397e+00,  2.7946e-01,
         2.4442e-01,  1.1832e+00,  4.4393e-04,  1.4189e-02, -4.7401e-02,
         3.3635e-02], device='cuda:0')
Original likelihood: -20.18113899230957
Adjusted likelihood: -20.18113899230957
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.25120960996719
Current ori: tensor([ 0.0004,  0.0142, -0.0474], device='cuda:0')
Middle force: tensor([0.7107, 1.8541, 0.6857, 0.5546, 0.9182, 0.5679, 0.5665, 0.5675, 0.5019,
        0.5137, 0.5011, 0.6274], device='cuda:0')
Thumb force: tensor([0.6323, 1.7354, 1.8968, 1.5672, 1.0606, 0.7250, 0.5073, 0.5094, 0.6092,
        0.5725, 0.6036, 0.5598], device='cuda:0')
Index force: tensor([0.7239, 1.3640, 0.5877, 0.5056, 1.3304, 0.6068, 0.5002, 0.6092, 0.6476,
        0.5383, 0.7014, 0.7257], device='cuda:0')
Storing NORMAL transition: reward=0.0529 (scaled=0.0529), steps=1
Reward stats updated: mean 0.0015 -> 0.0017, std: 0.0834
Collected 272 transitions for RL
SAC Update 1/5: Actor Loss=-0.0005, Q1 Loss=1.2583, Q2 Loss=1.2583, Entropy=0.0939, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8659
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.0933, Q2 Loss=1.0933, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1100
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1902, Q2 Loss=1.1902, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0367
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0460, Q2 Loss=1.0460, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8599
SAC Update 5/5: Actor Loss=-0.0083, Q1 Loss=1.1448, Q2 Loss=1.1448, Entropy=0.3423, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2454

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.5%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (19.4%)
Actor update: 0.10s (38.9%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.001760
Q1 loss: 1.146517
Q2 loss: 1.146517
Current threshold: -33.3865
Global Scale Offset: 0.0838
Reward stats: mean=0.0017, std=0.0834, count=272
----------------------------------------------
SAC Update - Actor Loss: -0.0018, Q1 Loss: 1.1465, Q2 Loss: 1.1465, Entropy: 0.0873, Mean TD Error: 0.8236, Threshold: -33.3865
tensor([ 0.1316,  0.6388,  0.5760,  0.4719, -0.0949,  0.4164,  0.8512,  1.0812,
         1.2945,  0.3284,  0.1878,  1.1294, -0.0154,  0.0138, -0.1006,  0.0451],
       device='cuda:0')
Original likelihood: -17.644935607910156
Adjusted likelihood: -17.644935607910156
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.604898959049024
Current ori: tensor([-0.0154,  0.0138, -0.1006], device='cuda:0')
Middle force: tensor([0.7682, 0.5370, 0.5194, 0.6237, 0.9331, 1.0117, 0.5797, 0.5013, 0.5019,
        0.5034, 0.6201], device='cuda:0')
Thumb force: tensor([2.3181, 0.6176, 1.4782, 1.0591, 0.8725, 1.9464, 0.5981, 0.7561, 0.5777,
        0.5501, 0.5693], device='cuda:0')
Index force: tensor([0.5003, 0.5889, 0.5554, 0.5676, 0.5154, 0.5786, 0.6114, 0.7541, 0.6070,
        0.5216, 0.5877], device='cuda:0')
Storing NORMAL transition: reward=0.1399 (scaled=0.1399), steps=1
Reward stats updated: mean 0.0017 -> 0.0022, std: 0.0836
Collected 273 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1396, Q2 Loss=1.1396, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1082
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3083, Q2 Loss=1.3083, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8444
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.6497, Q2 Loss=0.6497, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2529
SAC Update 4/5: Actor Loss=-0.0173, Q1 Loss=0.7254, Q2 Loss=0.7254, Entropy=0.2334, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6457
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3997, Q2 Loss=1.3997, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0657

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.5%)
Actor update: 0.10s (39.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.003463
Q1 loss: 1.044549
Q2 loss: 1.044549
Current threshold: -33.3987
Global Scale Offset: 0.0838
Reward stats: mean=0.0022, std=0.0836, count=273
----------------------------------------------
SAC Update - Actor Loss: -0.0035, Q1 Loss: 1.0445, Q2 Loss: 1.0445, Entropy: 0.0467, Mean TD Error: 0.7834, Threshold: -33.3987
tensor([ 0.1131,  0.6471,  0.5336,  0.4893, -0.0750,  0.3933,  0.9050,  1.1191,
         1.3934,  0.2449,  0.1720,  1.0089, -0.0195,  0.0289, -0.2419,  0.1053],
       device='cuda:0')
Original likelihood: -21.673202514648438
Adjusted likelihood: -21.673202514648438
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.077803933992982
Current ori: tensor([-0.0195,  0.0289, -0.2419], device='cuda:0')
Middle force: tensor([0.5206, 0.5128, 0.6299, 1.5132, 0.5463, 0.5018, 0.5260, 0.5267, 0.5191,
        0.5713], device='cuda:0')
Thumb force: tensor([0.5022, 0.8789, 0.8415, 1.0259, 0.7377, 0.5837, 1.0370, 0.6392, 0.5990,
        0.5536], device='cuda:0')
Index force: tensor([0.8265, 0.6175, 0.5034, 0.5280, 0.5035, 0.5735, 0.5258, 0.5709, 0.7024,
        0.6000], device='cuda:0')
Storing NORMAL transition: reward=-0.0144 (scaled=-0.0144), steps=1
Reward stats updated: mean 0.0022 -> 0.0022, std: 0.0835
Collected 274 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6460, Q2 Loss=0.6460, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8405
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.8878, Q2 Loss=1.8878, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5741
SAC Update 3/5: Actor Loss=-0.2360, Q1 Loss=1.0268, Q2 Loss=1.0268, Entropy=0.3309, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5293
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3417, Q2 Loss=1.3417, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1768
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.3099, Q2 Loss=2.3099, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8045

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.4%)
Q1 update: 0.06s (20.3%)
Q2 update: 0.06s (20.0%)
Actor update: 0.12s (42.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.047208
Q1 loss: 1.442442
Q2 loss: 1.442442
Current threshold: -33.4138
Global Scale Offset: 0.0837
Reward stats: mean=0.0022, std=0.0835, count=274
----------------------------------------------
SAC Update - Actor Loss: -0.0472, Q1 Loss: 1.4424, Q2 Loss: 1.4424, Entropy: 0.0662, Mean TD Error: 1.5850, Threshold: -33.4138
tensor([ 0.1731,  0.6584,  0.5388,  0.5404, -0.1300,  0.3167,  0.8684,  1.1777,
         1.4118,  0.1283,  0.2382,  1.0544, -0.0360,  0.0918, -0.2400, -3.2664],
       device='cuda:0')
Original likelihood: -34.773475646972656
Adjusted likelihood: -34.773475646972656
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 33.29365539550781
Projection step: 1, Loss: 33.721309661865234
Projection step: 2, Loss: 33.07098388671875
Projection step: 3, Loss: 33.183021545410156
Projection step: 4, Loss: 32.80972671508789
Projection step: 5, Loss: 32.07914733886719
Projection step: 6, Loss: 31.44931411743164
Projection step: 7, Loss: 31.277442932128906
Projection step: 8, Loss: 32.17671203613281
Projection step: 9, Loss: 31.960113525390625
Projection step: 10, Loss: 30.6101016998291
Projection step: 11, Loss: 29.764667510986328
Projection step: 12, Loss: 30.383350372314453
Projection step: 13, Loss: 29.563695907592773
Projection step: 14, Loss: 29.20770263671875
Projection step: 15, Loss: 29.707897186279297
Projection step: 16, Loss: 28.636920928955078
Projection step: 17, Loss: 28.38968849182129
Projection step: 18, Loss: 28.53689193725586
Projection step: 19, Loss: 28.203636169433594
Projection step: 20, Loss: 28.10934066772461
Projection step: 21, Loss: 27.691728591918945
Projection step: 22, Loss: 28.198076248168945
Projection step: 23, Loss: 27.251632690429688
Projection step: 24, Loss: 27.25342559814453
Final likelihood: tensor([-25.7464, -29.1458, -29.7319, -26.7187, -26.9874, -27.0929, -24.4511,
        -28.2960, -25.7460, -29.0857, -26.9119, -26.4860, -26.8228, -22.8670,
        -26.2965, -26.0293])
Final projection likelihood: -26.7760
1 mode projection succeeded
New goal: tensor([ 0.1559,  0.6699,  0.5342,  0.5659, -0.1405,  0.3438,  0.7375,  1.0601,
         1.3699,  0.1363,  0.2461,  1.0635, -0.0407,  0.0817,  0.1249],
       device='cuda:0')
tensor([[0.0067]], device='cuda:0') tensor([[0.0030]], device='cuda:0') tensor([[0.0038]], device='cuda:0')
Original likelihood: -25.116531372070312
Adjusted likelihood: -25.116531372070312
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 25.116531372070312}
Current yaw: tensor([-0.0360,  0.0918, -0.2400], device='cuda:0')
2 thumb_middle
tensor([ 0.1731,  0.6584,  0.5388,  0.5404, -0.1300,  0.3167,  0.8684,  1.1777,
         1.4118,  0.1283,  0.2382,  1.0544, -0.0360,  0.0918, -0.2400, -3.2664],
       device='cuda:0')
Solve time for step 1 8.96573052497115
Current ori: tensor([-0.0360,  0.0918, -0.2400], device='cuda:0')
Index force: tensor([0.5845, 0.5850, 0.5854, 0.5121], device='cuda:0')
tensor([ 0.1543,  0.6909,  0.5662,  0.5571, -0.2070,  0.3358,  0.7515,  1.0814,
         1.3611,  0.1111,  0.2142,  1.0512, -0.0935,  0.1910, -0.2523, -5.6564],
       device='cuda:0')
Solve time for step 2 3.588820045988541
Current ori: tensor([-0.0935,  0.1910, -0.2523], device='cuda:0')
Index force: tensor([0.5650, 0.5751, 0.5036], device='cuda:0')
tensor([ 0.0764,  0.7365,  0.6300,  0.6108, -0.2365,  0.3482,  0.7332,  1.0446,
         1.3974,  0.1322,  0.2328,  1.0535, -0.2330,  0.3973, -0.2754,  6.0376],
       device='cuda:0')
Solve time for step 3 3.455685974971857
Current ori: tensor([-0.2330,  0.3973, -0.2754], device='cuda:0')
Index force: tensor([0.5442, 0.5023], device='cuda:0')
tensor([-0.1136,  0.8347,  0.7133,  0.6668, -0.2257,  0.3381,  0.7395,  1.0525,
         1.4486,  0.1546,  0.2602,  1.0805, -0.5016,  0.8613, -0.2855, -4.8785],
       device='cuda:0')
Solve time for step 4 3.3919635040219873
Current ori: tensor([-0.5016,  0.8613, -0.2855], device='cuda:0')
Index force: tensor([0.5002], device='cuda:0')
Storing RECOVERY transition: reward=-1.3866 (scaled=-0.4622), steps=3
Reward stats updated: mean 0.0022 -> 0.0005, std: 0.0879
Collected 275 transitions for RL
SAC Update 1/5: Actor Loss=-0.0101, Q1 Loss=1.3012, Q2 Loss=1.3012, Entropy=0.3279, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7057
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.3415, Q2 Loss=1.3415, Entropy=0.0048, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7351
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1923, Q2 Loss=1.1923, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3247
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2106, Q2 Loss=1.2106, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0766
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4471, Q2 Loss=1.4471, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5480

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.04s (17.6%)
Actor update: 0.10s (39.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.002023
Q1 loss: 1.298546
Q2 loss: 1.298546
Current threshold: -33.4311
Global Scale Offset: 0.0836
Reward stats: mean=0.0005, std=0.0879, count=275
----------------------------------------------
SAC Update - Actor Loss: -0.0020, Q1 Loss: 1.2985, Q2 Loss: 1.2985, Entropy: 0.0665, Mean TD Error: 0.8780, Threshold: -33.4311
Original likelihood: -1045.4658203125
Adjusted likelihood: -1045.4658203125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 18
Loaded trajectory sampler
Current yaw: tensor([-0.0030,  0.0151, -0.0458], device='cuda:0')
Current yaw: tensor([-0.0030,  0.0151, -0.0458], device='cuda:0')
1 turn
Sampling time 3.603157678968273
tensor([ 0.1200,  0.6499,  0.5039,  0.5646, -0.1317,  0.5253,  0.9240,  0.9387,
         1.2217,  0.3077,  0.2505,  1.1838, -0.0030,  0.0151, -0.0458, -0.0415],
       device='cuda:0')
Original likelihood: -17.73885726928711
Adjusted likelihood: -17.73885726928711
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.729100149997976
Current ori: tensor([-0.0030,  0.0151, -0.0458], device='cuda:0')
Middle force: tensor([0.7314, 0.6759, 0.5010, 0.5218, 0.6652, 0.9965, 1.0053, 0.5568, 0.4918,
        0.5491, 0.5925, 0.5131], device='cuda:0')
Thumb force: tensor([0.5716, 2.4534, 0.6481, 1.5511, 1.0568, 0.8635, 2.0186, 0.6060, 0.6026,
        0.5914, 0.5693, 1.5728], device='cuda:0')
Index force: tensor([0.5674, 0.5148, 0.6276, 0.5581, 0.5625, 0.5087, 0.5591, 0.5999, 0.6289,
        0.6568, 0.5690, 0.5279], device='cuda:0')
Storing NORMAL transition: reward=0.0055 (scaled=0.0055), steps=1
Reward stats updated: mean 0.0005 -> 0.0005, std: 0.0877
Collected 276 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0360, Q2 Loss=1.0360, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6838
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.8914, Q2 Loss=2.8914, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5571
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.8699, Q2 Loss=1.8699, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8910
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0325, Q2 Loss=1.0325, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6527
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2234, Q2 Loss=1.2234, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0575

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.3%)
Actor update: 0.08s (40.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.610610
Q2 loss: 1.610610
Current threshold: -33.4414
Global Scale Offset: 0.0836
Reward stats: mean=0.0005, std=0.0877, count=276
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.6106, Q2 Loss: 1.6106, Entropy: 0.0000, Mean TD Error: 1.3684, Threshold: -33.4414
tensor([ 0.0389,  0.6010,  0.4850,  0.5814, -0.1604,  0.5717,  0.8021,  0.7852,
         1.3514,  0.2489,  0.2621,  1.0565, -0.0087,  0.0570, -0.0543,  0.0900],
       device='cuda:0')
Original likelihood: -25.507705688476562
Adjusted likelihood: -25.507705688476562
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.430467466008849
Current ori: tensor([-0.0087,  0.0570, -0.0543], device='cuda:0')
Middle force: tensor([0.5392, 0.5458, 0.9615, 0.5140, 0.5626, 0.5680, 1.2387, 1.0580, 0.5131,
        0.5341, 0.5439], device='cuda:0')
Thumb force: tensor([0.6050, 0.5445, 1.1196, 0.5889, 0.6670, 0.8918, 0.7187, 1.1158, 0.6189,
        0.5916, 0.6613], device='cuda:0')
Index force: tensor([0.5333, 0.5617, 0.9016, 0.5390, 0.5433, 0.5843, 0.8228, 1.1987, 0.6875,
        0.5512, 0.6320], device='cuda:0')
Storing NORMAL transition: reward=0.1038 (scaled=0.1038), steps=1
Reward stats updated: mean 0.0005 -> 0.0009, std: 0.0878
Collected 277 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8591, Q2 Loss=0.8591, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0640
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1581, Q2 Loss=1.1581, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9198
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.4308, Q2 Loss=2.4308, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6527
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2341, Q2 Loss=1.2341, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4371
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=3.1057, Q2 Loss=3.1057, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9621

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.05s (18.9%)
Actor update: 0.09s (38.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 1.757533
Q2 loss: 1.757533
Current threshold: -33.4474
Global Scale Offset: 0.0835
Reward stats: mean=0.0009, std=0.0878, count=277
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.7575, Q2 Loss: 1.7575, Entropy: 0.0000, Mean TD Error: 1.2071, Threshold: -33.4474
tensor([ 0.0580,  0.6116,  0.4780,  0.5994, -0.1475,  0.5672,  0.7919,  0.8542,
         1.3692,  0.2239,  0.2461,  1.0153, -0.0103,  0.0463, -0.1577,  0.2262],
       device='cuda:0')
Original likelihood: -22.34941864013672
Adjusted likelihood: -22.34941864013672
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.019308924966026
Current ori: tensor([-0.0103,  0.0463, -0.1577], device='cuda:0')
Middle force: tensor([0.5410, 0.9428, 0.5131, 0.5601, 0.5636, 1.2168, 1.0432, 0.5095, 0.5333,
        0.5441], device='cuda:0')
Thumb force: tensor([0.5403, 1.1030, 0.5811, 0.6604, 0.8814, 0.7112, 1.0901, 0.6168, 0.5839,
        0.6492], device='cuda:0')
Index force: tensor([0.5579, 0.8945, 0.5380, 0.5406, 0.5812, 0.8119, 1.1798, 0.6974, 0.5496,
        0.6277], device='cuda:0')
Storing NORMAL transition: reward=-0.0021 (scaled=-0.0021), steps=1
Reward stats updated: mean 0.0009 -> 0.0009, std: 0.0876
Collected 278 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5612, Q2 Loss=1.5612, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3605
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1594, Q2 Loss=1.1594, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7598
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2858, Q2 Loss=1.2858, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5778
SAC Update 4/5: Actor Loss=-0.0007, Q1 Loss=0.6823, Q2 Loss=0.6823, Entropy=0.3426, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5687
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6526, Q2 Loss=1.6526, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4247

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.3%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.05s (18.8%)
Actor update: 0.09s (38.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000143
Q1 loss: 1.268248
Q2 loss: 1.268248
Current threshold: -33.4513
Global Scale Offset: 0.0835
Reward stats: mean=0.0009, std=0.0876, count=278
----------------------------------------------
SAC Update - Actor Loss: -0.0001, Q1 Loss: 1.2682, Q2 Loss: 1.2682, Entropy: 0.0685, Mean TD Error: 0.9383, Threshold: -33.4513
tensor([-0.0110,  0.5422,  0.5080,  0.5956, -0.0852,  0.5629,  0.7925,  0.9816,
         1.3413,  0.1260,  0.2681,  1.0256,  0.0169,  0.0033, -0.1532,  2.0824],
       device='cuda:0')
Original likelihood: -13.639997482299805
Adjusted likelihood: -13.639997482299805
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.888908636989072
Current ori: tensor([ 0.0169,  0.0033, -0.1532], device='cuda:0')
Middle force: tensor([0.9770, 0.5247, 0.5886, 0.5856, 1.2392, 1.2381, 0.5150, 0.5635, 0.6075],
       device='cuda:0')
Thumb force: tensor([1.0209, 0.5462, 0.6179, 0.8423, 0.6931, 0.8682, 0.5846, 0.5512, 0.5819],
       device='cuda:0')
Index force: tensor([0.8598, 0.5345, 0.5350, 0.5666, 0.7789, 1.1521, 0.6767, 0.5390, 0.5975],
       device='cuda:0')
Storing NORMAL transition: reward=0.0846 (scaled=0.0846), steps=1
Reward stats updated: mean 0.0009 -> 0.0012, std: 0.0876
Collected 279 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6243, Q2 Loss=1.6243, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3143
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9681, Q2 Loss=0.9681, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0989
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5601, Q2 Loss=1.5601, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5379
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.1072, Q2 Loss=1.1072, Entropy=0.0001, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7852
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7734, Q2 Loss=0.7734, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5209

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (19.0%)
Actor update: 0.09s (38.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000000
Q1 loss: 1.206628
Q2 loss: 1.206628
Current threshold: -33.4539
Global Scale Offset: 0.0835
Reward stats: mean=0.0012, std=0.0876, count=279
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.2066, Q2 Loss: 1.2066, Entropy: 0.0000, Mean TD Error: 1.6514, Threshold: -33.4539
tensor([ 0.0075,  0.5653,  0.4817,  0.6156, -0.0757,  0.5555,  0.8168,  0.9772,
         1.3228,  0.1569,  0.2838,  0.9610,  0.0092, -0.0048, -0.2377,  2.1322],
       device='cuda:0')
Original likelihood: -14.424566268920898
Adjusted likelihood: -14.424566268920898
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.639431666000746
Current ori: tensor([ 0.0092, -0.0048, -0.2377], device='cuda:0')
Middle force: tensor([0.5378, 0.5115, 0.5624, 0.5460, 0.5225, 0.7363, 0.9183, 0.5339],
       device='cuda:0')
Thumb force: tensor([0.8635, 1.1070, 0.5409, 0.5123, 0.8431, 0.5370, 0.5184, 0.5354],
       device='cuda:0')
Index force: tensor([0.5392, 0.5138, 0.5401, 0.5339, 0.5109, 0.5135, 0.7818, 0.5301],
       device='cuda:0')
Storing NORMAL transition: reward=0.1141 (scaled=0.1141), steps=1
Reward stats updated: mean 0.0012 -> 0.0016, std: 0.0877
Collected 280 transitions for RL
SAC Update 1/5: Actor Loss=-0.0084, Q1 Loss=0.7293, Q2 Loss=0.7293, Entropy=0.3421, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7626
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0896, Q2 Loss=1.0896, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8337
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2934, Q2 Loss=1.2934, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4797
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.7601, Q2 Loss=1.7601, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4586
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6662, Q2 Loss=1.6662, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4039

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (19.0%)
Actor update: 0.09s (37.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.047729
Q1 loss: 1.307714
Q2 loss: 1.307714
Current threshold: -33.4616
Global Scale Offset: 0.0835
Reward stats: mean=0.0016, std=0.0877, count=280
----------------------------------------------
SAC Update - Actor Loss: -0.0477, Q1 Loss: 1.3077, Q2 Loss: 1.3077, Entropy: 0.0684, Mean TD Error: 1.1877, Threshold: -33.4616
tensor([ 0.0114,  0.5402,  0.5182,  0.6210, -0.0765,  0.5178,  0.8704,  0.9828,
         1.3721,  0.0976,  0.3059,  0.8491,  0.0070, -0.0063, -0.3518,  2.1883],
       device='cuda:0')
Original likelihood: -16.48028564453125
Adjusted likelihood: -16.48028564453125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.429051573970355
Current ori: tensor([ 0.0070, -0.0063, -0.3518], device='cuda:0')
Middle force: tensor([0.5103, 0.5582, 0.5434, 0.5195, 0.7275, 0.9081, 0.5310],
       device='cuda:0')
Thumb force: tensor([1.0653, 0.5349, 0.5098, 0.8207, 0.5299, 0.5165, 0.5326],
       device='cuda:0')
Index force: tensor([0.5116, 0.5365, 0.5306, 0.5089, 0.5109, 0.7673, 0.5268],
       device='cuda:0')
Storing NORMAL transition: reward=0.0669 (scaled=0.0669), steps=1
Reward stats updated: mean 0.0016 -> 0.0018, std: 0.0876
Collected 281 transitions for RL
SAC Update 1/5: Actor Loss=-0.0118, Q1 Loss=1.4030, Q2 Loss=1.4030, Entropy=0.3088, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3422
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1853, Q2 Loss=1.1853, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5166
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6969, Q2 Loss=1.6969, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3977
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8556, Q2 Loss=0.8556, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7561
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=2.1908, Q2 Loss=2.1908, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2126

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.4%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (20.0%)
Actor update: 0.12s (43.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.002354
Q1 loss: 1.466320
Q2 loss: 1.466320
Current threshold: -33.4741
Global Scale Offset: 0.0834
Reward stats: mean=0.0018, std=0.0876, count=281
----------------------------------------------
SAC Update - Actor Loss: -0.0024, Q1 Loss: 1.4663, Q2 Loss: 1.4663, Entropy: 0.0618, Mean TD Error: 1.2450, Threshold: -33.4741
tensor([-9.5926e-04,  5.0762e-01,  5.4529e-01,  6.3668e-01, -8.6765e-02,
         4.9697e-01,  8.9049e-01,  9.8426e-01,  1.3731e+00,  1.4694e-01,
         3.2596e-01,  7.8758e-01,  3.8303e-03,  9.5037e-04, -4.1852e-01,
         2.0763e+00], device='cuda:0')
Original likelihood: -14.893111228942871
Adjusted likelihood: -14.893111228942871
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.493036917003337
Current ori: tensor([ 0.0038,  0.0010, -0.4185], device='cuda:0')
Middle force: tensor([0.5531, 0.5416, 0.5165, 0.7174, 0.8940, 0.5286], device='cuda:0')
Thumb force: tensor([0.5293, 0.5074, 0.8009, 0.5252, 0.5151, 0.5305], device='cuda:0')
Index force: tensor([0.5329, 0.5278, 0.5074, 0.5091, 0.7559, 0.5239], device='cuda:0')
Storing NORMAL transition: reward=0.0312 (scaled=0.0312), steps=1
Reward stats updated: mean 0.0018 -> 0.0019, std: 0.0875
Collected 282 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.1137, Q2 Loss=1.1137, Entropy=0.0001, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6984
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9529, Q2 Loss=0.9529, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9950
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6279, Q2 Loss=0.6279, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0363
SAC Update 4/5: Actor Loss=-0.1037, Q1 Loss=1.1595, Q2 Loss=1.1595, Entropy=0.3005, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3946
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.0386, Q2 Loss=1.0386, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7731

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.6%)
Q1 update: 0.06s (19.9%)
Q2 update: 0.05s (19.0%)
Actor update: 0.12s (43.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.066786
Q1 loss: 0.978529
Q2 loss: 0.978529
Current threshold: -33.4853
Global Scale Offset: 0.0834
Reward stats: mean=0.0019, std=0.0875, count=282
----------------------------------------------
SAC Update - Actor Loss: -0.0668, Q1 Loss: 0.9785, Q2 Loss: 0.9785, Entropy: 0.0601, Mean TD Error: 0.7795, Threshold: -33.4853
tensor([-0.0036,  0.4938,  0.5579,  0.6457, -0.0896,  0.4926,  0.8964,  0.9816,
         1.3718,  0.1801,  0.3353,  0.7513,  0.0030,  0.0029, -0.4498,  2.1381],
       device='cuda:0')
Original likelihood: -14.965522766113281
Adjusted likelihood: -14.965522766113281
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.294451620022301
Current ori: tensor([ 0.0030,  0.0029, -0.4498], device='cuda:0')
Middle force: tensor([0.5392, 0.5142, 0.7084, 0.8801, 0.5264], device='cuda:0')
Thumb force: tensor([0.5053, 0.7826, 0.5210, 0.5139, 0.5285], device='cuda:0')
Index force: tensor([0.5251, 0.5062, 0.5076, 0.7459, 0.5214], device='cuda:0')
Storing NORMAL transition: reward=-0.0145 (scaled=-0.0145), steps=1
Reward stats updated: mean 0.0019 -> 0.0019, std: 0.0874
Collected 283 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.9997, Q2 Loss=0.9997, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0506
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2657, Q2 Loss=1.2657, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4370
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=2.4719, Q2 Loss=2.4719, Entropy=0.0001, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.3305
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1838, Q2 Loss=1.1838, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9246
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.9034, Q2 Loss=0.9034, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0148

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.4%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.05s (20.1%)
Actor update: 0.11s (41.8%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 1.364878
Q2 loss: 1.364878
Current threshold: -33.4963
Global Scale Offset: 0.0833
Reward stats: mean=0.0019, std=0.0874, count=283
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.3649, Q2 Loss: 1.3649, Entropy: 0.0000, Mean TD Error: 1.5515, Threshold: -33.4963
tensor([ 0.0816,  0.5113,  0.6056,  0.6658, -0.1945,  0.4636,  0.9859,  1.0356,
         1.3595,  0.2317,  0.3688,  0.7688,  0.0143,  0.0303, -0.4395,  1.0958],
       device='cuda:0')
Original likelihood: -28.894929885864258
Adjusted likelihood: -28.894929885864258
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 4.122137869999278
Current ori: tensor([ 0.0143,  0.0303, -0.4395], device='cuda:0')
Middle force: tensor([0.5120, 0.6715, 0.7467, 0.5231], device='cuda:0')
Thumb force: tensor([0.7643, 0.5222, 0.5354, 0.5275], device='cuda:0')
Index force: tensor([0.5049, 0.5066, 0.7333, 0.5195], device='cuda:0')
Storing NORMAL transition: reward=0.0273 (scaled=0.0273), steps=1
Reward stats updated: mean 0.0019 -> 0.0019, std: 0.0872
Collected 284 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.0729, Q2 Loss=1.0729, Entropy=0.0002, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6730
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1217, Q2 Loss=1.1217, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7778
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8580, Q2 Loss=0.8580, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3332
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0033, Q2 Loss=1.0033, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8276
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8430, Q2 Loss=0.8430, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5282

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.1%)
Q1 update: 0.06s (20.2%)
Q2 update: 0.06s (20.2%)
Actor update: 0.12s (42.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 0.979782
Q2 loss: 0.979782
Current threshold: -33.5029
Global Scale Offset: 0.0833
Reward stats: mean=0.0019, std=0.0872, count=284
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 0.9798, Q2 Loss: 0.9798, Entropy: 0.0000, Mean TD Error: 0.6279, Threshold: -33.5029
tensor([ 0.1757,  0.5753,  0.5594,  0.7456, -0.2227,  0.4522,  0.9839,  1.0188,
         1.4207,  0.1927,  0.3556,  0.7369,  0.0064,  0.0525, -0.4765,  0.7059],
       device='cuda:0')
Original likelihood: -42.05247497558594
Adjusted likelihood: -42.05247497558594
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 42.028961181640625
Projection step: 1, Loss: 41.02891159057617
Projection step: 2, Loss: 40.206539154052734
Projection step: 3, Loss: 40.46058654785156
Projection step: 4, Loss: 40.30276107788086
Projection step: 5, Loss: 38.13982391357422
Projection step: 6, Loss: 38.61891174316406
Projection step: 7, Loss: 38.7490348815918
Projection step: 8, Loss: 38.454429626464844
Projection step: 9, Loss: 38.48066711425781
Projection step: 10, Loss: 36.751930236816406
Projection step: 11, Loss: 36.70164108276367
Projection step: 12, Loss: 36.88386917114258
Projection step: 13, Loss: 35.80123519897461
Projection step: 14, Loss: 36.18345642089844
Projection step: 15, Loss: 34.856109619140625
Projection step: 16, Loss: 35.131107330322266
Projection step: 17, Loss: 34.732452392578125
Projection step: 18, Loss: 33.40664291381836
Projection step: 19, Loss: 34.00304412841797
Projection step: 20, Loss: 33.8454475402832
Projection step: 21, Loss: 33.58892822265625
Projection step: 22, Loss: 32.37257385253906
Projection step: 23, Loss: 30.96794891357422
Projection step: 24, Loss: 32.65453338623047
Final likelihood: tensor([-31.4749, -29.1590, -32.5001, -32.4802, -33.1322, -31.4572, -31.1686,
        -29.1700, -31.8638, -34.6044, -29.4484, -33.1465, -30.2965, -30.1961,
        -32.7094, -27.0579])
Final projection likelihood: -31.2416
1 mode projection succeeded
New goal: tensor([ 0.1571,  0.5313,  0.5408,  0.7216, -0.1659,  0.4761,  0.9827,  0.9633,
         1.3931,  0.1602,  0.3900,  0.8336,  0.0036,  0.0411, -0.8012],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -36.80113983154297
Adjusted likelihood: -36.80113983154297
Likelihood residual: 0.0
Original likelihood: -32.9761962890625
Adjusted likelihood: -32.9761962890625
Likelihood residual: 0.0
{'index': 32.9761962890625, 'thumb_middle': 36.80113983154297}
Current yaw: tensor([ 0.0064,  0.0525, -0.4765], device='cuda:0')
2 index
tensor([ 0.1757,  0.5753,  0.5594,  0.7456, -0.2227,  0.4522,  0.9839,  1.0188,
         1.4207,  0.1927,  0.3556,  0.7369,  0.0064,  0.0525, -0.4765,  0.7059],
       device='cuda:0')
Solve time for step 1 10.07147680100752
Current ori: tensor([ 0.0064,  0.0525, -0.4765], device='cuda:0')
Middle force: tensor([0.5433, 0.5434, 0.6053, 0.5745], device='cuda:0')
Thumb force: tensor([0.5096, 0.5560, 0.5268, 0.5249], device='cuda:0')
tensor([ 0.1778,  0.5019,  0.5127,  0.7094, -0.2273,  0.4452,  1.0018,  1.0014,
         1.4419,  0.1561,  0.3413,  0.7529,  0.0083,  0.0538, -0.4639,  0.7353],
       device='cuda:0')
Solve time for step 2 4.3240085750003345
Current ori: tensor([ 0.0083,  0.0538, -0.4639], device='cuda:0')
Middle force: tensor([0.5391, 0.6008, 0.5707], device='cuda:0')
Thumb force: tensor([0.5522, 0.5249, 0.5230], device='cuda:0')
tensor([ 0.1770,  0.5068,  0.5111,  0.7020, -0.1990,  0.4581,  1.0060,  0.9999,
         1.4211,  0.1698,  0.3104,  0.7912,  0.0057,  0.0348, -0.4668,  0.7277],
       device='cuda:0')
Solve time for step 3 4.150328262010589
Current ori: tensor([ 0.0057,  0.0348, -0.4668], device='cuda:0')
Middle force: tensor([0.6016, 0.5830], device='cuda:0')
Thumb force: tensor([0.5449, 0.5420], device='cuda:0')
tensor([ 0.1758,  0.5053,  0.5105,  0.7032, -0.1994,  0.4599,  1.0045,  0.9953,
         1.4206,  0.1728,  0.3147,  0.7802,  0.0035,  0.0356, -0.4696,  0.7062],
       device='cuda:0')
Solve time for step 4 3.952885670005344
Current ori: tensor([ 0.0035,  0.0356, -0.4696], device='cuda:0')
Middle force: tensor([0.5437], device='cuda:0')
Thumb force: tensor([0.5216], device='cuda:0')
Storing RECOVERY transition: reward=0.0028 (scaled=0.0003), steps=9
Reward stats updated: mean 0.0019 -> 0.0019, std: 0.0871
Collected 285 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.9167, Q2 Loss=1.9167, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1325
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=2.1039, Q2 Loss=2.1039, Entropy=0.0002, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8824
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7301, Q2 Loss=0.7301, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8037
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5313, Q2 Loss=1.5313, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0682
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.6137, Q2 Loss=1.6137, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2712

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.6%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (17.7%)
Actor update: 0.11s (40.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.579133
Q2 loss: 1.579133
Current threshold: -33.5067
Global Scale Offset: 0.0833
Reward stats: mean=0.0019, std=0.0871, count=285
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.5791, Q2 Loss: 1.5791, Entropy: 0.0000, Mean TD Error: 1.6316, Threshold: -33.5067
Original likelihood: -29.033924102783203
Adjusted likelihood: -29.033924102783203
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0051,  0.0287, -0.4695], device='cuda:0')
3 turn
Sampling time 3.6336874249973334
tensor([ 0.1325,  0.5613,  0.5534,  0.7289, -0.1864,  0.4766,  0.9953,  0.9793,
         1.4148,  0.1773,  0.3082,  0.7696, -0.0051,  0.0287, -0.4695,  0.7593],
       device='cuda:0')
Original likelihood: -28.291595458984375
Adjusted likelihood: -28.291595458984375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.11818236002
Current ori: tensor([-0.0051,  0.0287, -0.4695], device='cuda:0')
Middle force: tensor([0.5237, 0.5625, 0.5532, 1.8489, 0.5870, 1.2847, 0.5055, 0.5432, 0.8324,
        0.5446, 0.6591, 0.5984], device='cuda:0')
Thumb force: tensor([0.6486, 0.8378, 0.5346, 1.5712, 0.5565, 0.8062, 0.6556, 0.6003, 0.8087,
        1.1199, 0.5772, 0.6233], device='cuda:0')
Index force: tensor([0.6436, 0.5255, 0.4812, 0.5392, 0.5852, 0.6001, 0.7312, 0.6435, 0.5532,
        0.5833, 0.5583, 0.5936], device='cuda:0')
Storing NORMAL transition: reward=0.0004 (scaled=0.0004), steps=1
Reward stats updated: mean 0.0019 -> 0.0019, std: 0.0869
Collected 286 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6669, Q2 Loss=1.6669, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4859
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9686, Q2 Loss=0.9686, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6554
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9407, Q2 Loss=0.9407, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5688
SAC Update 4/5: Actor Loss=-0.0125, Q1 Loss=0.8378, Q2 Loss=0.8378, Entropy=0.2998, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7737
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7908, Q2 Loss=0.7908, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4126

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.7%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.1%)
Actor update: 0.09s (40.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.002496
Q1 loss: 1.040969
Q2 loss: 1.040969
Current threshold: -33.5129
Global Scale Offset: 0.0833
Reward stats: mean=0.0019, std=0.0869, count=286
----------------------------------------------
SAC Update - Actor Loss: -0.0025, Q1 Loss: 1.0410, Q2 Loss: 1.0410, Entropy: 0.0600, Mean TD Error: 0.9793, Threshold: -33.5129
tensor([ 0.1496,  0.5986,  0.4974,  0.7733, -0.2511,  0.5126,  1.0235,  1.1230,
         1.4640,  0.1783,  0.2313,  0.7278, -0.0084,  0.0186, -0.4695,  0.7759],
       device='cuda:0')
Original likelihood: -34.71888732910156
Adjusted likelihood: -34.71888732910156
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0001)
State is out of distribution
Projection step: 0, Loss: 34.549442291259766
Projection step: 1, Loss: 32.105098724365234
Projection step: 2, Loss: 33.28057098388672
Projection step: 3, Loss: 32.38695526123047
Projection step: 4, Loss: 30.276386260986328
Projection step: 5, Loss: 32.862548828125
Projection step: 6, Loss: 31.109447479248047
Projection step: 7, Loss: 30.3085880279541
Projection step: 8, Loss: 30.342077255249023
Projection step: 9, Loss: 34.23988342285156
Projection step: 10, Loss: 27.87769317626953
Projection step: 11, Loss: 27.758386611938477
Projection step: 12, Loss: 28.69219207763672
Projection step: 13, Loss: 28.439220428466797
Projection step: 14, Loss: 27.208782196044922
Projection step: 15, Loss: 25.6221923828125
Projection step: 16, Loss: 25.232450485229492
Projection step: 17, Loss: 26.001819610595703
Projection step: 18, Loss: 24.422822952270508
Projection step: 19, Loss: 24.7791748046875
Projection step: 20, Loss: 23.310598373413086
Projection step: 21, Loss: 24.496170043945312
Projection step: 22, Loss: 23.953857421875
Projection step: 23, Loss: 21.965965270996094
Projection step: 24, Loss: 22.12432861328125
Final likelihood: tensor([-23.5147, -19.4354, -25.5935, -24.9756, -25.4580, -24.6937, -23.9773,
        -22.7243, -23.9691, -22.2190, -24.1061, -22.1189, -23.5990, -22.9168,
        -23.1593, -30.3670])
Final projection likelihood: -23.9267
1 mode projection succeeded
New goal: tensor([ 0.1456,  0.5456,  0.5051,  0.7266, -0.1549,  0.5234,  1.0347,  1.0728,
         1.4660,  0.2223,  0.2172,  0.8164, -0.0083,  0.0155, -0.7018],
       device='cuda:0')
tensor([[0.0043]], device='cuda:0') tensor([[0.0045]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -29.250337600708008
Adjusted likelihood: -29.250337600708008
Likelihood residual: 0.0
Original likelihood: -27.05727767944336
Adjusted likelihood: -27.05727767944336
Likelihood residual: 0.0
{'index': 27.05727767944336, 'thumb_middle': 29.250337600708008}
Current yaw: tensor([-0.0084,  0.0186, -0.4695], device='cuda:0')
4 index
tensor([ 0.1496,  0.5986,  0.4974,  0.7733, -0.2511,  0.5126,  1.0235,  1.1230,
         1.4640,  0.1783,  0.2313,  0.7278, -0.0084,  0.0186, -0.4695,  0.7759],
       device='cuda:0')
Solve time for step 1 10.571690578013659
Current ori: tensor([-0.0084,  0.0186, -0.4695], device='cuda:0')
Middle force: tensor([0.5280, 0.5934, 0.5818, 0.5725], device='cuda:0')
Thumb force: tensor([0.5479, 0.5701, 0.5174, 0.6206], device='cuda:0')
tensor([ 0.1703,  0.5238,  0.4753,  0.7187, -0.2258,  0.5351,  1.0553,  1.0980,
         1.4621,  0.1869,  0.2068,  0.7882, -0.0098,  0.0080, -0.4583,  1.1789],
       device='cuda:0')
Solve time for step 2 4.045125226024538
Current ori: tensor([-0.0098,  0.0080, -0.4583], device='cuda:0')
Middle force: tensor([0.5891, 0.5779, 0.5682], device='cuda:0')
Thumb force: tensor([0.5655, 0.5164, 0.6139], device='cuda:0')
tensor([ 0.1702,  0.5248,  0.4748,  0.7101, -0.2118,  0.5426,  1.0588,  1.0901,
         1.4454,  0.2101,  0.1978,  0.7986, -0.0122, -0.0019, -0.4585,  1.4721],
       device='cuda:0')
Solve time for step 3 4.056615781970322
Current ori: tensor([-0.0122, -0.0019, -0.4585], device='cuda:0')
Middle force: tensor([0.5863, 0.5005], device='cuda:0')
Thumb force: tensor([0.5471, 0.5521], device='cuda:0')
tensor([ 0.1710,  0.5235,  0.4752,  0.7083, -0.2038,  0.5558,  1.0501,  1.0726,
         1.4686,  0.1636,  0.1851,  0.7759, -0.0223, -0.0051, -0.4661,  1.7243],
       device='cuda:0')
Solve time for step 4 3.9209283539676107
Current ori: tensor([-0.0223, -0.0051, -0.4661], device='cuda:0')
Middle force: tensor([0.5001], device='cuda:0')
Thumb force: tensor([0.5438], device='cuda:0')
Storing RECOVERY transition: reward=0.0076 (scaled=0.0076), steps=1
Reward stats updated: mean 0.0019 -> 0.0020, std: 0.0868
Collected 287 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1193, Q2 Loss=1.1193, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5624
SAC Update 2/5: Actor Loss=-0.0010, Q1 Loss=0.7091, Q2 Loss=0.7091, Entropy=0.1535, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6977
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0179, Q2 Loss=1.0179, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1050
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8202, Q2 Loss=0.8202, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4378
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.5339, Q2 Loss=1.5339, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2439

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.0%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (20.1%)
Actor update: 0.11s (42.4%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046244
Q1 loss: 1.040078
Q2 loss: 1.040078
Current threshold: -33.5221
Global Scale Offset: 0.0832
Reward stats: mean=0.0020, std=0.0868, count=287
----------------------------------------------
SAC Update - Actor Loss: -0.0462, Q1 Loss: 1.0401, Q2 Loss: 1.0401, Entropy: 0.0307, Mean TD Error: 1.2094, Threshold: -33.5221
Original likelihood: -25.682796478271484
Adjusted likelihood: -25.682796478271484
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0201, -0.0100, -0.4772], device='cuda:0')
5 turn
Sampling time 3.609983548987657
tensor([ 0.1224,  0.5796,  0.5183,  0.7337, -0.1975,  0.5560,  1.0534,  1.0823,
         1.4386,  0.2280,  0.1730,  0.8151, -0.0201, -0.0100, -0.4772,  1.8048],
       device='cuda:0')
Original likelihood: -24.58670425415039
Adjusted likelihood: -24.58670425415039
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.979737657005899
Current ori: tensor([-0.0201, -0.0100, -0.4772], device='cuda:0')
Middle force: tensor([1.0320, 1.0105, 0.5625, 1.0617, 1.7244, 1.0848, 0.5728, 0.5099, 0.7224,
        0.5387, 0.5703, 0.6013], device='cuda:0')
Thumb force: tensor([0.9913, 0.5956, 0.5778, 1.4174, 1.6375, 0.9548, 0.5108, 0.5423, 0.5760,
        0.6828, 0.6410, 0.6127], device='cuda:0')
Index force: tensor([0.5545, 0.6105, 0.5496, 0.5864, 0.5072, 0.5480, 0.5569, 0.7315, 0.5297,
        0.5754, 0.6093, 0.6216], device='cuda:0')
Storing NORMAL transition: reward=0.0184 (scaled=0.0184), steps=1
Reward stats updated: mean 0.0020 -> 0.0020, std: 0.0866
Collected 288 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4102, Q2 Loss=1.4102, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8988
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=2.0131, Q2 Loss=2.0131, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8699
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.9439, Q2 Loss=0.9439, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8901
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.4057, Q2 Loss=2.4057, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6599
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.5301, Q2 Loss=1.5301, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0505

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.9%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (19.7%)
Actor update: 0.09s (40.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 1.660594
Q2 loss: 1.660594
Current threshold: -33.5277
Global Scale Offset: 0.0832
Reward stats: mean=0.0020, std=0.0866, count=288
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.6606, Q2 Loss: 1.6606, Entropy: 0.0000, Mean TD Error: 1.6738, Threshold: -33.5277
tensor([ 0.0553,  0.5374,  0.5011,  0.7534, -0.2264,  0.5526,  1.0136,  1.1312,
         1.4787,  0.2068,  0.1796,  0.7903, -0.0144,  0.0119, -0.4954,  1.9835],
       device='cuda:0')
Original likelihood: -30.71677017211914
Adjusted likelihood: -30.71677017211914
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.722553731000517
Current ori: tensor([-0.0144,  0.0119, -0.4954], device='cuda:0')
Middle force: tensor([1.0089, 0.5791, 1.0502, 1.6979, 1.0740, 0.5725, 0.5076, 0.7192, 0.5372,
        0.5734, 0.5986], device='cuda:0')
Thumb force: tensor([0.5855, 0.5624, 1.3889, 1.6029, 0.9375, 0.5094, 0.5378, 0.5708, 0.6756,
        0.6291, 0.6080], device='cuda:0')
Index force: tensor([0.5993, 0.5416, 0.5808, 0.5067, 0.5453, 0.5521, 0.7473, 0.5275, 0.5721,
        0.6029, 0.6169], device='cuda:0')
Storing NORMAL transition: reward=0.0421 (scaled=0.0421), steps=1
Reward stats updated: mean 0.0020 -> 0.0021, std: 0.0865
Collected 289 transitions for RL
SAC Update 1/5: Actor Loss=-0.0107, Q1 Loss=1.3321, Q2 Loss=1.3321, Entropy=0.3210, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1445
SAC Update 2/5: Actor Loss=-0.0012, Q1 Loss=1.4126, Q2 Loss=1.4126, Entropy=0.1762, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1580
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.7159, Q2 Loss=1.7159, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4723
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=5.6734, Q2 Loss=5.6734, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.2061
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9693, Q2 Loss=0.9693, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4230

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.2%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.05s (20.0%)
Actor update: 0.09s (39.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.002388
Q1 loss: 2.220674
Q2 loss: 2.220674
Current threshold: -33.5399
Global Scale Offset: 0.0832
Reward stats: mean=0.0021, std=0.0865, count=289
----------------------------------------------
SAC Update - Actor Loss: -0.0024, Q1 Loss: 2.2207, Q2 Loss: 2.2207, Entropy: 0.0995, Mean TD Error: 1.8808, Threshold: -33.5399
tensor([ 0.1284,  0.4976,  0.5435,  0.7198, -0.1988,  0.5531,  1.0712,  1.0404,
         1.5000,  0.1668,  0.1653,  0.7185, -0.0348, -0.0045, -0.5385,  0.8395],
       device='cuda:0')
Original likelihood: -20.777320861816406
Adjusted likelihood: -20.777320861816406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.258104531036224
Current ori: tensor([-0.0348, -0.0045, -0.5385], device='cuda:0')
Middle force: tensor([0.5012, 0.8331, 0.6708, 0.7106, 0.5528, 0.5735, 1.0733, 0.5456, 0.9254,
        0.7588], device='cuda:0')
Thumb force: tensor([0.6327, 1.3675, 0.5239, 0.5432, 0.6972, 0.7513, 0.8268, 0.5827, 0.5147,
        0.9692], device='cuda:0')
Index force: tensor([0.6176, 0.5876, 0.5668, 0.5969, 0.5610, 0.8240, 0.5845, 0.5963, 0.5345,
        0.5631], device='cuda:0')
Storing NORMAL transition: reward=0.0098 (scaled=0.0098), steps=1
Reward stats updated: mean 0.0021 -> 0.0022, std: 0.0863
Collected 290 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.5528, Q2 Loss=1.5528, Entropy=0.0009, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6265
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.9107, Q2 Loss=0.9107, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6368
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8934, Q2 Loss=0.8934, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7933
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9439, Q2 Loss=0.9439, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2461
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0173, Q2 Loss=1.0173, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2373

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.5%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (19.0%)
Actor update: 0.11s (39.8%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.063626
Q2 loss: 1.063626
Current threshold: -33.5474
Global Scale Offset: 0.0832
Reward stats: mean=0.0022, std=0.0863, count=290
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.0636, Q2 Loss: 1.0636, Entropy: 0.0002, Mean TD Error: 0.9080, Threshold: -33.5474
tensor([ 0.1020,  0.5047,  0.5780,  0.7713, -0.2126,  0.5920,  0.9718,  1.1468,
         1.4316,  0.3972,  0.2363,  0.6497, -0.0317,  0.0022, -0.5481,  0.7087],
       device='cuda:0')
Original likelihood: -29.890666961669922
Adjusted likelihood: -29.890666961669922
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.013887528039049
Current ori: tensor([-0.0317,  0.0022, -0.5481], device='cuda:0')
Middle force: tensor([0.8206, 0.6645, 0.6934, 0.5496, 0.5600, 1.0566, 0.5393, 0.9150, 0.7495],
       device='cuda:0')
Thumb force: tensor([1.3441, 0.5222, 0.5451, 0.6902, 0.7596, 0.8186, 0.5829, 0.5138, 0.9593],
       device='cuda:0')
Index force: tensor([0.5808, 0.5644, 0.5924, 0.5582, 0.8170, 0.5814, 0.5961, 0.5325, 0.5602],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0232 (scaled=-0.0232), steps=1
Reward stats updated: mean 0.0022 -> 0.0021, std: 0.0862
Collected 291 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4550, Q2 Loss=1.4550, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3946
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.6513, Q2 Loss=1.6513, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9752
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8057, Q2 Loss=0.8057, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4761
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.9016, Q2 Loss=1.9016, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5281
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.7871, Q2 Loss=1.7871, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0739

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.1%)
Q1 update: 0.04s (18.3%)
Q2 update: 0.05s (20.0%)
Actor update: 0.10s (42.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.520145
Q2 loss: 1.520145
Current threshold: -33.5518
Global Scale Offset: 0.0832
Reward stats: mean=0.0021, std=0.0862, count=291
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.5201, Q2 Loss: 1.5201, Entropy: 0.0001, Mean TD Error: 2.0896, Threshold: -33.5518
tensor([ 0.1227,  0.5788,  0.4878,  0.7900, -0.1801,  0.5090,  0.9646,  1.2267,
         1.3979,  0.3604,  0.2604,  0.6339, -0.0464, -0.0103, -0.5262,  0.6962],
       device='cuda:0')
Original likelihood: -32.08163833618164
Adjusted likelihood: -32.08163833618164
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.914031382009853
Current ori: tensor([-0.0464, -0.0103, -0.5262], device='cuda:0')
Middle force: tensor([0.6543, 0.6922, 0.5467, 0.5556, 1.0457, 0.5340, 0.9007, 0.7414],
       device='cuda:0')
Thumb force: tensor([0.5195, 0.5386, 0.6829, 0.7346, 0.8071, 0.5774, 0.5129, 0.9482],
       device='cuda:0')
Index force: tensor([0.5631, 0.5940, 0.5552, 0.8263, 0.5787, 0.6014, 0.5315, 0.5579],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0047 (scaled=-0.0047), steps=1
Reward stats updated: mean 0.0021 -> 0.0021, std: 0.0861
Collected 292 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.7492, Q2 Loss=0.7492, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2656
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3611, Q2 Loss=1.3611, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8251
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.9419, Q2 Loss=1.9419, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1959
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1005, Q2 Loss=1.1005, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2943
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9795, Q2 Loss=0.9795, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0262

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (19.5%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (39.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000000
Q1 loss: 1.226449
Q2 loss: 1.226449
Current threshold: -33.5544
Global Scale Offset: 0.0832
Reward stats: mean=0.0021, std=0.0861, count=292
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.2264, Q2 Loss: 1.2264, Entropy: 0.0000, Mean TD Error: 1.1214, Threshold: -33.5544
tensor([ 0.1389,  0.5040,  0.5056,  0.9332, -0.1879,  0.3800,  1.0633,  1.3606,
         1.5000,  0.2307,  0.1129,  0.5936, -0.1071,  0.0110, -0.5313,  0.0648],
       device='cuda:0')
Original likelihood: -17.022897720336914
Adjusted likelihood: -17.022897720336914
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.6752874800004065
Current ori: tensor([-0.1071,  0.0110, -0.5313], device='cuda:0')
Middle force: tensor([0.9918, 0.5649, 0.5032, 0.6814, 0.5281, 0.5516, 0.5846],
       device='cuda:0')
Thumb force: tensor([0.8907, 0.5061, 0.5338, 0.5604, 0.6618, 0.6313, 0.5948],
       device='cuda:0')
Index force: tensor([0.5486, 0.5399, 0.7601, 0.5246, 0.5638, 0.5841, 0.6054],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0586 (scaled=-0.0586), steps=1
Reward stats updated: mean 0.0021 -> 0.0019, std: 0.0860
Collected 293 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6428, Q2 Loss=1.6428, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8200
SAC Update 2/5: Actor Loss=-0.0009, Q1 Loss=0.7369, Q2 Loss=0.7369, Entropy=0.1428, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4108
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.7124, Q2 Loss=1.7124, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4139
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.8938, Q2 Loss=1.8938, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1008
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0578, Q2 Loss=1.0578, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9565

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.9%)
Q1 update: 0.05s (20.4%)
Q2 update: 0.05s (20.5%)
Actor update: 0.10s (40.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000173
Q1 loss: 1.408740
Q2 loss: 1.408740
Current threshold: -33.5570
Global Scale Offset: 0.0832
Reward stats: mean=0.0019, std=0.0860, count=293
----------------------------------------------
SAC Update - Actor Loss: -0.0002, Q1 Loss: 1.4087, Q2 Loss: 1.4087, Entropy: 0.0286, Mean TD Error: 1.5404, Threshold: -33.5570
tensor([ 0.0978,  0.4543,  0.6783,  0.9092, -0.0834,  0.5041,  1.1077,  1.2617,
         1.5000,  0.1678,  0.1861,  0.5382, -0.2341,  0.0332, -0.5149,  0.3938],
       device='cuda:0')
Original likelihood: -111.43930053710938
Adjusted likelihood: -111.43930053710938
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 85.38566589355469
Projection step: 1, Loss: 100.49942779541016
Projection step: 2, Loss: 101.25849914550781
Projection step: 3, Loss: 90.76847076416016
Projection step: 4, Loss: 92.11607360839844
Projection step: 5, Loss: 90.95374298095703
Projection step: 6, Loss: 98.841796875
Projection step: 7, Loss: 91.67862701416016
Projection step: 8, Loss: 94.00247192382812
Projection step: 9, Loss: 99.80352783203125
Projection step: 10, Loss: 95.49302673339844
Projection step: 11, Loss: 84.5644302368164
Projection step: 12, Loss: 98.52662658691406
Projection step: 13, Loss: 88.83329772949219
Projection step: 14, Loss: 86.31399536132812
Projection step: 15, Loss: 92.64418029785156
Projection step: 16, Loss: 92.83958435058594
Projection step: 17, Loss: 98.21359252929688
Projection step: 18, Loss: 82.86097717285156
Projection step: 19, Loss: 86.61315155029297
Projection step: 20, Loss: 99.40237426757812
Projection step: 21, Loss: 96.38438415527344
Projection step: 22, Loss: 94.5727767944336
Projection step: 23, Loss: 92.8541259765625
Projection step: 24, Loss: 90.09646606445312
Final likelihood: tensor([ -80.7844,  -91.7667, -111.7172, -101.0019, -115.9837,  -93.3443,
         -82.5355, -112.2938, -147.9996,  -81.0758, -154.5883,  -52.6130,
         -76.3838,  -81.3319,  -75.8392, -123.4514])
Final projection likelihood: -98.9194
1 mode projection failed, trying anyway
New goal: tensor([ 0.1013,  0.4507,  0.6770,  0.8840, -0.0822,  0.4934,  1.0658,  1.2368,
         1.5005,  0.1942,  0.2080,  0.5296, -0.2316,  0.0329, -0.4227],
       device='cuda:0')
tensor([[0.0031]], device='cuda:0') tensor([[0.0207]], device='cuda:0') tensor([[0.0170]], device='cuda:0')
Original likelihood: -37.13624572753906
Adjusted likelihood: -37.13624572753906
Likelihood residual: 0.0
Original likelihood: -48.00725555419922
Adjusted likelihood: -48.00725555419922
Likelihood residual: 0.0
{'index': 48.00725555419922, 'thumb_middle': 37.13624572753906}
Current yaw: tensor([-0.2341,  0.0332, -0.5149], device='cuda:0')
6 thumb_middle
tensor([ 0.0978,  0.4543,  0.6783,  0.9092, -0.0834,  0.5041,  1.1077,  1.2617,
         1.5000,  0.1678,  0.1861,  0.5382, -0.2341,  0.0332, -0.5149,  0.3938],
       device='cuda:0')
Solve time for step 1 8.820473061001394
Current ori: tensor([-0.2341,  0.0332, -0.5149], device='cuda:0')
Index force: tensor([0.6420, 0.5065, 0.5005, 0.5353], device='cuda:0')
tensor([ 0.0681,  0.4939,  0.7028,  0.8981, -0.1559,  0.4888,  1.0631,  1.2374,
         1.4453,  0.1779,  0.1559,  0.5010, -0.5777,  0.0796, -0.5147,  1.8629],
       device='cuda:0')
Solve time for step 2 3.645690381003078
Current ori: tensor([-0.5777,  0.0796, -0.5147], device='cuda:0')
Index force: tensor([0.5040, 0.5000, 0.5202], device='cuda:0')
tensor([-0.1791,  0.7445,  0.8080,  0.9269, -0.1219,  0.5729,  1.0387,  1.1956,
         1.4534,  0.1823,  0.1380,  0.5189, -1.2806,  0.1412, -0.5147,  3.0700],
       device='cuda:0')
Solve time for step 3 3.40995932900114
Current ori: tensor([-1.2806,  0.1412, -0.5147], device='cuda:0')
Index force: tensor([0.5072, 0.5166], device='cuda:0')
tensor([-0.4076,  0.9940,  0.8643,  1.0019, -0.2859,  0.5818,  1.1194,  1.2261,
         1.4595,  0.1770,  0.1053,  0.5141, -1.9583,  0.1293, -0.5146,  2.3693],
       device='cuda:0')
Solve time for step 4 3.3869724809774198
Current ori: tensor([-1.9583,  0.1293, -0.5146], device='cuda:0')
Index force: tensor([0.5167], device='cuda:0')
Storing RECOVERY transition: reward=-1.6183 (scaled=-0.2697), steps=6
Reward stats updated: mean 0.0019 -> 0.0009, std: 0.0873
Collected 294 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.8265, Q2 Loss=0.8265, Entropy=0.0017, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3963
SAC Update 2/5: Actor Loss=-0.0118, Q1 Loss=0.8416, Q2 Loss=0.8416, Entropy=0.3085, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5622
SAC Update 3/5: Actor Loss=-0.2051, Q1 Loss=1.8243, Q2 Loss=1.8243, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6201
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9994, Q2 Loss=0.9994, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9921
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.8405, Q2 Loss=0.8405, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6695

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (17.2%)
Q1 update: 0.05s (18.8%)
Q2 update: 0.06s (19.7%)
Actor update: 0.12s (40.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.089426
Q1 loss: 1.066461
Q2 loss: 1.066461
Current threshold: -33.5656
Global Scale Offset: 0.0832
Reward stats: mean=0.0009, std=0.0873, count=294
----------------------------------------------
SAC Update - Actor Loss: -0.0894, Q1 Loss: 1.0665, Q2 Loss: 1.0665, Entropy: 0.0620, Mean TD Error: 0.8480, Threshold: -33.5656
Original likelihood: -1308.027587890625
Adjusted likelihood: -1308.027587890625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 19
Loaded trajectory sampler
Current yaw: tensor([-0.0027,  0.0149, -0.0448], device='cuda:0')
Current yaw: tensor([-0.0027,  0.0149, -0.0448], device='cuda:0')
1 turn
Sampling time 3.8105163230211474
tensor([ 0.1722,  0.6366,  0.5521,  0.6095, -0.1053,  0.5269,  0.8651,  0.9753,
         1.1975,  0.3157,  0.2666,  1.2027, -0.0027,  0.0149, -0.0448,  0.1239],
       device='cuda:0')
Original likelihood: -21.957447052001953
Adjusted likelihood: -21.957447052001953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.356610657996498
Current ori: tensor([-0.0027,  0.0149, -0.0448], device='cuda:0')
Middle force: tensor([0.9796, 0.5186, 0.4993, 0.6406, 0.6061, 0.5092, 0.5285, 0.5088, 0.7837,
        0.5557, 0.5413, 0.5307], device='cuda:0')
Thumb force: tensor([0.9957, 0.6105, 0.5952, 0.8372, 0.5456, 0.6723, 0.9716, 1.6842, 0.5778,
        0.6243, 0.6302, 0.6562], device='cuda:0')
Index force: tensor([0.5349, 0.5845, 0.7286, 0.6628, 0.5851, 0.6181, 0.5274, 0.5486, 0.6948,
        0.5928, 0.6035, 0.6320], device='cuda:0')
Storing NORMAL transition: reward=0.1846 (scaled=0.1846), steps=1
Reward stats updated: mean 0.0009 -> 0.0016, std: 0.0878
Collected 295 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9423, Q2 Loss=0.9423, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5412
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9226, Q2 Loss=0.9226, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3099
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3074, Q2 Loss=1.3074, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2060
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.7010, Q2 Loss=1.7010, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6705
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4216, Q2 Loss=1.4216, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4755

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (16.5%)
Q1 update: 0.06s (19.8%)
Q2 update: 0.05s (19.2%)
Actor update: 0.12s (41.5%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.258973
Q2 loss: 1.258973
Current threshold: -33.5720
Global Scale Offset: 0.0831
Reward stats: mean=0.0016, std=0.0878, count=295
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.2590, Q2 Loss: 1.2590, Entropy: 0.0000, Mean TD Error: 1.0406, Threshold: -33.5720
tensor([ 0.1867,  0.7056,  0.4567,  0.6333, -0.1263,  0.4831,  0.9218,  1.0887,
         1.2728,  0.3393,  0.2330,  1.0681, -0.0202,  0.0134, -0.2300,  0.3439],
       device='cuda:0')
Original likelihood: -22.442699432373047
Adjusted likelihood: -22.442699432373047
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.526803758984897
Current ori: tensor([-0.0202,  0.0134, -0.2300], device='cuda:0')
Middle force: tensor([0.5171, 0.5003, 0.6426, 0.6045, 0.5083, 0.5246, 0.5074, 0.7660, 0.5506,
        0.5365, 0.5275], device='cuda:0')
Thumb force: tensor([0.6039, 0.6088, 0.8262, 0.5409, 0.6695, 0.9749, 1.6656, 0.5793, 0.6299,
        0.6343, 0.6596], device='cuda:0')
Index force: tensor([0.5750, 0.7280, 0.6464, 0.5753, 0.6066, 0.5240, 0.5412, 0.6904, 0.5820,
        0.5937, 0.6223], device='cuda:0')
Storing NORMAL transition: reward=0.0952 (scaled=0.0952), steps=1
Reward stats updated: mean 0.0016 -> 0.0019, std: 0.0878
Collected 296 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.8189, Q2 Loss=1.8189, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5263
SAC Update 2/5: Actor Loss=-0.0498, Q1 Loss=0.8232, Q2 Loss=0.8232, Entropy=0.3010, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1658
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.9026, Q2 Loss=1.9026, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0401
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1052, Q2 Loss=1.1052, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6200
SAC Update 5/5: Actor Loss=-0.0262, Q1 Loss=0.7123, Q2 Loss=0.7123, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6431

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.5%)
Q1 update: 0.05s (20.6%)
Q2 update: 0.05s (19.8%)
Actor update: 0.10s (40.9%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.015213
Q1 loss: 1.272444
Q2 loss: 1.272444
Current threshold: -33.5829
Global Scale Offset: 0.0831
Reward stats: mean=0.0019, std=0.0878, count=296
----------------------------------------------
SAC Update - Actor Loss: -0.0152, Q1 Loss: 1.2724, Q2 Loss: 1.2724, Entropy: 0.0602, Mean TD Error: 1.3991, Threshold: -33.5829
tensor([ 0.1143,  0.6502,  0.3765,  0.5624, -0.0784,  0.4376,  1.0381,  0.9255,
         1.4025,  0.1507,  0.1718,  0.9551, -0.0577,  0.0082, -0.3314,  2.6612],
       device='cuda:0')
Original likelihood: -16.3354434967041
Adjusted likelihood: -16.3354434967041
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.263555906014517
Current ori: tensor([-0.0577,  0.0082, -0.3314], device='cuda:0')
Middle force: tensor([0.5009, 0.7127, 0.6155, 0.5167, 0.5244, 0.5068, 0.7990, 0.5701, 0.5441,
        0.5461], device='cuda:0')
Thumb force: tensor([0.5623, 0.7081, 0.5305, 0.6011, 0.9404, 1.6244, 0.5423, 0.5850, 0.5943,
        0.5941], device='cuda:0')
Index force: tensor([0.7341, 0.6525, 0.5698, 0.5996, 0.5241, 0.5387, 0.7174, 0.5834, 0.5994,
        0.6254], device='cuda:0')
Storing NORMAL transition: reward=0.0016 (scaled=0.0016), steps=1
Reward stats updated: mean 0.0019 -> 0.0019, std: 0.0877
Collected 297 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3922, Q2 Loss=1.3922, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2138
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2862, Q2 Loss=1.2862, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9338
SAC Update 3/5: Actor Loss=-0.0129, Q1 Loss=1.3506, Q2 Loss=1.3506, Entropy=0.2941, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7342
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0575, Q2 Loss=1.0575, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8192
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2008, Q2 Loss=1.2008, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0036

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (16.6%)
Q1 update: 0.05s (20.4%)
Q2 update: 0.05s (19.5%)
Actor update: 0.10s (40.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.002582
Q1 loss: 1.257461
Q2 loss: 1.257461
Current threshold: -33.5966
Global Scale Offset: 0.0830
Reward stats: mean=0.0019, std=0.0877, count=297
----------------------------------------------
SAC Update - Actor Loss: -0.0026, Q1 Loss: 1.2575, Q2 Loss: 1.2575, Entropy: 0.0588, Mean TD Error: 0.9409, Threshold: -33.5966
tensor([ 1.0398e-01,  6.4525e-01,  4.5196e-01,  6.4860e-01, -3.2151e-02,
         3.3766e-01,  9.8062e-01,  7.7958e-01,  1.3383e+00,  3.5337e-01,
         1.3073e-01,  8.8006e-01, -5.1255e-02,  1.2531e-03, -3.3148e-01,
         2.8701e+00], device='cuda:0')
Original likelihood: -19.821941375732422
Adjusted likelihood: -19.821941375732422
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.94048594601918
Current ori: tensor([-0.0513,  0.0013, -0.3315], device='cuda:0')
Middle force: tensor([0.6784, 0.5992, 0.5127, 0.5213, 0.5053, 0.7806, 0.5572, 0.5379, 0.5361],
       device='cuda:0')
Thumb force: tensor([0.6913, 0.5284, 0.5981, 0.9389, 1.6166, 0.5393, 0.5859, 0.5906, 0.5972],
       device='cuda:0')
Index force: tensor([0.6610, 0.5709, 0.6054, 0.5215, 0.5315, 0.7194, 0.5847, 0.6003, 0.6283],
       device='cuda:0')
Storing NORMAL transition: reward=0.0568 (scaled=0.0568), steps=1
Reward stats updated: mean 0.0019 -> 0.0021, std: 0.0876
Collected 298 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2493, Q2 Loss=1.2493, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1507
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5983, Q2 Loss=1.5983, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4123
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1857, Q2 Loss=1.1857, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6123
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.4884, Q2 Loss=1.4884, Entropy=0.0006, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5074
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.8727, Q2 Loss=1.8727, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5609

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (18.2%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (39.2%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000000
Q1 loss: 1.478872
Q2 loss: 1.478872
Current threshold: -33.6076
Global Scale Offset: 0.0830
Reward stats: mean=0.0021, std=0.0876, count=298
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.4789, Q2 Loss: 1.4789, Entropy: 0.0001, Mean TD Error: 1.4487, Threshold: -33.6076
tensor([ 0.1140,  0.5331,  0.5909,  0.6894,  0.0060,  0.4653,  0.9258,  0.8669,
         1.4079,  0.3307,  0.1669,  0.7909, -0.0481, -0.0162, -0.3899,  2.7190],
       device='cuda:0')
Original likelihood: -17.851932525634766
Adjusted likelihood: -17.851932525634766
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.581663406977896
Current ori: tensor([-0.0481, -0.0162, -0.3899], device='cuda:0')
Middle force: tensor([0.5972, 0.5161, 0.5200, 0.5048, 0.7892, 0.5669, 0.5407, 0.5473],
       device='cuda:0')
Thumb force: tensor([0.5242, 0.5814, 0.9264, 1.5878, 0.5335, 0.5726, 0.5804, 0.5785],
       device='cuda:0')
Index force: tensor([0.5651, 0.5915, 0.5196, 0.5280, 0.7077, 0.5749, 0.5915, 0.6119],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0170 (scaled=-0.0170), steps=1
Reward stats updated: mean 0.0021 -> 0.0020, std: 0.0874
Collected 299 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.5307, Q2 Loss=1.5307, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3654
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1166, Q2 Loss=1.1166, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7146
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=2.2404, Q2 Loss=2.2404, Entropy=0.0011, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.2548
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.4276, Q2 Loss=1.4276, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1515
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.3778, Q2 Loss=1.3778, Entropy=0.0006, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1472

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (13.8%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.06s (20.6%)
Actor update: 0.12s (42.5%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.538630
Q2 loss: 1.538630
Current threshold: -33.6142
Global Scale Offset: 0.0829
Reward stats: mean=0.0020, std=0.0874, count=299
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.5386, Q2 Loss: 1.5386, Entropy: 0.0003, Mean TD Error: 1.9267, Threshold: -33.6142
tensor([ 0.1533,  0.5584,  0.5798,  0.7224,  0.0439,  0.5086,  0.8356,  0.9572,
         1.3828,  0.2582,  0.1770,  0.8064, -0.0488, -0.0366, -0.3752,  2.7385],
       device='cuda:0')
Original likelihood: -23.36324691772461
Adjusted likelihood: -23.36324691772461
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.583800207008608
Current ori: tensor([-0.0488, -0.0366, -0.3752], device='cuda:0')
Middle force: tensor([0.5129, 0.5870, 0.5548, 0.7568, 0.5744, 1.1232, 0.9163],
       device='cuda:0')
Thumb force: tensor([0.5762, 0.5766, 0.5348, 0.6588, 0.5345, 0.5369, 0.7792],
       device='cuda:0')
Index force: tensor([0.5895, 0.5039, 0.6367, 0.7988, 0.5394, 0.6209, 0.5059],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0172 (scaled=-0.0172), steps=1
Reward stats updated: mean 0.0020 -> 0.0019, std: 0.0873
Collected 300 transitions for RL
SAC Update 1/5: Actor Loss=-0.0170, Q1 Loss=1.2250, Q2 Loss=1.2250, Entropy=0.2379, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3696
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7532, Q2 Loss=0.7532, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2858
SAC Update 3/5: Actor Loss=-0.1353, Q1 Loss=1.1378, Q2 Loss=1.1378, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8905
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2537, Q2 Loss=1.2537, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6775
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3127, Q2 Loss=1.3127, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0809

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.2%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.5%)
Actor update: 0.10s (38.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.030452
Q1 loss: 1.136477
Q2 loss: 1.136477
Current threshold: -33.6286
Global Scale Offset: 0.0828
Reward stats: mean=0.0019, std=0.0873, count=300
----------------------------------------------
SAC Update - Actor Loss: -0.0305, Q1 Loss: 1.1365, Q2 Loss: 1.1365, Entropy: 0.0476, Mean TD Error: 1.4609, Threshold: -33.6286
tensor([ 1.2179e-01,  4.3077e-01,  6.1296e-01,  9.4634e-01, -5.1756e-03,
         4.9659e-01,  8.3078e-01,  1.0521e+00,  1.4035e+00,  1.6540e-01,
         2.2652e-01,  8.1880e-01, -1.5193e-03, -2.6372e-02, -3.5023e-01,
         2.7827e+00], device='cuda:0')
Original likelihood: -16.359800338745117
Adjusted likelihood: -16.359800338745117
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.37824307999108
Current ori: tensor([-0.0015, -0.0264, -0.3502], device='cuda:0')
Middle force: tensor([0.5837, 0.5548, 0.7646, 0.5764, 1.1216, 0.9193], device='cuda:0')
Thumb force: tensor([0.5707, 0.5318, 0.6417, 0.5309, 0.5327, 0.7669], device='cuda:0')
Index force: tensor([0.5034, 0.6257, 0.7867, 0.5356, 0.6132, 0.5049], device='cuda:0')
Storing NORMAL transition: reward=0.0120 (scaled=0.0120), steps=1
Reward stats updated: mean 0.0019 -> 0.0020, std: 0.0872
Collected 301 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5483, Q2 Loss=1.5483, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2215
SAC Update 2/5: Actor Loss=-0.0001, Q1 Loss=1.1764, Q2 Loss=1.1764, Entropy=0.0242, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4883
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.7327, Q2 Loss=1.7327, Entropy=0.0008, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7288
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0557, Q2 Loss=1.0557, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3657
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.8428, Q2 Loss=2.8428, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5899

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.3%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (19.5%)
Actor update: 0.10s (40.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000017
Q1 loss: 1.671195
Q2 loss: 1.671195
Current threshold: -33.6370
Global Scale Offset: 0.0827
Reward stats: mean=0.0020, std=0.0872, count=301
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.6712, Q2 Loss: 1.6712, Entropy: 0.0050, Mean TD Error: 1.4789, Threshold: -33.6370
tensor([ 0.1105,  0.4331,  0.6067,  0.9240, -0.0161,  0.4943,  0.8301,  1.0345,
         1.4429,  0.1153,  0.2428,  0.7516, -0.0078, -0.0184, -0.3616,  2.7264],
       device='cuda:0')
Original likelihood: -14.78605842590332
Adjusted likelihood: -14.78605842590332
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.528524729015771
Current ori: tensor([-0.0078, -0.0184, -0.3616], device='cuda:0')
Middle force: tensor([0.5512, 0.7568, 0.5728, 1.1055, 0.9072], device='cuda:0')
Thumb force: tensor([0.5294, 0.6363, 0.5290, 0.5308, 0.7577], device='cuda:0')
Index force: tensor([0.6176, 0.7768, 0.5334, 0.6086, 0.5044], device='cuda:0')
Storing NORMAL transition: reward=0.1184 (scaled=0.1184), steps=1
Reward stats updated: mean 0.0020 -> 0.0023, std: 0.0873
Collected 302 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.2424, Q2 Loss=1.2424, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8753
SAC Update 2/5: Actor Loss=-0.0009, Q1 Loss=0.7099, Q2 Loss=0.7099, Entropy=0.1477, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5256
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4003, Q2 Loss=1.4003, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8916
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6738, Q2 Loss=1.6738, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3652
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2413, Q2 Loss=1.2413, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0243

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (16.7%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (19.4%)
Actor update: 0.10s (41.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046234
Q1 loss: 1.253566
Q2 loss: 1.253566
Current threshold: -33.6430
Global Scale Offset: 0.0827
Reward stats: mean=0.0023, std=0.0873, count=302
----------------------------------------------
SAC Update - Actor Loss: -0.0462, Q1 Loss: 1.2536, Q2 Loss: 1.2536, Entropy: 0.0295, Mean TD Error: 0.9364, Threshold: -33.6430
tensor([ 0.1328,  0.3867,  0.6249,  0.8046,  0.0959,  0.3514,  0.9947,  1.2068,
         1.4010,  0.1195,  0.3313,  0.6718,  0.0590, -0.0666, -0.5769,  3.9333],
       device='cuda:0')
Original likelihood: -31.884946823120117
Adjusted likelihood: -31.884946823120117
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 3.974277045985218
Current ori: tensor([ 0.0590, -0.0666, -0.5769], device='cuda:0')
Middle force: tensor([0.5601, 0.6846, 0.5286, 0.5697], device='cuda:0')
Thumb force: tensor([0.6994, 0.5507, 0.5673, 0.5753], device='cuda:0')
Index force: tensor([0.5544, 0.5430, 0.6002, 0.5735], device='cuda:0')
Storing NORMAL transition: reward=0.0047 (scaled=0.0047), steps=1
Reward stats updated: mean 0.0023 -> 0.0024, std: 0.0871
Collected 303 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.8326, Q2 Loss=0.8326, Entropy=0.0004, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6633
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.7550, Q2 Loss=0.7550, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2593
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1841, Q2 Loss=1.1841, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8685
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8250, Q2 Loss=0.8250, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4655
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2159, Q2 Loss=1.2159, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9174

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (16.4%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.05s (19.5%)
Actor update: 0.11s (41.6%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 0.962519
Q2 loss: 0.962519
Current threshold: -33.6468
Global Scale Offset: 0.0827
Reward stats: mean=0.0024, std=0.0871, count=303
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 0.9625, Q2 Loss: 0.9625, Entropy: 0.0001, Mean TD Error: 0.6348, Threshold: -33.6468
tensor([ 0.0883,  0.4058,  0.6797,  0.7890,  0.0907,  0.3635,  1.0049,  1.1572,
         1.3881,  0.1439,  0.3379,  0.6452,  0.0504, -0.0681, -0.5704,  4.0902],
       device='cuda:0')
Original likelihood: -30.278030395507812
Adjusted likelihood: -30.278030395507812
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 10 3.7415653049829416
Current ori: tensor([ 0.0504, -0.0681, -0.5704], device='cuda:0')
Middle force: tensor([0.6813, 0.5287, 0.5659], device='cuda:0')
Thumb force: tensor([0.5477, 0.5636, 0.5719], device='cuda:0')
Index force: tensor([0.5379, 0.5924, 0.5687], device='cuda:0')
Storing NORMAL transition: reward=-0.0002 (scaled=-0.0002), steps=1
Reward stats updated: mean 0.0024 -> 0.0023, std: 0.0870
Collected 304 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.4965, Q2 Loss=1.4965, Entropy=0.0003, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4687
SAC Update 2/5: Actor Loss=-0.0019, Q1 Loss=0.6941, Q2 Loss=0.6941, Entropy=0.2591, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1114
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1984, Q2 Loss=1.1984, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3807
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5214, Q2 Loss=1.5214, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4315
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2177, Q2 Loss=1.2177, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9198

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.4%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.7%)
Actor update: 0.09s (39.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000380
Q1 loss: 1.225611
Q2 loss: 1.225611
Current threshold: -33.6500
Global Scale Offset: 0.0827
Reward stats: mean=0.0023, std=0.0870, count=304
----------------------------------------------
SAC Update - Actor Loss: -0.0004, Q1 Loss: 1.2256, Q2 Loss: 1.2256, Entropy: 0.0519, Mean TD Error: 0.8624, Threshold: -33.6500
tensor([ 0.0319,  0.3389,  0.7471,  0.7645,  0.1090,  0.4116,  0.9477,  1.1361,
         1.3737,  0.1706,  0.3066,  0.6647,  0.0357, -0.0769, -0.5634,  3.6921],
       device='cuda:0')
Original likelihood: -35.939476013183594
Adjusted likelihood: -35.939476013183594
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 32.99755096435547
Projection step: 1, Loss: 30.004146575927734
Projection step: 2, Loss: 31.25071144104004
Projection step: 3, Loss: 34.017398834228516
Projection step: 4, Loss: 30.17955207824707
Projection step: 5, Loss: 31.147974014282227
Projection step: 6, Loss: 29.667591094970703
Projection step: 7, Loss: 28.606243133544922
Projection step: 8, Loss: 30.371095657348633
Projection step: 9, Loss: 27.741455078125
Projection step: 10, Loss: 28.073335647583008
Projection step: 11, Loss: 27.125274658203125
Projection step: 12, Loss: 29.393394470214844
Projection step: 13, Loss: 30.322635650634766
Projection step: 14, Loss: 28.72628402709961
Projection step: 15, Loss: 28.18100929260254
Projection step: 16, Loss: 28.84430694580078
Projection step: 17, Loss: 29.235803604125977
Projection step: 18, Loss: 27.20449447631836
Projection step: 19, Loss: 26.456867218017578
Projection step: 20, Loss: 28.162925720214844
Projection step: 21, Loss: 26.03460121154785
Projection step: 22, Loss: 27.191566467285156
Projection step: 23, Loss: 26.406835556030273
Projection step: 24, Loss: 29.474130630493164
Final likelihood: tensor([-29.0998, -25.7193, -32.1086, -34.8958, -24.9910, -31.0187, -28.6870,
        -33.6927, -25.4967, -24.3555, -24.6200, -16.0221, -30.2280, -34.4362,
        -25.5904, -28.9524])
Final projection likelihood: -28.1196
1 mode projection succeeded
New goal: tensor([ 0.0525,  0.3364,  0.7496,  0.7889,  0.0712,  0.4813,  0.8944,  1.0638,
         1.4112,  0.0791,  0.2951,  0.8001,  0.0332, -0.0726, -0.2611],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -27.694204330444336
Adjusted likelihood: -27.694204330444336
Likelihood residual: 0.0
Original likelihood: -37.93159866333008
Adjusted likelihood: -37.93159866333008
Likelihood residual: 0.0
{'index': 37.93159866333008, 'thumb_middle': 27.694204330444336}
Current yaw: tensor([ 0.0357, -0.0769, -0.5634], device='cuda:0')
2 thumb_middle
tensor([ 0.0319,  0.3389,  0.7471,  0.7645,  0.1090,  0.4116,  0.9477,  1.1361,
         1.3737,  0.1706,  0.3066,  0.6647,  0.0357, -0.0769, -0.5634,  3.6921],
       device='cuda:0')
Solve time for step 1 9.081543413980398
Current ori: tensor([ 0.0357, -0.0769, -0.5634], device='cuda:0')
Index force: tensor([0.5931, 0.5814, 0.6100, 0.5951], device='cuda:0')
tensor([ 3.2499e-03,  3.8056e-01,  7.8423e-01,  7.9506e-01, -1.8171e-02,
         4.4310e-01,  8.5905e-01,  1.0533e+00,  1.3390e+00,  6.6221e-02,
         1.9468e-01,  7.3335e-01,  8.8267e-02, -2.1416e-01, -5.6271e-01,
         4.1471e+00], device='cuda:0')
Solve time for step 2 3.6541472049430013
Current ori: tensor([ 0.0883, -0.2142, -0.5627], device='cuda:0')
Index force: tensor([0.5014, 0.5841, 0.5686], device='cuda:0')
tensor([-0.0151,  0.4185,  0.8153,  0.8051,  0.0422,  0.5079,  0.8919,  1.0590,
         1.3241,  0.0575,  0.1347,  0.7348,  0.1552, -0.3244, -0.6293,  4.6850],
       device='cuda:0')
Solve time for step 3 3.7453098770347424
Current ori: tensor([ 0.1552, -0.3244, -0.6293], device='cuda:0')
Index force: tensor([0.5670, 0.5575], device='cuda:0')
tensor([-0.0496,  0.5391,  0.9015,  0.8512,  0.0949,  0.5587,  0.8955,  1.0373,
         1.3182,  0.0584,  0.1216,  0.7273,  0.1685, -0.3312, -0.7187,  4.8273],
       device='cuda:0')
Solve time for step 4 3.720532611012459
Current ori: tensor([ 0.1685, -0.3312, -0.7187], device='cuda:0')
Index force: tensor([0.5546], device='cuda:0')
Storing RECOVERY transition: reward=-0.3409 (scaled=-0.0341), steps=10
Reward stats updated: mean 0.0023 -> 0.0022, std: 0.0869
Collected 305 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8728, Q2 Loss=0.8728, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6326
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.9017, Q2 Loss=1.9017, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2425
SAC Update 3/5: Actor Loss=-0.0765, Q1 Loss=0.9647, Q2 Loss=0.9647, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0294
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1140, Q2 Loss=1.1140, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3466
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.8451, Q2 Loss=1.8451, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5597

------ SAC Update Summary (5 iterations) ------
Total time: 0.31s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (15.0%)
Q1 update: 0.07s (21.3%)
Q2 update: 0.06s (20.2%)
Actor update: 0.13s (40.9%)
Target update: 0.00s (1.1%)
Priority update: 0.00s (0.1%)
Actor loss: -0.015303
Q1 loss: 1.339667
Q2 loss: 1.339667
Current threshold: -33.6522
Global Scale Offset: 0.0827
Reward stats: mean=0.0022, std=0.0869, count=305
----------------------------------------------
SAC Update - Actor Loss: -0.0153, Q1 Loss: 1.3397, Q2 Loss: 1.3397, Entropy: 0.0000, Mean TD Error: 1.1622, Threshold: -33.6522
Original likelihood: -219.51751708984375
Adjusted likelihood: -219.51751708984375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 213.59776306152344
Projection step: 1, Loss: 200.60385131835938
Projection step: 2, Loss: 196.55313110351562
Projection step: 3, Loss: 188.48361206054688
Projection step: 4, Loss: 202.1540069580078
Projection step: 5, Loss: 208.220947265625
Projection step: 6, Loss: 180.0123291015625
Projection step: 7, Loss: 217.8294677734375
Projection step: 8, Loss: 192.0421142578125
Projection step: 9, Loss: 182.7763671875
Projection step: 10, Loss: 201.89637756347656
Projection step: 11, Loss: 198.89019775390625
Projection step: 12, Loss: 218.33193969726562
Projection step: 13, Loss: 217.47378540039062
Projection step: 14, Loss: 178.43942260742188
Projection step: 15, Loss: 209.77447509765625
Projection step: 16, Loss: 199.73876953125
Projection step: 17, Loss: 200.0705108642578
Projection step: 18, Loss: 192.9510498046875
Projection step: 19, Loss: 220.42791748046875
Projection step: 20, Loss: 211.01248168945312
Projection step: 21, Loss: 210.25234985351562
Projection step: 22, Loss: 202.7825927734375
Projection step: 23, Loss: 192.85720825195312
Projection step: 24, Loss: 178.866455078125
Final likelihood: tensor([-191.3447, -225.1539, -272.0341, -123.8896, -136.2022, -234.0368,
        -216.4092, -193.8631, -164.9635, -264.1585, -242.3331, -263.9769,
        -287.2235, -213.2843, -197.5239, -180.5924])
Final projection likelihood: -212.9369
1 mode projection failed, trying anyway
New goal: tensor([ 0.0115,  0.6337,  0.9612,  0.8694,  0.1483,  0.6924,  0.9222,  1.0285,
         1.3209,  0.0690,  0.2210,  0.7902,  0.2062, -0.2888, -0.7455],
       device='cuda:0')
tensor([[0.0037]], device='cuda:0') tensor([[0.0225]], device='cuda:0') tensor([[0.0149]], device='cuda:0')
Original likelihood: -145.16429138183594
Adjusted likelihood: -145.16429138183594
Likelihood residual: 0.0
Original likelihood: -129.2676239013672
Adjusted likelihood: -129.2676239013672
Likelihood residual: 0.0
{'index': 129.2676239013672, 'thumb_middle': 145.16429138183594}
Current yaw: tensor([ 0.2068, -0.2885, -0.8035], device='cuda:0')
3 index
tensor([ 2.9376e-03,  6.4169e-01,  9.6459e-01,  8.6915e-01,  1.4785e-01,
         6.9224e-01,  9.2878e-01,  1.0168e+00,  1.3382e+00,  6.4581e-02,
         2.2714e-01,  7.8171e-01,  2.0685e-01, -2.8847e-01, -8.0346e-01,
         4.7367e+00], device='cuda:0')
Solve time for step 1 10.0841357819736
Current ori: tensor([ 0.2068, -0.2885, -0.8035], device='cuda:0')
Middle force: tensor([0.5214, 0.5377, 0.5708, 0.5727], device='cuda:0')
Thumb force: tensor([0.5687, 0.5239, 0.5155, 0.5242], device='cuda:0')
tensor([ 0.0167,  0.7050,  0.9586,  0.8512,  0.1720,  0.8007,  0.9872,  1.0479,
         1.3471,  0.0848,  0.2709,  0.7604,  0.2531, -0.2707, -0.9259,  5.0096],
       device='cuda:0')
Solve time for step 2 4.171557068009861
Current ori: tensor([ 0.2531, -0.2707, -0.9259], device='cuda:0')
Middle force: tensor([0.5408, 0.5699, 0.5802], device='cuda:0')
Thumb force: tensor([0.5183, 0.5132, 0.5175], device='cuda:0')
tensor([-4.9553e-03,  6.8584e-01,  9.3128e-01,  8.3908e-01,  1.6267e-01,
         8.5670e-01,  9.8501e-01,  1.0143e+00,  1.3582e+00,  1.0207e-01,
         3.3773e-01,  8.4660e-01,  3.3225e-01, -2.5278e-01, -1.1106e+00,
         5.5427e+00], device='cuda:0')
Solve time for step 3 4.054976664017886
Current ori: tensor([ 0.3322, -0.2528, -1.1106], device='cuda:0')
Middle force: tensor([0.5676, 0.5770], device='cuda:0')
Thumb force: tensor([0.5117, 0.5162], device='cuda:0')
tensor([ 0.0078,  0.5592,  1.0254,  0.8970,  0.1829,  0.9252,  0.9674,  0.9385,
         1.3422,  0.0936,  0.2943,  0.8694,  0.3555, -0.2700, -1.0578,  5.6103],
       device='cuda:0')
Solve time for step 4 3.9163631630362943
Current ori: tensor([ 0.3555, -0.2700, -1.0578], device='cuda:0')
Middle force: tensor([0.5784], device='cuda:0')
Thumb force: tensor([0.5131], device='cuda:0')
Storing RECOVERY transition: reward=-0.5681 (scaled=-0.0568), steps=10
Reward stats updated: mean 0.0022 -> 0.0020, std: 0.0868
Collected 306 transitions for RL
SAC Update 1/5: Actor Loss=-0.1840, Q1 Loss=1.5975, Q2 Loss=1.5975, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4330
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.9299, Q2 Loss=1.9299, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9636
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.9258, Q2 Loss=1.9258, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8559
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6138, Q2 Loss=1.6138, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2928
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2678, Q2 Loss=1.2678, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4393

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.9%)
Target Q: 0.05s (20.9%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.04s (16.9%)
Actor update: 0.09s (39.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.036804
Q1 loss: 1.666966
Q2 loss: 1.666966
Current threshold: -33.6534
Global Scale Offset: 0.0827
Reward stats: mean=0.0020, std=0.0868, count=306
----------------------------------------------
SAC Update - Actor Loss: -0.0368, Q1 Loss: 1.6670, Q2 Loss: 1.6670, Entropy: 0.0000, Mean TD Error: 1.5969, Threshold: -33.6534
Original likelihood: -168.62445068359375
Adjusted likelihood: -168.62445068359375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 187.1790313720703
Projection step: 1, Loss: 175.85238647460938
Projection step: 2, Loss: 182.47479248046875
Projection step: 3, Loss: 184.47666931152344
Projection step: 4, Loss: 186.3616943359375
Projection step: 5, Loss: 186.26426696777344
Projection step: 6, Loss: 215.033935546875
Projection step: 7, Loss: 193.93228149414062
Projection step: 8, Loss: 179.26617431640625
Projection step: 9, Loss: 199.10752868652344
Projection step: 10, Loss: 178.9217071533203
Projection step: 11, Loss: 160.37245178222656
Projection step: 12, Loss: 200.19483947753906
Projection step: 13, Loss: 196.14927673339844
Projection step: 14, Loss: 188.57723999023438
Projection step: 15, Loss: 180.68017578125
Projection step: 16, Loss: 175.04510498046875
Projection step: 17, Loss: 179.98513793945312
Projection step: 18, Loss: 183.67543029785156
Projection step: 19, Loss: 196.74391174316406
Projection step: 20, Loss: 189.46075439453125
Projection step: 21, Loss: 176.02880859375
Projection step: 22, Loss: 192.51931762695312
Projection step: 23, Loss: 179.83009338378906
Projection step: 24, Loss: 195.31820678710938
Final likelihood: tensor([-189.3887, -208.3809, -223.6499, -178.1520, -188.0350, -130.7717,
        -142.7349, -162.8972, -195.8615, -249.0577, -230.6045, -182.2719,
        -294.3456, -212.6196, -193.0593, -169.9952])
Final projection likelihood: -196.9891
1 mode projection failed, trying anyway
New goal: tensor([ 0.0954,  0.6976,  1.1378,  0.9801,  0.1477,  0.9192,  0.9794,  0.9921,
         1.3151,  0.1385,  0.2989,  0.9115,  0.3562, -0.2753, -1.0640],
       device='cuda:0')
tensor([[0.0030]], device='cuda:0') tensor([[0.0162]], device='cuda:0') tensor([[0.0053]], device='cuda:0')
Original likelihood: -244.1068572998047
Adjusted likelihood: -244.1068572998047
Likelihood residual: 0.0
Original likelihood: -242.34010314941406
Adjusted likelihood: -242.34010314941406
Likelihood residual: 0.0
{'index': 242.34010314941406, 'thumb_middle': 244.1068572998047}
Current yaw: tensor([ 0.3561, -0.2749, -1.0417], device='cuda:0')
4 index
tensor([ 0.0814,  0.7013,  1.1533,  0.9870,  0.1517,  0.9296,  0.9792,  0.9546,
         1.3298,  0.1321,  0.3042,  0.9005,  0.3561, -0.2749, -1.0417,  5.3998],
       device='cuda:0')
Solve time for step 1 10.273379432968795
Current ori: tensor([ 0.3561, -0.2749, -1.0417], device='cuda:0')
Middle force: tensor([0.5500, 0.5380, 0.5614, 0.5737], device='cuda:0')
Thumb force: tensor([0.5433, 0.6361, 0.5925, 0.5483], device='cuda:0')
tensor([ 0.1902,  0.7006,  1.1088,  0.9491,  0.1803,  1.0107,  0.9269,  0.8951,
         1.2999,  0.1506,  0.2559,  0.9003,  0.3612, -0.2978, -0.9707,  5.2632],
       device='cuda:0')
Solve time for step 2 4.143336729030125
Current ori: tensor([ 0.3612, -0.2978, -0.9707], device='cuda:0')
Middle force: tensor([0.5943, 0.5758, 0.6122], device='cuda:0')
Thumb force: tensor([0.5347, 0.5701, 0.6008], device='cuda:0')
tensor([ 0.1984,  0.7676,  1.1274,  0.9551,  0.1745,  1.0286,  0.9417,  0.8960,
         1.2996,  0.1510,  0.2581,  0.8997,  0.3612, -0.2976, -0.9713,  5.3604],
       device='cuda:0')
Solve time for step 3 4.043443396978546
Current ori: tensor([ 0.3612, -0.2976, -0.9713], device='cuda:0')
Middle force: tensor([0.5179, 0.5860], device='cuda:0')
Thumb force: tensor([0.5905, 0.6653], device='cuda:0')
tensor([ 0.1760,  0.7692,  1.1183,  0.9495,  0.1774,  1.0635,  0.9476,  0.8608,
         1.2877,  0.1651,  0.2480,  0.8936,  0.3635, -0.3047, -0.9397,  5.3411],
       device='cuda:0')
Solve time for step 4 3.8395138240302913
Current ori: tensor([ 0.3635, -0.3047, -0.9397], device='cuda:0')
Middle force: tensor([0.5779], device='cuda:0')
Thumb force: tensor([0.6499], device='cuda:0')
Storing RECOVERY transition: reward=-0.5616 (scaled=-0.0562), steps=10
Reward stats updated: mean 0.0020 -> 0.0018, std: 0.0867
Collected 307 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.6188, Q2 Loss=1.6188, Entropy=0.0010, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2971
SAC Update 2/5: Actor Loss=-0.0189, Q1 Loss=0.7995, Q2 Loss=0.7995, Entropy=0.2120, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3268
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.8971, Q2 Loss=0.8971, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9502
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8427, Q2 Loss=0.8427, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7281
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9039, Q2 Loss=0.9039, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7532

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.6%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (41.2%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.049837
Q1 loss: 1.012407
Q2 loss: 1.012407
Current threshold: -33.6639
Global Scale Offset: 0.0826
Reward stats: mean=0.0018, std=0.0867, count=307
----------------------------------------------
SAC Update - Actor Loss: -0.0498, Q1 Loss: 1.0124, Q2 Loss: 1.0124, Entropy: 0.0426, Mean TD Error: 0.8111, Threshold: -33.6639
Original likelihood: -195.59686279296875
Adjusted likelihood: -195.59686279296875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 197.643310546875
Projection step: 1, Loss: 216.78982543945312
Projection step: 2, Loss: 193.85635375976562
Projection step: 3, Loss: 216.04212951660156
Projection step: 4, Loss: 204.16436767578125
Projection step: 5, Loss: 233.99771118164062
Projection step: 6, Loss: 201.5069580078125
Projection step: 7, Loss: 222.0327606201172
Projection step: 8, Loss: 207.3692169189453
Projection step: 9, Loss: 233.0387725830078
Projection step: 10, Loss: 216.09579467773438
Projection step: 11, Loss: 215.08384704589844
Projection step: 12, Loss: 204.402099609375
Projection step: 13, Loss: 227.587158203125
Projection step: 14, Loss: 238.50650024414062
Projection step: 15, Loss: 209.84695434570312
Projection step: 16, Loss: 211.2249755859375
Projection step: 17, Loss: 216.2360382080078
Projection step: 18, Loss: 208.43771362304688
Projection step: 19, Loss: 224.54083251953125
Projection step: 20, Loss: 238.6016845703125
Projection step: 21, Loss: 211.1395263671875
Projection step: 22, Loss: 214.74082946777344
Projection step: 23, Loss: 218.6446533203125
Projection step: 24, Loss: 216.81512451171875
Final likelihood: tensor([-193.7781, -202.2669, -245.8698, -153.2005, -143.9618, -224.1544,
        -279.6073, -224.4622, -218.5915, -164.2631, -170.0293, -273.1261,
        -140.8081, -277.0740, -255.0440, -202.6002])
Final projection likelihood: -210.5523
1 mode projection failed, trying anyway
New goal: tensor([ 0.1364,  0.7365,  1.1549,  0.9840,  0.1426,  1.0125,  1.0610,  1.0488,
         1.2966,  0.1531,  0.3040,  0.9025,  0.3574, -0.2857, -1.0244],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0021]], device='cuda:0') tensor([[0.0060]], device='cuda:0')
Original likelihood: -258.3028259277344
Adjusted likelihood: -258.3028259277344
Likelihood residual: 0.0
{'index': 258.3028259277344, 'thumb_middle': inf}
Current yaw: tensor([ 0.3573, -0.2852, -1.0255], device='cuda:0')
5 index
tensor([ 0.1256,  0.7379,  1.1567,  0.9830,  0.1451,  1.0183,  1.0666,  1.0286,
         1.3100,  0.1482,  0.3041,  0.8910,  0.3573, -0.2852, -1.0255,  5.3673],
       device='cuda:0')
Solve time for step 1 10.294264831987675
Current ori: tensor([ 0.3573, -0.2852, -1.0255], device='cuda:0')
Middle force: tensor([0.5808, 0.5350, 0.5720, 0.5290], device='cuda:0')
Thumb force: tensor([0.6513, 0.6634, 0.6295, 0.5007], device='cuda:0')
tensor([ 0.2145,  0.8037,  1.1152,  0.9460,  0.1617,  1.0662,  1.0425,  0.9816,
         1.2985,  0.1811,  0.2848,  0.8988,  0.3614, -0.2964, -1.0485,  5.4149],
       device='cuda:0')
Solve time for step 2 4.121706273988821
Current ori: tensor([ 0.3614, -0.2964, -1.0485], device='cuda:0')
Middle force: tensor([0.5420, 0.5733, 0.5254], device='cuda:0')
Thumb force: tensor([0.6557, 0.6328, 0.5010], device='cuda:0')
tensor([ 0.2879,  0.8015,  1.1121,  0.9427,  0.1665,  1.0946,  1.0300,  0.9363,
         1.2961,  0.1834,  0.2615,  0.8991,  0.3597, -0.3028, -1.0640,  5.4410],
       device='cuda:0')
Solve time for step 3 3.9416269390494563
Current ori: tensor([ 0.3597, -0.3028, -1.0640], device='cuda:0')
Middle force: tensor([0.5755, 0.5249], device='cuda:0')
Thumb force: tensor([0.6365, 0.5014], device='cuda:0')
tensor([ 0.2233,  0.8032,  1.1281,  0.9491,  0.1721,  1.1156,  1.0485,  0.8707,
         1.2800,  0.2438,  0.2740,  0.8990,  0.3667, -0.3108, -1.0697,  5.4165],
       device='cuda:0')
Solve time for step 4 3.8144402820034884
Current ori: tensor([ 0.3667, -0.3108, -1.0697], device='cuda:0')
Middle force: tensor([0.5228], device='cuda:0')
Thumb force: tensor([0.5010], device='cuda:0')
Storing RECOVERY transition: reward=-0.6510 (scaled=-0.0651), steps=10
Reward stats updated: mean 0.0018 -> 0.0016, std: 0.0866
Collected 308 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2207, Q2 Loss=1.2207, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3853
SAC Update 2/5: Actor Loss=-0.0196, Q1 Loss=0.8116, Q2 Loss=0.8116, Entropy=0.2038, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6618
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7303, Q2 Loss=0.7303, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2068
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7560, Q2 Loss=0.7560, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2554
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7467, Q2 Loss=0.7467, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7624

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.0%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.04s (18.0%)
Actor update: 0.09s (38.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.049964
Q1 loss: 0.853066
Q2 loss: 0.853066
Current threshold: -33.6818
Global Scale Offset: 0.0824
Reward stats: mean=0.0016, std=0.0866, count=308
----------------------------------------------
SAC Update - Actor Loss: -0.0500, Q1 Loss: 0.8531, Q2 Loss: 0.8531, Entropy: 0.0408, Mean TD Error: 0.8543, Threshold: -33.6818
Original likelihood: -229.18408203125
Adjusted likelihood: -229.18408203125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 263.94512939453125
Projection step: 1, Loss: 241.86328125
Projection step: 2, Loss: 240.92153930664062
Projection step: 3, Loss: 256.1822509765625
Projection step: 4, Loss: 255.99563598632812
Projection step: 5, Loss: 254.5524444580078
Projection step: 6, Loss: 243.03857421875
Projection step: 7, Loss: 233.91348266601562
Projection step: 8, Loss: 237.81930541992188
Projection step: 9, Loss: 252.38787841796875
Projection step: 10, Loss: 271.8634033203125
Projection step: 11, Loss: 251.38031005859375
Projection step: 12, Loss: 249.09825134277344
Projection step: 13, Loss: 242.57717895507812
Projection step: 14, Loss: 253.307861328125
Projection step: 15, Loss: 232.0343475341797
Projection step: 16, Loss: 234.14004516601562
Projection step: 17, Loss: 244.2831573486328
Projection step: 18, Loss: 248.48838806152344
Projection step: 19, Loss: 259.43756103515625
Projection step: 20, Loss: 249.1399688720703
Projection step: 21, Loss: 245.77694702148438
Projection step: 22, Loss: 237.25564575195312
Projection step: 23, Loss: 248.854736328125
Projection step: 24, Loss: 242.30874633789062
Final likelihood: tensor([-251.4017, -258.2478, -289.9170, -288.5158, -238.7871, -216.6785,
        -174.7458, -281.5609, -291.4874, -259.1530, -264.7348, -192.0703,
        -251.9607, -228.8230, -245.5786, -221.4915])
Final projection likelihood: -247.1971
1 mode projection failed, trying anyway
New goal: tensor([ 0.1440,  0.7912,  1.1789,  0.9886,  0.1274,  1.0731,  1.0207,  1.0180,
         1.2861,  0.2191,  0.3189,  0.9088,  0.3611, -0.2935, -1.1346],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0058]], device='cuda:0')
Original likelihood: -271.90435791015625
Adjusted likelihood: -271.90435791015625
Likelihood residual: 0.0
{'index': 271.90435791015625, 'thumb_middle': inf}
Current yaw: tensor([ 0.3610, -0.2930, -1.1388], device='cuda:0')
6 index
tensor([ 0.1357,  0.7923,  1.1746,  0.9852,  0.1283,  1.0745,  1.0267,  1.0074,
         1.2973,  0.2175,  0.3181,  0.9001,  0.3610, -0.2930, -1.1388,  5.4932],
       device='cuda:0')
Solve time for step 1 10.329713885963429
Current ori: tensor([ 0.3610, -0.2930, -1.1388], device='cuda:0')
Middle force: tensor([0.5282, 0.5285, 0.6335, 0.6174], device='cuda:0')
Thumb force: tensor([0.6053, 0.6335, 0.5844, 0.5087], device='cuda:0')
tensor([ 0.2550,  0.8385,  1.1205,  0.9403,  0.1517,  1.1070,  1.0040,  0.9713,
         1.2932,  0.2271,  0.2948,  0.9030,  0.3632, -0.2998, -1.1077,  5.4976],
       device='cuda:0')
Solve time for step 2 4.033689798961859
Current ori: tensor([ 0.3632, -0.2998, -1.1077], device='cuda:0')
Middle force: tensor([0.5257, 0.6287, 0.6177], device='cuda:0')
Thumb force: tensor([0.6284, 0.5811, 0.5094], device='cuda:0')
tensor([ 0.2447,  0.8590,  1.1173,  0.9361,  0.1581,  1.1373,  0.9866,  0.9387,
         1.2899,  0.2310,  0.2726,  0.9028,  0.3652, -0.3053, -1.0919,  5.4567],
       device='cuda:0')
Solve time for step 3 4.076683218998369
Current ori: tensor([ 0.3652, -0.3053, -1.0919], device='cuda:0')
Middle force: tensor([0.6247, 0.6193], device='cuda:0')
Thumb force: tensor([0.5754, 0.5114], device='cuda:0')
tensor([ 0.2157,  0.8339,  1.1157,  0.9334,  0.1475,  1.1598,  0.9966,  0.8897,
         1.2786,  0.2725,  0.2684,  0.8967,  0.3681, -0.3137, -1.0892,  5.4259],
       device='cuda:0')
Solve time for step 4 4.004497568006627
Current ori: tensor([ 0.3681, -0.3137, -1.0892], device='cuda:0')
Middle force: tensor([0.5753], device='cuda:0')
Thumb force: tensor([0.5439], device='cuda:0')
Storing RECOVERY transition: reward=-0.6564 (scaled=-0.0656), steps=10
Reward stats updated: mean 0.0016 -> 0.0014, std: 0.0866
Collected 309 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0917, Q2 Loss=1.0917, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7009
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.4294, Q2 Loss=1.4294, Entropy=0.0002, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0025
SAC Update 3/5: Actor Loss=-0.1760, Q1 Loss=1.5584, Q2 Loss=1.5584, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4250
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6460, Q2 Loss=0.6460, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9390
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.8925, Q2 Loss=1.8925, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6589

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (16.6%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (18.8%)
Actor update: 0.10s (41.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.035204
Q1 loss: 1.323592
Q2 loss: 1.323592
Current threshold: -33.6942
Global Scale Offset: 0.0823
Reward stats: mean=0.0014, std=0.0866, count=309
----------------------------------------------
SAC Update - Actor Loss: -0.0352, Q1 Loss: 1.3236, Q2 Loss: 1.3236, Entropy: 0.0000, Mean TD Error: 1.3453, Threshold: -33.6942
Original likelihood: -276.30535888671875
Adjusted likelihood: -276.30535888671875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 277.03253173828125
Projection step: 1, Loss: 267.56182861328125
Projection step: 2, Loss: 273.97174072265625
Projection step: 3, Loss: 259.52392578125
Projection step: 4, Loss: 263.6864013671875
Projection step: 5, Loss: 274.66851806640625
Projection step: 6, Loss: 268.3274841308594
Projection step: 7, Loss: 252.34268188476562
Projection step: 8, Loss: 268.11834716796875
Projection step: 9, Loss: 260.4006042480469
Projection step: 10, Loss: 274.52069091796875
Projection step: 11, Loss: 263.22491455078125
Projection step: 12, Loss: 254.88323974609375
Projection step: 13, Loss: 268.06878662109375
Projection step: 14, Loss: 254.06484985351562
Projection step: 15, Loss: 270.69085693359375
Projection step: 16, Loss: 248.6234893798828
Projection step: 17, Loss: 256.44464111328125
Projection step: 18, Loss: 256.93963623046875
Projection step: 19, Loss: 259.1516418457031
Projection step: 20, Loss: 253.48582458496094
Projection step: 21, Loss: 278.2529296875
Projection step: 22, Loss: 269.75018310546875
Projection step: 23, Loss: 273.461669921875
Projection step: 24, Loss: 265.91766357421875
Final likelihood: tensor([-313.5772, -330.7935, -318.3206, -252.6625, -183.8815, -308.6295,
        -258.5246, -244.2613, -299.8470, -262.0519, -248.7700, -278.7570,
        -280.0760, -159.6283, -232.7361, -257.4405])
Final projection likelihood: -264.3723
1 mode projection failed, trying anyway
New goal: tensor([ 0.1506,  0.8448,  1.2032,  0.9895,  0.1138,  1.1091,  0.9941,  1.0104,
         1.2745,  0.2471,  0.3165,  0.9072,  0.3633, -0.3012, -1.1364],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0061]], device='cuda:0')
Original likelihood: -293.11077880859375
Adjusted likelihood: -293.11077880859375
Likelihood residual: 0.0
{'index': 293.11077880859375, 'thumb_middle': inf}
Current yaw: tensor([ 0.3632, -0.3007, -1.1404], device='cuda:0')
7 index
tensor([ 0.1438,  0.8458,  1.1980,  0.9864,  0.1141,  1.1092,  0.9996,  1.0026,
         1.2846,  0.2458,  0.3160,  0.9003,  0.3632, -0.3007, -1.1404,  5.4979],
       device='cuda:0')
Solve time for step 1 10.683258928009309
Current ori: tensor([ 0.3632, -0.3007, -1.1404], device='cuda:0')
Middle force: tensor([0.5804, 0.5092, 0.5640, 0.5066], device='cuda:0')
Thumb force: tensor([0.5335, 0.5468, 0.5810, 0.5106], device='cuda:0')
tensor([ 0.2988,  0.8690,  1.1443,  0.9405,  0.1263,  1.1320,  0.9863,  0.9751,
         1.2787,  0.2673,  0.3058,  0.9036,  0.3652, -0.3065, -1.1242,  5.5043],
       device='cuda:0')
Solve time for step 2 4.125533988990355
Current ori: tensor([ 0.3652, -0.3065, -1.1242], device='cuda:0')
Middle force: tensor([0.5075, 0.5607, 0.5055], device='cuda:0')
Thumb force: tensor([0.5473, 0.5765, 0.5089], device='cuda:0')
tensor([ 0.2949,  0.8735,  1.0967,  0.9117,  0.1430,  1.1678,  0.9620,  0.9424,
         1.2745,  0.2701,  0.2776,  0.9037,  0.3682, -0.3140, -1.0842,  5.4342],
       device='cuda:0')
Solve time for step 3 4.010870594996959
Current ori: tensor([ 0.3682, -0.3140, -1.0842], device='cuda:0')
Middle force: tensor([0.5422, 0.5097], device='cuda:0')
Thumb force: tensor([0.5289, 0.5218], device='cuda:0')
tensor([ 0.2483,  0.9029,  1.1241,  0.9192,  0.1398,  1.1821,  0.9523,  0.9333,
         1.2702,  0.2853,  0.2779,  0.8951,  0.3692, -0.3168, -1.0832,  5.4166],
       device='cuda:0')
Solve time for step 4 3.889728144975379
Current ori: tensor([ 0.3692, -0.3168, -1.0832], device='cuda:0')
Middle force: tensor([0.5076], device='cuda:0')
Thumb force: tensor([0.5196], device='cuda:0')
Storing RECOVERY transition: reward=-0.6218 (scaled=-0.0622), steps=10
Reward stats updated: mean 0.0014 -> 0.0012, std: 0.0865
Collected 310 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9635, Q2 Loss=0.9635, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3157
SAC Update 2/5: Actor Loss=-0.0140, Q1 Loss=1.0347, Q2 Loss=1.0347, Entropy=0.1657, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4775
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1651, Q2 Loss=1.1651, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0827
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.3078, Q2 Loss=1.3078, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0199
SAC Update 5/5: Actor Loss=-0.0013, Q1 Loss=0.7672, Q2 Loss=0.7672, Entropy=0.1888, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4522

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.4%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (19.7%)
Actor update: 0.09s (40.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.049116
Q1 loss: 1.047652
Q2 loss: 1.047652
Current threshold: -33.7088
Global Scale Offset: 0.0821
Reward stats: mean=0.0012, std=0.0865, count=310
----------------------------------------------
SAC Update - Actor Loss: -0.0491, Q1 Loss: 1.0477, Q2 Loss: 1.0477, Entropy: 0.0709, Mean TD Error: 0.8696, Threshold: -33.7088
Original likelihood: -299.21942138671875
Adjusted likelihood: -299.21942138671875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 302.3847351074219
Projection step: 1, Loss: 309.0985107421875
Projection step: 2, Loss: 299.84185791015625
Projection step: 3, Loss: 287.10772705078125
Projection step: 4, Loss: 314.09112548828125
Projection step: 5, Loss: 300.78912353515625
Projection step: 6, Loss: 304.27020263671875
Projection step: 7, Loss: 302.844482421875
Projection step: 8, Loss: 307.8648681640625
Projection step: 9, Loss: 300.9015197753906
Projection step: 10, Loss: 307.32080078125
Projection step: 11, Loss: 297.62432861328125
Projection step: 12, Loss: 303.8006591796875
Projection step: 13, Loss: 308.6598815917969
Projection step: 14, Loss: 318.1263427734375
Projection step: 15, Loss: 309.4927978515625
Projection step: 16, Loss: 311.0543212890625
Projection step: 17, Loss: 303.099853515625
Projection step: 18, Loss: 302.0556640625
Projection step: 19, Loss: 316.937255859375
Projection step: 20, Loss: 304.83380126953125
Projection step: 21, Loss: 309.48980712890625
Projection step: 22, Loss: 309.15399169921875
Projection step: 23, Loss: 315.8638916015625
Projection step: 24, Loss: 308.25604248046875
Final likelihood: tensor([-315.0653, -304.7037, -323.9430, -321.6872, -325.9285, -205.5158,
        -266.9376, -321.0191, -354.4504, -251.0696, -306.5899, -304.5138,
        -336.8707, -307.2260, -294.3899, -316.4420])
Final projection likelihood: -303.5220
1 mode projection failed, trying anyway
New goal: tensor([ 0.1927,  0.9679,  1.2146,  0.9617,  0.1194,  1.1616,  0.9743,  0.9599,
         1.2491,  0.3329,  0.3162,  0.8921,  0.3697, -0.3189, -1.0762],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0022]], device='cuda:0')
Original likelihood: -330.24114990234375
Adjusted likelihood: -330.24114990234375
Likelihood residual: 0.0
Original likelihood: -279.654052734375
Adjusted likelihood: -279.654052734375
Likelihood residual: 0.0
{'index': 279.654052734375, 'thumb_middle': 330.24114990234375}
Current yaw: tensor([ 0.3696, -0.3186, -1.0796], device='cuda:0')
8 index
tensor([ 0.1888,  0.9684,  1.2093,  0.9606,  0.1190,  1.1610,  0.9780,  0.9557,
         1.2565,  0.3313,  0.3172,  0.8883,  0.3696, -0.3186, -1.0796,  5.4303],
       device='cuda:0')
Solve time for step 1 10.43226502899779
Current ori: tensor([ 0.3696, -0.3186, -1.0796], device='cuda:0')
Middle force: tensor([0.6304, 0.5548, 0.5416, 0.5971], device='cuda:0')
Thumb force: tensor([0.5783, 0.5672, 0.5928, 0.6055], device='cuda:0')
tensor([ 0.3022,  0.9063,  1.1159,  0.8923,  0.1210,  1.1927,  0.9633,  0.9199,
         1.2498,  0.3546,  0.2988,  0.8867,  0.3727, -0.3275, -1.0786,  5.4274],
       device='cuda:0')
Solve time for step 2 4.146671310008969
Current ori: tensor([ 0.3727, -0.3275, -1.0786], device='cuda:0')
Middle force: tensor([0.5272, 0.5929, 0.5812], device='cuda:0')
Thumb force: tensor([0.5621, 0.5333, 0.5736], device='cuda:0')
tensor([ 0.2698,  0.9658,  1.0978,  0.8778,  0.1247,  1.2210,  0.9407,  0.8977,
         1.2493,  0.3572,  0.2800,  0.8864,  0.3739, -0.3316, -1.0636,  5.3889],
       device='cuda:0')
Solve time for step 3 4.084019440051634
Current ori: tensor([ 0.3739, -0.3316, -1.0636], device='cuda:0')
Middle force: tensor([0.5474, 0.5039], device='cuda:0')
Thumb force: tensor([0.5393, 0.5143], device='cuda:0')
tensor([ 0.2499,  0.9910,  1.1231,  0.8909,  0.1239,  1.2222,  0.9362,  0.9031,
         1.2490,  0.3576,  0.2825,  0.8836,  0.3738, -0.3312, -1.0651,  5.3934],
       device='cuda:0')
Solve time for step 4 3.826491045008879
Current ori: tensor([ 0.3738, -0.3312, -1.0651], device='cuda:0')
Middle force: tensor([0.5578], device='cuda:0')
Thumb force: tensor([0.5071], device='cuda:0')
Storing RECOVERY transition: reward=-0.6343 (scaled=-0.0634), steps=10
Reward stats updated: mean 0.0012 -> 0.0010, std: 0.0865
Collected 311 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7582, Q2 Loss=0.7582, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5862
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4288, Q2 Loss=1.4288, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2913
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9934, Q2 Loss=0.9934, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3231
SAC Update 4/5: Actor Loss=-0.0028, Q1 Loss=0.7082, Q2 Loss=0.7082, Entropy=0.2781, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7280
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6471, Q2 Loss=1.6471, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4064

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.3%)
Q1 update: 0.05s (20.4%)
Q2 update: 0.05s (18.0%)
Actor update: 0.10s (40.2%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000561
Q1 loss: 1.107131
Q2 loss: 1.107131
Current threshold: -33.7214
Global Scale Offset: 0.0820
Reward stats: mean=0.0010, std=0.0865, count=311
----------------------------------------------
SAC Update - Actor Loss: -0.0006, Q1 Loss: 1.1071, Q2 Loss: 1.1071, Entropy: 0.0556, Mean TD Error: 0.8670, Threshold: -33.7214
Original likelihood: -317.2801208496094
Adjusted likelihood: -317.2801208496094
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 20
Loaded trajectory sampler
Current yaw: tensor([-0.0021,  0.0148, -0.0223], device='cuda:0')
Current yaw: tensor([-0.0021,  0.0148, -0.0223], device='cuda:0')
1 turn
Sampling time 3.656402954016812
tensor([ 0.1429,  0.6157,  0.5646,  0.5841, -0.1240,  0.5689,  0.8646,  0.9164,
         1.2042,  0.3409,  0.2836,  1.1260, -0.0021,  0.0148, -0.0223,  0.1918],
       device='cuda:0')
Original likelihood: -18.627269744873047
Adjusted likelihood: -18.627269744873047
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.531304080039263
Current ori: tensor([-0.0021,  0.0148, -0.0223], device='cuda:0')
Middle force: tensor([0.7139, 0.7955, 0.5295, 0.5266, 0.6377, 0.9731, 1.0162, 0.5862, 0.4996,
        0.5166, 0.5592, 0.8384], device='cuda:0')
Thumb force: tensor([0.5903, 2.3808, 0.6383, 1.4962, 1.0406, 0.8826, 1.9929, 0.6102, 0.7377,
        0.6710, 0.6593, 0.5542], device='cuda:0')
Index force: tensor([0.5893, 0.5037, 0.5949, 0.5615, 0.5756, 0.5168, 0.5770, 0.6114, 0.7197,
        0.6841, 0.6190, 0.5284], device='cuda:0')
Storing NORMAL transition: reward=0.0113 (scaled=0.0113), steps=1
Reward stats updated: mean 0.0010 -> 0.0010, std: 0.0863
Collected 312 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7419, Q2 Loss=0.7419, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3028
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9025, Q2 Loss=0.9025, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2164
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3717, Q2 Loss=1.3717, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1497
SAC Update 4/5: Actor Loss=-0.0785, Q1 Loss=0.8925, Q2 Loss=0.8925, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7208
SAC Update 5/5: Actor Loss=-0.1098, Q1 Loss=0.9002, Q2 Loss=0.9002, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1695

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.9%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (19.0%)
Actor update: 0.09s (39.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.037668
Q1 loss: 0.961764
Q2 loss: 0.961764
Current threshold: -33.7306
Global Scale Offset: 0.0820
Reward stats: mean=0.0010, std=0.0863, count=312
----------------------------------------------
SAC Update - Actor Loss: -0.0377, Q1 Loss: 0.9618, Q2 Loss: 0.9618, Entropy: 0.0000, Mean TD Error: 0.9119, Threshold: -33.7306
tensor([ 0.1202,  0.6723,  0.5399,  0.4025, -0.0273,  0.3583,  0.9675,  1.1422,
         1.2594,  0.1495,  0.2895,  1.1104,  0.0147, -0.0081, -0.0337, -0.6920],
       device='cuda:0')
Original likelihood: -26.422447204589844
Adjusted likelihood: -26.422447204589844
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.509078163013328
Current ori: tensor([ 0.0147, -0.0081, -0.0337], device='cuda:0')
Middle force: tensor([0.7863, 0.5376, 0.5262, 0.6350, 0.9493, 1.0251, 0.6110, 0.5014, 0.5229,
        0.5769, 0.8430], device='cuda:0')
Thumb force: tensor([2.3245, 0.5976, 1.4546, 1.0207, 0.8734, 1.9442, 0.5692, 0.6597, 0.6111,
        0.6144, 0.5451], device='cuda:0')
Index force: tensor([0.5032, 0.5981, 0.5610, 0.5726, 0.5176, 0.5772, 0.6202, 0.6997, 0.6917,
        0.6200, 0.5285], device='cuda:0')
Storing NORMAL transition: reward=0.1006 (scaled=0.1006), steps=1
Reward stats updated: mean 0.0010 -> 0.0013, std: 0.0864
Collected 313 transitions for RL
SAC Update 1/5: Actor Loss=-0.0030, Q1 Loss=0.6867, Q2 Loss=0.6867, Entropy=0.2862, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6225
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0058, Q2 Loss=1.0058, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4032
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=2.1067, Q2 Loss=2.1067, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8570
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6275, Q2 Loss=0.6275, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6082
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1864, Q2 Loss=1.1864, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8724

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.3%)
Q1 update: 0.04s (15.5%)
Q2 update: 0.09s (32.8%)
Actor update: 0.10s (33.3%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046652
Q1 loss: 1.122642
Q2 loss: 1.122642
Current threshold: -33.7394
Global Scale Offset: 0.0820
Reward stats: mean=0.0013, std=0.0864, count=313
----------------------------------------------
SAC Update - Actor Loss: -0.0467, Q1 Loss: 1.1226, Q2 Loss: 1.1226, Entropy: 0.0572, Mean TD Error: 0.8726, Threshold: -33.7394
tensor([ 0.0808,  0.6877,  0.4777,  0.3920, -0.1770,  0.3662,  0.9025,  1.1270,
         1.4264, -0.0372,  0.3325,  0.9706,  0.0080,  0.0225, -0.1347, -0.7799],
       device='cuda:0')
Original likelihood: -24.221323013305664
Adjusted likelihood: -24.221323013305664
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.231019125960302
Current ori: tensor([ 0.0080,  0.0225, -0.1347], device='cuda:0')
Middle force: tensor([0.5307, 0.5251, 0.6345, 0.9438, 1.0124, 0.6004, 0.5008, 0.5174, 0.5696,
        0.8292], device='cuda:0')
Thumb force: tensor([0.5944, 1.4130, 0.9907, 0.8560, 1.9063, 0.5696, 0.6615, 0.6117, 0.6154,
        0.5444], device='cuda:0')
Index force: tensor([0.5952, 0.5611, 0.5728, 0.5169, 0.5744, 0.6161, 0.7085, 0.6981, 0.6155,
        0.5266], device='cuda:0')
Storing NORMAL transition: reward=-0.0026 (scaled=-0.0026), steps=1
Reward stats updated: mean 0.0013 -> 0.0013, std: 0.0862
Collected 314 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0135, Q2 Loss=1.0135, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8520
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6248, Q2 Loss=1.6248, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2808
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6244, Q2 Loss=1.6244, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4901
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1442, Q2 Loss=1.1442, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0921
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2972, Q2 Loss=1.2972, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6513

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.1%)
Q1 update: 0.05s (18.8%)
Q2 update: 0.05s (19.5%)
Actor update: 0.10s (40.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.340804
Q2 loss: 1.340804
Current threshold: -33.7445
Global Scale Offset: 0.0820
Reward stats: mean=0.0013, std=0.0862, count=314
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.3408, Q2 Loss: 1.3408, Entropy: 0.0000, Mean TD Error: 1.0732, Threshold: -33.7445
tensor([ 0.2073,  0.7312,  0.5318,  0.3785, -0.2220,  0.4860,  0.8931,  1.0096,
         1.4161,  0.1033,  0.3450,  0.7927,  0.0124,  0.0642, -0.1366,  4.5826],
       device='cuda:0')
Original likelihood: -45.121337890625
Adjusted likelihood: -45.121337890625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 45.69907760620117
Projection step: 1, Loss: 45.69376754760742
Projection step: 2, Loss: 44.75421142578125
Projection step: 3, Loss: 42.75736999511719
Projection step: 4, Loss: 42.6135139465332
Projection step: 5, Loss: 42.699554443359375
Projection step: 6, Loss: 41.2594108581543
Projection step: 7, Loss: 42.16029357910156
Projection step: 8, Loss: 40.81128692626953
Projection step: 9, Loss: 40.22749710083008
Projection step: 10, Loss: 39.98161315917969
Projection step: 11, Loss: 39.75182342529297
Projection step: 12, Loss: 43.02125930786133
Projection step: 13, Loss: 39.12370681762695
Projection step: 14, Loss: 38.75679397583008
Projection step: 15, Loss: 37.80975341796875
Projection step: 16, Loss: 41.45384979248047
Projection step: 17, Loss: 36.82795333862305
Projection step: 18, Loss: 36.59899139404297
Projection step: 19, Loss: 36.77015686035156
Projection step: 20, Loss: 37.348567962646484
Projection step: 21, Loss: 36.76295471191406
Projection step: 22, Loss: 36.29352569580078
Projection step: 23, Loss: 36.1389045715332
Projection step: 24, Loss: 36.11711883544922
Final likelihood: tensor([-38.7046, -36.2761, -37.2601, -34.8052, -33.9812, -39.4772, -32.4219,
        -34.0072, -33.5873, -36.1212, -36.6912, -33.0850, -37.0167, -35.4502,
        -37.6244, -35.2186])
Final projection likelihood: -35.7330
1 mode projection failed, trying anyway
New goal: tensor([ 0.1966,  0.7254,  0.5102,  0.4301, -0.1925,  0.4484,  0.7973,  0.9729,
         1.3788,  0.0647,  0.3957,  0.9854,  0.0109,  0.0550,  2.2964],
       device='cuda:0')
tensor([[0.0069]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0030]], device='cuda:0')
Original likelihood: -34.47802734375
Adjusted likelihood: -34.47802734375
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 34.47802734375}
Current yaw: tensor([ 0.0124,  0.0642, -0.1366], device='cuda:0')
2 thumb_middle
tensor([ 0.2073,  0.7312,  0.5318,  0.3785, -0.2220,  0.4860,  0.8931,  1.0096,
         1.4161,  0.1033,  0.3450,  0.7927,  0.0124,  0.0642, -0.1366,  4.5826],
       device='cuda:0')
Solve time for step 1 9.09430102800252
Current ori: tensor([ 0.0124,  0.0642, -0.1366], device='cuda:0')
Index force: tensor([0.5223, 0.4993, 0.5659, 0.5566], device='cuda:0')
tensor([ 0.2015,  0.7527,  0.4933,  0.5130, -0.2596,  0.4792,  0.8163,  0.9849,
         1.3553,  0.0432,  0.3170,  0.9200,  0.0300,  0.0962, -0.1365,  3.9190],
       device='cuda:0')
Solve time for step 2 3.567448698973749
Current ori: tensor([ 0.0300,  0.0962, -0.1365], device='cuda:0')
Index force: tensor([0.5010, 0.4999, 0.5584], device='cuda:0')
tensor([ 0.1933,  0.7669,  0.5580,  0.4820, -0.2764,  0.4793,  0.8102,  0.9666,
         1.3571,  0.0352,  0.3471,  0.9663,  0.0323,  0.1151, -0.1227,  4.1005],
       device='cuda:0')
Solve time for step 3 3.346745954011567
Current ori: tensor([ 0.0323,  0.1151, -0.1227], device='cuda:0')
Index force: tensor([0.5008, 0.5709], device='cuda:0')
tensor([ 0.1636,  0.8151,  0.5596,  0.4182, -0.2858,  0.4782,  0.7992,  0.9619,
         1.3721,  0.0427,  0.3570,  0.9761,  0.0332,  0.1232, -0.1094,  4.6791],
       device='cuda:0')
Solve time for step 4 3.1971395179862157
Current ori: tensor([ 0.0332,  0.1232, -0.1094], device='cuda:0')
Index force: tensor([0.5552], device='cuda:0')
Storing RECOVERY transition: reward=-0.0225 (scaled=-0.0075), steps=3
Reward stats updated: mean 0.0013 -> 0.0013, std: 0.0861
Collected 315 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1025, Q2 Loss=1.1025, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6314
SAC Update 2/5: Actor Loss=-0.0896, Q1 Loss=1.1263, Q2 Loss=1.1263, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3433
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2763, Q2 Loss=1.2763, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0546
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0463, Q2 Loss=1.0463, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7502
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7182, Q2 Loss=0.7182, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2762

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.0%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (40.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.063963
Q1 loss: 1.053927
Q2 loss: 1.053927
Current threshold: -33.7476
Global Scale Offset: 0.0820
Reward stats: mean=0.0013, std=0.0861, count=315
----------------------------------------------
SAC Update - Actor Loss: -0.0640, Q1 Loss: 1.0539, Q2 Loss: 1.0539, Entropy: 0.0000, Mean TD Error: 0.8111, Threshold: -33.7476
Original likelihood: -41.38156509399414
Adjusted likelihood: -41.38156509399414
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 41.72188186645508
Projection step: 1, Loss: 40.997100830078125
Projection step: 2, Loss: 39.653717041015625
Projection step: 3, Loss: 41.51988983154297
Projection step: 4, Loss: 42.445228576660156
Projection step: 5, Loss: 38.66111373901367
Projection step: 6, Loss: 37.73857498168945
Projection step: 7, Loss: 37.62440490722656
Projection step: 8, Loss: 37.67345428466797
Projection step: 9, Loss: 39.49760437011719
Projection step: 10, Loss: 36.38850402832031
Projection step: 11, Loss: 37.030364990234375
Projection step: 12, Loss: 34.98830032348633
Projection step: 13, Loss: 37.31237030029297
Projection step: 14, Loss: 37.69408416748047
Projection step: 15, Loss: 37.57676696777344
Projection step: 16, Loss: 37.53620147705078
Projection step: 17, Loss: 38.83741760253906
Projection step: 18, Loss: 37.75969314575195
Projection step: 19, Loss: 37.486087799072266
Projection step: 20, Loss: 38.90311050415039
Projection step: 21, Loss: 36.51123046875
Projection step: 22, Loss: 36.74542999267578
Projection step: 23, Loss: 38.48345947265625
Projection step: 24, Loss: 33.770179748535156
Final likelihood: tensor([-40.5709, -39.9592, -36.9519, -31.3884, -32.0110, -21.9707, -31.7761,
        -39.9665, -29.4148, -39.9459, -41.3288, -30.5800, -21.5676, -41.0618,
        -32.9502, -37.4432])
Final projection likelihood: -34.3054
1 mode projection failed, trying anyway
New goal: tensor([ 0.1495,  0.7904,  0.4835,  0.3292, -0.2433,  0.4719,  0.7386,  0.9002,
         1.4142,  0.0250,  0.3982,  0.9933,  0.0360,  0.1007,  0.7176],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0017]], device='cuda:0')
Original likelihood: -28.37181854248047
Adjusted likelihood: -28.37181854248047
Likelihood residual: 0.0
Original likelihood: -31.92147445678711
Adjusted likelihood: -31.92147445678711
Likelihood residual: 0.0
{'index': 31.92147445678711, 'thumb_middle': 28.37181854248047}
Current yaw: tensor([ 0.0320,  0.1086, -0.1238], device='cuda:0')
3 thumb_middle
tensor([ 0.1648,  0.8399,  0.5530,  0.4242, -0.2632,  0.4801,  0.8079,  0.9720,
         1.4397,  0.0881,  0.3990,  0.9923,  0.0320,  0.1086, -0.1238,  4.7517],
       device='cuda:0')
Solve time for step 1 8.81794760300545
Current ori: tensor([ 0.0320,  0.1086, -0.1238], device='cuda:0')
Index force: tensor([0.5255, 0.5347, 0.5428, 0.5946], device='cuda:0')
tensor([ 0.1004,  0.8516,  0.5170,  0.3832, -0.3142,  0.4942,  0.7646,  0.9022,
         1.4234,  0.0192,  0.3840,  0.9860,  0.0250,  0.1491, -0.1057,  5.1551],
       device='cuda:0')
Solve time for step 2 3.529926047020126
Current ori: tensor([ 0.0250,  0.1491, -0.1057], device='cuda:0')
Index force: tensor([0.5328, 0.5372, 0.6291], device='cuda:0')
tensor([ 0.0721,  0.8728,  0.5225,  0.3402, -0.3243,  0.5056,  0.7460,  0.8880,
         1.4336,  0.0352,  0.3729,  0.9955,  0.0203,  0.1577, -0.0963,  5.4421],
       device='cuda:0')
Solve time for step 3 3.3774165459908545
Current ori: tensor([ 0.0203,  0.1577, -0.0963], device='cuda:0')
Index force: tensor([0.5302, 0.6144], device='cuda:0')
tensor([ 0.0509,  0.8935,  0.5470,  0.3646, -0.3357,  0.5069,  0.7390,  0.8818,
         1.4545,  0.0274,  0.3737,  0.9814,  0.0202,  0.1654, -0.0918,  5.3257],
       device='cuda:0')
Solve time for step 4 3.2740647500031628
Current ori: tensor([ 0.0202,  0.1654, -0.0918], device='cuda:0')
Index force: tensor([0.5932], device='cuda:0')
Storing RECOVERY transition: reward=-0.0454 (scaled=-0.0151), steps=3
Reward stats updated: mean 0.0013 -> 0.0013, std: 0.0860
Collected 316 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2871, Q2 Loss=1.2871, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4142
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7768, Q2 Loss=0.7768, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3591
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7656, Q2 Loss=0.7656, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9124
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1138, Q2 Loss=1.1138, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3504
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.3613, Q2 Loss=2.3613, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9532

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.4%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (18.8%)
Actor update: 0.09s (40.9%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.260927
Q2 loss: 1.260927
Current threshold: -33.7494
Global Scale Offset: 0.0820
Reward stats: mean=0.0013, std=0.0860, count=316
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.2609, Q2 Loss: 1.2609, Entropy: 0.0000, Mean TD Error: 1.3979, Threshold: -33.7494
Original likelihood: -35.69983673095703
Adjusted likelihood: -35.69983673095703
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 35.41716766357422
Projection step: 1, Loss: 35.258262634277344
Projection step: 2, Loss: 34.7178840637207
Projection step: 3, Loss: 34.442386627197266
Projection step: 4, Loss: 33.81779479980469
Projection step: 5, Loss: 34.88540267944336
Projection step: 6, Loss: 33.025108337402344
Projection step: 7, Loss: 34.266658782958984
Projection step: 8, Loss: 32.394317626953125
Projection step: 9, Loss: 32.905250549316406
Projection step: 10, Loss: 30.99566650390625
Projection step: 11, Loss: 31.613121032714844
Projection step: 12, Loss: 31.1634464263916
Projection step: 13, Loss: 31.171432495117188
Projection step: 14, Loss: 31.547115325927734
Projection step: 15, Loss: 32.1877326965332
Projection step: 16, Loss: 30.056652069091797
Projection step: 17, Loss: 29.992549896240234
Projection step: 18, Loss: 30.26740264892578
Projection step: 19, Loss: 30.12691879272461
Projection step: 20, Loss: 29.898147583007812
Projection step: 21, Loss: 28.747398376464844
Projection step: 22, Loss: 29.702728271484375
Projection step: 23, Loss: 29.719608306884766
Projection step: 24, Loss: 29.887126922607422
Final likelihood: tensor([-30.0194, -30.0961, -34.0009, -29.2552, -27.1507, -28.8518, -30.5229,
        -23.5605, -27.0959, -23.0246, -25.5556, -28.0033, -28.5150, -29.9117,
        -25.0453, -31.8524])
Final projection likelihood: -28.2788
1 mode projection succeeded
New goal: tensor([ 7.4727e-02,  8.5817e-01,  4.2228e-01,  2.6961e-01, -2.8671e-01,
         4.9186e-01,  6.6284e-01,  8.5397e-01,  1.4743e+00,  5.3380e-04,
         4.1936e-01,  9.8083e-01,  1.0035e-02,  1.5415e-01,  2.2479e-01],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -35.563533782958984
Adjusted likelihood: -35.563533782958984
Likelihood residual: 0.0
Original likelihood: -28.529573440551758
Adjusted likelihood: -28.529573440551758
Likelihood residual: 0.0
{'index': 28.529573440551758, 'thumb_middle': 35.563533782958984}
Current yaw: tensor([ 0.0110,  0.1566, -0.1136], device='cuda:0')
4 index
tensor([ 0.0643,  0.9233,  0.5402,  0.3158, -0.3182,  0.5153,  0.7410,  0.8706,
         1.4986,  0.0708,  0.4310,  1.0055,  0.0110,  0.1566, -0.1136,  5.4207],
       device='cuda:0')
Solve time for step 1 10.452663258998655
Current ori: tensor([ 0.0110,  0.1566, -0.1136], device='cuda:0')
Middle force: tensor([0.5145, 0.5988, 0.5930, 0.5480], device='cuda:0')
Thumb force: tensor([0.5016, 0.5418, 0.5725, 0.6235], device='cuda:0')
tensor([ 1.8715e-01,  8.9909e-01,  4.5171e-01,  2.7996e-01, -2.9907e-01,
         5.2360e-01,  7.3397e-01,  8.9072e-01,  1.5000e+00,  5.5444e-02,
         4.2378e-01,  9.4969e-01, -2.2166e-03,  1.4585e-01, -1.9265e-01,
         6.1508e+00], device='cuda:0')
Solve time for step 2 4.211034752021078
Current ori: tensor([-0.0022,  0.1458, -0.1927], device='cuda:0')
Middle force: tensor([0.5450, 0.5229, 0.5772], device='cuda:0')
Thumb force: tensor([0.5708, 0.5096, 0.6083], device='cuda:0')
tensor([ 0.2117,  0.9038,  0.4410,  0.2740, -0.2832,  0.5304,  0.7332,  0.8924,
         1.5000,  0.0409,  0.4128,  0.9505, -0.0063,  0.1363, -0.2027, -5.5921],
       device='cuda:0')
Solve time for step 3 3.9841887790244073
Current ori: tensor([-0.0063,  0.1363, -0.2027], device='cuda:0')
Middle force: tensor([0.5579, 0.5219], device='cuda:0')
Thumb force: tensor([0.6292, 0.5898], device='cuda:0')
tensor([ 0.2116,  0.9037,  0.4390,  0.2716, -0.2819,  0.5288,  0.7355,  0.8962,
         1.5000,  0.0372,  0.4176,  0.9432, -0.0060,  0.1352, -0.2088, -5.1776],
       device='cuda:0')
Solve time for step 4 3.7121670600026846
Current ori: tensor([-0.0060,  0.1352, -0.2088], device='cuda:0')
Middle force: tensor([0.5881], device='cuda:0')
Thumb force: tensor([0.5611], device='cuda:0')
Storing RECOVERY transition: reward=0.0411 (scaled=0.0137), steps=3
Reward stats updated: mean 0.0013 -> 0.0013, std: 0.0858
Collected 317 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5854, Q2 Loss=1.5854, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3842
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0456, Q2 Loss=1.0456, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3068
SAC Update 3/5: Actor Loss=-0.0640, Q1 Loss=0.8157, Q2 Loss=0.8157, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6304
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.8163, Q2 Loss=1.8163, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5438
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.2045, Q2 Loss=1.2045, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8945

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.2%)
Q1 update: 0.04s (19.6%)
Q2 update: 0.04s (18.0%)
Actor update: 0.08s (39.2%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.012791
Q1 loss: 1.293496
Q2 loss: 1.293496
Current threshold: -33.7505
Global Scale Offset: 0.0820
Reward stats: mean=0.0013, std=0.0858, count=317
----------------------------------------------
SAC Update - Actor Loss: -0.0128, Q1 Loss: 1.2935, Q2 Loss: 1.2935, Entropy: 0.0000, Mean TD Error: 1.1519, Threshold: -33.7505
Original likelihood: -28.9722900390625
Adjusted likelihood: -28.9722900390625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0056,  0.1339, -0.1974], device='cuda:0')
5 turn
Sampling time 3.6593934679985978
tensor([ 0.1192,  0.9019,  0.4382,  0.2709, -0.2798,  0.5317,  0.7324,  0.8956,
         1.5000,  0.0357,  0.4075,  0.9691, -0.0056,  0.1339, -0.1974, -5.2878],
       device='cuda:0')
Original likelihood: -29.01394271850586
Adjusted likelihood: -29.01394271850586
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.151405707001686
Current ori: tensor([-0.0056,  0.1339, -0.1974], device='cuda:0')
Middle force: tensor([0.5226, 0.5094, 0.7060, 0.6292, 0.5456, 0.7940, 0.7926, 0.7156, 0.4399,
        0.4986, 0.4687, 0.7632], device='cuda:0')
Thumb force: tensor([0.8726, 0.6522, 0.8171, 0.5539, 0.5362, 0.5216, 0.6635, 0.5343, 0.5779,
        0.8929, 0.6310, 0.5194], device='cuda:0')
Index force: tensor([0.6932, 0.5177, 0.6259, 0.5302, 0.5416, 0.5278, 0.7135, 0.4895, 0.5713,
        0.6479, 1.0304, 0.5132], device='cuda:0')
Storing NORMAL transition: reward=-0.0178 (scaled=-0.0178), steps=1
Reward stats updated: mean 0.0013 -> 0.0012, std: 0.0857
Collected 318 transitions for RL
SAC Update 1/5: Actor Loss=-0.0141, Q1 Loss=1.1492, Q2 Loss=1.1492, Entropy=0.2776, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0001
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4057, Q2 Loss=1.4057, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0985
SAC Update 3/5: Actor Loss=-0.0208, Q1 Loss=1.3077, Q2 Loss=1.3077, Entropy=0.1882, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7591
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9540, Q2 Loss=0.9540, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3737
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.6285, Q2 Loss=0.6285, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3746

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (18.1%)
Actor update: 0.09s (40.1%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.006987
Q1 loss: 1.088993
Q2 loss: 1.088993
Current threshold: -33.7693
Global Scale Offset: 0.0818
Reward stats: mean=0.0012, std=0.0857, count=318
----------------------------------------------
SAC Update - Actor Loss: -0.0070, Q1 Loss: 1.0890, Q2 Loss: 1.0890, Entropy: 0.0931, Mean TD Error: 1.1212, Threshold: -33.7693
tensor([ 0.1089,  0.9354,  0.4381,  0.1532, -0.3070,  0.5670,  0.7216,  0.9068,
         1.3841,  0.1975,  0.4787,  0.9216, -0.0156,  0.1401, -0.1809, -5.3478],
       device='cuda:0')
Original likelihood: -31.06403350830078
Adjusted likelihood: -31.06403350830078
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.492979186004959
Current ori: tensor([-0.0156,  0.1401, -0.1809], device='cuda:0')
Middle force: tensor([0.5232, 0.4984, 0.7557, 0.5273, 0.7037, 0.8449, 1.1369, 0.4985, 0.5065,
        0.5002, 0.5299], device='cuda:0')
Thumb force: tensor([1.0027, 1.1791, 0.5562, 0.7138, 0.6034, 0.9220, 0.5893, 0.5603, 1.1914,
        1.3994, 1.0869], device='cuda:0')
Index force: tensor([0.6270, 0.7392, 0.7878, 0.5368, 0.5208, 0.5357, 0.5066, 0.5250, 0.8606,
        1.0021, 0.5306], device='cuda:0')
Storing NORMAL transition: reward=-0.1144 (scaled=-0.1144), steps=1
Reward stats updated: mean 0.0012 -> 0.0009, std: 0.0858
Collected 319 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.0032, Q2 Loss=1.0032, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7112
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6971, Q2 Loss=1.6971, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7523
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3454, Q2 Loss=1.3454, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2714
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.9198, Q2 Loss=0.9198, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6675
SAC Update 5/5: Actor Loss=-0.0215, Q1 Loss=0.6218, Q2 Loss=0.6218, Entropy=0.1801, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6236

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (16.8%)
Q1 update: 0.04s (17.8%)
Q2 update: 0.05s (19.9%)
Actor update: 0.10s (41.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.096401
Q1 loss: 1.117451
Q2 loss: 1.117451
Current threshold: -33.7877
Global Scale Offset: 0.0817
Reward stats: mean=0.0009, std=0.0858, count=319
----------------------------------------------
SAC Update - Actor Loss: -0.0964, Q1 Loss: 1.1175, Q2 Loss: 1.1175, Entropy: 0.0360, Mean TD Error: 1.0052, Threshold: -33.7877
tensor([ 0.1346,  1.0153,  0.5946, -0.0546, -0.2437,  0.7136,  0.5711,  0.7825,
         1.5000,  0.0256,  0.3399,  1.0597, -0.0553,  0.1139, -0.0604, -5.4888],
       device='cuda:0')
Original likelihood: -30.49732208251953
Adjusted likelihood: -30.49732208251953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.322571710043121
Current ori: tensor([-0.0553,  0.1139, -0.0604], device='cuda:0')
Middle force: tensor([1.0413, 0.7252, 0.5673, 1.1527, 1.3080, 1.0682, 0.5346, 0.5065, 0.5025,
        0.5143], device='cuda:0')
Thumb force: tensor([1.2837, 0.6058, 0.5278, 1.2912, 0.9783, 0.7978, 0.8911, 0.6154, 1.3104,
        0.6108], device='cuda:0')
Index force: tensor([0.5540, 0.6487, 0.5358, 0.5007, 0.5493, 0.6574, 0.5401, 0.5266, 0.8847,
        0.5073], device='cuda:0')
Storing NORMAL transition: reward=0.0871 (scaled=0.0871), steps=1
Reward stats updated: mean 0.0009 -> 0.0011, std: 0.0858
Collected 320 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0517, Q2 Loss=1.0517, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1392
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7676, Q2 Loss=0.7676, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7657
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.1713, Q2 Loss=1.1713, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1378
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3214, Q2 Loss=1.3214, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7851
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.0883, Q2 Loss=2.0883, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5825

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.3%)
Q1 update: 0.06s (20.2%)
Q2 update: 0.06s (20.0%)
Actor update: 0.12s (42.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.280075
Q2 loss: 1.280075
Current threshold: -33.8083
Global Scale Offset: 0.0814
Reward stats: mean=0.0011, std=0.0858, count=320
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.2801, Q2 Loss: 1.2801, Entropy: 0.0000, Mean TD Error: 1.2821, Threshold: -33.8083
tensor([ 0.0771,  0.9713,  0.5705,  0.2317, -0.2553,  0.5996,  0.6505,  0.8769,
         1.5000, -0.0815,  0.4040,  0.9816, -0.0544,  0.1302, -0.1527, -5.4169],
       device='cuda:0')
Original likelihood: -36.632362365722656
Adjusted likelihood: -36.632362365722656
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 31.88658332824707
Projection step: 1, Loss: 33.638336181640625
Projection step: 2, Loss: 33.696739196777344
Projection step: 3, Loss: 30.611061096191406
Projection step: 4, Loss: 30.761581420898438
Projection step: 5, Loss: 31.77993392944336
Projection step: 6, Loss: 31.359752655029297
Projection step: 7, Loss: 29.074676513671875
Projection step: 8, Loss: 29.342403411865234
Projection step: 9, Loss: 29.432838439941406
Projection step: 10, Loss: 31.03903579711914
Projection step: 11, Loss: 30.092090606689453
Projection step: 12, Loss: 29.597143173217773
Projection step: 13, Loss: 29.992368698120117
Projection step: 14, Loss: 28.055110931396484
Projection step: 15, Loss: 28.637821197509766
Projection step: 16, Loss: 26.42210578918457
Projection step: 17, Loss: 27.737911224365234
Projection step: 18, Loss: 28.394197463989258
Projection step: 19, Loss: 27.337984085083008
Projection step: 20, Loss: 27.117277145385742
Projection step: 21, Loss: 28.297632217407227
Projection step: 22, Loss: 24.668678283691406
Projection step: 23, Loss: 29.37222671508789
Projection step: 24, Loss: 26.42334747314453
Final likelihood: tensor([-25.4562, -38.3279, -24.2729, -23.8544, -25.3822, -23.1965, -22.5635,
        -24.5166, -24.2102, -35.3679, -21.4875, -24.3757, -36.6621, -35.9841,
        -25.2694, -24.4752])
Final projection likelihood: -27.2126
1 mode projection succeeded
New goal: tensor([ 0.0843,  0.8853,  0.4685,  0.1828, -0.2316,  0.5524,  0.5820,  0.8370,
         1.4753, -0.0546,  0.4021,  0.9988, -0.0549,  0.1267,  0.4952],
       device='cuda:0')
tensor([[0.0105]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0018]], device='cuda:0')
Original likelihood: -34.93022155761719
Adjusted likelihood: -34.93022155761719
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 34.93022155761719}
Current yaw: tensor([-0.0544,  0.1302, -0.1527], device='cuda:0')
6 thumb_middle
tensor([ 0.0771,  0.9713,  0.5705,  0.2317, -0.2553,  0.5996,  0.6505,  0.8769,
         1.5000, -0.0815,  0.4040,  0.9816, -0.0544,  0.1302, -0.1527, -5.4169],
       device='cuda:0')
Solve time for step 1 8.89652400003979
Current ori: tensor([-0.0544,  0.1302, -0.1527], device='cuda:0')
Index force: tensor([0.5278, 0.5313, 0.5489, 0.5500], device='cuda:0')
tensor([ 0.0302,  1.0238,  0.5716,  0.2087, -0.2739,  0.6149,  0.6072,  0.8451,
         1.4808, -0.0646,  0.3973,  1.0081, -0.0820,  0.1557, -0.1964, -5.3671],
       device='cuda:0')
Solve time for step 2 3.6036675999639556
Current ori: tensor([-0.0820,  0.1557, -0.1964], device='cuda:0')
Index force: tensor([0.5230, 0.5384, 0.5417], device='cuda:0')
tensor([ 3.0317e-03,  1.0503e+00,  5.5135e-01,  2.0446e-01, -2.6671e-01,
         6.0558e-01,  5.9681e-01,  8.3236e-01,  1.4981e+00, -6.0453e-02,
         3.9040e-01,  1.0171e+00, -1.0740e-01,  1.6663e-01, -2.6547e-01,
        -5.2153e+00], device='cuda:0')
Solve time for step 3 3.575695702980738
Current ori: tensor([-0.1074,  0.1666, -0.2655], device='cuda:0')
Index force: tensor([0.5417, 0.5592], device='cuda:0')
tensor([-0.0263,  1.0717,  0.5313,  0.2105, -0.2724,  0.6219,  0.5957,  0.8340,
         1.5000, -0.0611,  0.4098,  1.0173, -0.1637,  0.1895, -0.3438, -4.9876],
       device='cuda:0')
Solve time for step 4 3.5789826219552197
Current ori: tensor([-0.1637,  0.1895, -0.3438], device='cuda:0')
Index force: tensor([0.5445], device='cuda:0')
Storing RECOVERY transition: reward=-0.0075 (scaled=-0.0025), steps=3
Reward stats updated: mean 0.0011 -> 0.0011, std: 0.0857
Collected 321 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7173, Q2 Loss=0.7173, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2952
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=2.7633, Q2 Loss=2.7633, Entropy=0.0034, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.4404
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.6811, Q2 Loss=0.6811, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6962
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.6172, Q2 Loss=1.6172, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1206
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1899, Q2 Loss=1.1899, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9394

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.7%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (41.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092104
Q1 loss: 1.393736
Q2 loss: 1.393736
Current threshold: -33.8205
Global Scale Offset: 0.0813
Reward stats: mean=0.0011, std=0.0857, count=321
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.3937, Q2 Loss: 1.3937, Entropy: 0.0007, Mean TD Error: 1.2984, Threshold: -33.8205
Original likelihood: -279.94140625
Adjusted likelihood: -279.94140625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 279.95758056640625
Projection step: 1, Loss: 276.9306945800781
Projection step: 2, Loss: 278.62628173828125
Projection step: 3, Loss: 278.52728271484375
Projection step: 4, Loss: 277.4437255859375
Projection step: 5, Loss: 278.10223388671875
Projection step: 6, Loss: 275.0047912597656
Projection step: 7, Loss: 274.74725341796875
Projection step: 8, Loss: 277.26129150390625
Projection step: 9, Loss: 276.95819091796875
Projection step: 10, Loss: 280.20855712890625
Projection step: 11, Loss: 279.0746765136719
Projection step: 12, Loss: 277.987060546875
Projection step: 13, Loss: 280.56884765625
Projection step: 14, Loss: 283.7717590332031
Projection step: 15, Loss: 275.5252380371094
Projection step: 16, Loss: 273.39178466796875
Projection step: 17, Loss: 275.5062255859375
Projection step: 18, Loss: 274.5964660644531
Projection step: 19, Loss: 277.6786193847656
Projection step: 20, Loss: 278.42889404296875
Projection step: 21, Loss: 278.06475830078125
Projection step: 22, Loss: 274.0380859375
Projection step: 23, Loss: 272.9450988769531
Projection step: 24, Loss: 273.08160400390625
Final likelihood: tensor([-275.8394, -275.1032, -274.1719, -274.7655, -270.9360, -281.9869,
        -284.3053, -264.5360, -267.7486, -264.9987, -278.8308, -273.9377,
        -271.0096, -278.5008, -269.0105, -269.0818])
Final projection likelihood: -273.4227
1 mode projection failed, trying anyway
New goal: tensor([-0.0636,  1.0616,  0.5548,  0.2238, -0.2168,  0.6682,  0.6269,  0.8558,
         1.4982,  0.0451,  0.4157,  1.0281, -0.3714,  0.3168, -0.3894],
       device='cuda:0')
tensor([[0.0414]], device='cuda:0') tensor([[0.0130]], device='cuda:0') tensor([[0.0375]], device='cuda:0')
Original likelihood: -215.13446044921875
Adjusted likelihood: -215.13446044921875
Likelihood residual: 0.0
Original likelihood: -260.09564208984375
Adjusted likelihood: -260.09564208984375
Likelihood residual: 0.0
{'index': 260.09564208984375, 'thumb_middle': 215.13446044921875}
Current yaw: tensor([-0.3715,  0.3168, -0.3842], device='cuda:0')
7 thumb_middle
tensor([-0.0646,  1.0610,  0.5546,  0.2233, -0.2163,  0.6681,  0.6267,  0.8524,
         1.5000,  0.0443,  0.4163,  1.0289, -0.3715,  0.3168, -0.3842, -4.6861],
       device='cuda:0')
Solve time for step 1 8.75935551902512
Current ori: tensor([-0.3715,  0.3168, -0.3842], device='cuda:0')
Index force: tensor([0.6998, 0.5012, 0.5043, 0.5520], device='cuda:0')
tensor([-0.1398,  1.0836,  0.5940,  0.2486, -0.2320,  0.6708,  0.6203,  0.8520,
         1.5000,  0.0842,  0.4158,  1.0334, -0.8150,  0.6737, -0.3841, -4.4340],
       device='cuda:0')
Solve time for step 2 3.7027310880366713
Current ori: tensor([-0.8150,  0.6737, -0.3841], device='cuda:0')
Index force: tensor([0.5014, 0.5032, 0.5433], device='cuda:0')
tensor([-0.2866,  1.0580,  0.5663,  0.3856, -0.2534,  0.6459,  0.6095,  0.8398,
         1.5000,  0.1987,  0.4511,  1.0976, -1.3557,  0.9962, -0.3841, -4.3025],
       device='cuda:0')
Solve time for step 3 3.427535527967848
Current ori: tensor([-1.3557,  0.9962, -0.3841], device='cuda:0')
Index force: tensor([0.5073, 0.5555], device='cuda:0')
tensor([-0.2698,  1.1336,  0.4721,  0.3899, -0.1533,  0.7993,  0.6664,  0.8289,
         1.4903,  0.2011,  0.4396,  1.0930, -1.7525,  0.9927, -0.3842, -3.8140],
       device='cuda:0')
Solve time for step 4 3.70376224903157
Current ori: tensor([-1.7525,  0.9927, -0.3842], device='cuda:0')
Index force: tensor([0.5145], device='cuda:0')
Storing RECOVERY transition: reward=-1.5034 (scaled=-0.5011), steps=3
Reward stats updated: mean 0.0011 -> -0.0004, std: 0.0900
Collected 322 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6208, Q2 Loss=1.6208, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6467
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9878, Q2 Loss=0.9878, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6817
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6765, Q2 Loss=1.6765, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3017
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1103, Q2 Loss=1.1103, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5113
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2890, Q2 Loss=1.2890, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1648

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (1.0%)
Target Q: 0.05s (19.4%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (18.4%)
Actor update: 0.10s (39.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.336892
Q2 loss: 1.336892
Current threshold: -33.8278
Global Scale Offset: 0.0812
Reward stats: mean=-0.0004, std=0.0900, count=322
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.3369, Q2 Loss: 1.3369, Entropy: 0.0000, Mean TD Error: 1.0612, Threshold: -33.8278
Original likelihood: -1310.7220458984375
Adjusted likelihood: -1310.7220458984375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 21
Loaded trajectory sampler
Current yaw: tensor([ 0.0004,  0.0141, -0.0480], device='cuda:0')
Current yaw: tensor([ 0.0004,  0.0141, -0.0480], device='cuda:0')
1 turn
Sampling time 3.6313788130064495
tensor([ 1.3295e-01,  6.3691e-01,  5.5332e-01,  5.2783e-01, -1.4105e-01,
         5.6768e-01,  8.7335e-01,  9.5236e-01,  1.2231e+00,  2.5946e-01,
         2.6341e-01,  1.2304e+00,  3.7568e-04,  1.4118e-02, -4.8037e-02,
         4.6632e-01], device='cuda:0')
Original likelihood: -21.101808547973633
Adjusted likelihood: -21.101808547973633
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.217934153974056
Current ori: tensor([ 0.0004,  0.0141, -0.0480], device='cuda:0')
Middle force: tensor([0.5887, 0.5746, 1.1574, 0.5685, 1.1363, 0.6381, 0.5368, 0.5413, 0.5155,
        0.8288, 0.7127, 0.6101], device='cuda:0')
Thumb force: tensor([0.8841, 0.8721, 0.7815, 1.0707, 0.9933, 0.6553, 0.5252, 0.8766, 0.5431,
        0.5410, 0.7358, 0.6129], device='cuda:0')
Index force: tensor([0.6005, 0.6022, 0.5605, 0.5752, 0.8157, 0.5264, 1.0416, 0.9397, 0.5966,
        0.5939, 0.7786, 0.6204], device='cuda:0')
Storing NORMAL transition: reward=-0.0029 (scaled=-0.0029), steps=1
Reward stats updated: mean -0.0004 -> -0.0004, std: 0.0899
Collected 323 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.1967, Q2 Loss=2.1967, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5163
SAC Update 2/5: Actor Loss=-0.0181, Q1 Loss=1.0154, Q2 Loss=1.0154, Entropy=0.0895, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5775
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.2083, Q2 Loss=1.2083, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8049
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.1845, Q2 Loss=2.1845, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8649
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8795, Q2 Loss=0.8795, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8692

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.7%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (19.8%)
Actor update: 0.09s (40.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.049672
Q1 loss: 1.496891
Q2 loss: 1.496891
Current threshold: -33.8400
Global Scale Offset: 0.0811
Reward stats: mean=-0.0004, std=0.0899, count=323
----------------------------------------------
SAC Update - Actor Loss: -0.0497, Q1 Loss: 1.4969, Q2 Loss: 1.4969, Entropy: 0.0179, Mean TD Error: 1.9266, Threshold: -33.8400
tensor([ 0.0781,  0.5987,  0.5921,  0.4450, -0.2468,  0.6119,  0.6992,  0.9594,
         1.2693,  0.3052,  0.2534,  1.0804,  0.0041,  0.0449, -0.0469,  0.3736],
       device='cuda:0')
Original likelihood: -36.89874267578125
Adjusted likelihood: -36.89874267578125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.580421447753906
Projection step: 1, Loss: 37.09813690185547
Projection step: 2, Loss: 35.33513259887695
Projection step: 3, Loss: 34.52825927734375
Projection step: 4, Loss: 33.87164306640625
Projection step: 5, Loss: 33.75259780883789
Projection step: 6, Loss: 32.94989013671875
Projection step: 7, Loss: 32.34697723388672
Projection step: 8, Loss: 29.467863082885742
Projection step: 9, Loss: 31.47402000427246
Projection step: 10, Loss: 30.477338790893555
Projection step: 11, Loss: 29.26547622680664
Projection step: 12, Loss: 28.8016357421875
Projection step: 13, Loss: 28.605710983276367
Projection step: 14, Loss: 26.522445678710938
Projection step: 15, Loss: 28.663330078125
Projection step: 16, Loss: 25.907512664794922
Projection step: 17, Loss: 26.36800765991211
Projection step: 18, Loss: 24.219623565673828
Projection step: 19, Loss: 24.293609619140625
Projection step: 20, Loss: 23.60372543334961
Projection step: 21, Loss: 24.640243530273438
Projection step: 22, Loss: 22.97328758239746
Projection step: 23, Loss: 22.48470687866211
Projection step: 24, Loss: 22.70660400390625
Final likelihood: tensor([-17.9131, -23.8882, -24.0583, -17.6414, -23.9545, -24.7187, -17.8145,
        -17.7636, -25.8646, -23.5434, -22.1901, -18.9551, -19.6451, -24.7766,
        -18.7755, -17.7079])
Final projection likelihood: -21.2007
1 mode projection succeeded
New goal: tensor([ 6.0625e-02,  5.7460e-01,  5.8323e-01,  5.1909e-01, -1.6280e-01,
         6.2643e-01,  7.2309e-01,  8.3878e-01,  1.2941e+00,  2.3604e-01,
         2.2507e-01,  1.1864e+00,  4.4814e-04,  3.5756e-02, -6.6395e-01],
       device='cuda:0')
tensor([[0.0067]], device='cuda:0') tensor([[0.0095]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -31.215484619140625
Adjusted likelihood: -31.215484619140625
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 31.215484619140625}
Current yaw: tensor([ 0.0041,  0.0449, -0.0469], device='cuda:0')
2 thumb_middle
tensor([ 0.0781,  0.5987,  0.5921,  0.4450, -0.2468,  0.6119,  0.6992,  0.9594,
         1.2693,  0.3052,  0.2534,  1.0804,  0.0041,  0.0449, -0.0469,  0.3736],
       device='cuda:0')
Solve time for step 1 9.184712894028053
Current ori: tensor([ 0.0041,  0.0449, -0.0469], device='cuda:0')
Index force: tensor([0.5664, 0.5879, 0.5806, 0.5636], device='cuda:0')
tensor([ 0.0708,  0.6028,  0.5698,  0.4633, -0.2475,  0.6180,  0.7159,  0.8396,
         1.2734,  0.2249,  0.1703,  1.1508,  0.0046,  0.0493, -0.0469,  0.3615],
       device='cuda:0')
Solve time for step 2 3.7638083940255456
Current ori: tensor([ 0.0046,  0.0493, -0.0469], device='cuda:0')
Index force: tensor([0.5747, 0.5697, 0.5544], device='cuda:0')
tensor([ 0.0806,  0.5781,  0.5791,  0.5302, -0.2444,  0.6193,  0.7291,  0.8486,
         1.2670,  0.2290,  0.1615,  1.1586,  0.0143,  0.0458, -0.0469,  0.3972],
       device='cuda:0')
Solve time for step 3 3.3940622009686194
Current ori: tensor([ 0.0143,  0.0458, -0.0469], device='cuda:0')
Index force: tensor([0.5530, 0.5393], device='cuda:0')
tensor([ 0.0894,  0.5927,  0.5802,  0.5057, -0.2312,  0.6284,  0.7176,  0.8244,
         1.2596,  0.2265,  0.1508,  1.1777,  0.0090,  0.0400, -0.0469,  0.4022],
       device='cuda:0')
Solve time for step 4 3.244091517990455
Current ori: tensor([ 0.0090,  0.0400, -0.0469], device='cuda:0')
Index force: tensor([0.5557], device='cuda:0')
Storing RECOVERY transition: reward=-0.0113 (scaled=-0.0113), steps=1
Reward stats updated: mean -0.0004 -> -0.0005, std: 0.0897
Collected 324 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6142, Q2 Loss=1.6142, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7332
SAC Update 2/5: Actor Loss=-0.1878, Q1 Loss=1.1003, Q2 Loss=1.1003, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1973
SAC Update 3/5: Actor Loss=-0.0325, Q1 Loss=1.2566, Q2 Loss=1.2566, Entropy=0.0821, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0100
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0523, Q2 Loss=1.0523, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4940
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.8634, Q2 Loss=0.8634, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4345

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.1%)
Q1 update: 0.05s (18.9%)
Q2 update: 0.05s (18.6%)
Actor update: 0.11s (40.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.044066
Q1 loss: 1.177363
Q2 loss: 1.177363
Current threshold: -33.8597
Global Scale Offset: 0.0807
Reward stats: mean=-0.0005, std=0.0897, count=324
----------------------------------------------
SAC Update - Actor Loss: -0.0441, Q1 Loss: 1.1774, Q2 Loss: 1.1774, Entropy: 0.0164, Mean TD Error: 0.7738, Threshold: -33.8597
Original likelihood: -30.687702178955078
Adjusted likelihood: -30.687702178955078
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0143,  0.0569, -0.0368], device='cuda:0')
3 turn
Sampling time 3.7256587719894014
tensor([ 0.0578,  0.5823,  0.5547,  0.5239, -0.2095,  0.6683,  0.7467,  0.8464,
         1.3293,  0.2703,  0.2219,  1.2009,  0.0143,  0.0569, -0.0368,  0.3842],
       device='cuda:0')
Original likelihood: -36.06433868408203
Adjusted likelihood: -36.06433868408203
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 32.54224395751953
Projection step: 1, Loss: 31.4881591796875
Projection step: 2, Loss: 29.212453842163086
Projection step: 3, Loss: 30.16182518005371
Projection step: 4, Loss: 27.928043365478516
Projection step: 5, Loss: 27.971433639526367
Projection step: 6, Loss: 27.95052719116211
Projection step: 7, Loss: 26.322757720947266
Projection step: 8, Loss: 26.830476760864258
Projection step: 9, Loss: 27.145198822021484
Projection step: 10, Loss: 27.62820816040039
Projection step: 11, Loss: 26.385955810546875
Projection step: 12, Loss: 24.95891571044922
Projection step: 13, Loss: 26.483936309814453
Projection step: 14, Loss: 24.55877685546875
Projection step: 15, Loss: 24.359638214111328
Projection step: 16, Loss: 23.821073532104492
Projection step: 17, Loss: 26.164621353149414
Projection step: 18, Loss: 23.453617095947266
Projection step: 19, Loss: 24.330257415771484
Projection step: 20, Loss: 25.384658813476562
Projection step: 21, Loss: 25.263397216796875
Projection step: 22, Loss: 23.771804809570312
Projection step: 23, Loss: 23.222776412963867
Projection step: 24, Loss: 22.486480712890625
Final likelihood: tensor([-32.1514, -27.1241, -30.2626, -20.0690, -21.0856, -19.8672, -21.5315,
        -20.1899, -20.6308, -19.8454, -20.5015, -33.3141, -20.7197, -20.2879,
        -19.7415, -20.3067])
Final projection likelihood: -22.9768
1 mode projection succeeded
New goal: tensor([ 0.0562,  0.5788,  0.5816,  0.5411, -0.1706,  0.6442,  0.7252,  0.7626,
         1.3066,  0.2388,  0.2176,  1.2221,  0.0179,  0.0437, -0.2053],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -30.567611694335938
Adjusted likelihood: -30.567611694335938
Likelihood residual: 0.0
Original likelihood: -28.99886703491211
Adjusted likelihood: -28.99886703491211
Likelihood residual: 0.0
{'index': 28.99886703491211, 'thumb_middle': 30.567611694335938}
Current yaw: tensor([ 0.0143,  0.0569, -0.0368], device='cuda:0')
4 index
tensor([ 0.0578,  0.5823,  0.5547,  0.5239, -0.2095,  0.6683,  0.7467,  0.8464,
         1.3293,  0.2703,  0.2219,  1.2009,  0.0143,  0.0569, -0.0368,  0.3842],
       device='cuda:0')
Solve time for step 1 10.490355657006148
Current ori: tensor([ 0.0143,  0.0569, -0.0368], device='cuda:0')
Middle force: tensor([0.6010, 0.5554, 0.5056, 0.6017], device='cuda:0')
Thumb force: tensor([0.5677, 0.5709, 0.5252, 0.5974], device='cuda:0')
tensor([ 0.1130,  0.5113,  0.5198,  0.5148, -0.2027,  0.6699,  0.7621,  0.8047,
         1.3482,  0.2532,  0.2005,  1.1923,  0.0058,  0.0537, -0.0613,  1.1522],
       device='cuda:0')
Solve time for step 2 4.249220710014924
Current ori: tensor([ 0.0058,  0.0537, -0.0613], device='cuda:0')
Middle force: tensor([0.5513, 0.5050, 0.5969], device='cuda:0')
Thumb force: tensor([0.5649, 0.5233, 0.5931], device='cuda:0')
tensor([ 0.1156,  0.5115,  0.5209,  0.5128, -0.1929,  0.6796,  0.7594,  0.7881,
         1.3503,  0.2464,  0.1878,  1.1957,  0.0018,  0.0481, -0.0617,  1.4165],
       device='cuda:0')
Solve time for step 3 4.065544538025279
Current ori: tensor([ 0.0018,  0.0481, -0.0617], device='cuda:0')
Middle force: tensor([0.5310, 0.5105], device='cuda:0')
Thumb force: tensor([0.5603, 0.5099], device='cuda:0')
tensor([ 0.1137,  0.5098,  0.5227,  0.5143, -0.2052,  0.6662,  0.7670,  0.8002,
         1.3542,  0.2527,  0.1902,  1.2046,  0.0051,  0.0555, -0.0743,  1.3538],
       device='cuda:0')
Solve time for step 4 3.9721256130142137
Current ori: tensor([ 0.0051,  0.0555, -0.0743], device='cuda:0')
Middle force: tensor([0.5354], device='cuda:0')
Thumb force: tensor([0.5484], device='cuda:0')
Storing RECOVERY transition: reward=0.0243 (scaled=0.0243), steps=0
Reward stats updated: mean -0.0005 -> -0.0004, std: 0.0896
Collected 325 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5191, Q2 Loss=1.5191, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1456
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6837, Q2 Loss=0.6837, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1679
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1513, Q2 Loss=1.1513, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5363
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9626, Q2 Loss=0.9626, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3343
SAC Update 5/5: Actor Loss=-0.0482, Q1 Loss=0.7292, Q2 Loss=0.7292, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2108

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (19.6%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (17.5%)
Actor update: 0.09s (39.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.009638
Q1 loss: 1.009180
Q2 loss: 1.009180
Current threshold: -33.8770
Global Scale Offset: 0.0805
Reward stats: mean=-0.0004, std=0.0896, count=325
----------------------------------------------
SAC Update - Actor Loss: -0.0096, Q1 Loss: 1.0092, Q2 Loss: 1.0092, Entropy: 0.0000, Mean TD Error: 0.4790, Threshold: -33.8770
Original likelihood: -29.030391693115234
Adjusted likelihood: -29.030391693115234
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0008,  0.0514, -0.0605], device='cuda:0')
5 turn
Sampling time 3.7616350619937293
tensor([ 6.4545e-02,  5.7530e-01,  5.6730e-01,  5.3402e-01, -1.9542e-01,
         6.8246e-01,  7.5359e-01,  7.8075e-01,  1.3498e+00,  2.5486e-01,
         1.8370e-01,  1.2078e+00,  8.1008e-04,  5.1414e-02, -6.0461e-02,
         1.2715e+00], device='cuda:0')
Original likelihood: -32.258567810058594
Adjusted likelihood: -32.258567810058594
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.53996919101337
Current ori: tensor([ 0.0008,  0.0514, -0.0605], device='cuda:0')
Middle force: tensor([0.5353, 0.6571, 0.5686, 0.5095, 0.7971, 1.1082, 0.5759, 0.5545, 0.5633,
        0.5450, 0.5242, 0.5954], device='cuda:0')
Thumb force: tensor([0.8668, 0.6852, 1.6074, 2.6067, 0.8977, 1.7822, 0.6005, 0.7710, 0.5165,
        1.6222, 0.5845, 0.5939], device='cuda:0')
Index force: tensor([0.5546, 0.8590, 0.6084, 0.5931, 0.6027, 0.6037, 0.5850, 0.5190, 0.4981,
        0.8089, 0.5248, 0.5626], device='cuda:0')
Storing NORMAL transition: reward=0.1052 (scaled=0.1052), steps=1
Reward stats updated: mean -0.0004 -> -0.0001, std: 0.0897
Collected 326 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.3439, Q2 Loss=2.3439, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.3630
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4123, Q2 Loss=1.4123, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7947
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2460, Q2 Loss=1.2460, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5488
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.5254, Q2 Loss=2.5254, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4142
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4332, Q2 Loss=1.4332, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5119

------ SAC Update Summary (5 iterations) ------
Total time: 0.31s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.06s (18.0%)
Q1 update: 0.06s (20.5%)
Q2 update: 0.06s (19.7%)
Actor update: 0.12s (38.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.792158
Q2 loss: 1.792158
Current threshold: -33.8872
Global Scale Offset: 0.0803
Reward stats: mean=-0.0001, std=0.0897, count=326
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.7922, Q2 Loss: 1.7922, Entropy: 0.0000, Mean TD Error: 2.1265, Threshold: -33.8872
tensor([ 2.9619e-02,  5.6179e-01,  5.6206e-01,  5.1643e-01, -1.4384e-01,
         6.5422e-01,  8.5555e-01,  7.3902e-01,  1.2831e+00,  3.3051e-01,
         1.6352e-01,  1.2064e+00, -2.0499e-03,  8.4591e-03, -1.6323e-01,
         2.7152e+00], device='cuda:0')
Original likelihood: -20.426292419433594
Adjusted likelihood: -20.426292419433594
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.717590237036347
Current ori: tensor([-0.0020,  0.0085, -0.1632], device='cuda:0')
Middle force: tensor([0.7185, 0.5659, 0.5188, 0.7921, 1.0897, 0.6018, 0.5609, 0.5785, 0.6227,
        0.5333, 0.6131], device='cuda:0')
Thumb force: tensor([0.5903, 1.5697, 2.5196, 0.8774, 1.7471, 0.5628, 0.7349, 0.5107, 1.5158,
        0.5664, 0.5743], device='cuda:0')
Index force: tensor([0.8565, 0.6025, 0.5687, 0.5983, 0.5990, 0.5826, 0.5186, 0.5012, 0.7238,
        0.5211, 0.5578], device='cuda:0')
Storing NORMAL transition: reward=-0.0777 (scaled=-0.0777), steps=1
Reward stats updated: mean -0.0001 -> -0.0003, std: 0.0896
Collected 327 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1087, Q2 Loss=1.1087, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5284
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1995, Q2 Loss=1.1995, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9589
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.5985, Q2 Loss=1.5985, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6299
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2004, Q2 Loss=1.2004, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5169
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5288, Q2 Loss=1.5288, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2323

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.8%)
Q1 update: 0.06s (20.0%)
Q2 update: 0.06s (20.1%)
Actor update: 0.12s (42.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.327190
Q2 loss: 1.327190
Current threshold: -33.8933
Global Scale Offset: 0.0802
Reward stats: mean=-0.0003, std=0.0896, count=327
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.3272, Q2 Loss: 1.3272, Entropy: 0.0000, Mean TD Error: 1.5733, Threshold: -33.8933
tensor([ 0.0161,  0.5546,  0.5651,  0.4987, -0.1209,  0.7358,  0.6897,  0.8986,
         1.1829,  0.3141,  0.1874,  1.1521,  0.0061, -0.0079, -0.0856,  2.8673],
       device='cuda:0')
Original likelihood: -28.80040168762207
Adjusted likelihood: -28.80040168762207
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.258941936015617
Current ori: tensor([ 0.0061, -0.0079, -0.0856], device='cuda:0')
Middle force: tensor([0.5020, 0.5157, 0.5652, 0.5901, 0.9387, 0.8070, 0.7354, 0.5710, 0.5162,
        0.5356], device='cuda:0')
Thumb force: tensor([1.1172, 0.5503, 0.9327, 0.6916, 1.1947, 0.5364, 0.6528, 0.6133, 0.5017,
        0.7832], device='cuda:0')
Index force: tensor([0.7757, 0.5907, 0.5448, 0.5193, 0.5555, 0.5121, 0.5410, 0.5475, 0.5590,
        0.5327], device='cuda:0')
Storing NORMAL transition: reward=0.0471 (scaled=0.0471), steps=1
Reward stats updated: mean -0.0003 -> -0.0002, std: 0.0895
Collected 328 transitions for RL
SAC Update 1/5: Actor Loss=-0.0060, Q1 Loss=0.6366, Q2 Loss=0.6366, Entropy=0.3443, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1216
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7166, Q2 Loss=0.7166, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7051
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.5940, Q2 Loss=1.5940, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0110
SAC Update 4/5: Actor Loss=-0.0290, Q1 Loss=0.7330, Q2 Loss=0.7330, Entropy=0.1067, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0614
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9219, Q2 Loss=0.9219, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8291

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.6%)
Q1 update: 0.05s (20.4%)
Q2 update: 0.05s (19.0%)
Actor update: 0.11s (42.6%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.053051
Q1 loss: 0.920424
Q2 loss: 0.920424
Current threshold: -33.9098
Global Scale Offset: 0.0800
Reward stats: mean=-0.0002, std=0.0895, count=328
----------------------------------------------
SAC Update - Actor Loss: -0.0531, Q1 Loss: 0.9204, Q2 Loss: 0.9204, Entropy: 0.0902, Mean TD Error: 0.7456, Threshold: -33.9098
tensor([ 0.0614,  0.5508,  0.6160,  0.4882, -0.0828,  0.6615,  0.6187,  0.9371,
         1.1372,  0.4128,  0.3818,  1.1263,  0.0529,  0.0195, -0.1361,  3.1711],
       device='cuda:0')
Original likelihood: -27.520217895507812
Adjusted likelihood: -27.520217895507812
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.958858708036132
Current ori: tensor([ 0.0529,  0.0195, -0.1361], device='cuda:0')
Middle force: tensor([0.5039, 0.5661, 0.5708, 0.8969, 0.7336, 0.7303, 0.5555, 0.5651, 0.5209],
       device='cuda:0')
Thumb force: tensor([0.5596, 0.9408, 0.6853, 1.1629, 0.5396, 0.6379, 0.6053, 0.5262, 0.5454],
       device='cuda:0')
Index force: tensor([0.6067, 0.5486, 0.5135, 0.5491, 0.5131, 0.5332, 0.5386, 0.5008, 0.5709],
       device='cuda:0')
Storing NORMAL transition: reward=0.0659 (scaled=0.0659), steps=1
Reward stats updated: mean -0.0002 -> 0.0000, std: 0.0895
Collected 329 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.8882, Q2 Loss=1.8882, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1774
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.4024, Q2 Loss=1.4024, Entropy=0.0002, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2633
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.4783, Q2 Loss=1.4783, Entropy=0.0101, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1893
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7192, Q2 Loss=0.7192, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7161
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0700, Q2 Loss=1.0700, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0666

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.3%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (19.8%)
Actor update: 0.11s (42.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000005
Q1 loss: 1.311600
Q2 loss: 1.311600
Current threshold: -33.9280
Global Scale Offset: 0.0798
Reward stats: mean=0.0000, std=0.0895, count=329
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.3116, Q2 Loss: 1.3116, Entropy: 0.0021, Mean TD Error: 1.2826, Threshold: -33.9280
tensor([ 0.0447,  0.5894,  0.5892,  0.4055, -0.0757,  0.5376,  0.7264,  0.9831,
         1.2602,  0.3689,  0.3604,  0.9043,  0.0376,  0.0230, -0.2010,  3.1873],
       device='cuda:0')
Original likelihood: -16.969951629638672
Adjusted likelihood: -16.969951629638672
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.688145592983346
Current ori: tensor([ 0.0376,  0.0230, -0.2010], device='cuda:0')
Middle force: tensor([0.5592, 0.5672, 0.8453, 0.6902, 0.7086, 0.5481, 0.5521, 0.5178],
       device='cuda:0')
Thumb force: tensor([0.9086, 0.6707, 1.1390, 0.5427, 0.6231, 0.5944, 0.5253, 0.5391],
       device='cuda:0')
Index force: tensor([0.5434, 0.5098, 0.5451, 0.5117, 0.5285, 0.5344, 0.5004, 0.5644],
       device='cuda:0')
Storing NORMAL transition: reward=0.1019 (scaled=0.1019), steps=1
Reward stats updated: mean 0.0000 -> 0.0003, std: 0.0895
Collected 330 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6739, Q2 Loss=0.6739, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5005
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0031, Q2 Loss=1.0031, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8903
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9257, Q2 Loss=0.9257, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3283
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1716, Q2 Loss=1.1716, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9131
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5750, Q2 Loss=1.5750, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3095

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.9%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (41.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 1.069849
Q2 loss: 1.069849
Current threshold: -33.9388
Global Scale Offset: 0.0797
Reward stats: mean=0.0003, std=0.0895, count=330
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.0698, Q2 Loss: 1.0698, Entropy: 0.0000, Mean TD Error: 0.7884, Threshold: -33.9388
tensor([ 0.0699,  0.6243,  0.5228,  0.4857, -0.0592,  0.5183,  0.7568,  1.0201,
         1.3254,  0.2977,  0.3318,  0.8471,  0.0360,  0.0110, -0.3029,  3.3500],
       device='cuda:0')
Original likelihood: -14.527206420898438
Adjusted likelihood: -14.527206420898438
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.647013169014826
Current ori: tensor([ 0.0360,  0.0110, -0.3029], device='cuda:0')
Middle force: tensor([0.5688, 0.8290, 0.6899, 0.7061, 0.5602, 0.5027, 0.5237],
       device='cuda:0')
Thumb force: tensor([0.6607, 1.1153, 0.5448, 0.6180, 0.5975, 0.5734, 1.1469],
       device='cuda:0')
Index force: tensor([0.5171, 0.5503, 0.5126, 0.5396, 0.5364, 0.8294, 0.5118],
       device='cuda:0')
Storing NORMAL transition: reward=0.1819 (scaled=0.1819), steps=1
Reward stats updated: mean 0.0003 -> 0.0009, std: 0.0899
Collected 331 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2022, Q2 Loss=1.2022, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4747
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.2691, Q2 Loss=1.2691, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1487
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1518, Q2 Loss=1.1518, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9099
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.9813, Q2 Loss=1.9813, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.2250
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.0312, Q2 Loss=1.0312, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4835

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.7%)
Q1 update: 0.06s (19.9%)
Q2 update: 0.06s (20.3%)
Actor update: 0.12s (42.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.327130
Q2 loss: 1.327130
Current threshold: -33.9452
Global Scale Offset: 0.0796
Reward stats: mean=0.0009, std=0.0899, count=331
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.3271, Q2 Loss: 1.3271, Entropy: 0.0000, Mean TD Error: 1.6484, Threshold: -33.9452
tensor([-0.1018,  0.5279,  0.4855,  0.6104,  0.0278,  0.5034,  0.8632,  0.9194,
         1.2895,  0.3791,  0.3590,  0.6932,  0.0190, -0.0354, -0.4905,  3.9033],
       device='cuda:0')
Original likelihood: -23.666034698486328
Adjusted likelihood: -23.666034698486328
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.443791627010796
Current ori: tensor([ 0.0190, -0.0354, -0.4905], device='cuda:0')
Middle force: tensor([0.5058, 0.5024, 0.6497, 0.5313, 0.5119, 0.5179], device='cuda:0')
Thumb force: tensor([0.5433, 0.5437, 0.6732, 0.5541, 0.5045, 0.5640], device='cuda:0')
Index force: tensor([0.5603, 0.7802, 0.5256, 0.5483, 0.5011, 0.5953], device='cuda:0')
Storing NORMAL transition: reward=-0.0106 (scaled=-0.0106), steps=1
Reward stats updated: mean 0.0009 -> 0.0009, std: 0.0898
Collected 332 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9746, Q2 Loss=0.9746, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8202
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.3708, Q2 Loss=1.3708, Entropy=0.0002, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2366
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7270, Q2 Loss=0.7270, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8946
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7788, Q2 Loss=0.7788, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4294
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8098, Q2 Loss=0.8098, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1820

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.8%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (42.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 0.932195
Q2 loss: 0.932195
Current threshold: -33.9490
Global Scale Offset: 0.0795
Reward stats: mean=0.0009, std=0.0898, count=332
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 0.9322, Q2 Loss: 0.9322, Entropy: 0.0000, Mean TD Error: 0.7126, Threshold: -33.9490
tensor([-0.0891,  0.5612,  0.5932,  0.6316,  0.1001,  0.5540,  0.7573,  0.9229,
         1.3225,  0.2388,  0.2906,  0.7092,  0.0430, -0.1016, -0.5626,  4.2193],
       device='cuda:0')
Original likelihood: -41.5390625
Adjusted likelihood: -41.5390625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 40.709800720214844
Projection step: 1, Loss: 38.90681838989258
Projection step: 2, Loss: 38.952674865722656
Projection step: 3, Loss: 39.45330047607422
Projection step: 4, Loss: 39.65453338623047
Projection step: 5, Loss: 39.64033889770508
Projection step: 6, Loss: 40.104469299316406
Projection step: 7, Loss: 38.62382507324219
Projection step: 8, Loss: 36.674957275390625
Projection step: 9, Loss: 38.528289794921875
Projection step: 10, Loss: 38.45973587036133
Projection step: 11, Loss: 36.985504150390625
Projection step: 12, Loss: 36.25359344482422
Projection step: 13, Loss: 37.97489929199219
Projection step: 14, Loss: 36.930484771728516
Projection step: 15, Loss: 35.77423858642578
Projection step: 16, Loss: 36.150123596191406
Projection step: 17, Loss: 36.534149169921875
Projection step: 18, Loss: 34.91767120361328
Projection step: 19, Loss: 35.42649459838867
Projection step: 20, Loss: 35.1846923828125
Projection step: 21, Loss: 36.98006820678711
Projection step: 22, Loss: 35.11433792114258
Projection step: 23, Loss: 33.99492645263672
Projection step: 24, Loss: 33.65685272216797
Final likelihood: tensor([-32.8401, -37.7050, -35.8632, -37.1648, -33.6718, -37.8394, -32.8364,
        -37.8211, -31.9328, -33.4953, -31.6667, -32.6367, -34.3376, -36.5872,
        -36.9859, -33.5609])
Final projection likelihood: -34.8091
1 mode projection failed, trying anyway
New goal: tensor([-0.0333,  0.4988,  0.6011,  0.6312,  0.0710,  0.5740,  0.7731,  0.8670,
         1.3694,  0.1955,  0.2748,  0.7689,  0.0398, -0.0963, -0.1043],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0104]], device='cuda:0') tensor([[0.0021]], device='cuda:0')
Original likelihood: -41.132083892822266
Adjusted likelihood: -41.132083892822266
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 41.132083892822266}
Current yaw: tensor([ 0.0430, -0.1016, -0.5626], device='cuda:0')
6 thumb_middle
tensor([-0.0891,  0.5612,  0.5932,  0.6316,  0.1001,  0.5540,  0.7573,  0.9229,
         1.3225,  0.2388,  0.2906,  0.7092,  0.0430, -0.1016, -0.5626,  4.2193],
       device='cuda:0')
Solve time for step 1 9.094518747006077
Current ori: tensor([ 0.0430, -0.1016, -0.5626], device='cuda:0')
Index force: tensor([0.5897, 0.5737, 0.5793, 0.5904], device='cuda:0')
tensor([-0.1043,  0.5734,  0.6710,  0.6832,  0.0447,  0.5879,  0.7598,  0.8636,
         1.3084,  0.1892,  0.1769,  0.7177,  0.1074, -0.2556, -0.5647,  4.5860],
       device='cuda:0')
Solve time for step 2 3.7413973940419964
Current ori: tensor([ 0.1074, -0.2556, -0.5647], device='cuda:0')
Index force: tensor([0.5596, 0.5660, 0.5794], device='cuda:0')
tensor([-0.1054,  0.6474,  0.7392,  0.7024,  0.0482,  0.6533,  0.7953,  0.8781,
         1.2923,  0.1978,  0.1335,  0.7133,  0.1668, -0.3475, -0.6324,  4.6309],
       device='cuda:0')
Solve time for step 3 3.524289946013596
Current ori: tensor([ 0.1668, -0.3475, -0.6324], device='cuda:0')
Index force: tensor([0.5597, 0.5569], device='cuda:0')
tensor([ 0.0046,  0.8661,  0.8619,  0.7166,  0.1454,  0.7067,  0.8392,  0.8894,
         1.2727,  0.1743,  0.0435,  0.6820,  0.1730, -0.3814, -0.7100,  4.5298],
       device='cuda:0')
Solve time for step 4 3.285055263026152
Current ori: tensor([ 0.1730, -0.3814, -0.7100], device='cuda:0')
Index force: tensor([0.5480], device='cuda:0')
Storing RECOVERY transition: reward=-0.3221 (scaled=-0.0460), steps=7
Reward stats updated: mean 0.0009 -> 0.0007, std: 0.0897
Collected 333 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5220, Q2 Loss=1.5220, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1470
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2378, Q2 Loss=1.2378, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2166
SAC Update 3/5: Actor Loss=-0.0259, Q1 Loss=1.1861, Q2 Loss=1.1861, Entropy=0.0742, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7210
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8652, Q2 Loss=0.8652, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7646
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.9278, Q2 Loss=0.9278, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8183

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (20.5%)
Q2 update: 0.05s (18.7%)
Actor update: 0.09s (38.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.051237
Q1 loss: 1.147796
Q2 loss: 1.147796
Current threshold: -33.9601
Global Scale Offset: 0.0794
Reward stats: mean=0.0007, std=0.0897, count=333
----------------------------------------------
SAC Update - Actor Loss: -0.0512, Q1 Loss: 1.1478, Q2 Loss: 1.1478, Entropy: 0.0148, Mean TD Error: 0.9335, Threshold: -33.9601
Original likelihood: -314.01739501953125
Adjusted likelihood: -314.01739501953125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 22
Loaded trajectory sampler
Current yaw: tensor([-0.0018,  0.0148, -0.0223], device='cuda:0')
Current yaw: tensor([-0.0018,  0.0148, -0.0223], device='cuda:0')
1 turn
Sampling time 3.7991951649892144
tensor([ 0.1438,  0.6252,  0.5786,  0.5321, -0.0990,  0.4908,  0.9327,  0.9051,
         1.2627,  0.2453,  0.2123,  1.2260, -0.0018,  0.0148, -0.0223,  0.3724],
       device='cuda:0')
Original likelihood: -17.790752410888672
Adjusted likelihood: -17.790752410888672
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.967689864977729
Current ori: tensor([-0.0018,  0.0148, -0.0223], device='cuda:0')
Middle force: tensor([0.6616, 0.7107, 0.5343, 0.5205, 0.6295, 0.9788, 1.0278, 0.5786, 0.5019,
        1.2841, 0.5046, 0.5120], device='cuda:0')
Thumb force: tensor([0.5553, 2.3779, 0.6329, 1.5427, 1.0661, 0.8642, 1.9924, 0.6131, 0.7445,
        0.5741, 0.5915, 0.6928], device='cuda:0')
Index force: tensor([0.5894, 0.5079, 0.5978, 0.5603, 0.5696, 0.5222, 0.5880, 0.6105, 0.7294,
        0.5197, 0.5265, 0.6356], device='cuda:0')
Storing NORMAL transition: reward=0.0172 (scaled=0.0172), steps=1
Reward stats updated: mean 0.0007 -> 0.0008, std: 0.0896
Collected 334 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8673, Q2 Loss=0.8673, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7694
SAC Update 2/5: Actor Loss=-0.1017, Q1 Loss=1.2176, Q2 Loss=1.2176, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4136
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.5745, Q2 Loss=1.5745, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3374
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9212, Q2 Loss=0.9212, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6377
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3603, Q2 Loss=1.3603, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0804

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (19.4%)
Q1 update: 0.04s (19.6%)
Q2 update: 0.04s (17.7%)
Actor update: 0.08s (39.1%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.066383
Q1 loss: 1.188166
Q2 loss: 1.188166
Current threshold: -33.9712
Global Scale Offset: 0.0792
Reward stats: mean=0.0008, std=0.0896, count=334
----------------------------------------------
SAC Update - Actor Loss: -0.0664, Q1 Loss: 1.1882, Q2 Loss: 1.1882, Entropy: 0.0000, Mean TD Error: 1.0477, Threshold: -33.9712
tensor([ 0.1255,  0.6770,  0.4898,  0.5116, -0.0980,  0.2881,  1.0599,  1.0272,
         1.3991,  0.2327,  0.1539,  1.1237, -0.0129,  0.0484, -0.0417, -0.0155],
       device='cuda:0')
Original likelihood: -29.073131561279297
Adjusted likelihood: -29.073131561279297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.563112268981058
Current ori: tensor([-0.0129,  0.0484, -0.0417], device='cuda:0')
Middle force: tensor([0.7044, 0.5332, 0.5197, 0.6180, 0.9500, 0.9964, 0.5754, 0.5012, 1.2486,
        0.5040, 0.5114], device='cuda:0')
Thumb force: tensor([2.3183, 0.6291, 1.5207, 1.0680, 0.8677, 1.9731, 0.6140, 0.7702, 0.5714,
        0.5882, 0.6850], device='cuda:0')
Index force: tensor([0.5073, 0.5880, 0.5556, 0.5632, 0.5215, 0.5816, 0.6000, 0.7115, 0.5213,
        0.5252, 0.6299], device='cuda:0')
Storing NORMAL transition: reward=0.2430 (scaled=0.2430), steps=1
Reward stats updated: mean 0.0008 -> 0.0015, std: 0.0904
Collected 335 transitions for RL
SAC Update 1/5: Actor Loss=-0.0443, Q1 Loss=0.8371, Q2 Loss=0.8371, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1474
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.1574, Q2 Loss=1.1574, Entropy=0.0003, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8243
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=3.4463, Q2 Loss=3.4463, Entropy=0.0127, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.5951
SAC Update 4/5: Actor Loss=-0.0066, Q1 Loss=0.6755, Q2 Loss=0.6755, Entropy=0.3464, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2991
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9812, Q2 Loss=0.9812, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6742

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.4%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (19.4%)
Actor update: 0.10s (40.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.010192
Q1 loss: 1.419496
Q2 loss: 1.419496
Current threshold: -33.9807
Global Scale Offset: 0.0791
Reward stats: mean=0.0015, std=0.0904, count=335
----------------------------------------------
SAC Update - Actor Loss: -0.0102, Q1 Loss: 1.4195, Q2 Loss: 1.4195, Entropy: 0.0719, Mean TD Error: 1.3080, Threshold: -33.9807
tensor([ 0.2175,  0.6194,  0.4503,  0.3984, -0.1283,  0.2303,  1.0836,  1.1287,
         1.4851,  0.2168,  0.0865,  1.1063, -0.0280,  0.0830, -0.2973, -0.8060],
       device='cuda:0')
Original likelihood: -39.46800994873047
Adjusted likelihood: -39.46800994873047
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 40.64161682128906
Projection step: 1, Loss: 38.02887725830078
Projection step: 2, Loss: 38.18815612792969
Projection step: 3, Loss: 35.34965515136719
Projection step: 4, Loss: 35.722267150878906
Projection step: 5, Loss: 35.94950866699219
Projection step: 6, Loss: 35.70275115966797
Projection step: 7, Loss: 36.007041931152344
Projection step: 8, Loss: 34.47096633911133
Projection step: 9, Loss: 33.91791534423828
Projection step: 10, Loss: 34.65516662597656
Projection step: 11, Loss: 36.149635314941406
Projection step: 12, Loss: 32.99626159667969
Projection step: 13, Loss: 31.717105865478516
Projection step: 14, Loss: 31.671245574951172
Projection step: 15, Loss: 30.585365295410156
Projection step: 16, Loss: 31.806739807128906
Projection step: 17, Loss: 30.77831268310547
Projection step: 18, Loss: 30.22398567199707
Projection step: 19, Loss: 30.200387954711914
Projection step: 20, Loss: 30.429903030395508
Projection step: 21, Loss: 30.363086700439453
Projection step: 22, Loss: 29.96227264404297
Projection step: 23, Loss: 30.0744571685791
Projection step: 24, Loss: 29.317291259765625
Final likelihood: tensor([-27.9186, -28.5261, -31.0102, -30.4954, -32.8563, -27.6575, -27.1528,
        -28.9121, -28.4538, -27.8683, -29.6305, -26.1991, -27.1031, -30.5942,
        -29.9376, -29.7317])
Final projection likelihood: -29.0030
1 mode projection succeeded
New goal: tensor([ 0.1860,  0.6568,  0.5005,  0.4886, -0.1554,  0.2552,  0.9294,  1.0854,
         1.4629,  0.2903,  0.0886,  1.1464, -0.0340,  0.0731, -0.7481],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0139]], device='cuda:0')
Original likelihood: -27.691293716430664
Adjusted likelihood: -27.691293716430664
Likelihood residual: 0.0
{'index': 27.691293716430664, 'thumb_middle': inf}
Current yaw: tensor([-0.0280,  0.0830, -0.2973], device='cuda:0')
2 index
tensor([ 0.2175,  0.6194,  0.4503,  0.3984, -0.1283,  0.2303,  1.0836,  1.1287,
         1.4851,  0.2168,  0.0865,  1.1063, -0.0280,  0.0830, -0.2973, -0.8060],
       device='cuda:0')
Solve time for step 1 10.357103742018808
Current ori: tensor([-0.0280,  0.0830, -0.2973], device='cuda:0')
Middle force: tensor([0.5685, 0.5452, 0.5568, 0.5793], device='cuda:0')
Thumb force: tensor([0.6005, 0.5370, 0.5952, 0.6050], device='cuda:0')
tensor([ 0.2153,  0.6211,  0.4680,  0.4602, -0.1354,  0.2673,  1.0213,  1.1501,
         1.4613,  0.2627,  0.1104,  1.0892, -0.0353,  0.0869, -0.2694, -1.0368],
       device='cuda:0')
Solve time for step 2 4.127772420994006
Current ori: tensor([-0.0353,  0.0869, -0.2694], device='cuda:0')
Middle force: tensor([0.5425, 0.5534, 0.5755], device='cuda:0')
Thumb force: tensor([0.5322, 0.5903, 0.6021], device='cuda:0')
tensor([ 0.2118,  0.6260,  0.4726,  0.4724, -0.1271,  0.2909,  1.0023,  1.1363,
         1.4675,  0.2514,  0.1037,  1.0713, -0.0456,  0.0837, -0.2713, -0.7050],
       device='cuda:0')
Solve time for step 3 4.064239100029226
Current ori: tensor([-0.0456,  0.0837, -0.2713], device='cuda:0')
Middle force: tensor([0.5523, 0.5713], device='cuda:0')
Thumb force: tensor([0.5791, 0.5990], device='cuda:0')
tensor([ 0.2138,  0.6269,  0.4728,  0.4731, -0.1120,  0.3018,  0.9985,  1.1337,
         1.4739,  0.2432,  0.0810,  1.0834, -0.0547,  0.0787, -0.3168,  0.0225],
       device='cuda:0')
Solve time for step 4 3.957278940011747
Current ori: tensor([-0.0547,  0.0787, -0.3168], device='cuda:0')
Middle force: tensor([0.5136], device='cuda:0')
Thumb force: tensor([0.5009], device='cuda:0')
Storing RECOVERY transition: reward=0.0216 (scaled=0.0108), steps=2
Reward stats updated: mean 0.0015 -> 0.0015, std: 0.0903
Collected 336 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.7352, Q2 Loss=1.7352, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3510
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.9226, Q2 Loss=1.9226, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8147
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.8977, Q2 Loss=1.8977, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0379
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=2.9336, Q2 Loss=2.9336, Entropy=0.0139, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.4084
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.7516, Q2 Loss=2.7516, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8591

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.3%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (18.5%)
Actor update: 0.10s (39.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046056
Q1 loss: 2.248135
Q2 loss: 2.248135
Current threshold: -33.9897
Global Scale Offset: 0.0790
Reward stats: mean=0.0015, std=0.0903, count=336
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 2.2481, Q2 Loss: 2.2481, Entropy: 0.0028, Mean TD Error: 2.2942, Threshold: -33.9897
Original likelihood: -30.19363021850586
Adjusted likelihood: -30.19363021850586
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0594,  0.0777, -0.3246], device='cuda:0')
3 turn
Sampling time 3.73151749803219
tensor([ 0.1387,  0.6797,  0.5117,  0.4928, -0.1094,  0.3101,  0.9945,  1.1298,
         1.4614,  0.2642,  0.0777,  1.1067, -0.0594,  0.0777, -0.3246,  0.3302],
       device='cuda:0')
Original likelihood: -28.890838623046875
Adjusted likelihood: -28.890838623046875
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.131278115033638
Current ori: tensor([-0.0594,  0.0777, -0.3246], device='cuda:0')
Middle force: tensor([0.5597, 0.5288, 1.0841, 0.5602, 0.6055, 0.5442, 0.8359, 0.5686, 0.5211,
        0.5205, 0.5612, 0.5013], device='cuda:0')
Thumb force: tensor([0.6363, 0.6545, 0.6060, 0.5581, 0.9937, 0.6293, 0.6106, 0.5789, 0.5753,
        0.6157, 0.5232, 0.5618], device='cuda:0')
Index force: tensor([0.5668, 0.9790, 0.7076, 0.5215, 0.5784, 0.6054, 0.5123, 0.5882, 0.6623,
        0.5291, 0.4925, 0.6897], device='cuda:0')
Storing NORMAL transition: reward=-0.0398 (scaled=-0.0398), steps=1
Reward stats updated: mean 0.0015 -> 0.0014, std: 0.0902
Collected 337 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=3.5740, Q2 Loss=3.5740, Entropy=0.0000, Time=0.10sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0910
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1080, Q2 Loss=1.1080, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5095
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9145, Q2 Loss=0.9145, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4912
SAC Update 4/5: Actor Loss=-0.1666, Q1 Loss=1.6624, Q2 Loss=1.6624, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6222
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.6535, Q2 Loss=0.6535, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7902

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.6%)
Q1 update: 0.04s (16.4%)
Q2 update: 0.04s (14.9%)
Actor update: 0.12s (48.0%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.079363
Q1 loss: 1.582474
Q2 loss: 1.582474
Current threshold: -33.9950
Global Scale Offset: 0.0790
Reward stats: mean=0.0014, std=0.0902, count=337
----------------------------------------------
SAC Update - Actor Loss: -0.0794, Q1 Loss: 1.5825, Q2 Loss: 1.5825, Entropy: 0.0000, Mean TD Error: 1.7008, Threshold: -33.9950
tensor([ 0.1233,  0.6680,  0.5032,  0.5163, -0.1174,  0.3705,  0.9392,  1.0856,
         1.4646,  0.2529,  0.0481,  1.1515, -0.0684,  0.0745, -0.2847,  0.4807],
       device='cuda:0')
Original likelihood: -29.055288314819336
Adjusted likelihood: -29.055288314819336
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.442680887004826
Current ori: tensor([-0.0684,  0.0745, -0.2847], device='cuda:0')
Middle force: tensor([0.5287, 1.0521, 0.5572, 0.6026, 0.5388, 0.8238, 0.5637, 0.5196, 0.5191,
        0.5616, 0.5005], device='cuda:0')
Thumb force: tensor([0.6355, 0.6044, 0.5551, 0.9654, 0.6294, 0.6046, 0.5721, 0.5674, 0.6068,
        0.5198, 0.5462], device='cuda:0')
Index force: tensor([0.9587, 0.6944, 0.5205, 0.5789, 0.5983, 0.5111, 0.5847, 0.6548, 0.5258,
        0.5044, 0.7296], device='cuda:0')
Storing NORMAL transition: reward=0.0706 (scaled=0.0706), steps=1
Reward stats updated: mean 0.0014 -> 0.0016, std: 0.0901
Collected 338 transitions for RL
SAC Update 1/5: Actor Loss=-0.0001, Q1 Loss=1.0512, Q2 Loss=1.0512, Entropy=0.0180, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7075
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.9432, Q2 Loss=0.9432, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7378
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.7578, Q2 Loss=2.7578, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8465
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=3.3767, Q2 Loss=3.3767, Entropy=0.0151, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6217
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7029, Q2 Loss=0.7029, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3895

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.3%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (18.6%)
Actor update: 0.11s (41.6%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046068
Q1 loss: 1.766357
Q2 loss: 1.766357
Current threshold: -33.9983
Global Scale Offset: 0.0790
Reward stats: mean=0.0016, std=0.0901, count=338
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.7664, Q2 Loss: 1.7664, Entropy: 0.0066, Mean TD Error: 1.6606, Threshold: -33.9983
tensor([ 0.1596,  0.6841,  0.5987,  0.6406, -0.1524,  0.2940,  0.9730,  1.1513,
         1.5000,  0.1917,  0.0588,  1.2096, -0.0566,  0.1067, -0.3611,  0.3831],
       device='cuda:0')
Original likelihood: -36.73089599609375
Adjusted likelihood: -36.73089599609375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 38.29739761352539
Projection step: 1, Loss: 33.476966857910156
Projection step: 2, Loss: 36.70064163208008
Projection step: 3, Loss: 34.840057373046875
Projection step: 4, Loss: 34.33293914794922
Projection step: 5, Loss: 36.48503875732422
Projection step: 6, Loss: 35.53589630126953
Projection step: 7, Loss: 33.338436126708984
Projection step: 8, Loss: 33.82508850097656
Projection step: 9, Loss: 33.33727264404297
Projection step: 10, Loss: 33.97895431518555
Projection step: 11, Loss: 33.08222198486328
Projection step: 12, Loss: 33.16218566894531
Projection step: 13, Loss: 32.2529182434082
Projection step: 14, Loss: 32.43629837036133
Projection step: 15, Loss: 32.71950149536133
Projection step: 16, Loss: 31.022624969482422
Projection step: 17, Loss: 31.83453941345215
Projection step: 18, Loss: 29.925251007080078
Projection step: 19, Loss: 31.866477966308594
Projection step: 20, Loss: 33.213401794433594
Projection step: 21, Loss: 31.264286041259766
Projection step: 22, Loss: 31.116565704345703
Projection step: 23, Loss: 32.56542205810547
Projection step: 24, Loss: 30.858837127685547
Final likelihood: tensor([-28.2773, -32.7062, -38.8717, -32.1996, -34.8754, -32.1466, -28.4518,
        -34.1895, -26.9058, -36.7880, -32.8100, -31.3477, -26.9093, -33.5135,
        -31.2299, -29.4425])
Final projection likelihood: -31.9166
1 mode projection succeeded
New goal: tensor([ 0.1456,  0.6599,  0.5466,  0.5983, -0.1608,  0.3047,  0.8346,  1.0459,
         1.4725,  0.2314,  0.0839,  1.1536, -0.0580,  0.0991,  0.3016],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[0.0020]], device='cuda:0')
Original likelihood: -26.325912475585938
Adjusted likelihood: -26.325912475585938
Likelihood residual: 0.0
Original likelihood: -29.508161544799805
Adjusted likelihood: -29.508161544799805
Likelihood residual: 0.0
{'index': 29.508161544799805, 'thumb_middle': 26.325912475585938}
Current yaw: tensor([-0.0566,  0.1067, -0.3611], device='cuda:0')
4 thumb_middle
tensor([ 0.1596,  0.6841,  0.5987,  0.6406, -0.1524,  0.2940,  0.9730,  1.1513,
         1.5000,  0.1917,  0.0588,  1.2096, -0.0566,  0.1067, -0.3611,  0.3831],
       device='cuda:0')
Solve time for step 1 9.213425141992047
Current ori: tensor([-0.0566,  0.1067, -0.3611], device='cuda:0')
Index force: tensor([0.4997, 0.5806, 0.4999, 0.5010], device='cuda:0')
tensor([ 0.1362,  0.7018,  0.6133,  0.6295, -0.2332,  0.2900,  0.8443,  1.0568,
         1.4363,  0.1989,  0.0276,  1.1488, -0.1443,  0.2751, -0.3619, -0.0733],
       device='cuda:0')
Solve time for step 2 3.570138794952072
Current ori: tensor([-0.1443,  0.2751, -0.3619], device='cuda:0')
Index force: tensor([0.5521, 0.5000, 0.5003], device='cuda:0')
tensor([ 0.0295,  0.7625,  0.6491,  0.6817, -0.2540,  0.2944,  0.8299,  1.0440,
         1.4621,  0.2152,  0.0475,  1.1471, -0.3407,  0.5743, -0.3912,  1.1678],
       device='cuda:0')
Solve time for step 3 3.561417802993674
Current ori: tensor([-0.3407,  0.5743, -0.3912], device='cuda:0')
Index force: tensor([0.5002, 0.5042], device='cuda:0')
tensor([-0.2661,  0.9594,  0.8483,  0.7654, -0.2445,  0.3019,  0.8354,  1.0466,
         1.4868,  0.2277,  0.0598,  1.1386, -0.6144,  1.1935, -0.3912,  1.9058],
       device='cuda:0')
Solve time for step 4 3.235215556982439
Current ori: tensor([-0.6144,  1.1935, -0.3912], device='cuda:0')
Index force: tensor([0.5049], device='cuda:0')
Storing RECOVERY transition: reward=-1.4894 (scaled=-0.7447), steps=2
Reward stats updated: mean 0.0016 -> -0.0006, std: 0.0986
Collected 339 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.1420, Q2 Loss=1.1420, Entropy=0.0004, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0343
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1188, Q2 Loss=1.1188, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1164
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2431, Q2 Loss=1.2431, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4282
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2451, Q2 Loss=1.2451, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8859
SAC Update 5/5: Actor Loss=-0.0001, Q1 Loss=1.0332, Q2 Loss=1.0332, Entropy=0.0187, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9493

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.1%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.1%)
Actor update: 0.08s (39.2%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000012
Q1 loss: 1.156435
Q2 loss: 1.156435
Current threshold: -34.0004
Global Scale Offset: 0.0789
Reward stats: mean=-0.0006, std=0.0986, count=339
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.1564, Q2 Loss: 1.1564, Entropy: 0.0038, Mean TD Error: 1.0828, Threshold: -34.0004
Original likelihood: -1123.94970703125
Adjusted likelihood: -1123.94970703125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 23
Loaded trajectory sampler
Current yaw: tensor([-0.0012,  0.0145, -0.0304], device='cuda:0')
Current yaw: tensor([-0.0012,  0.0145, -0.0304], device='cuda:0')
1 turn
Sampling time 3.6062775569735095
tensor([ 0.1250,  0.6308,  0.5431,  0.5494, -0.1115,  0.5092,  0.9268,  0.9206,
         1.2233,  0.2733,  0.2574,  1.1958, -0.0012,  0.0145, -0.0304,  0.0389],
       device='cuda:0')
Original likelihood: -18.37611198425293
Adjusted likelihood: -18.37611198425293
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.867107721976936
Current ori: tensor([-0.0012,  0.0145, -0.0304], device='cuda:0')
Middle force: tensor([0.7911, 0.5097, 0.5491, 0.8912, 0.9359, 0.9336, 0.8706, 0.7867, 0.5907,
        1.0241, 0.5753, 0.5380], device='cuda:0')
Thumb force: tensor([0.8000, 3.3693, 1.3559, 0.5943, 0.6964, 0.9024, 0.8449, 0.7393, 0.5233,
        0.7317, 0.9745, 0.6104], device='cuda:0')
Index force: tensor([0.7334, 0.5286, 0.5452, 0.8144, 0.5580, 0.5592, 0.5613, 0.5433, 0.5784,
        0.5357, 0.5850, 0.7307], device='cuda:0')
Storing NORMAL transition: reward=0.0012 (scaled=0.0012), steps=1
Reward stats updated: mean -0.0006 -> -0.0006, std: 0.0985
Collected 340 transitions for RL
SAC Update 1/5: Actor Loss=-0.1701, Q1 Loss=1.0757, Q2 Loss=1.0757, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1211
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0995, Q2 Loss=1.0995, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1495
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0467, Q2 Loss=1.0467, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6168
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7773, Q2 Loss=0.7773, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5811
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0104, Q2 Loss=1.0104, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5238

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.2%)
Q1 update: 0.05s (19.0%)
Q2 update: 0.05s (18.3%)
Actor update: 0.11s (41.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.034014
Q1 loss: 1.001902
Q2 loss: 1.001902
Current threshold: -34.0017
Global Scale Offset: 0.0789
Reward stats: mean=-0.0006, std=0.0985, count=340
----------------------------------------------
SAC Update - Actor Loss: -0.0340, Q1 Loss: 1.0019, Q2 Loss: 1.0019, Entropy: 0.0000, Mean TD Error: 0.5985, Threshold: -34.0017
tensor([ 0.1955,  0.6043,  0.6067,  0.6413, -0.3031,  0.4656,  1.0491,  0.9701,
         1.2377,  0.3255,  0.1677,  1.1062,  0.0130, -0.0226, -0.0320,  0.1654],
       device='cuda:0')
Original likelihood: -49.72344207763672
Adjusted likelihood: -49.72344207763672
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 50.68536376953125
Projection step: 1, Loss: 49.248069763183594
Projection step: 2, Loss: 49.57581329345703
Projection step: 3, Loss: 48.1826171875
Projection step: 4, Loss: 48.20722198486328
Projection step: 5, Loss: 45.83995819091797
Projection step: 6, Loss: 45.868045806884766
Projection step: 7, Loss: 45.173118591308594
Projection step: 8, Loss: 44.55231475830078
Projection step: 9, Loss: 44.02659606933594
Projection step: 10, Loss: 45.39775466918945
Projection step: 11, Loss: 42.723270416259766
Projection step: 12, Loss: 41.60188293457031
Projection step: 13, Loss: 40.89543914794922
Projection step: 14, Loss: 40.068572998046875
Projection step: 15, Loss: 40.20962905883789
Projection step: 16, Loss: 43.61231994628906
Projection step: 17, Loss: 41.05091094970703
Projection step: 18, Loss: 39.771217346191406
Projection step: 19, Loss: 37.11189270019531
Projection step: 20, Loss: 37.78226852416992
Projection step: 21, Loss: 35.234893798828125
Projection step: 22, Loss: 35.87051773071289
Projection step: 23, Loss: 34.3068733215332
Projection step: 24, Loss: 35.707679748535156
Final likelihood: tensor([-33.7020, -36.3978, -32.1651, -30.5561, -31.8091, -33.3525, -33.0666,
        -34.7485, -31.7519, -31.9640, -33.5520, -30.0772, -37.3795, -31.1098,
        -31.6722, -32.5427])
Final projection likelihood: -32.8654
1 mode projection succeeded
New goal: tensor([ 0.1696,  0.5854,  0.5652,  0.6157, -0.1872,  0.5233,  1.0449,  1.0379,
         1.2685,  0.3162,  0.1786,  1.0722,  0.0109, -0.0168, -0.0753],
       device='cuda:0')
tensor([[0.0064]], device='cuda:0') tensor([[0.0160]], device='cuda:0') tensor([[0.0030]], device='cuda:0')
Original likelihood: -28.253625869750977
Adjusted likelihood: -28.253625869750977
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 28.253625869750977}
Current yaw: tensor([ 0.0130, -0.0226, -0.0320], device='cuda:0')
2 thumb_middle
tensor([ 0.1955,  0.6043,  0.6067,  0.6413, -0.3031,  0.4656,  1.0491,  0.9701,
         1.2377,  0.3255,  0.1677,  1.1062,  0.0130, -0.0226, -0.0320,  0.1654],
       device='cuda:0')
Solve time for step 1 8.927959996974096
Current ori: tensor([ 0.0130, -0.0226, -0.0320], device='cuda:0')
Index force: tensor([0.5738, 0.5938, 0.5874, 0.5046], device='cuda:0')
tensor([ 0.1931,  0.6141,  0.5939,  0.6363, -0.2825,  0.4945,  1.0204,  1.0153,
         1.2434,  0.3083,  0.1111,  1.0485,  0.0103, -0.0222, -0.0320,  0.1591],
       device='cuda:0')
Solve time for step 2 3.547547564958222
Current ori: tensor([ 0.0103, -0.0222, -0.0320], device='cuda:0')
Index force: tensor([0.5009, 0.5826, 0.5904], device='cuda:0')
tensor([ 0.1902,  0.6236,  0.5849,  0.6225, -0.2827,  0.5018,  1.0147,  1.0234,
         1.2497,  0.3056,  0.1066,  1.0404,  0.0070, -0.0216, -0.0320,  0.1494],
       device='cuda:0')
Solve time for step 3 3.399316215014551
Current ori: tensor([ 0.0070, -0.0216, -0.0320], device='cuda:0')
Index force: tensor([0.5717, 0.5806], device='cuda:0')
tensor([ 0.1822,  0.6258,  0.5785,  0.6127, -0.2828,  0.5048,  1.0138,  1.0198,
         1.2543,  0.3071,  0.1045,  1.0316,  0.0055, -0.0176, -0.0320,  0.1360],
       device='cuda:0')
Solve time for step 4 3.2491688239970244
Current ori: tensor([ 0.0055, -0.0176, -0.0320], device='cuda:0')
Index force: tensor([0.5710], device='cuda:0')
Storing RECOVERY transition: reward=0.0061 (scaled=0.0061), steps=1
Reward stats updated: mean -0.0006 -> -0.0006, std: 0.0984
Collected 341 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6832, Q2 Loss=0.6832, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7388
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.4213, Q2 Loss=2.4213, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6099
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.0820, Q2 Loss=1.0820, Entropy=0.0157, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5037
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4083, Q2 Loss=1.4083, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1894
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7376, Q2 Loss=0.7376, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5337

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (19.9%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.04s (18.2%)
Actor update: 0.09s (38.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000001
Q1 loss: 1.266480
Q2 loss: 1.266480
Current threshold: -34.0025
Global Scale Offset: 0.0789
Reward stats: mean=-0.0006, std=0.0984, count=341
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.2665, Q2 Loss: 1.2665, Entropy: 0.0031, Mean TD Error: 1.3151, Threshold: -34.0025
Original likelihood: -33.9075927734375
Adjusted likelihood: -33.9075927734375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.6243)
Current yaw: tensor([ 0.0141, -0.0016, -0.0377], device='cuda:0')
3 turn
Sampling time 3.7565078010084108
tensor([ 0.1549,  0.5938,  0.5778,  0.6413, -0.2303,  0.5452,  1.0567,  1.0355,
         1.3281,  0.3249,  0.1773,  1.0496,  0.0141, -0.0016, -0.0377,  0.1491],
       device='cuda:0')
Original likelihood: -33.467079162597656
Adjusted likelihood: -33.467079162597656
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9631)
Solve time for step 1 13.874759029014967
Current ori: tensor([ 0.0141, -0.0016, -0.0377], device='cuda:0')
Middle force: tensor([0.5489, 1.5874, 0.5408, 0.5118, 0.6558, 1.5775, 0.5605, 0.5717, 0.5000,
        0.5748, 0.5350, 0.5575], device='cuda:0')
Thumb force: tensor([0.7795, 0.8500, 0.4971, 0.9500, 0.8527, 1.0055, 0.7155, 0.9211, 0.7944,
        0.8444, 0.5978, 0.6520], device='cuda:0')
Index force: tensor([0.5261, 0.6773, 0.8403, 0.5980, 0.5053, 0.5120, 0.5001, 0.5362, 0.5474,
        0.6009, 0.5743, 0.5413], device='cuda:0')
Storing NORMAL transition: reward=-0.0660 (scaled=-0.0660), steps=1
Reward stats updated: mean -0.0006 -> -0.0008, std: 0.0983
Collected 342 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8367, Q2 Loss=0.8367, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4299
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2978, Q2 Loss=1.2978, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6507
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.3366, Q2 Loss=1.3366, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6555
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.7117, Q2 Loss=0.7117, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2166
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9167, Q2 Loss=0.9167, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3731

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.6%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.04s (18.1%)
Actor update: 0.10s (39.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.019890
Q2 loss: 1.019890
Current threshold: -34.0029
Global Scale Offset: 0.0789
Reward stats: mean=-0.0008, std=0.0983, count=342
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.0199, Q2 Loss: 1.0199, Entropy: 0.0000, Mean TD Error: 1.0652, Threshold: -34.0029
tensor([ 0.1567,  0.6182,  0.5603,  0.6142, -0.2237,  0.5970,  0.9692,  1.0586,
         1.3823,  0.2379,  0.1152,  1.0910,  0.0066, -0.0039,  0.0285,  0.0696],
       device='cuda:0')
Original likelihood: -32.44685363769531
Adjusted likelihood: -32.44685363769531
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.605064511008095
Current ori: tensor([ 0.0066, -0.0039,  0.0285], device='cuda:0')
Middle force: tensor([1.5420, 0.5387, 0.5103, 0.6472, 1.5317, 0.5557, 0.5673, 0.5000, 0.5709,
        0.5333, 0.5538], device='cuda:0')
Thumb force: tensor([0.8236, 0.5014, 0.9221, 0.8332, 0.9754, 0.7083, 0.9104, 0.7802, 0.8305,
        0.5909, 0.6488], device='cuda:0')
Index force: tensor([0.6602, 0.8351, 0.5921, 0.5046, 0.5103, 0.5001, 0.5332, 0.5435, 0.5957,
        0.5699, 0.5372], device='cuda:0')
Storing NORMAL transition: reward=-0.2345 (scaled=-0.2345), steps=1
Reward stats updated: mean -0.0008 -> -0.0014, std: 0.0989
Collected 343 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2073, Q2 Loss=1.2073, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9073
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4941, Q2 Loss=1.4941, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2961
SAC Update 3/5: Actor Loss=-0.0462, Q1 Loss=1.1007, Q2 Loss=1.1007, Entropy=0.0277, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2112
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8195, Q2 Loss=0.8195, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6759
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3848, Q2 Loss=1.3848, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8621

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.9%)
Q1 update: 0.05s (18.4%)
Q2 update: 0.05s (20.3%)
Actor update: 0.11s (43.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.009233
Q1 loss: 1.201291
Q2 loss: 1.201291
Current threshold: -34.0174
Global Scale Offset: 0.0786
Reward stats: mean=-0.0014, std=0.0989, count=343
----------------------------------------------
SAC Update - Actor Loss: -0.0092, Q1 Loss: 1.2013, Q2 Loss: 1.2013, Entropy: 0.0055, Mean TD Error: 1.1905, Threshold: -34.0174
tensor([ 0.0066,  0.5527,  0.4758,  0.6738, -0.2577,  0.6597,  0.8249,  1.0321,
         1.4781,  0.0533,  0.1863,  1.0144,  0.0197,  0.0339,  0.2621,  0.2375],
       device='cuda:0')
Original likelihood: -36.700408935546875
Adjusted likelihood: -36.700408935546875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 35.292747497558594
Projection step: 1, Loss: 33.20562744140625
Projection step: 2, Loss: 34.1552734375
Projection step: 3, Loss: 33.957489013671875
Projection step: 4, Loss: 31.117507934570312
Projection step: 5, Loss: 30.480283737182617
Projection step: 6, Loss: 29.434206008911133
Projection step: 7, Loss: 30.87562370300293
Projection step: 8, Loss: 29.3221435546875
Projection step: 9, Loss: 28.433185577392578
Projection step: 10, Loss: 27.12891387939453
Projection step: 11, Loss: 25.864412307739258
Projection step: 12, Loss: 24.367008209228516
Projection step: 13, Loss: 24.4536190032959
Projection step: 14, Loss: 24.50048065185547
Projection step: 15, Loss: 24.57619285583496
Projection step: 16, Loss: 22.57042121887207
Projection step: 17, Loss: 22.919204711914062
Projection step: 18, Loss: 21.464609146118164
Projection step: 19, Loss: 22.616254806518555
Projection step: 20, Loss: 21.009830474853516
Projection step: 21, Loss: 20.752960205078125
Projection step: 22, Loss: 19.74748992919922
Projection step: 23, Loss: 20.215213775634766
Projection step: 24, Loss: 19.22696876525879
Final likelihood: tensor([-19.8444, -16.8726, -18.7580, -25.2329, -21.1460, -23.7741, -18.0869,
        -17.4365, -16.5006, -19.6147, -22.6811, -15.6849, -18.9308, -18.2050,
        -21.4268, -22.6819])
Final projection likelihood: -19.8048
1 mode projection succeeded
New goal: tensor([ 0.0274,  0.5790,  0.5378,  0.5364, -0.1790,  0.6090,  0.7676,  0.9020,
         1.4319,  0.0717,  0.1780,  1.1084,  0.0207,  0.0276, -0.4994],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -24.25554656982422
Adjusted likelihood: -24.25554656982422
Likelihood residual: 0.0
Original likelihood: -28.432384490966797
Adjusted likelihood: -28.432384490966797
Likelihood residual: 0.0
{'index': 28.432384490966797, 'thumb_middle': 24.25554656982422}
Current yaw: tensor([0.0197, 0.0339, 0.2621], device='cuda:0')
4 thumb_middle
tensor([ 0.0066,  0.5527,  0.4758,  0.6738, -0.2577,  0.6597,  0.8249,  1.0321,
         1.4781,  0.0533,  0.1863,  1.0144,  0.0197,  0.0339,  0.2621,  0.2375],
       device='cuda:0')
Solve time for step 1 8.86124243598897
Current ori: tensor([0.0197, 0.0339, 0.2621], device='cuda:0')
Index force: tensor([0.5889, 0.5994, 0.5551, 0.5543], device='cuda:0')
tensor([ 0.0036,  0.5688,  0.5142,  0.5477, -0.2591,  0.6190,  0.7720,  0.9141,
         1.3995,  0.0376,  0.1008,  1.0664,  0.0047,  0.0369,  0.2633,  0.1486],
       device='cuda:0')
Solve time for step 2 3.5429380040150136
Current ori: tensor([0.0047, 0.0369, 0.2633], device='cuda:0')
Index force: tensor([0.5903, 0.5482, 0.5536], device='cuda:0')
tensor([-6.7393e-03,  5.7508e-01,  5.1402e-01,  5.1349e-01, -2.5813e-01,
         6.2219e-01,  7.5544e-01,  8.9426e-01,  1.4087e+00,  5.6404e-02,
         8.6162e-02,  1.0604e+00,  5.5169e-04,  4.2300e-02,  2.6325e-01,
         1.2808e-01], device='cuda:0')
Solve time for step 3 3.502379984012805
Current ori: tensor([0.0006, 0.0423, 0.2632], device='cuda:0')
Index force: tensor([0.5369, 0.5410], device='cuda:0')
tensor([ 0.0065,  0.5609,  0.5334,  0.5369, -0.2571,  0.6250,  0.7657,  0.8898,
         1.4081,  0.0484,  0.0795,  1.0632,  0.0055,  0.0354,  0.2633,  0.1533],
       device='cuda:0')
Solve time for step 4 3.4336634459905326
Current ori: tensor([0.0055, 0.0354, 0.2633], device='cuda:0')
Index force: tensor([0.5270], device='cuda:0')
Storing RECOVERY transition: reward=0.0124 (scaled=0.0062), steps=2
Reward stats updated: mean -0.0014 -> -0.0014, std: 0.0988
Collected 344 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0346, Q2 Loss=1.0346, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8216
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0165, Q2 Loss=1.0165, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9243
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6547, Q2 Loss=1.6547, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3100
SAC Update 4/5: Actor Loss=-0.0107, Q1 Loss=1.1489, Q2 Loss=1.1489, Entropy=0.3217, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0991
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.1013, Q2 Loss=2.1013, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8548

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (21.0%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (17.9%)
Actor update: 0.08s (37.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.002136
Q1 loss: 1.391199
Q2 loss: 1.391199
Current threshold: -34.0372
Global Scale Offset: 0.0783
Reward stats: mean=-0.0014, std=0.0988, count=344
----------------------------------------------
SAC Update - Actor Loss: -0.0021, Q1 Loss: 1.3912, Q2 Loss: 1.3912, Entropy: 0.0643, Mean TD Error: 1.2019, Threshold: -34.0372
Original likelihood: -28.047897338867188
Adjusted likelihood: -28.047897338867188
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([0.0028, 0.0287, 0.2501], device='cuda:0')
5 turn
Sampling time 3.6265829369658604
tensor([ 0.0175,  0.5724,  0.5264,  0.5382, -0.1908,  0.6576,  0.7797,  0.9123,
         1.4728,  0.0714,  0.1227,  1.0805,  0.0028,  0.0287,  0.2501,  0.1898],
       device='cuda:0')
Original likelihood: -24.29627227783203
Adjusted likelihood: -24.29627227783203
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.025532349012792
Current ori: tensor([0.0028, 0.0287, 0.2501], device='cuda:0')
Middle force: tensor([1.3950, 0.5013, 0.5067, 0.5156, 0.5447, 0.5745, 1.1500, 0.8586, 0.8183,
        0.5702, 0.5196, 0.5178], device='cuda:0')
Thumb force: tensor([1.9717, 1.9436, 1.3964, 0.5740, 1.0949, 0.8239, 1.5299, 0.5465, 0.7346,
        0.6885, 0.5326, 0.5677], device='cuda:0')
Index force: tensor([0.5621, 0.7723, 0.6859, 0.6150, 0.5480, 0.5370, 0.5503, 0.5167, 0.5888,
        0.5477, 0.6699, 0.5882], device='cuda:0')
Storing NORMAL transition: reward=-0.2095 (scaled=-0.2095), steps=1
Reward stats updated: mean -0.0014 -> -0.0020, std: 0.0993
Collected 345 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.8408, Q2 Loss=1.8408, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8421
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7920, Q2 Loss=0.7920, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1814
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9403, Q2 Loss=0.9403, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2278
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7997, Q2 Loss=0.7997, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5351
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1830, Q2 Loss=1.1830, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3145

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.0%)
Q1 update: 0.04s (18.1%)
Q2 update: 0.04s (18.8%)
Actor update: 0.09s (41.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.111184
Q2 loss: 1.111184
Current threshold: -34.0536
Global Scale Offset: 0.0780
Reward stats: mean=-0.0020, std=0.0993, count=345
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.1112, Q2 Loss: 1.1112, Entropy: 0.0000, Mean TD Error: 1.0202, Threshold: -34.0536
tensor([-0.0536,  0.5077,  0.5331,  0.5736, -0.1458,  0.7437,  0.6691,  0.8218,
         1.3849,  0.1923,  0.0931,  1.2937,  0.0175,  0.0249,  0.4597,  0.2119],
       device='cuda:0')
Original likelihood: -27.8077392578125
Adjusted likelihood: -27.8077392578125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.625607524008956
Current ori: tensor([0.0175, 0.0249, 0.4597], device='cuda:0')
Middle force: tensor([0.5014, 0.5084, 0.5176, 0.5452, 0.5859, 1.1445, 0.8816, 0.8321, 0.5765,
        0.5271, 0.5273], device='cuda:0')
Thumb force: tensor([1.8969, 1.3613, 0.5652, 1.0692, 0.8079, 1.4941, 0.5401, 0.7162, 0.6732,
        0.5262, 0.5585], device='cuda:0')
Index force: tensor([0.7494, 0.6584, 0.6066, 0.5464, 0.5293, 0.5484, 0.5142, 0.5815, 0.5432,
        0.6475, 0.5653], device='cuda:0')
Storing NORMAL transition: reward=-0.0877 (scaled=-0.0877), steps=1
Reward stats updated: mean -0.0020 -> -0.0023, std: 0.0992
Collected 346 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0342, Q2 Loss=1.0342, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6415
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9170, Q2 Loss=0.9170, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0958
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0762, Q2 Loss=1.0762, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9403
SAC Update 4/5: Actor Loss=-0.1671, Q1 Loss=1.0404, Q2 Loss=1.0404, Entropy=0.0169, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8304
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.9606, Q2 Loss=2.9606, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0405

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (16.3%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (19.6%)
Actor update: 0.10s (40.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.079478
Q1 loss: 1.405677
Q2 loss: 1.405677
Current threshold: -34.0739
Global Scale Offset: 0.0776
Reward stats: mean=-0.0023, std=0.0992, count=346
----------------------------------------------
SAC Update - Actor Loss: -0.0795, Q1 Loss: 1.4057, Q2 Loss: 1.4057, Entropy: 0.0034, Mean TD Error: 1.5097, Threshold: -34.0739
tensor([-1.6879e-01,  4.5324e-01,  5.2649e-01,  5.7735e-01, -2.2765e-01,
         7.7845e-01,  5.6944e-01,  8.6088e-01,  1.3078e+00,  2.9056e-01,
         1.6075e-01,  1.4833e+00,  3.5426e-02,  7.8271e-02,  5.4353e-01,
        -9.1339e-04], device='cuda:0')
Original likelihood: -25.66692543029785
Adjusted likelihood: -25.66692543029785
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.316212262026966
Current ori: tensor([0.0354, 0.0783, 0.5435], device='cuda:0')
Middle force: tensor([0.5078, 0.5166, 0.5450, 0.5807, 1.1381, 0.8953, 0.8200, 0.5721, 0.5231,
        0.5265], device='cuda:0')
Thumb force: tensor([1.3296, 0.5636, 1.0409, 0.8022, 1.4611, 0.5360, 0.7169, 0.6686, 0.5246,
        0.5577], device='cuda:0')
Index force: tensor([0.6680, 0.6061, 0.5490, 0.5287, 0.5478, 0.5120, 0.5784, 0.5414, 0.6525,
        0.5617], device='cuda:0')
Storing NORMAL transition: reward=0.0605 (scaled=0.0605), steps=1
Reward stats updated: mean -0.0023 -> -0.0021, std: 0.0992
Collected 347 transitions for RL
SAC Update 1/5: Actor Loss=-0.0001, Q1 Loss=1.1299, Q2 Loss=1.1299, Entropy=0.0305, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6255
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2715, Q2 Loss=1.2715, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2927
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2380, Q2 Loss=1.2380, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4097
SAC Update 4/5: Actor Loss=-0.0464, Q1 Loss=1.2068, Q2 Loss=1.2068, Entropy=0.0272, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8610
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3993, Q2 Loss=1.3993, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9362

------ SAC Update Summary (5 iterations) ------
Total time: 0.31s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (14.9%)
Q1 update: 0.06s (20.3%)
Q2 update: 0.06s (20.4%)
Actor update: 0.13s (41.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.009299
Q1 loss: 1.249102
Q2 loss: 1.249102
Current threshold: -34.1080
Global Scale Offset: 0.0769
Reward stats: mean=-0.0021, std=0.0992, count=347
----------------------------------------------
SAC Update - Actor Loss: -0.0093, Q1 Loss: 1.2491, Q2 Loss: 1.2491, Entropy: 0.0115, Mean TD Error: 1.0250, Threshold: -34.1080
tensor([-0.1805,  0.5572,  0.3841,  0.5570, -0.2064,  0.7468,  0.6173,  0.9630,
         1.2949,  0.3784,  0.0918,  1.5404,  0.0490,  0.0535,  0.4841,  0.5019],
       device='cuda:0')
Original likelihood: -30.1395263671875
Adjusted likelihood: -30.1395263671875
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.05374928296078
Current ori: tensor([0.0490, 0.0535, 0.4841], device='cuda:0')
Middle force: tensor([0.5158, 0.5456, 0.5980, 1.1202, 0.8989, 0.8334, 0.5785, 0.5271, 0.5353],
       device='cuda:0')
Thumb force: tensor([0.5594, 1.0205, 0.7920, 1.4363, 0.5331, 0.7031, 0.6567, 0.5225, 0.5554],
       device='cuda:0')
Index force: tensor([0.6031, 0.5475, 0.5210, 0.5469, 0.5105, 0.5700, 0.5369, 0.6338, 0.5444],
       device='cuda:0')
Storing NORMAL transition: reward=0.0105 (scaled=0.0105), steps=1
Reward stats updated: mean -0.0021 -> -0.0021, std: 0.0990
Collected 348 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5471, Q2 Loss=1.5471, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5097
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.7603, Q2 Loss=1.7603, Entropy=0.0013, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4151
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1593, Q2 Loss=1.1593, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2136
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6946, Q2 Loss=1.6946, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5469
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.1985, Q2 Loss=1.1985, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8365

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.7%)
Q1 update: 0.06s (20.2%)
Q2 update: 0.06s (20.1%)
Actor update: 0.12s (42.3%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000001
Q1 loss: 1.471976
Q2 loss: 1.471976
Current threshold: -34.1394
Global Scale Offset: 0.0762
Reward stats: mean=-0.0021, std=0.0990, count=348
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.4720, Q2 Loss: 1.4720, Entropy: 0.0003, Mean TD Error: 1.3044, Threshold: -34.1394
tensor([-0.1674,  0.5708,  0.3730,  0.5503, -0.2035,  0.7364,  0.6491,  0.9222,
         1.2615,  0.4224,  0.1219,  1.5150,  0.0458,  0.0516,  0.4739,  0.4839],
       device='cuda:0')
Original likelihood: -32.27113342285156
Adjusted likelihood: -32.27113342285156
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.662316372967325
Current ori: tensor([0.0458, 0.0516, 0.4739], device='cuda:0')
Middle force: tensor([0.5451, 0.5974, 1.1009, 0.8836, 0.8233, 0.5769, 0.5243, 0.5345],
       device='cuda:0')
Thumb force: tensor([0.9881, 0.7932, 1.4233, 0.5320, 0.7008, 0.6525, 0.5218, 0.5543],
       device='cuda:0')
Index force: tensor([0.5534, 0.5186, 0.5457, 0.5104, 0.5665, 0.5348, 0.6335, 0.5417],
       device='cuda:0')
Storing NORMAL transition: reward=0.0586 (scaled=0.0586), steps=1
Reward stats updated: mean -0.0021 -> -0.0019, std: 0.0989
Collected 349 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9094, Q2 Loss=0.9094, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4284
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5261, Q2 Loss=1.5261, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2576
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.1799, Q2 Loss=1.1799, Entropy=0.0017, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8056
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6609, Q2 Loss=1.6609, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0734
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.4164, Q2 Loss=1.4164, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0083

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.3%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (18.9%)
Actor update: 0.09s (38.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000001
Q1 loss: 1.338546
Q2 loss: 1.338546
Current threshold: -34.1581
Global Scale Offset: 0.0758
Reward stats: mean=-0.0019, std=0.0989, count=349
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.3385, Q2 Loss: 1.3385, Entropy: 0.0003, Mean TD Error: 1.3147, Threshold: -34.1581
tensor([-0.1099,  0.5066,  0.4272,  0.7078, -0.2445,  0.6503,  0.7706,  0.8411,
         1.3015,  0.4390,  0.1076,  1.4914,  0.0455,  0.0772,  0.4126,  1.7458],
       device='cuda:0')
Original likelihood: -28.945064544677734
Adjusted likelihood: -28.945064544677734
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.50439825898502
Current ori: tensor([0.0455, 0.0772, 0.4126], device='cuda:0')
Middle force: tensor([0.5368, 0.5501, 0.6432, 0.5002, 0.8022, 0.5242, 0.5166],
       device='cuda:0')
Thumb force: tensor([0.5184, 0.5212, 0.6305, 0.5396, 0.5011, 0.6656, 0.5953],
       device='cuda:0')
Index force: tensor([0.5172, 0.5051, 0.5089, 0.5352, 0.5123, 0.5003, 0.5136],
       device='cuda:0')
Storing NORMAL transition: reward=0.1473 (scaled=0.1473), steps=1
Reward stats updated: mean -0.0019 -> -0.0015, std: 0.0991
Collected 350 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.3303, Q2 Loss=1.3303, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6674
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0544, Q2 Loss=1.0544, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0416
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.1780, Q2 Loss=1.1780, Entropy=0.0019, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0644
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8894, Q2 Loss=0.8894, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6425
SAC Update 5/5: Actor Loss=-0.1000, Q1 Loss=1.0272, Q2 Loss=1.0272, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9564

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.7%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (40.0%)
Target update: 0.00s (1.9%)
Priority update: 0.00s (0.2%)
Actor loss: -0.020006
Q1 loss: 1.095853
Q2 loss: 1.095853
Current threshold: -34.1691
Global Scale Offset: 0.0756
Reward stats: mean=-0.0015, std=0.0991, count=350
----------------------------------------------
SAC Update - Actor Loss: -0.0200, Q1 Loss: 1.0959, Q2 Loss: 1.0959, Entropy: 0.0004, Mean TD Error: 0.8744, Threshold: -34.1691
tensor([-0.1047,  0.4320,  0.5152,  0.7594, -0.3015,  0.5840,  0.8030,  1.0027,
         1.3461,  0.4536,  0.1122,  1.3833,  0.0566,  0.0921,  0.2603,  2.0082],
       device='cuda:0')
Original likelihood: -35.851806640625
Adjusted likelihood: -35.851806640625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 34.92781066894531
Projection step: 1, Loss: 32.58272933959961
Projection step: 2, Loss: 32.64306640625
Projection step: 3, Loss: 31.44464111328125
Projection step: 4, Loss: 31.281776428222656
Projection step: 5, Loss: 30.12024688720703
Projection step: 6, Loss: 30.632644653320312
Projection step: 7, Loss: 30.368919372558594
Projection step: 8, Loss: 29.172927856445312
Projection step: 9, Loss: 29.22921371459961
Projection step: 10, Loss: 28.334444046020508
Projection step: 11, Loss: 27.29849624633789
Projection step: 12, Loss: 26.827293395996094
Projection step: 13, Loss: 27.647380828857422
Projection step: 14, Loss: 27.338115692138672
Projection step: 15, Loss: 27.116743087768555
Projection step: 16, Loss: 26.218093872070312
Projection step: 17, Loss: 26.404226303100586
Projection step: 18, Loss: 24.323474884033203
Projection step: 19, Loss: 25.343006134033203
Projection step: 20, Loss: 24.69463539123535
Projection step: 21, Loss: 24.213346481323242
Projection step: 22, Loss: 25.2823429107666
Projection step: 23, Loss: 24.242677688598633
Projection step: 24, Loss: 22.954692840576172
Final likelihood: tensor([-24.5143, -24.5358, -25.5942, -23.3802, -25.2531, -24.4399, -22.4365,
        -24.0311, -24.3411, -25.1908, -24.0219, -24.8380, -23.5974, -24.3577,
        -24.7727, -23.6241])
Final projection likelihood: -24.3080
1 mode projection succeeded
New goal: tensor([-0.1223,  0.4339,  0.4232,  0.8304, -0.2300,  0.6060,  0.7241,  1.0020,
         1.3975,  0.4381,  0.1734,  1.2919,  0.0552,  0.0926,  0.9246],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -23.782773971557617
Adjusted likelihood: -23.782773971557617
Likelihood residual: 0.0
Original likelihood: -27.924297332763672
Adjusted likelihood: -27.924297332763672
Likelihood residual: 0.0
{'index': 27.924297332763672, 'thumb_middle': 23.782773971557617}
Current yaw: tensor([0.0566, 0.0921, 0.2603], device='cuda:0')
6 thumb_middle
tensor([-0.1047,  0.4320,  0.5152,  0.7594, -0.3015,  0.5840,  0.8030,  1.0027,
         1.3461,  0.4536,  0.1122,  1.3833,  0.0566,  0.0921,  0.2603,  2.0082],
       device='cuda:0')
Solve time for step 1 8.799251114018261
Current ori: tensor([0.0566, 0.0921, 0.2603], device='cuda:0')
Index force: tensor([0.5719, 0.5802, 0.5830, 0.5894], device='cuda:0')
tensor([-0.1137,  0.4457,  0.4570,  0.8264, -0.3139,  0.6058,  0.7308,  0.9896,
         1.3308,  0.4199,  0.0605,  1.2662,  0.0622,  0.0964,  0.2603,  2.0115],
       device='cuda:0')
Solve time for step 2 3.491610050026793
Current ori: tensor([0.0622, 0.0964, 0.2603], device='cuda:0')
Index force: tensor([0.5672, 0.5716, 0.5783], device='cuda:0')
tensor([-0.1103,  0.4498,  0.4399,  0.8534, -0.3049,  0.6144,  0.7113,  0.9640,
         1.3284,  0.4338,  0.0770,  1.2217,  0.0639,  0.0946,  0.2603,  2.0203],
       device='cuda:0')
Solve time for step 3 3.4164200489758514
Current ori: tensor([0.0639, 0.0946, 0.2603], device='cuda:0')
Index force: tensor([0.5735, 0.5484], device='cuda:0')
tensor([-0.1146,  0.4618,  0.4333,  0.8258, -0.3080,  0.6153,  0.7093,  0.9739,
         1.3350,  0.4228,  0.0684,  1.2304,  0.0585,  0.0974,  0.2603,  2.0078],
       device='cuda:0')
Solve time for step 4 3.243464266008232
Current ori: tensor([0.0585, 0.0974, 0.2603], device='cuda:0')
Index force: tensor([0.5341], device='cuda:0')
Storing RECOVERY transition: reward=0.0015 (scaled=0.0003), steps=6
Reward stats updated: mean -0.0015 -> -0.0015, std: 0.0990
Collected 351 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3425, Q2 Loss=1.3425, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7822
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.7265, Q2 Loss=0.7265, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1682
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.1735, Q2 Loss=1.1735, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6993
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.7028, Q2 Loss=1.7028, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6706
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7235, Q2 Loss=0.7235, Entropy=0.0214, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5241

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.9%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (19.4%)
Actor update: 0.09s (40.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046054
Q1 loss: 1.133744
Q2 loss: 1.133744
Current threshold: -34.1757
Global Scale Offset: 0.0754
Reward stats: mean=-0.0015, std=0.0990, count=351
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1337, Q2 Loss: 1.1337, Entropy: 0.0043, Mean TD Error: 1.1689, Threshold: -34.1757
Original likelihood: -28.704509735107422
Adjusted likelihood: -28.704509735107422
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([0.0580, 0.0986, 0.2574], device='cuda:0')
7 turn
Sampling time 3.870751894020941
tensor([-0.1257,  0.4586,  0.4295,  0.8275, -0.2824,  0.6278,  0.7145,  0.9876,
         1.3989,  0.4336,  0.1154,  1.2656,  0.0580,  0.0986,  0.2574,  1.9275],
       device='cuda:0')
Original likelihood: -28.345508575439453
Adjusted likelihood: -28.345508575439453
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.23370923299808
Current ori: tensor([0.0580, 0.0986, 0.2574], device='cuda:0')
Middle force: tensor([0.7703, 0.5634, 0.5316, 0.6845, 1.3451, 0.5409, 0.5749, 0.7282, 0.8521,
        0.5892, 0.8715, 0.5848], device='cuda:0')
Thumb force: tensor([1.8235, 1.1400, 0.7316, 0.7487, 1.2243, 0.8407, 0.5689, 0.5267, 0.6700,
        0.5517, 0.5226, 0.6057], device='cuda:0')
Index force: tensor([0.6947, 0.8447, 0.8769, 0.5871, 0.5834, 0.8233, 0.5647, 0.7576, 0.7820,
        0.6121, 0.5412, 0.5883], device='cuda:0')
Storing NORMAL transition: reward=0.0367 (scaled=0.0367), steps=1
Reward stats updated: mean -0.0015 -> -0.0013, std: 0.0989
Collected 352 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9642, Q2 Loss=0.9642, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0404
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1875, Q2 Loss=1.1875, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9740
SAC Update 3/5: Actor Loss=-0.0001, Q1 Loss=3.1682, Q2 Loss=3.1682, Entropy=0.0483, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.3682
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6065, Q2 Loss=1.6065, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8803
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4874, Q2 Loss=1.4874, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1661

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.8%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (19.7%)
Actor update: 0.10s (38.8%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000021
Q1 loss: 1.682755
Q2 loss: 1.682755
Current threshold: -34.1797
Global Scale Offset: 0.0753
Reward stats: mean=-0.0013, std=0.0989, count=352
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.6828, Q2 Loss: 1.6828, Entropy: 0.0097, Mean TD Error: 1.8858, Threshold: -34.1797
tensor([-0.0864,  0.4737,  0.4211,  0.8572, -0.2591,  0.6426,  0.6987,  1.0205,
         1.3979,  0.4295,  0.0835,  1.2762,  0.0536,  0.0801,  0.2237,  2.0185],
       device='cuda:0')
Original likelihood: -28.911762237548828
Adjusted likelihood: -28.911762237548828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.769763575983234
Current ori: tensor([0.0536, 0.0801, 0.2237], device='cuda:0')
Middle force: tensor([0.6053, 0.5279, 1.1209, 0.9619, 0.5231, 0.7001, 1.0616, 0.5027, 0.6974,
        0.5498, 0.9942], device='cuda:0')
Thumb force: tensor([1.8521, 0.9997, 0.5693, 1.2811, 0.5422, 0.9269, 0.7924, 0.8429, 0.6792,
        0.6199, 0.7623], device='cuda:0')
Index force: tensor([0.8874, 0.6737, 1.0085, 0.7048, 0.7596, 0.5959, 0.5323, 1.0558, 0.6431,
        0.5919, 0.5310], device='cuda:0')
Storing NORMAL transition: reward=0.2457 (scaled=0.2457), steps=1
Reward stats updated: mean -0.0013 -> -0.0006, std: 0.0996
Collected 353 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.7944, Q2 Loss=1.7944, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8621
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1914, Q2 Loss=1.1914, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3345
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5842, Q2 Loss=1.5842, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3180
SAC Update 4/5: Actor Loss=-0.0002, Q1 Loss=1.5527, Q2 Loss=1.5527, Entropy=0.0575, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5615
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.1923, Q2 Loss=1.1923, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4642

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.3%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (18.8%)
Actor update: 0.09s (39.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000050
Q1 loss: 1.462987
Q2 loss: 1.462987
Current threshold: -34.1824
Global Scale Offset: 0.0753
Reward stats: mean=-0.0006, std=0.0996, count=353
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.4630, Q2 Loss: 1.4630, Entropy: 0.0115, Mean TD Error: 1.1081, Threshold: -34.1824
tensor([-0.2034,  0.4953,  0.3577,  0.7620, -0.1795,  0.6111,  0.8036,  1.0431,
         1.3120,  0.5645,  0.1268,  1.1460,  0.0288,  0.0174, -0.0149,  1.9243],
       device='cuda:0')
Original likelihood: -35.09492111206055
Adjusted likelihood: -35.09492111206055
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0008)
State is out of distribution
Projection step: 0, Loss: 34.10643768310547
Projection step: 1, Loss: 36.20662307739258
Projection step: 2, Loss: 32.36186218261719
Projection step: 3, Loss: 34.625465393066406
Projection step: 4, Loss: 32.42253875732422
Projection step: 5, Loss: 31.597145080566406
Projection step: 6, Loss: 31.85373306274414
Projection step: 7, Loss: 31.568679809570312
Projection step: 8, Loss: 32.185359954833984
Projection step: 9, Loss: 31.47402572631836
Projection step: 10, Loss: 31.159809112548828
Projection step: 11, Loss: 29.709674835205078
Projection step: 12, Loss: 30.828651428222656
Projection step: 13, Loss: 30.03594970703125
Projection step: 14, Loss: 29.97918128967285
Projection step: 15, Loss: 28.110902786254883
Projection step: 16, Loss: 29.514236450195312
Projection step: 17, Loss: 28.826257705688477
Projection step: 18, Loss: 28.060440063476562
Projection step: 19, Loss: 26.354511260986328
Projection step: 20, Loss: 27.93585205078125
Projection step: 21, Loss: 26.391061782836914
Projection step: 22, Loss: 26.59038734436035
Projection step: 23, Loss: 26.096023559570312
Projection step: 24, Loss: 25.577686309814453
Final likelihood: tensor([-21.0288, -23.2626, -27.5124, -26.8450, -21.5044, -26.2715, -27.1226,
        -25.9899, -27.1848, -24.0359, -27.9019, -24.7855, -26.4427, -25.4423,
        -26.6904, -24.7703])
Final projection likelihood: -25.4244
1 mode projection succeeded
New goal: tensor([-0.1191,  0.4531,  0.4811,  0.7929, -0.1515,  0.5835,  0.7032,  1.0400,
         1.2509,  0.4628,  0.1306,  1.1716,  0.0269,  0.0181,  0.2224],
       device='cuda:0')
tensor([[0.0030]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0035]], device='cuda:0')
Original likelihood: -29.573062896728516
Adjusted likelihood: -29.573062896728516
Likelihood residual: 0.0
Original likelihood: -32.00066375732422
Adjusted likelihood: -32.00066375732422
Likelihood residual: 0.0
{'index': 32.00066375732422, 'thumb_middle': 29.573062896728516}
Current yaw: tensor([ 0.0288,  0.0174, -0.0149], device='cuda:0')
8 thumb_middle
tensor([-0.2034,  0.4953,  0.3577,  0.7620, -0.1795,  0.6111,  0.8036,  1.0431,
         1.3120,  0.5645,  0.1268,  1.1460,  0.0288,  0.0174, -0.0149,  1.9243],
       device='cuda:0')
Solve time for step 1 8.976516644004732
Current ori: tensor([ 0.0288,  0.0174, -0.0149], device='cuda:0')
Index force: tensor([0.5966, 0.6065, 0.5758, 0.5954], device='cuda:0')
tensor([-0.1703,  0.4664,  0.4564,  0.7889, -0.2088,  0.5899,  0.7091,  1.0314,
         1.2453,  0.4781,  0.0760,  1.1407,  0.0409, -0.0140, -0.0026,  1.9037],
       device='cuda:0')
Solve time for step 2 3.644422426004894
Current ori: tensor([ 0.0409, -0.0140, -0.0026], device='cuda:0')
Index force: tensor([0.6007, 0.5708, 0.5901], device='cuda:0')
tensor([-0.1821,  0.4520,  0.4677,  0.7921, -0.2036,  0.5993,  0.7010,  1.0308,
         1.2484,  0.4643,  0.0736,  1.1461,  0.0479, -0.0093,  0.0170,  1.9554],
       device='cuda:0')
Solve time for step 3 3.436388857953716
Current ori: tensor([ 0.0479, -0.0093,  0.0170], device='cuda:0')
Index force: tensor([0.5606, 0.5781], device='cuda:0')
tensor([-0.1868,  0.4636,  0.4587,  0.7671, -0.2062,  0.5957,  0.6965,  1.0296,
         1.2460,  0.4630,  0.0731,  1.1474,  0.0457, -0.0060,  0.0406,  1.9517],
       device='cuda:0')
Solve time for step 4 3.3376432639779523
Current ori: tensor([ 0.0457, -0.0060,  0.0406], device='cuda:0')
Index force: tensor([0.5628], device='cuda:0')
Storing RECOVERY transition: reward=-0.0711 (scaled=-0.0355), steps=2
Reward stats updated: mean -0.0006 -> -0.0007, std: 0.0995
Collected 354 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6607, Q2 Loss=0.6607, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4266
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2135, Q2 Loss=1.2135, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0048
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1719, Q2 Loss=1.1719, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7614
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.6167, Q2 Loss=1.6167, Entropy=0.0501, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6648
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0634, Q2 Loss=1.0634, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6676

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.9%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (18.9%)
Actor update: 0.08s (40.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046063
Q1 loss: 1.145264
Q2 loss: 1.145264
Current threshold: -34.1842
Global Scale Offset: 0.0753
Reward stats: mean=-0.0007, std=0.0995, count=354
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1453, Q2 Loss: 1.1453, Entropy: 0.0100, Mean TD Error: 1.1050, Threshold: -34.1842
Original likelihood: -34.94594955444336
Adjusted likelihood: -34.94594955444336
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0042)
State is out of distribution
Projection step: 0, Loss: 34.65386962890625
Projection step: 1, Loss: 35.779335021972656
Projection step: 2, Loss: 34.34861755371094
Projection step: 3, Loss: 35.48509216308594
Projection step: 4, Loss: 32.277809143066406
Projection step: 5, Loss: 31.285614013671875
Projection step: 6, Loss: 31.316970825195312
Projection step: 7, Loss: 31.42184066772461
Projection step: 8, Loss: 31.79375457763672
Projection step: 9, Loss: 30.79795265197754
Projection step: 10, Loss: 29.92254066467285
Projection step: 11, Loss: 30.004730224609375
Projection step: 12, Loss: 29.592496871948242
Projection step: 13, Loss: 31.1313533782959
Projection step: 14, Loss: 31.590726852416992
Projection step: 15, Loss: 29.387508392333984
Projection step: 16, Loss: 29.641149520874023
Projection step: 17, Loss: 29.338027954101562
Projection step: 18, Loss: 26.279043197631836
Projection step: 19, Loss: 28.175283432006836
Projection step: 20, Loss: 28.202219009399414
Projection step: 21, Loss: 27.155086517333984
Projection step: 22, Loss: 27.583744049072266
Projection step: 23, Loss: 27.77526092529297
Projection step: 24, Loss: 27.24808120727539
Final likelihood: tensor([-27.2861, -29.8788, -28.5372, -26.9613, -29.1361, -20.2403, -19.8870,
        -26.2018, -27.4895, -25.5258, -24.0140, -27.3198, -28.6188, -29.8949,
        -28.0397, -28.7751])
Final projection likelihood: -26.7379
1 mode projection succeeded
New goal: tensor([-1.2482e-01,  4.2793e-01,  5.3926e-01,  8.2817e-01, -1.2558e-01,
         6.0633e-01,  6.7219e-01,  1.0629e+00,  1.2392e+00,  3.7999e-01,
         1.5246e-01,  1.2042e+00,  4.3500e-02,  9.5969e-05,  2.1446e-01],
       device='cuda:0')
tensor([[0.0030]], device='cuda:0') tensor([[0.0030]], device='cuda:0') tensor([[0.0019]], device='cuda:0')
Original likelihood: -31.291152954101562
Adjusted likelihood: -31.291152954101562
Likelihood residual: 0.0
Original likelihood: -32.04002380371094
Adjusted likelihood: -32.04002380371094
Likelihood residual: 0.0
{'index': 32.04002380371094, 'thumb_middle': 31.291152954101562}
Current yaw: tensor([ 0.0478, -0.0024,  0.0549], device='cuda:0')
9 thumb_middle
tensor([-0.1943,  0.4633,  0.4553,  0.7634, -0.1308,  0.6498,  0.7384,  1.0531,
         1.2982,  0.4729,  0.1266,  1.1813,  0.0478, -0.0024,  0.0549,  1.9340],
       device='cuda:0')
Solve time for step 1 9.215477185964119
Current ori: tensor([ 0.0478, -0.0024,  0.0549], device='cuda:0')
Index force: tensor([0.5006, 0.5801, 0.6019, 0.6019], device='cuda:0')
tensor([-0.1581,  0.4563,  0.4838,  0.7841, -0.1778,  0.6126,  0.6718,  1.0495,
         1.2269,  0.3895,  0.0867,  1.1709,  0.0512, -0.0207,  0.0784,  1.9139],
       device='cuda:0')
Solve time for step 2 3.6310105780139565
Current ori: tensor([ 0.0512, -0.0207,  0.0784], device='cuda:0')
Index force: tensor([0.5702, 0.5928, 0.5919], device='cuda:0')
tensor([-0.1642,  0.4068,  0.5306,  0.8351, -0.1747,  0.6185,  0.6680,  1.0551,
         1.2344,  0.3798,  0.0833,  1.1712,  0.0711, -0.0221,  0.0984,  1.9051],
       device='cuda:0')
Solve time for step 3 3.615821181971114
Current ori: tensor([ 0.0711, -0.0221,  0.0984], device='cuda:0')
Index force: tensor([0.5831, 0.5832], device='cuda:0')
tensor([-0.1651,  0.4133,  0.5254,  0.8239, -0.1755,  0.6167,  0.6679,  1.0540,
         1.2321,  0.3780,  0.0814,  1.1738,  0.0710, -0.0209,  0.1223,  1.9059],
       device='cuda:0')
Solve time for step 4 3.5023245249758475
Current ori: tensor([ 0.0710, -0.0209,  0.1223], device='cuda:0')
Index force: tensor([0.5526], device='cuda:0')
Storing RECOVERY transition: reward=-0.1576 (scaled=-0.0788), steps=2
Reward stats updated: mean -0.0007 -> -0.0010, std: 0.0994
Collected 355 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.7080, Q2 Loss=1.7080, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8618
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6820, Q2 Loss=1.6820, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.8904
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5128, Q2 Loss=1.5128, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7715
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6364, Q2 Loss=0.6364, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6101
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7366, Q2 Loss=0.7366, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1032

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (18.9%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (17.8%)
Actor update: 0.10s (38.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.255164
Q2 loss: 1.255164
Current threshold: -34.1854
Global Scale Offset: 0.0753
Reward stats: mean=-0.0010, std=0.0994, count=355
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.2552, Q2 Loss: 1.2552, Entropy: 0.0000, Mean TD Error: 1.8474, Threshold: -34.1854
Original likelihood: -33.14875030517578
Adjusted likelihood: -33.14875030517578
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9998)
Current yaw: tensor([ 0.0707, -0.0179,  0.1384], device='cuda:0')
10 turn
Sampling time 3.7833993890089914
tensor([-0.1703,  0.4176,  0.5210,  0.8109, -0.1056,  0.6740,  0.7206,  1.0856,
         1.2921,  0.3834,  0.1395,  1.2023,  0.0707, -0.0179,  0.1384,  1.9063],
       device='cuda:0')
Original likelihood: -34.734397888183594
Adjusted likelihood: -34.734397888183594
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0286)
State is out of distribution
Projection step: 0, Loss: 33.83203125
Projection step: 1, Loss: 33.96135330200195
Projection step: 2, Loss: 35.709693908691406
Projection step: 3, Loss: 34.757320404052734
Projection step: 4, Loss: 32.900447845458984
Projection step: 5, Loss: 32.772216796875
Projection step: 6, Loss: 32.840789794921875
Projection step: 7, Loss: 32.982975006103516
Projection step: 8, Loss: 32.356285095214844
Projection step: 9, Loss: 31.516542434692383
Projection step: 10, Loss: 33.034908294677734
Projection step: 11, Loss: 31.909168243408203
Projection step: 12, Loss: 30.198001861572266
Projection step: 13, Loss: 32.041988372802734
Projection step: 14, Loss: 32.096466064453125
Projection step: 15, Loss: 30.719741821289062
Projection step: 16, Loss: 35.36511993408203
Projection step: 17, Loss: 30.57274055480957
Projection step: 18, Loss: 30.73601722717285
Projection step: 19, Loss: 30.090686798095703
Projection step: 20, Loss: 29.971378326416016
Projection step: 21, Loss: 29.780046463012695
Projection step: 22, Loss: 29.666561126708984
Projection step: 23, Loss: 29.262420654296875
Projection step: 24, Loss: 28.217979431152344
Final likelihood: tensor([-31.0922, -26.6885, -27.4918, -34.8340, -29.4887, -25.8629, -27.2436,
        -34.4257, -29.2694, -26.7431, -30.2242, -29.9760, -29.4354, -28.3346,
        -28.0141, -30.5035])
Final projection likelihood: -29.3517
1 mode projection succeeded
New goal: tensor([-0.1175,  0.3966,  0.5687,  0.9006, -0.1032,  0.6234,  0.6655,  1.0825,
         1.2528,  0.2978,  0.1560,  1.2551,  0.0662, -0.0169,  0.1545],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0029]], device='cuda:0') tensor([[0.0021]], device='cuda:0')
Original likelihood: -32.95133590698242
Adjusted likelihood: -32.95133590698242
Likelihood residual: 0.0
Original likelihood: -34.471771240234375
Adjusted likelihood: -34.471771240234375
Likelihood residual: 0.0
{'index': 34.471771240234375, 'thumb_middle': 32.95133590698242}
Current yaw: tensor([ 0.0707, -0.0179,  0.1384], device='cuda:0')
11 thumb_middle
tensor([-0.1703,  0.4176,  0.5210,  0.8109, -0.1056,  0.6740,  0.7206,  1.0856,
         1.2921,  0.3834,  0.1395,  1.2023,  0.0707, -0.0179,  0.1384,  1.9063],
       device='cuda:0')
Solve time for step 1 8.741343581001274
Current ori: tensor([ 0.0707, -0.0179,  0.1384], device='cuda:0')
Index force: tensor([0.5738, 0.5027, 0.5157, 0.6180], device='cuda:0')
tensor([-0.1367,  0.3679,  0.5670,  0.9371, -0.1399,  0.6334,  0.6721,  1.0768,
         1.2320,  0.3098,  0.0703,  1.2160,  0.0972, -0.0397,  0.1608,  1.9165],
       device='cuda:0')
Solve time for step 2 3.5586289220373146
Current ori: tensor([ 0.0972, -0.0397,  0.1608], device='cuda:0')
Index force: tensor([0.5017, 0.5801, 0.5013], device='cuda:0')
tensor([-0.1445,  0.3972,  0.5708,  0.9424, -0.1482,  0.6320,  0.6798,  1.0830,
         1.2393,  0.2908,  0.0726,  1.2071,  0.2245, -0.0936,  0.1643,  2.2870],
       device='cuda:0')
Solve time for step 3 3.4454462209832855
Current ori: tensor([ 0.2245, -0.0936,  0.1643], device='cuda:0')
Index force: tensor([0.5689, 0.5009], device='cuda:0')
tensor([-0.0802,  0.4706,  0.7737,  0.9976, -0.0518,  0.6801,  0.7234,  1.1099,
         1.2564,  0.3010,  0.0911,  1.2195,  0.2770, -0.1402,  0.1654,  2.6978],
       device='cuda:0')
Solve time for step 4 3.3721183290472254
Current ori: tensor([ 0.2770, -0.1402,  0.1654], device='cuda:0')
Index force: tensor([0.5008], device='cuda:0')
Storing RECOVERY transition: reward=-0.2349 (scaled=-0.2349), steps=0
Reward stats updated: mean -0.0010 -> -0.0016, std: 0.1000
Collected 356 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.8240, Q2 Loss=1.8240, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1496
SAC Update 2/5: Actor Loss=-0.1312, Q1 Loss=1.1015, Q2 Loss=1.1015, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8434
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.8841, Q2 Loss=0.8841, Entropy=0.0023, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6538
SAC Update 4/5: Actor Loss=-0.1600, Q1 Loss=1.5353, Q2 Loss=1.5353, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4919
SAC Update 5/5: Actor Loss=-0.0003, Q1 Loss=0.8344, Q2 Loss=0.8344, Entropy=0.0591, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4482

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (18.8%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (40.1%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.058296
Q1 loss: 1.235850
Q2 loss: 1.235850
Current threshold: -34.1863
Global Scale Offset: 0.0753
Reward stats: mean=-0.0016, std=0.1000, count=356
----------------------------------------------
SAC Update - Actor Loss: -0.0583, Q1 Loss: 1.2359, Q2 Loss: 1.2359, Entropy: 0.0123, Mean TD Error: 1.1174, Threshold: -34.1863
Original likelihood: -147.7191162109375
Adjusted likelihood: -147.7191162109375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 154.63497924804688
Projection step: 1, Loss: 154.56976318359375
Projection step: 2, Loss: 150.51034545898438
Projection step: 3, Loss: 150.1741180419922
Projection step: 4, Loss: 143.4234161376953
Projection step: 5, Loss: 162.5234832763672
Projection step: 6, Loss: 157.1241455078125
Projection step: 7, Loss: 159.03762817382812
Projection step: 8, Loss: 162.7895050048828
Projection step: 9, Loss: 177.17572021484375
Projection step: 10, Loss: 153.35812377929688
Projection step: 11, Loss: 154.36900329589844
Projection step: 12, Loss: 165.59815979003906
Projection step: 13, Loss: 152.95965576171875
Projection step: 14, Loss: 143.35733032226562
Projection step: 15, Loss: 158.06210327148438
Projection step: 16, Loss: 155.02743530273438
Projection step: 17, Loss: 148.21359252929688
Projection step: 18, Loss: 145.40737915039062
Projection step: 19, Loss: 150.5341796875
Projection step: 20, Loss: 143.16708374023438
Projection step: 21, Loss: 153.58029174804688
Projection step: 22, Loss: 173.80117797851562
Projection step: 23, Loss: 149.32827758789062
Projection step: 24, Loss: 163.72012329101562
Final likelihood: tensor([-151.7470, -146.4916, -171.0303, -108.9120, -231.3384, -132.3919,
        -202.7061, -195.5663, -152.4115, -127.1355, -118.2691, -131.2579,
        -143.1337, -168.3815, -123.5437, -142.1045])
Final projection likelihood: -152.9013
1 mode projection failed, trying anyway
New goal: tensor([-0.0017,  0.5513,  0.9580,  1.1060,  0.1321,  0.8019,  0.8912,  1.2346,
         1.2919,  0.3042,  0.1869,  1.3159,  0.3548, -0.2617,  0.2653],
       device='cuda:0')
tensor([[0.0065]], device='cuda:0') tensor([[0.0222]], device='cuda:0') tensor([[0.0039]], device='cuda:0')
Original likelihood: -111.59114837646484
Adjusted likelihood: -111.59114837646484
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 111.59114837646484}
Current yaw: tensor([ 0.3551, -0.2613,  0.2382], device='cuda:0')
12 thumb_middle
tensor([-0.0201,  0.5547,  0.9864,  1.1301,  0.1386,  0.8126,  0.8785,  1.1859,
         1.3029,  0.2958,  0.1975,  1.3190,  0.3551, -0.2613,  0.2382,  3.8346],
       device='cuda:0')
Solve time for step 1 9.22904748498695
Current ori: tensor([ 0.3551, -0.2613,  0.2382], device='cuda:0')
Index force: tensor([0.5683, 0.5936, 0.5901, 0.5884], device='cuda:0')
tensor([-0.0523,  0.6006,  0.9912,  1.1201,  0.1037,  0.8415,  0.8972,  1.2177,
         1.2732,  0.3212,  0.1014,  1.3005,  0.3779, -0.3448,  0.4023,  4.3001],
       device='cuda:0')
Solve time for step 2 3.5552267400198616
Current ori: tensor([ 0.3779, -0.3448,  0.4023], device='cuda:0')
Index force: tensor([0.5714, 0.5458, 0.5488], device='cuda:0')
tensor([-0.0847,  0.6189,  0.9925,  1.1089,  0.0708,  0.8439,  0.9147,  1.2395,
         1.2504,  0.3341,  0.1073,  1.3221,  0.3812, -0.3532,  0.4273,  3.9170],
       device='cuda:0')
Solve time for step 3 3.827800869010389
Current ori: tensor([ 0.3812, -0.3532,  0.4273], device='cuda:0')
Index force: tensor([0.5867, 0.5920], device='cuda:0')
tensor([-0.0858,  0.6302,  0.9930,  1.1131,  0.0690,  0.8520,  0.9235,  1.2368,
         1.2490,  0.3362,  0.1037,  1.3355,  0.3816, -0.3540,  0.4363,  3.6353],
       device='cuda:0')
Solve time for step 4 3.381113593990449
Current ori: tensor([ 0.3816, -0.3540,  0.4363], device='cuda:0')
Index force: tensor([0.5615], device='cuda:0')
Storing RECOVERY transition: reward=-0.3982 (scaled=-0.3982), steps=0
Reward stats updated: mean -0.0016 -> -0.0027, std: 0.1021
Collected 357 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7210, Q2 Loss=0.7210, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8749
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.7608, Q2 Loss=0.7608, Entropy=0.0654, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2702
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2352, Q2 Loss=1.2352, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3755
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.4412, Q2 Loss=1.4412, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9458
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.9065, Q2 Loss=2.9065, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.2611

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (21.2%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.5%)
Actor update: 0.08s (37.4%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000006
Q1 loss: 1.412910
Q2 loss: 1.412910
Current threshold: -34.1872
Global Scale Offset: 0.0753
Reward stats: mean=-0.0027, std=0.1021, count=357
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.4129, Q2 Loss: 1.4129, Entropy: 0.0131, Mean TD Error: 1.5455, Threshold: -34.1872
Original likelihood: -287.8323974609375
Adjusted likelihood: -287.8323974609375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 321.29254150390625
Projection step: 1, Loss: 299.04241943359375
Projection step: 2, Loss: 329.6941833496094
Projection step: 3, Loss: 314.19940185546875
Projection step: 4, Loss: 320.26849365234375
Projection step: 5, Loss: 320.2396545410156
Projection step: 6, Loss: 318.72833251953125
Projection step: 7, Loss: 317.9803466796875
Projection step: 8, Loss: 315.4638671875
Projection step: 9, Loss: 305.345947265625
Projection step: 10, Loss: 318.8883056640625
Projection step: 11, Loss: 328.25225830078125
Projection step: 12, Loss: 311.0980529785156
Projection step: 13, Loss: 315.0750732421875
Projection step: 14, Loss: 324.99658203125
Projection step: 15, Loss: 322.0934753417969
Projection step: 16, Loss: 316.8811950683594
Projection step: 17, Loss: 305.223388671875
Projection step: 18, Loss: 314.91925048828125
Projection step: 19, Loss: 327.83062744140625
Projection step: 20, Loss: 298.0309753417969
Projection step: 21, Loss: 321.19921875
Projection step: 22, Loss: 308.3446350097656
Projection step: 23, Loss: 317.39495849609375
Projection step: 24, Loss: 324.1644287109375
Final likelihood: tensor([-321.7019, -290.6268, -332.4188, -324.2321, -288.0729, -312.1604,
        -365.8344, -352.8882, -365.9292, -335.2557, -301.3530, -254.7712,
        -329.8496, -335.3307, -298.1437, -365.2375])
Final projection likelihood: -323.3629
1 mode projection failed, trying anyway
New goal: tensor([-0.0315,  0.6658,  1.0326,  1.1239,  0.1171,  0.9098,  0.9401,  1.1996,
         1.2779,  0.3265,  0.0869,  1.4310,  0.3776, -0.3445,  0.4049],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0205]], device='cuda:0') tensor([[0.0067]], device='cuda:0')
Original likelihood: -309.7626953125
Adjusted likelihood: -309.7626953125
Likelihood residual: 0.0
Original likelihood: -288.70355224609375
Adjusted likelihood: -288.70355224609375
Likelihood residual: 0.0
{'index': 288.70355224609375, 'thumb_middle': 309.7626953125}
Current yaw: tensor([ 0.3776, -0.3442,  0.3881], device='cuda:0')
13 index
tensor([-0.0335,  0.6662,  1.0300,  1.1236,  0.1170,  0.9103,  0.9421,  1.1973,
         1.2826,  0.3233,  0.0881,  1.4290,  0.3776, -0.3442,  0.3881,  3.9972],
       device='cuda:0')
Solve time for step 1 10.577713040984236
Current ori: tensor([ 0.3776, -0.3442,  0.3881], device='cuda:0')
Middle force: tensor([0.5712, 0.5633, 0.5217, 0.5440], device='cuda:0')
Thumb force: tensor([0.5873, 0.5490, 0.5004, 0.5011], device='cuda:0')
tensor([-0.0057,  0.6614,  1.0255,  1.1166,  0.1100,  0.9938,  0.8654,  1.0714,
         1.2573,  0.3831,  0.0928,  1.3195,  0.3829, -0.3565,  0.4599,  3.9858],
       device='cuda:0')
Solve time for step 2 4.268378139007837
Current ori: tensor([ 0.3829, -0.3565,  0.4599], device='cuda:0')
Middle force: tensor([0.5823, 0.5030, 0.5496], device='cuda:0')
Thumb force: tensor([0.5500, 0.5111, 0.5489], device='cuda:0')
tensor([ 0.0243,  0.6743,  1.0391,  1.1256,  0.1274,  1.0725,  0.8359,  0.9946,
         1.2341,  0.4205,  0.0231,  1.3594,  0.3906, -0.3770,  0.4800,  3.7311],
       device='cuda:0')
Solve time for step 3 4.22146375401644
Current ori: tensor([ 0.3906, -0.3770,  0.4800], device='cuda:0')
Middle force: tensor([0.5270, 0.5906], device='cuda:0')
Thumb force: tensor([0.5347, 0.6051], device='cuda:0')
tensor([ 0.0283,  0.6773,  1.0433,  1.1271,  0.1007,  1.1037,  0.8507,  0.9941,
         1.2166,  0.4207,  0.0369,  1.3960,  0.3921, -0.3809,  0.4827,  3.7583],
       device='cuda:0')
Solve time for step 4 3.8678650369984098
Current ori: tensor([ 0.3921, -0.3809,  0.4827], device='cuda:0')
Middle force: tensor([0.5544], device='cuda:0')
Thumb force: tensor([0.5702], device='cuda:0')
Storing RECOVERY transition: reward=-0.4625 (scaled=-0.4625), steps=0
Reward stats updated: mean -0.0027 -> -0.0040, std: 0.1048
Collected 358 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1522, Q2 Loss=1.1522, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4360
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7239, Q2 Loss=0.7239, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0651
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=3.1013, Q2 Loss=3.1013, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.1833
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2300, Q2 Loss=1.2300, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5871
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2673, Q2 Loss=1.2673, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4373

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (17.7%)
Q1 update: 0.05s (20.3%)
Q2 update: 0.05s (19.2%)
Actor update: 0.10s (39.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.494940
Q2 loss: 1.494940
Current threshold: -34.1878
Global Scale Offset: 0.0753
Reward stats: mean=-0.0040, std=0.1048, count=358
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.4949, Q2 Loss: 1.4949, Entropy: 0.0000, Mean TD Error: 2.1418, Threshold: -34.1878
Original likelihood: -333.00677490234375
Adjusted likelihood: -333.00677490234375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 24
Loaded trajectory sampler
Current yaw: tensor([-0.0012,  0.0145, -0.0305], device='cuda:0')
Current yaw: tensor([-0.0012,  0.0145, -0.0305], device='cuda:0')
1 turn
Sampling time 3.6212612229865044
tensor([ 1.5539e-01,  5.7368e-01,  6.1114e-01,  6.3062e-01, -1.1855e-01,
         5.2761e-01,  9.4463e-01,  8.4299e-01,  1.2285e+00,  2.9583e-01,
         2.7583e-01,  1.1231e+00, -1.1562e-03,  1.4488e-02, -3.0475e-02,
         3.8849e-02], device='cuda:0')
Original likelihood: -22.355003356933594
Adjusted likelihood: -22.355003356933594
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.365334999980405
Current ori: tensor([-0.0012,  0.0145, -0.0305], device='cuda:0')
Middle force: tensor([1.4215, 1.4204, 1.2420, 0.5300, 0.5445, 1.1435, 1.5940, 0.6821, 0.5951,
        0.4947, 0.5651, 0.5975], device='cuda:0')
Thumb force: tensor([1.7433, 1.2002, 2.1853, 0.6501, 0.5607, 0.5493, 0.6020, 0.5369, 0.5960,
        0.6186, 0.5981, 0.5892], device='cuda:0')
Index force: tensor([0.9634, 0.6326, 0.5633, 0.5813, 0.6739, 0.6117, 0.5044, 0.7095, 0.5531,
        0.7131, 0.6027, 0.6513], device='cuda:0')
Storing NORMAL transition: reward=-0.1173 (scaled=-0.1173), steps=1
Reward stats updated: mean -0.0040 -> -0.0043, std: 0.1048
Collected 359 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1707, Q2 Loss=1.1707, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8839
SAC Update 2/5: Actor Loss=-0.0198, Q1 Loss=0.7478, Q2 Loss=0.7478, Entropy=0.2026, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7597
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.2675, Q2 Loss=1.2675, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7357
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8926, Q2 Loss=0.8926, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1518
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2606, Q2 Loss=1.2606, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9100

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (18.1%)
Q2 update: 0.05s (18.9%)
Actor update: 0.10s (40.7%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.050020
Q1 loss: 1.067827
Q2 loss: 1.067827
Current threshold: -34.1993
Global Scale Offset: 0.0752
Reward stats: mean=-0.0043, std=0.1048, count=359
----------------------------------------------
SAC Update - Actor Loss: -0.0500, Q1 Loss: 1.0678, Q2 Loss: 1.0678, Entropy: 0.0405, Mean TD Error: 0.6882, Threshold: -34.1993
tensor([ 0.1049,  0.6154,  0.5452,  0.5434, -0.1443,  0.5586,  0.8233,  0.9175,
         1.2599,  0.1879,  0.3443,  1.1698,  0.0079,  0.0311,  0.0862, -0.3582],
       device='cuda:0')
Original likelihood: -26.116729736328125
Adjusted likelihood: -26.116729736328125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.628628157021012
Current ori: tensor([0.0079, 0.0311, 0.0862], device='cuda:0')
Middle force: tensor([0.5764, 0.5601, 1.8325, 0.5657, 1.3305, 0.5082, 1.1154, 0.6033, 0.5384,
        0.5189, 0.6106], device='cuda:0')
Thumb force: tensor([0.8748, 0.6132, 1.4579, 0.5487, 0.8170, 0.5963, 1.2816, 1.9005, 0.5677,
        0.5777, 0.5579], device='cuda:0')
Index force: tensor([0.5096, 0.5010, 0.5284, 0.5790, 0.6356, 0.7334, 0.5987, 0.5640, 0.5404,
        0.7082, 0.6011], device='cuda:0')
Storing NORMAL transition: reward=0.1687 (scaled=0.1687), steps=1
Reward stats updated: mean -0.0043 -> -0.0039, std: 0.1050
Collected 360 transitions for RL
SAC Update 1/5: Actor Loss=-0.2302, Q1 Loss=0.6408, Q2 Loss=0.6408, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7101
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9825, Q2 Loss=0.9825, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8336
SAC Update 3/5: Actor Loss=-0.1176, Q1 Loss=0.9413, Q2 Loss=0.9413, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2764
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.2349, Q2 Loss=1.2349, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7809
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8212, Q2 Loss=0.8212, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0928

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.0%)
Q1 update: 0.06s (20.5%)
Q2 update: 0.05s (19.9%)
Actor update: 0.11s (41.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.069563
Q1 loss: 0.924138
Q2 loss: 0.924138
Current threshold: -34.2087
Global Scale Offset: 0.0751
Reward stats: mean=-0.0039, std=0.1050, count=360
----------------------------------------------
SAC Update - Actor Loss: -0.0696, Q1 Loss: 0.9241, Q2 Loss: 0.9241, Entropy: 0.0000, Mean TD Error: 0.5387, Threshold: -34.2087
tensor([ 0.0928,  0.6491,  0.5020,  0.5130, -0.1554,  0.5075,  0.8702,  0.9787,
         1.2283,  0.2509,  0.3919,  1.0394, -0.0039,  0.0357, -0.0830, -0.1795],
       device='cuda:0')
Original likelihood: -27.475399017333984
Adjusted likelihood: -27.475399017333984
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.287716805993114
Current ori: tensor([-0.0039,  0.0357, -0.0830], device='cuda:0')
Middle force: tensor([0.5574, 1.7979, 0.5632, 1.3135, 0.5074, 1.0999, 0.5974, 0.5353, 0.5131,
        0.6066], device='cuda:0')
Thumb force: tensor([0.6070, 1.4234, 0.5453, 0.8044, 0.5924, 1.2573, 1.8641, 0.5657, 0.5789,
        0.5551], device='cuda:0')
Index force: tensor([0.5008, 0.5286, 0.5737, 0.6283, 0.7257, 0.5939, 0.5606, 0.5381, 0.7183,
        0.5970], device='cuda:0')
Storing NORMAL transition: reward=-0.0029 (scaled=-0.0029), steps=1
Reward stats updated: mean -0.0039 -> -0.0039, std: 0.1049
Collected 361 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0131, Q2 Loss=1.0131, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1927
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.4446, Q2 Loss=1.4446, Entropy=0.0042, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3234
SAC Update 3/5: Actor Loss=-0.0001, Q1 Loss=1.6272, Q2 Loss=1.6272, Entropy=0.0753, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7744
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.7098, Q2 Loss=1.7098, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6972
SAC Update 5/5: Actor Loss=-0.0001, Q1 Loss=1.9918, Q2 Loss=1.9918, Entropy=0.0601, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7334

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (1.1%)
Target Q: 0.04s (15.7%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (20.3%)
Actor update: 0.10s (40.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000039
Q1 loss: 1.557301
Q2 loss: 1.557301
Current threshold: -34.2144
Global Scale Offset: 0.0750
Reward stats: mean=-0.0039, std=0.1049, count=361
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.5573, Q2 Loss: 1.5573, Entropy: 0.0279, Mean TD Error: 1.7442, Threshold: -34.2144
tensor([ 0.0541,  0.7019,  0.3643,  0.5443, -0.1365,  0.4734,  0.8333,  0.9000,
         1.2577,  0.2683,  0.3721,  1.0235, -0.0083,  0.0581, -0.0823, -0.2528],
       device='cuda:0')
Original likelihood: -29.534584045410156
Adjusted likelihood: -29.534584045410156
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.933539982012007
Current ori: tensor([-0.0083,  0.0581, -0.0823], device='cuda:0')
Middle force: tensor([1.7295, 0.5591, 1.2802, 0.5067, 1.0691, 0.5887, 0.5312, 0.5098, 0.6008],
       device='cuda:0')
Thumb force: tensor([1.4059, 0.5460, 0.7988, 0.5918, 1.2482, 1.8387, 0.5680, 0.5867, 0.5529],
       device='cuda:0')
Index force: tensor([0.5334, 0.5675, 0.6330, 0.7182, 0.5899, 0.5561, 0.5353, 0.7281, 0.5945],
       device='cuda:0')
Storing NORMAL transition: reward=0.1626 (scaled=0.1626), steps=1
Reward stats updated: mean -0.0039 -> -0.0034, std: 0.1051
Collected 362 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.2806, Q2 Loss=2.2806, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3928
SAC Update 2/5: Actor Loss=-0.1492, Q1 Loss=1.3852, Q2 Loss=1.3852, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3269
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1576, Q2 Loss=1.1576, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7686
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5421, Q2 Loss=1.5421, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3236
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9573, Q2 Loss=0.9573, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2715

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.1%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.6%)
Actor update: 0.08s (40.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.029834
Q1 loss: 1.464575
Q2 loss: 1.464575
Current threshold: -34.2180
Global Scale Offset: 0.0750
Reward stats: mean=-0.0034, std=0.1051, count=362
----------------------------------------------
SAC Update - Actor Loss: -0.0298, Q1 Loss: 1.4646, Q2 Loss: 1.4646, Entropy: 0.0000, Mean TD Error: 1.4167, Threshold: -34.2180
tensor([-0.0428,  0.6453,  0.3434,  0.5815, -0.1346,  0.4473,  0.8290,  1.0461,
         1.3411,  0.2533,  0.3790,  0.8003, -0.0085,  0.0520, -0.2458, -0.0932],
       device='cuda:0')
Original likelihood: -27.598859786987305
Adjusted likelihood: -27.598859786987305
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.695801393012516
Current ori: tensor([-0.0085,  0.0520, -0.2458], device='cuda:0')
Middle force: tensor([0.5011, 0.5865, 0.5012, 0.5188, 0.5965, 0.5882, 0.5771, 0.5631],
       device='cuda:0')
Thumb force: tensor([1.4625, 0.5675, 0.5553, 0.5188, 0.5695, 0.5803, 0.5146, 0.5563],
       device='cuda:0')
Index force: tensor([0.6064, 0.5370, 0.7252, 0.5002, 0.5327, 0.5844, 0.5529, 0.5504],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0031 (scaled=-0.0031), steps=1
Reward stats updated: mean -0.0034 -> -0.0034, std: 0.1050
Collected 363 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.7660, Q2 Loss=1.7660, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8375
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.4024, Q2 Loss=1.4024, Entropy=0.0045, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1210
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7637, Q2 Loss=0.7637, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2011
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.9832, Q2 Loss=2.9832, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.1107
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2065, Q2 Loss=1.2065, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4093

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.3%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.04s (18.6%)
Actor update: 0.09s (39.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000001
Q1 loss: 1.624366
Q2 loss: 1.624366
Current threshold: -34.2201
Global Scale Offset: 0.0749
Reward stats: mean=-0.0034, std=0.1050, count=363
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.6244, Q2 Loss: 1.6244, Entropy: 0.0009, Mean TD Error: 1.9359, Threshold: -34.2201
tensor([-2.9599e-02,  5.8519e-01,  4.5429e-01,  5.6188e-01, -1.2522e-01,
         4.3782e-01,  8.5656e-01,  1.0250e+00,  1.3851e+00,  2.1138e-01,
         3.1097e-01,  8.5496e-01, -1.1013e-03,  4.5541e-02, -2.4156e-01,
        -8.4612e-02], device='cuda:0')
Original likelihood: -25.98708724975586
Adjusted likelihood: -25.98708724975586
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.6220836349530146
Current ori: tensor([-0.0011,  0.0455, -0.2416], device='cuda:0')
Middle force: tensor([0.5800, 0.5009, 0.5175, 0.5965, 0.5838, 0.5738, 0.5589],
       device='cuda:0')
Thumb force: tensor([0.5601, 0.5497, 0.5157, 0.5632, 0.5747, 0.5127, 0.5521],
       device='cuda:0')
Index force: tensor([0.5352, 0.7183, 0.5000, 0.5285, 0.5802, 0.5485, 0.5468],
       device='cuda:0')
Storing NORMAL transition: reward=0.0703 (scaled=0.0703), steps=1
Reward stats updated: mean -0.0034 -> -0.0032, std: 0.1049
Collected 364 transitions for RL
SAC Update 1/5: Actor Loss=-0.0470, Q1 Loss=1.1859, Q2 Loss=1.1859, Entropy=0.0078, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6462
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9933, Q2 Loss=0.9933, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6307
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0632, Q2 Loss=1.0632, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4719
SAC Update 4/5: Actor Loss=-0.0001, Q1 Loss=1.0497, Q2 Loss=1.0497, Entropy=0.0849, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3027
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.7969, Q2 Loss=0.7969, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8616

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (20.7%)
Q1 update: 0.04s (18.0%)
Q2 update: 0.04s (19.9%)
Actor update: 0.08s (37.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.009423
Q1 loss: 1.017816
Q2 loss: 1.017816
Current threshold: -34.2417
Global Scale Offset: 0.0744
Reward stats: mean=-0.0032, std=0.1049, count=364
----------------------------------------------
SAC Update - Actor Loss: -0.0094, Q1 Loss: 1.0178, Q2 Loss: 1.0178, Entropy: 0.0185, Mean TD Error: 0.9826, Threshold: -34.2417
tensor([-8.9152e-04,  5.5583e-01,  5.4495e-01,  5.1492e-01, -9.9663e-02,
         4.3786e-01,  8.6666e-01,  1.0589e+00,  1.4496e+00,  1.9814e-01,
         1.7328e-01,  9.2874e-01, -2.7084e-03,  2.7914e-02, -3.1020e-01,
        -1.2196e-01], device='cuda:0')
Original likelihood: -17.850765228271484
Adjusted likelihood: -17.850765228271484
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.529393756994978
Current ori: tensor([-0.0027,  0.0279, -0.3102], device='cuda:0')
Middle force: tensor([0.5008, 0.5160, 0.5904, 0.5790, 0.5674, 0.5520], device='cuda:0')
Thumb force: tensor([0.5444, 0.5132, 0.5594, 0.5698, 0.5118, 0.5506], device='cuda:0')
Index force: tensor([0.7002, 0.5000, 0.5255, 0.5748, 0.5448, 0.5438], device='cuda:0')
Storing NORMAL transition: reward=-0.0504 (scaled=-0.0504), steps=1
Reward stats updated: mean -0.0032 -> -0.0033, std: 0.1048
Collected 365 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9254, Q2 Loss=0.9254, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7618
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9701, Q2 Loss=0.9701, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8146
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.1897, Q2 Loss=1.1897, Entropy=0.0057, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3274
SAC Update 4/5: Actor Loss=-0.2304, Q1 Loss=1.6442, Q2 Loss=1.6442, Entropy=0.0918, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5485
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.7245, Q2 Loss=1.7245, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4297

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.5%)
Q1 update: 0.06s (19.7%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (40.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046084
Q1 loss: 1.290778
Q2 loss: 1.290778
Current threshold: -34.2548
Global Scale Offset: 0.0741
Reward stats: mean=-0.0033, std=0.1048, count=365
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.2908, Q2 Loss: 1.2908, Entropy: 0.0195, Mean TD Error: 1.1764, Threshold: -34.2548
tensor([-0.0180,  0.6030,  0.4389,  0.5614, -0.1105,  0.4101,  0.9553,  0.9003,
         1.4518,  0.1984,  0.1842,  0.9207, -0.0072,  0.0382, -0.2607, -0.1328],
       device='cuda:0')
Original likelihood: -21.38925552368164
Adjusted likelihood: -21.38925552368164
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.384091521031223
Current ori: tensor([-0.0072,  0.0382, -0.2607], device='cuda:0')
Middle force: tensor([0.7138, 0.5850, 0.5048, 0.5665, 0.5958], device='cuda:0')
Thumb force: tensor([0.5070, 0.5610, 0.5283, 0.5598, 0.5508], device='cuda:0')
Index force: tensor([0.6985, 0.5363, 0.6484, 0.5701, 0.6154], device='cuda:0')
Storing NORMAL transition: reward=-0.0446 (scaled=-0.0446), steps=1
Reward stats updated: mean -0.0033 -> -0.0034, std: 0.1047
Collected 366 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8900, Q2 Loss=0.8900, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1779
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1375, Q2 Loss=1.1375, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5276
SAC Update 3/5: Actor Loss=-0.0534, Q1 Loss=1.0727, Q2 Loss=1.0727, Entropy=0.0080, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3923
SAC Update 4/5: Actor Loss=-0.1860, Q1 Loss=1.5838, Q2 Loss=1.5838, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3972
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2449, Q2 Loss=1.2449, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5264

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.3%)
Q1 update: 0.06s (20.2%)
Q2 update: 0.05s (19.8%)
Actor update: 0.12s (42.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.047889
Q1 loss: 1.185764
Q2 loss: 1.185764
Current threshold: -34.2763
Global Scale Offset: 0.0735
Reward stats: mean=-0.0034, std=0.1047, count=366
----------------------------------------------
SAC Update - Actor Loss: -0.0479, Q1 Loss: 1.1858, Q2 Loss: 1.1858, Entropy: 0.0016, Mean TD Error: 1.0043, Threshold: -34.2763
tensor([ 0.0395,  0.5975,  0.5588,  0.4281, -0.2134,  0.4232,  0.8990,  0.9327,
         1.4692,  0.1679,  0.2883,  0.8805,  0.0230,  0.0869, -0.2257,  0.2218],
       device='cuda:0')
Original likelihood: -49.398521423339844
Adjusted likelihood: -49.398521423339844
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 51.42101287841797
Projection step: 1, Loss: 50.47835159301758
Projection step: 2, Loss: 47.002838134765625
Projection step: 3, Loss: 48.276611328125
Projection step: 4, Loss: 46.844825744628906
Projection step: 5, Loss: 47.11918640136719
Projection step: 6, Loss: 45.63799285888672
Projection step: 7, Loss: 44.46698760986328
Projection step: 8, Loss: 43.100059509277344
Projection step: 9, Loss: 44.05277633666992
Projection step: 10, Loss: 43.04131317138672
Projection step: 11, Loss: 43.677703857421875
Projection step: 12, Loss: 43.86713790893555
Projection step: 13, Loss: 42.06146240234375
Projection step: 14, Loss: 42.20305633544922
Projection step: 15, Loss: 41.02141189575195
Projection step: 16, Loss: 43.00425720214844
Projection step: 17, Loss: 41.99619674682617
Projection step: 18, Loss: 40.78484344482422
Projection step: 19, Loss: 40.25840377807617
Projection step: 20, Loss: 40.957210540771484
Projection step: 21, Loss: 39.419822692871094
Projection step: 22, Loss: 40.80332565307617
Projection step: 23, Loss: 38.776092529296875
Projection step: 24, Loss: 39.146034240722656
Final likelihood: tensor([-37.1636, -38.9959, -37.0993, -40.0071, -38.1397, -38.6468, -33.1329,
        -35.9529, -39.6103, -40.2845, -46.1019, -32.9789, -37.3952, -39.4039,
        -38.8073, -45.7468])
Final projection likelihood: -38.7167
1 mode projection failed, trying anyway
New goal: tensor([ 0.0169,  0.6121,  0.5119,  0.5028, -0.1902,  0.4378,  0.7870,  0.8737,
         1.4535,  0.1331,  0.3406,  1.0026,  0.0274,  0.0770, -0.1144],
       device='cuda:0')
tensor([[0.0030]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0023]], device='cuda:0')
Original likelihood: -42.697898864746094
Adjusted likelihood: -42.697898864746094
Likelihood residual: 0.0
Original likelihood: -43.446510314941406
Adjusted likelihood: -43.446510314941406
Likelihood residual: 0.0
{'index': 43.446510314941406, 'thumb_middle': 42.697898864746094}
Current yaw: tensor([ 0.0230,  0.0869, -0.2257], device='cuda:0')
2 thumb_middle
tensor([ 0.0395,  0.5975,  0.5588,  0.4281, -0.2134,  0.4232,  0.8990,  0.9327,
         1.4692,  0.1679,  0.2883,  0.8805,  0.0230,  0.0869, -0.2257,  0.2218],
       device='cuda:0')
Solve time for step 1 8.981889181013685
Current ori: tensor([ 0.0230,  0.0869, -0.2257], device='cuda:0')
Index force: tensor([0.5689, 0.5570, 0.5910, 0.5927], device='cuda:0')
tensor([ 0.0303,  0.6102,  0.5109,  0.4728, -0.2595,  0.4517,  0.8161,  0.8800,
         1.3968,  0.0999,  0.2563,  0.9528,  0.0240,  0.0921, -0.2256,  0.2233],
       device='cuda:0')
Solve time for step 2 3.59709551098058
Current ori: tensor([ 0.0240,  0.0921, -0.2256], device='cuda:0')
Index force: tensor([0.5491, 0.5827, 0.5841], device='cuda:0')
tensor([ 0.0306,  0.6070,  0.5060,  0.4917, -0.2584,  0.4620,  0.8105,  0.8763,
         1.3913,  0.0921,  0.2531,  0.9643,  0.0259,  0.0922, -0.2256,  0.2302],
       device='cuda:0')
Solve time for step 3 3.4764817359973677
Current ori: tensor([ 0.0259,  0.0922, -0.2256], device='cuda:0')
Index force: tensor([0.5714, 0.5728], device='cuda:0')
tensor([ 0.0279,  0.5919,  0.5174,  0.5059, -0.2599,  0.4627,  0.8092,  0.8746,
         1.3936,  0.0862,  0.2546,  0.9713,  0.0302,  0.0942, -0.2256,  0.2317],
       device='cuda:0')
Solve time for step 4 3.5672532359603792
Current ori: tensor([ 0.0302,  0.0942, -0.2256], device='cuda:0')
Index force: tensor([0.5498], device='cuda:0')
Storing RECOVERY transition: reward=-0.0027 (scaled=-0.0003), steps=8
Reward stats updated: mean -0.0034 -> -0.0034, std: 0.1045
Collected 367 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4671, Q2 Loss=1.4671, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5919
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.9220, Q2 Loss=0.9220, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4159
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5425, Q2 Loss=1.5425, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6679
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1430, Q2 Loss=1.1430, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9916
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0874, Q2 Loss=1.0874, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4084

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (16.0%)
Q1 update: 0.04s (17.2%)
Q2 update: 0.04s (16.1%)
Actor update: 0.09s (34.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.232411
Q2 loss: 1.232411
Current threshold: -34.2961
Global Scale Offset: 0.0729
Reward stats: mean=-0.0034, std=0.1045, count=367
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.2324, Q2 Loss: 1.2324, Entropy: 0.0000, Mean TD Error: 1.2152, Threshold: -34.2961
Original likelihood: -37.227779388427734
Adjusted likelihood: -37.227779388427734
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 45.64030075073242
Projection step: 1, Loss: 44.93545150756836
Projection step: 2, Loss: 42.43096160888672
Projection step: 3, Loss: 39.23996353149414
Projection step: 4, Loss: 44.95410919189453
Projection step: 5, Loss: 42.057640075683594
Projection step: 6, Loss: 38.79038619995117
Projection step: 7, Loss: 40.905738830566406
Projection step: 8, Loss: 38.67340087890625
Projection step: 9, Loss: 42.80950927734375
Projection step: 10, Loss: 42.28330612182617
Projection step: 11, Loss: 39.103233337402344
Projection step: 12, Loss: 40.410736083984375
Projection step: 13, Loss: 43.62459182739258
Projection step: 14, Loss: 37.74859619140625
Projection step: 15, Loss: 38.183502197265625
Projection step: 16, Loss: 40.94371032714844
Projection step: 17, Loss: 41.143611907958984
Projection step: 18, Loss: 39.37957763671875
Projection step: 19, Loss: 35.44586181640625
Projection step: 20, Loss: 37.51511764526367
Projection step: 21, Loss: 39.783447265625
Projection step: 22, Loss: 40.65837097167969
Projection step: 23, Loss: 41.751094818115234
Projection step: 24, Loss: 40.00022506713867
Final likelihood: tensor([-35.0857, -37.4944, -34.1383, -40.0677, -37.5255, -16.1913, -23.1743,
        -41.8490, -17.8494, -43.2561, -18.2368, -28.9544, -40.3455, -30.6227,
        -38.1625, -41.2677])
Final projection likelihood: -32.7638
1 mode projection succeeded
New goal: tensor([-0.0047,  0.6160,  0.4844,  0.5142, -0.1950,  0.4805,  0.7565,  0.8204,
         1.4342,  0.0809,  0.3184,  1.0435,  0.0291,  0.0867,  0.1292],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0023]], device='cuda:0')
Original likelihood: -39.964576721191406
Adjusted likelihood: -39.964576721191406
Likelihood residual: 0.0
Original likelihood: -38.47052764892578
Adjusted likelihood: -38.47052764892578
Likelihood residual: 0.0
{'index': 38.47052764892578, 'thumb_middle': 39.964576721191406}
Current yaw: tensor([ 0.0251,  0.0951, -0.2253], device='cuda:0')
3 index
tensor([ 0.0167,  0.6202,  0.4844,  0.4730, -0.2133,  0.4896,  0.8098,  0.8774,
         1.4579,  0.1187,  0.3022,  0.9972,  0.0251,  0.0951, -0.2253,  0.1133],
       device='cuda:0')
Solve time for step 1 10.580901754030492
Current ori: tensor([ 0.0251,  0.0951, -0.2253], device='cuda:0')
Middle force: tensor([0.5188, 0.6212, 0.5087, 0.5571], device='cuda:0')
Thumb force: tensor([0.5990, 0.5175, 0.5198, 0.5714], device='cuda:0')
tensor([ 0.0445,  0.5388,  0.4277,  0.4857, -0.2170,  0.4878,  0.8109,  0.8770,
         1.4829,  0.0894,  0.2895,  0.9932,  0.0240,  0.0971, -0.2367, -0.8059],
       device='cuda:0')
Solve time for step 2 4.141380934976041
Current ori: tensor([ 0.0240,  0.0971, -0.2367], device='cuda:0')
Middle force: tensor([0.6158, 0.5078, 0.5537], device='cuda:0')
Thumb force: tensor([0.5155, 0.5182, 0.5684], device='cuda:0')
tensor([ 0.0463,  0.5403,  0.4260,  0.4906, -0.2072,  0.4994,  0.8061,  0.8633,
         1.4783,  0.0899,  0.2804,  1.0023,  0.0206,  0.0921, -0.2252, -0.9606],
       device='cuda:0')
Solve time for step 3 4.213029492995702
Current ori: tensor([ 0.0206,  0.0921, -0.2252], device='cuda:0')
Middle force: tensor([0.5484, 0.5187], device='cuda:0')
Thumb force: tensor([0.5950, 0.5059], device='cuda:0')
tensor([ 0.0460,  0.5401,  0.4259,  0.4907, -0.2118,  0.5011,  0.8012,  0.8589,
         1.4800,  0.0919,  0.2871,  0.9935,  0.0198,  0.0956, -0.2213, -0.5772],
       device='cuda:0')
Solve time for step 4 4.365765216003638
Current ori: tensor([ 0.0198,  0.0956, -0.2213], device='cuda:0')
Middle force: tensor([0.5619], device='cuda:0')
Thumb force: tensor([0.5612], device='cuda:0')
Storing RECOVERY transition: reward=-0.0162 (scaled=-0.0020), steps=8
Reward stats updated: mean -0.0034 -> -0.0034, std: 0.1044
Collected 368 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.1073, Q2 Loss=2.1073, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9482
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.0177, Q2 Loss=1.0177, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4146
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9291, Q2 Loss=0.9291, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5414
SAC Update 4/5: Actor Loss=-0.2304, Q1 Loss=1.1315, Q2 Loss=1.1315, Entropy=0.1174, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3649
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.0302, Q2 Loss=1.0302, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3486

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.0%)
Q1 update: 0.06s (20.8%)
Q2 update: 0.05s (20.0%)
Actor update: 0.11s (41.0%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092124
Q1 loss: 1.243151
Q2 loss: 1.243151
Current threshold: -34.3079
Global Scale Offset: 0.0726
Reward stats: mean=-0.0034, std=0.1044, count=368
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.2432, Q2 Loss: 1.2432, Entropy: 0.0235, Mean TD Error: 1.3235, Threshold: -34.3079
Original likelihood: -37.678794860839844
Adjusted likelihood: -37.678794860839844
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.39684295654297
Projection step: 1, Loss: 41.63909149169922
Projection step: 2, Loss: 35.34892272949219
Projection step: 3, Loss: 35.27122116088867
Projection step: 4, Loss: 41.285499572753906
Projection step: 5, Loss: 33.70166015625
Projection step: 6, Loss: 32.81114959716797
Projection step: 7, Loss: 36.86895751953125
Projection step: 8, Loss: 31.44320297241211
Projection step: 9, Loss: 41.010772705078125
Projection step: 10, Loss: 33.590576171875
Projection step: 11, Loss: 35.35818099975586
Projection step: 12, Loss: 37.82746124267578
Projection step: 13, Loss: 36.48846435546875
Projection step: 14, Loss: 35.51725769042969
Projection step: 15, Loss: 34.96206283569336
Projection step: 16, Loss: 39.655426025390625
Projection step: 17, Loss: 36.79192352294922
Projection step: 18, Loss: 36.55585479736328
Projection step: 19, Loss: 39.43223571777344
Projection step: 20, Loss: 39.71685028076172
Projection step: 21, Loss: 40.90458679199219
Projection step: 22, Loss: 32.656089782714844
Projection step: 23, Loss: 36.20392608642578
Projection step: 24, Loss: 39.32416534423828
Final likelihood: tensor([-37.0444, -20.1364, -37.9282, -17.6938, -37.8582, -36.8030, -36.5737,
        -23.8767, -39.0015, -28.8778, -37.5745, -38.4369, -36.8848, -38.1831,
        -36.6674, -40.3994])
Final projection likelihood: -33.9962
1 mode projection succeeded
New goal: tensor([-0.0140,  0.6060,  0.4821,  0.5318, -0.1892,  0.4918,  0.7424,  0.7905,
         1.4458,  0.0461,  0.3032,  1.0267,  0.0166,  0.0884, -0.2182],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0023]], device='cuda:0')
Original likelihood: -33.766117095947266
Adjusted likelihood: -33.766117095947266
Likelihood residual: 0.0
Original likelihood: -35.75443649291992
Adjusted likelihood: -35.75443649291992
Likelihood residual: 0.0
{'index': 35.75443649291992, 'thumb_middle': 33.766117095947266}
Current yaw: tensor([ 0.0178,  0.0958, -0.2110], device='cuda:0')
4 thumb_middle
tensor([ 9.4013e-04,  6.1107e-01,  4.6841e-01,  5.0781e-01, -2.1086e-01,
         5.0899e-01,  7.9427e-01,  8.5054e-01,  1.4799e+00,  9.1690e-02,
         2.8721e-01,  9.9224e-01,  1.7756e-02,  9.5835e-02, -2.1099e-01,
        -4.0594e-01], device='cuda:0')
Solve time for step 1 9.050914924999233
Current ori: tensor([ 0.0178,  0.0958, -0.2110], device='cuda:0')
Index force: tensor([0.5875, 0.5017, 0.5951, 0.6027], device='cuda:0')
tensor([ 0.0128,  0.5602,  0.4955,  0.6151, -0.2446,  0.5125,  0.7635,  0.7988,
         1.4230,  0.0160,  0.2544,  1.0006,  0.0352,  0.0921, -0.2109, -0.3686],
       device='cuda:0')
Solve time for step 2 3.7129118249868043
Current ori: tensor([ 0.0352,  0.0921, -0.2109], device='cuda:0')
Index force: tensor([0.5012, 0.5870, 0.5937], device='cuda:0')
tensor([ 0.0030,  0.5653,  0.4964,  0.5817, -0.2511,  0.5218,  0.7516,  0.7877,
         1.4237,  0.0245,  0.2521,  1.0100,  0.0325,  0.0966, -0.2109, -0.3877],
       device='cuda:0')
Solve time for step 3 3.530798332998529
Current ori: tensor([ 0.0325,  0.0966, -0.2109], device='cuda:0')
Index force: tensor([0.5755, 0.5827], device='cuda:0')
tensor([ 0.0017,  0.5998,  0.4750,  0.5274, -0.2485,  0.5224,  0.7542,  0.8022,
         1.4224,  0.0206,  0.2444,  1.0076,  0.0214,  0.0960, -0.2109, -0.4065],
       device='cuda:0')
Solve time for step 4 3.5030329750152305
Current ori: tensor([ 0.0214,  0.0960, -0.2109], device='cuda:0')
Index force: tensor([0.5640], device='cuda:0')
Storing RECOVERY transition: reward=-0.0177 (scaled=-0.0022), steps=8
Reward stats updated: mean -0.0034 -> -0.0034, std: 0.1042
Collected 369 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7215, Q2 Loss=0.7215, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4529
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.1250, Q2 Loss=1.1250, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1659
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1811, Q2 Loss=1.1811, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3437
SAC Update 4/5: Actor Loss=-0.0313, Q1 Loss=0.7927, Q2 Loss=0.7927, Entropy=0.0899, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0211
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5836, Q2 Loss=1.5836, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7787

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (16.9%)
Q1 update: 0.06s (18.9%)
Q2 update: 0.05s (18.6%)
Actor update: 0.12s (41.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.006255
Q1 loss: 1.080786
Q2 loss: 1.080786
Current threshold: -34.3233
Global Scale Offset: 0.0723
Reward stats: mean=-0.0034, std=0.1042, count=369
----------------------------------------------
SAC Update - Actor Loss: -0.0063, Q1 Loss: 1.0808, Q2 Loss: 1.0808, Entropy: 0.0180, Mean TD Error: 1.1525, Threshold: -34.3233
Original likelihood: -38.80897521972656
Adjusted likelihood: -38.80897521972656
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 43.12715148925781
Projection step: 1, Loss: 31.913986206054688
Projection step: 2, Loss: 36.09947967529297
Projection step: 3, Loss: 31.976083755493164
Projection step: 4, Loss: 35.810848236083984
Projection step: 5, Loss: 35.035858154296875
Projection step: 6, Loss: 35.450775146484375
Projection step: 7, Loss: 32.70795440673828
Projection step: 8, Loss: 34.22821044921875
Projection step: 9, Loss: 34.75282669067383
Projection step: 10, Loss: 31.339462280273438
Projection step: 11, Loss: 35.85723114013672
Projection step: 12, Loss: 32.703792572021484
Projection step: 13, Loss: 34.332984924316406
Projection step: 14, Loss: 35.35435104370117
Projection step: 15, Loss: 37.103214263916016
Projection step: 16, Loss: 36.34429168701172
Projection step: 17, Loss: 34.35546112060547
Projection step: 18, Loss: 31.366016387939453
Projection step: 19, Loss: 34.209564208984375
Projection step: 20, Loss: 32.22114944458008
Projection step: 21, Loss: 34.97627258300781
Projection step: 22, Loss: 33.459144592285156
Projection step: 23, Loss: 34.926307678222656
Projection step: 24, Loss: 31.314599990844727
Final likelihood: tensor([-43.7215, -25.3483, -39.1282, -36.7181, -17.5078, -34.1048, -37.9204,
        -37.3598, -44.5425, -21.6458, -21.7171, -18.7853, -43.0063, -40.0126,
        -38.4527, -20.7745])
Final projection likelihood: -32.5466
1 mode projection succeeded
New goal: tensor([-0.0166,  0.5881,  0.4991,  0.5396, -0.1935,  0.5133,  0.7397,  0.7576,
         1.4549,  0.0360,  0.3112,  1.0331,  0.0233,  0.0917, -0.1263],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -40.53043746948242
Adjusted likelihood: -40.53043746948242
Likelihood residual: 0.0
Original likelihood: -39.50232696533203
Adjusted likelihood: -39.50232696533203
Likelihood residual: 0.0
{'index': 39.50232696533203, 'thumb_middle': 40.53043746948242}
Current yaw: tensor([ 0.0225,  0.0979, -0.2103], device='cuda:0')
5 index
tensor([-0.0024,  0.5957,  0.4786,  0.5252, -0.2098,  0.5421,  0.7714,  0.7959,
         1.4851,  0.0675,  0.2967,  1.0183,  0.0225,  0.0979, -0.2103, -0.4109],
       device='cuda:0')
Solve time for step 1 10.426968730986118
Current ori: tensor([ 0.0225,  0.0979, -0.2103], device='cuda:0')
Middle force: tensor([0.5482, 0.5412, 0.5114, 0.5584], device='cuda:0')
Thumb force: tensor([0.5193, 0.5658, 0.6008, 0.5195], device='cuda:0')
tensor([ 0.0275,  0.5185,  0.4398,  0.5136, -0.2126,  0.5391,  0.7777,  0.7893,
         1.4987,  0.0499,  0.2997,  0.9979,  0.0205,  0.0992, -0.2253, -1.0410],
       device='cuda:0')
Solve time for step 2 4.178736174944788
Current ori: tensor([ 0.0205,  0.0992, -0.2253], device='cuda:0')
Middle force: tensor([0.5377, 0.5103, 0.5550], device='cuda:0')
Thumb force: tensor([0.5611, 0.5962, 0.5178], device='cuda:0')
tensor([ 0.0293,  0.5171,  0.4405,  0.5135, -0.2106,  0.5393,  0.7790,  0.7891,
         1.5000,  0.0464,  0.2965,  1.0008,  0.0204,  0.0979, -0.2257, -0.9304],
       device='cuda:0')
Solve time for step 3 4.1214617129880935
Current ori: tensor([ 0.0204,  0.0979, -0.2257], device='cuda:0')
Middle force: tensor([0.5090, 0.5517], device='cuda:0')
Thumb force: tensor([0.5898, 0.5161], device='cuda:0')
tensor([ 0.0274,  0.5163,  0.4406,  0.5138, -0.2096,  0.5396,  0.7791,  0.7880,
         1.5000,  0.0458,  0.2951,  1.0036,  0.0204,  0.0975, -0.2234, -0.3781],
       device='cuda:0')
Solve time for step 4 3.959292001032736
Current ori: tensor([ 0.0204,  0.0975, -0.2234], device='cuda:0')
Middle force: tensor([0.5557], device='cuda:0')
Thumb force: tensor([0.5672], device='cuda:0')
Storing RECOVERY transition: reward=-0.0180 (scaled=-0.0023), steps=8
Reward stats updated: mean -0.0034 -> -0.0034, std: 0.1041
Collected 370 transitions for RL
SAC Update 1/5: Actor Loss=-0.0851, Q1 Loss=0.7464, Q2 Loss=0.7464, Entropy=0.0010, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2509
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.1887, Q2 Loss=2.1887, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0081
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.6902, Q2 Loss=0.6902, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1741
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1761, Q2 Loss=1.1761, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2831
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.6770, Q2 Loss=0.6770, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0869

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.6%)
Q1 update: 0.05s (18.6%)
Q2 update: 0.05s (19.2%)
Actor update: 0.10s (40.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.063070
Q1 loss: 1.095683
Q2 loss: 1.095683
Current threshold: -34.3725
Global Scale Offset: 0.0710
Reward stats: mean=-0.0034, std=0.1041, count=370
----------------------------------------------
SAC Update - Actor Loss: -0.0631, Q1 Loss: 1.0957, Q2 Loss: 1.0957, Entropy: 0.0002, Mean TD Error: 0.9606, Threshold: -34.3725
Original likelihood: -38.724952697753906
Adjusted likelihood: -38.724952697753906
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 40.247100830078125
Projection step: 1, Loss: 37.768882751464844
Projection step: 2, Loss: 36.742530822753906
Projection step: 3, Loss: 34.36170196533203
Projection step: 4, Loss: 33.87203598022461
Projection step: 5, Loss: 37.041015625
Projection step: 6, Loss: 32.21784973144531
Projection step: 7, Loss: 36.257484436035156
Projection step: 8, Loss: 36.10588073730469
Projection step: 9, Loss: 37.3541259765625
Projection step: 10, Loss: 29.556835174560547
Projection step: 11, Loss: 32.045860290527344
Projection step: 12, Loss: 35.69001388549805
Projection step: 13, Loss: 36.684165954589844
Projection step: 14, Loss: 34.69175338745117
Projection step: 15, Loss: 36.3656120300293
Projection step: 16, Loss: 35.391563415527344
Projection step: 17, Loss: 34.19150924682617
Projection step: 18, Loss: 33.815391540527344
Projection step: 19, Loss: 32.103275299072266
Projection step: 20, Loss: 32.071632385253906
Projection step: 21, Loss: 31.18001365661621
Projection step: 22, Loss: 35.695350646972656
Projection step: 23, Loss: 33.400123596191406
Projection step: 24, Loss: 31.771541595458984
Final likelihood: tensor([-43.0020, -32.5325, -25.6850, -37.0043, -19.9348, -35.1486, -41.3773,
        -42.7432, -36.5432, -36.6768, -37.3816, -23.9888, -37.1447, -24.5985,
        -36.1389, -35.5249])
Final projection likelihood: -34.0891
1 mode projection succeeded
New goal: tensor([-0.0197,  0.5809,  0.5073,  0.5420, -0.1829,  0.5234,  0.7244,  0.7435,
         1.4568,  0.0226,  0.3082,  1.0202,  0.0156,  0.0878, -0.2806],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -38.030845642089844
Adjusted likelihood: -38.030845642089844
Likelihood residual: 0.0
Original likelihood: -37.00286102294922
Adjusted likelihood: -37.00286102294922
Likelihood residual: 0.0
{'index': 37.00286102294922, 'thumb_middle': 38.030845642089844}
Current yaw: tensor([ 0.0168,  0.0948, -0.2087], device='cuda:0')
6 index
tensor([-0.0117,  0.5838,  0.4858,  0.5320, -0.2031,  0.5510,  0.7700,  0.7768,
         1.4947,  0.0494,  0.2903,  1.0082,  0.0168,  0.0948, -0.2087, -0.1904],
       device='cuda:0')
Solve time for step 1 10.375407931976952
Current ori: tensor([ 0.0168,  0.0948, -0.2087], device='cuda:0')
Middle force: tensor([0.5014, 0.5908, 0.6170, 0.5191], device='cuda:0')
Thumb force: tensor([0.5280, 0.6256, 0.5713, 0.6031], device='cuda:0')
tensor([ 0.0277,  0.5102,  0.4492,  0.5146, -0.2021,  0.5553,  0.7659,  0.7743,
         1.4933,  0.0517,  0.2919,  0.9981,  0.0140,  0.0943, -0.2148, -0.2736],
       device='cuda:0')
Solve time for step 2 4.008271721017081
Current ori: tensor([ 0.0140,  0.0943, -0.2148], device='cuda:0')
Middle force: tensor([0.5873, 0.6123, 0.5174], device='cuda:0')
Thumb force: tensor([0.6168, 0.5686, 0.5987], device='cuda:0')
tensor([ 0.0276,  0.5103,  0.4476,  0.5153, -0.2025,  0.5527,  0.7684,  0.7763,
         1.5000,  0.0419,  0.2901,  0.9980,  0.0147,  0.0944, -0.2186, -0.0301],
       device='cuda:0')
Solve time for step 3 4.0145624789875
Current ori: tensor([ 0.0147,  0.0944, -0.2186], device='cuda:0')
Middle force: tensor([0.6115, 0.5156], device='cuda:0')
Thumb force: tensor([0.5608, 0.5933], device='cuda:0')
tensor([ 0.0267,  0.5090,  0.4469,  0.5139, -0.2019,  0.5592,  0.7617,  0.7686,
         1.5000,  0.0431,  0.2925,  0.9872,  0.0112,  0.0948, -0.2180,  0.3084],
       device='cuda:0')
Solve time for step 4 3.987028844014276
Current ori: tensor([ 0.0112,  0.0948, -0.2180], device='cuda:0')
Middle force: tensor([0.5446], device='cuda:0')
Thumb force: tensor([0.5921], device='cuda:0')
Storing RECOVERY transition: reward=-0.0173 (scaled=-0.0022), steps=8
Reward stats updated: mean -0.0034 -> -0.0034, std: 0.1040
Collected 371 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7915, Q2 Loss=0.7915, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.5763
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2921, Q2 Loss=1.2921, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0812
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6926, Q2 Loss=0.6926, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5172
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0787, Q2 Loss=1.0787, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9433
SAC Update 5/5: Actor Loss=-0.0100, Q1 Loss=0.6759, Q2 Loss=0.6759, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2680

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (20.4%)
Q1 update: 0.04s (19.6%)
Q2 update: 0.04s (17.8%)
Actor update: 0.08s (38.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.001995
Q1 loss: 0.906143
Q2 loss: 0.906143
Current threshold: -34.4025
Global Scale Offset: 0.0702
Reward stats: mean=-0.0034, std=0.1040, count=371
----------------------------------------------
SAC Update - Actor Loss: -0.0020, Q1 Loss: 0.9061, Q2 Loss: 0.9061, Entropy: 0.0000, Mean TD Error: 1.2772, Threshold: -34.4025
Original likelihood: -35.50395965576172
Adjusted likelihood: -35.50395965576172
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 36.08736801147461
Projection step: 1, Loss: 38.458099365234375
Projection step: 2, Loss: 31.050945281982422
Projection step: 3, Loss: 33.97965621948242
Projection step: 4, Loss: 30.644947052001953
Projection step: 5, Loss: 30.683094024658203
Projection step: 6, Loss: 27.523929595947266
Projection step: 7, Loss: 28.479455947875977
Projection step: 8, Loss: 29.622177124023438
Projection step: 9, Loss: 33.18891906738281
Projection step: 10, Loss: 34.17824172973633
Projection step: 11, Loss: 30.704307556152344
Projection step: 12, Loss: 32.66510009765625
Projection step: 13, Loss: 35.16318893432617
Projection step: 14, Loss: 29.8657169342041
Projection step: 15, Loss: 34.862056732177734
Projection step: 16, Loss: 33.06395721435547
Projection step: 17, Loss: 34.54027557373047
Projection step: 18, Loss: 32.18682861328125
Projection step: 19, Loss: 33.399078369140625
Projection step: 20, Loss: 34.598121643066406
Projection step: 21, Loss: 29.515830993652344
Projection step: 22, Loss: 32.703956604003906
Projection step: 23, Loss: 32.79242706298828
Projection step: 24, Loss: 33.19060516357422
Final likelihood: tensor([-32.5691, -33.1971, -35.8904, -31.8100, -32.9122, -32.0036, -35.9690,
        -37.7328, -27.9158, -31.4745, -34.0777, -31.1271, -30.3941, -22.1605,
        -35.7228, -37.8866])
Final projection likelihood: -32.6777
1 mode projection succeeded
New goal: tensor([-2.0575e-02,  5.7970e-01,  5.1145e-01,  5.3901e-01, -1.7520e-01,
         5.4986e-01,  6.9496e-01,  7.2139e-01,  1.4492e+00,  3.7382e-02,
         2.9333e-01,  1.0096e+00,  3.5208e-04,  8.3480e-02, -3.1625e-01],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -38.0283317565918
Adjusted likelihood: -38.0283317565918
Likelihood residual: 0.0
Original likelihood: -33.170509338378906
Adjusted likelihood: -33.170509338378906
Likelihood residual: 0.0
{'index': 33.170509338378906, 'thumb_middle': 38.0283317565918}
Current yaw: tensor([ 0.0057,  0.0916, -0.2082], device='cuda:0')
7 index
tensor([-0.0150,  0.5776,  0.4921,  0.5344, -0.1953,  0.5733,  0.7517,  0.7554,
         1.5000,  0.0465,  0.2806,  0.9807,  0.0057,  0.0916, -0.2082,  0.3989],
       device='cuda:0')
Solve time for step 1 10.494955383997876
Current ori: tensor([ 0.0057,  0.0916, -0.2082], device='cuda:0')
Middle force: tensor([0.5694, 0.5807, 0.5265, 0.5303], device='cuda:0')
Thumb force: tensor([0.5861, 0.5501, 0.6101, 0.6062], device='cuda:0')
tensor([ 0.0270,  0.5070,  0.4513,  0.5115, -0.1926,  0.5797,  0.7427,  0.7591,
         1.4956,  0.0513,  0.2793,  0.9813,  0.0044,  0.0900, -0.2058,  0.4526],
       device='cuda:0')
Solve time for step 2 4.048538455972448
Current ori: tensor([ 0.0044,  0.0900, -0.2058], device='cuda:0')
Middle force: tensor([0.5738, 0.5231, 0.5273], device='cuda:0')
Thumb force: tensor([0.5446, 0.6050, 0.6004], device='cuda:0')
tensor([ 0.0302,  0.5074,  0.4515,  0.5113, -0.1917,  0.5784,  0.7453,  0.7590,
         1.5000,  0.0437,  0.2763,  0.9831,  0.0045,  0.0893, -0.2077,  0.4992],
       device='cuda:0')
Solve time for step 3 3.89264254498994
Current ori: tensor([ 0.0045,  0.0893, -0.2077], device='cuda:0')
Middle force: tensor([0.5782, 0.5000], device='cuda:0')
Thumb force: tensor([0.5690, 0.5003], device='cuda:0')
tensor([ 0.0289,  0.5068,  0.4515,  0.5115, -0.1913,  0.5816,  0.7410,  0.7586,
         1.4974,  0.0475,  0.2772,  0.9820,  0.0037,  0.0893, -0.2052,  0.5115],
       device='cuda:0')
Solve time for step 4 3.7923958059982397
Current ori: tensor([ 0.0037,  0.0893, -0.2052], device='cuda:0')
Middle force: tensor([0.5001], device='cuda:0')
Thumb force: tensor([0.5001], device='cuda:0')
Storing RECOVERY transition: reward=-0.0237 (scaled=-0.0030), steps=8
Reward stats updated: mean -0.0034 -> -0.0034, std: 0.1038
Collected 372 transitions for RL
SAC Update 1/5: Actor Loss=-0.0001, Q1 Loss=1.1229, Q2 Loss=1.1229, Entropy=0.1484, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2845
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.0846, Q2 Loss=2.0846, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9587
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6958, Q2 Loss=0.6958, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1766
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2169, Q2 Loss=1.2169, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6327
SAC Update 5/5: Actor Loss=-0.0002, Q1 Loss=1.1836, Q2 Loss=1.1836, Entropy=0.1873, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4086

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (20.2%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.04s (17.9%)
Actor update: 0.09s (38.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000073
Q1 loss: 1.260747
Q2 loss: 1.260747
Current threshold: -34.4231
Global Scale Offset: 0.0697
Reward stats: mean=-0.0034, std=0.1038, count=372
----------------------------------------------
SAC Update - Actor Loss: -0.0001, Q1 Loss: 1.2607, Q2 Loss: 1.2607, Entropy: 0.0671, Mean TD Error: 1.4922, Threshold: -34.4231
Original likelihood: -33.644874572753906
Adjusted likelihood: -33.644874572753906
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9979)
Current yaw: tensor([-0.0017,  0.0848, -0.1999], device='cuda:0')
8 turn
Sampling time 3.6078600550536066
tensor([-0.0147,  0.5767,  0.4962,  0.5313, -0.1824,  0.5941,  0.7329,  0.7444,
         1.4933,  0.0486,  0.2717,  0.9779, -0.0017,  0.0848, -0.1999,  0.5021],
       device='cuda:0')
Original likelihood: -32.85602569580078
Adjusted likelihood: -32.85602569580078
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.001767558045685
Current ori: tensor([-0.0017,  0.0848, -0.1999], device='cuda:0')
Middle force: tensor([0.7188, 0.9901, 0.5884, 0.7180, 0.5062, 0.5163, 0.5123, 0.5722, 0.7756,
        0.5683, 0.5002, 0.5162], device='cuda:0')
Thumb force: tensor([1.7001, 0.5464, 0.7471, 0.5136, 0.5242, 0.5088, 0.6319, 0.5712, 1.1097,
        0.6525, 0.5465, 1.2084], device='cuda:0')
Index force: tensor([1.0717, 0.5195, 0.5502, 0.5503, 0.5493, 0.5975, 0.5109, 0.6040, 0.6004,
        0.5987, 0.7065, 0.5553], device='cuda:0')
Storing NORMAL transition: reward=-0.0296 (scaled=-0.0296), steps=1
Reward stats updated: mean -0.0034 -> -0.0035, std: 0.1037
Collected 373 transitions for RL
SAC Update 1/5: Actor Loss=-0.0886, Q1 Loss=1.2913, Q2 Loss=1.2913, Entropy=0.0004, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6186
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5558, Q2 Loss=1.5558, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1279
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.7067, Q2 Loss=1.7067, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6065
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.5189, Q2 Loss=1.5189, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3115
SAC Update 5/5: Actor Loss=-0.0010, Q1 Loss=3.4164, Q2 Loss=3.4164, Entropy=0.2138, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.3932

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.4%)
Q1 update: 0.04s (18.4%)
Q2 update: 0.04s (17.5%)
Actor update: 0.09s (41.8%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.063981
Q1 loss: 1.897787
Q2 loss: 1.897787
Current threshold: -34.4665
Global Scale Offset: 0.0683
Reward stats: mean=-0.0035, std=0.1037, count=373
----------------------------------------------
SAC Update - Actor Loss: -0.0640, Q1 Loss: 1.8978, Q2 Loss: 1.8978, Entropy: 0.0429, Mean TD Error: 1.6115, Threshold: -34.4665
tensor([-0.0621,  0.5695,  0.4799,  0.4973, -0.1444,  0.6340,  0.6675,  0.7570,
         1.2616,  0.2370,  0.3627,  0.9522, -0.0062,  0.0683, -0.1676,  1.0773],
       device='cuda:0')
Original likelihood: -28.842472076416016
Adjusted likelihood: -28.842472076416016
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.674247306014877
Current ori: tensor([-0.0062,  0.0683, -0.1676], device='cuda:0')
Middle force: tensor([1.0071, 0.7112, 0.7463, 0.9128, 0.5889, 0.5204, 0.6306, 0.7461, 0.5331,
        0.5032, 0.8815], device='cuda:0')
Thumb force: tensor([0.5170, 1.1421, 0.7125, 0.6843, 0.5912, 0.5603, 0.5746, 1.2230, 0.6308,
        0.9145, 0.5406], device='cuda:0')
Index force: tensor([0.5541, 0.5265, 0.6325, 0.5521, 0.5968, 0.5744, 0.5636, 0.5263, 0.5655,
        0.6238, 0.5973], device='cuda:0')
Storing NORMAL transition: reward=-0.0551 (scaled=-0.0551), steps=1
Reward stats updated: mean -0.0035 -> -0.0036, std: 0.1036
Collected 374 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6888, Q2 Loss=0.6888, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4068
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0998, Q2 Loss=1.0998, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7505
SAC Update 3/5: Actor Loss=-0.0003, Q1 Loss=1.3762, Q2 Loss=1.3762, Entropy=0.1978, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4981
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4242, Q2 Loss=1.4242, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2307
SAC Update 5/5: Actor Loss=-0.1188, Q1 Loss=1.0555, Q2 Loss=1.0555, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9780

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.4%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.7%)
Actor update: 0.09s (41.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.023819
Q1 loss: 1.128917
Q2 loss: 1.128917
Current threshold: -34.5023
Global Scale Offset: 0.0673
Reward stats: mean=-0.0036, std=0.1036, count=374
----------------------------------------------
SAC Update - Actor Loss: -0.0238, Q1 Loss: 1.1289, Q2 Loss: 1.1289, Entropy: 0.0396, Mean TD Error: 1.1728, Threshold: -34.5023
tensor([-0.1456,  0.5085,  0.4634,  0.5909, -0.2151,  0.5546,  0.7849,  0.7092,
         1.1737,  0.2665,  0.4416,  0.9462,  0.0222,  0.1031, -0.1175,  1.0781],
       device='cuda:0')
Original likelihood: -34.34534454345703
Adjusted likelihood: -34.34534454345703
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.7234)
Solve time for step 3 5.309752995963208
Current ori: tensor([ 0.0222,  0.1031, -0.1175], device='cuda:0')
Middle force: tensor([0.5990, 0.7270, 0.5169, 0.8288, 0.5786, 0.9196, 0.5220, 0.5243, 0.5347,
        0.5723], device='cuda:0')
Thumb force: tensor([0.6869, 0.8369, 0.5547, 0.5357, 0.5592, 0.5624, 0.9034, 0.5849, 0.5207,
        1.0904], device='cuda:0')
Index force: tensor([0.6580, 0.5104, 0.6160, 0.5226, 0.5630, 0.5330, 0.5632, 0.5906, 0.7105,
        0.5346], device='cuda:0')
Storing NORMAL transition: reward=0.0703 (scaled=0.0703), steps=1
Reward stats updated: mean -0.0036 -> -0.0034, std: 0.1035
Collected 375 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9784, Q2 Loss=0.9784, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6555
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0971, Q2 Loss=1.0971, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4988
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6220, Q2 Loss=1.6220, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4232
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7493, Q2 Loss=0.7493, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2900
SAC Update 5/5: Actor Loss=-0.0013, Q1 Loss=2.2330, Q2 Loss=2.2330, Entropy=0.2661, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8963

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.1%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.04s (18.7%)
Actor update: 0.10s (41.5%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000258
Q1 loss: 1.335966
Q2 loss: 1.335966
Current threshold: -34.5502
Global Scale Offset: 0.0657
Reward stats: mean=-0.0034, std=0.1035, count=375
----------------------------------------------
SAC Update - Actor Loss: -0.0003, Q1 Loss: 1.3360, Q2 Loss: 1.3360, Entropy: 0.0532, Mean TD Error: 0.9527, Threshold: -34.5502
tensor([-0.2062,  0.5039,  0.4849,  0.4607, -0.1402,  0.4212,  0.8725,  0.7353,
         1.3310,  0.3025,  0.4303,  0.8310,  0.0319,  0.0844, -0.1862,  1.5894],
       device='cuda:0')
Original likelihood: -27.880176544189453
Adjusted likelihood: -27.880176544189453
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.238042548997328
Current ori: tensor([ 0.0319,  0.0844, -0.1862], device='cuda:0')
Middle force: tensor([0.7495, 0.9007, 0.5869, 0.5189, 0.6443, 0.7651, 0.5390, 0.5086, 0.8616],
       device='cuda:0')
Thumb force: tensor([0.6733, 0.6948, 0.5893, 0.5538, 0.5609, 1.1210, 0.6030, 0.8507, 0.5304],
       device='cuda:0')
Index force: tensor([0.7069, 0.5629, 0.6013, 0.5698, 0.5498, 0.5217, 0.5521, 0.5437, 0.5875],
       device='cuda:0')
Storing NORMAL transition: reward=0.1065 (scaled=0.1065), steps=1
Reward stats updated: mean -0.0034 -> -0.0031, std: 0.1035
Collected 376 transitions for RL
SAC Update 1/5: Actor Loss=-0.1231, Q1 Loss=1.5676, Q2 Loss=1.5676, Entropy=0.2751, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5965
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.7318, Q2 Loss=1.7318, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8641
SAC Update 3/5: Actor Loss=-0.0012, Q1 Loss=1.7095, Q2 Loss=1.7095, Entropy=0.2930, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4091
SAC Update 4/5: Actor Loss=-0.1338, Q1 Loss=1.1604, Q2 Loss=1.1604, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0085
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.9355, Q2 Loss=2.9355, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.2333

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (14.0%)
Q1 update: 0.06s (19.8%)
Q2 update: 0.06s (20.5%)
Actor update: 0.12s (42.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.097684
Q1 loss: 1.820964
Q2 loss: 1.820964
Current threshold: -34.6296
Global Scale Offset: 0.0628
Reward stats: mean=-0.0031, std=0.1035, count=376
----------------------------------------------
SAC Update - Actor Loss: -0.0977, Q1 Loss: 1.8210, Q2 Loss: 1.8210, Entropy: 0.1136, Mean TD Error: 2.4223, Threshold: -34.6296
tensor([-0.2505,  0.3647,  0.5543,  0.5877, -0.0562,  0.4955,  0.8325,  0.8141,
         1.4188,  0.1967,  0.2834,  0.7938,  0.0059,  0.0225, -0.2857, -0.2395],
       device='cuda:0')
Original likelihood: -32.519439697265625
Adjusted likelihood: -32.519439697265625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.610185363970231
Current ori: tensor([ 0.0059,  0.0225, -0.2857], device='cuda:0')
Middle force: tensor([0.5034, 0.5131, 0.5172, 0.6115, 0.7514, 0.5711, 0.5164, 0.5084],
       device='cuda:0')
Thumb force: tensor([0.5195, 0.5063, 0.5700, 0.5261, 0.9811, 0.6072, 0.5100, 1.1285],
       device='cuda:0')
Index force: tensor([0.5426, 0.6018, 0.5051, 0.5806, 0.5833, 0.5729, 0.5446, 0.5431],
       device='cuda:0')
Storing NORMAL transition: reward=0.0809 (scaled=0.0809), steps=1
Reward stats updated: mean -0.0031 -> -0.0029, std: 0.1035
Collected 377 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5882, Q2 Loss=1.5882, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0280
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5107, Q2 Loss=1.5107, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7063
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.1105, Q2 Loss=1.1105, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9378
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.7155, Q2 Loss=1.7155, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8224
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.9775, Q2 Loss=0.9775, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3921

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.9%)
Q1 update: 0.04s (19.7%)
Q2 update: 0.04s (18.2%)
Actor update: 0.09s (39.2%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000000
Q1 loss: 1.380483
Q2 loss: 1.380483
Current threshold: -34.6955
Global Scale Offset: 0.0605
Reward stats: mean=-0.0029, std=0.1035, count=377
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.3805, Q2 Loss: 1.3805, Entropy: 0.0000, Mean TD Error: 1.5773, Threshold: -34.6955
tensor([-2.0350e-01,  3.8792e-01,  6.3701e-01,  6.8314e-01, -7.0354e-03,
         5.2009e-01,  8.3460e-01,  8.4842e-01,  1.3862e+00,  2.4461e-01,
         2.6209e-01,  7.7808e-01, -8.1275e-04, -1.2439e-02, -3.6623e-01,
        -7.5411e-01], device='cuda:0')
Original likelihood: -30.456802368164062
Adjusted likelihood: -30.456802368164062
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.5416075920220464
Current ori: tensor([-0.0008, -0.0124, -0.3662], device='cuda:0')
Middle force: tensor([0.5320, 1.0834, 0.5010, 0.5043, 0.5604, 0.5277, 0.5028],
       device='cuda:0')
Thumb force: tensor([0.5081, 0.7565, 0.6785, 0.5031, 0.5585, 0.5636, 0.5061],
       device='cuda:0')
Index force: tensor([0.6129, 1.0183, 0.6373, 0.7983, 0.5657, 0.7240, 0.9126],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0127 (scaled=-0.0127), steps=1
Reward stats updated: mean -0.0029 -> -0.0029, std: 0.1034
Collected 378 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4405, Q2 Loss=1.4405, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0845
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3088, Q2 Loss=1.3088, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5927
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.9313, Q2 Loss=0.9313, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5505
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6750, Q2 Loss=0.6750, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2300
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.3642, Q2 Loss=2.3642, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1033

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.4%)
Q1 update: 0.06s (19.9%)
Q2 update: 0.05s (18.9%)
Actor update: 0.12s (40.8%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.343952
Q2 loss: 1.343952
Current threshold: -34.7345
Global Scale Offset: 0.0592
Reward stats: mean=-0.0029, std=0.1034, count=378
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.3440, Q2 Loss: 1.3440, Entropy: 0.0000, Mean TD Error: 0.9122, Threshold: -34.7345
tensor([-0.2214,  0.4679,  0.4750,  0.6631,  0.1187,  0.6375,  0.7796,  0.9238,
         1.2246,  0.3260,  0.3040,  0.7382,  0.0078, -0.1050, -0.3686, -1.2670],
       device='cuda:0')
Original likelihood: -43.910606384277344
Adjusted likelihood: -43.910606384277344
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 45.83650207519531
Projection step: 1, Loss: 42.67713165283203
Projection step: 2, Loss: 42.940399169921875
Projection step: 3, Loss: 43.18028259277344
Projection step: 4, Loss: 41.995880126953125
Projection step: 5, Loss: 41.59524917602539
Projection step: 6, Loss: 42.023372650146484
Projection step: 7, Loss: 41.20804977416992
Projection step: 8, Loss: 40.41327667236328
Projection step: 9, Loss: 40.88440704345703
Projection step: 10, Loss: 39.99665832519531
Projection step: 11, Loss: 41.011070251464844
Projection step: 12, Loss: 38.881507873535156
Projection step: 13, Loss: 38.92839813232422
Projection step: 14, Loss: 39.53575897216797
Projection step: 15, Loss: 38.37873077392578
Projection step: 16, Loss: 38.48062515258789
Projection step: 17, Loss: 38.03388977050781
Projection step: 18, Loss: 37.581321716308594
Projection step: 19, Loss: 37.07231140136719
Projection step: 20, Loss: 36.764862060546875
Projection step: 21, Loss: 36.63276290893555
Projection step: 22, Loss: 36.09061050415039
Projection step: 23, Loss: 36.47331237792969
Projection step: 24, Loss: 37.599205017089844
Final likelihood: tensor([-40.4321, -35.5232, -35.9203, -33.9201, -35.8736, -35.6484, -35.5041,
        -35.2244, -37.1762, -35.3485, -34.9610, -35.7559, -35.3765, -35.7620,
        -35.0931, -34.8627])
Final projection likelihood: -35.7739
1 mode projection failed, trying anyway
New goal: tensor([-0.1666,  0.4209,  0.4977,  0.6627,  0.0882,  0.6671,  0.7503,  1.0091,
         1.2556,  0.3066,  0.2862,  0.8020,  0.0035, -0.1006,  0.0536],
       device='cuda:0')
tensor([[0.0021]], device='cuda:0') tensor([[0.0021]], device='cuda:0') tensor([[0.0212]], device='cuda:0')
Original likelihood: -45.166568756103516
Adjusted likelihood: -45.166568756103516
Likelihood residual: 0.0
{'index': 45.166568756103516, 'thumb_middle': inf}
Current yaw: tensor([ 0.0078, -0.1050, -0.3686], device='cuda:0')
9 index
tensor([-0.2214,  0.4679,  0.4750,  0.6631,  0.1187,  0.6375,  0.7796,  0.9238,
         1.2246,  0.3260,  0.3040,  0.7382,  0.0078, -0.1050, -0.3686, -1.2670],
       device='cuda:0')
Solve time for step 1 10.394059040001594
Current ori: tensor([ 0.0078, -0.1050, -0.3686], device='cuda:0')
Middle force: tensor([0.5771, 0.5317, 0.5198, 0.5220], device='cuda:0')
Thumb force: tensor([0.6126, 0.5716, 0.5657, 0.5002], device='cuda:0')
tensor([-0.1897,  0.4756,  0.4977,  0.6579,  0.1149,  0.6463,  0.7515,  0.9935,
         1.2504,  0.2767,  0.2762,  0.7663,  0.0148, -0.1079, -0.3854, -1.9922],
       device='cuda:0')
Solve time for step 2 4.138283309992403
Current ori: tensor([ 0.0148, -0.1079, -0.3854], device='cuda:0')
Middle force: tensor([0.5279, 0.5173, 0.5195], device='cuda:0')
Thumb force: tensor([0.5653, 0.5621, 0.5001], device='cuda:0')
tensor([-0.1870,  0.4896,  0.5096,  0.6568,  0.1054,  0.6670,  0.7409,  0.9981,
         1.2534,  0.2742,  0.2739,  0.7512,  0.0103, -0.1110, -0.3977, -2.5959],
       device='cuda:0')
Solve time for step 3 4.131669159978628
Current ori: tensor([ 0.0103, -0.1110, -0.3977], device='cuda:0')
Middle force: tensor([0.5470, 0.5513], device='cuda:0')
Thumb force: tensor([0.5120, 0.6063], device='cuda:0')
tensor([-0.1874,  0.5130,  0.5188,  0.6568,  0.0906,  0.6917,  0.7391,  0.9879,
         1.2454,  0.2897,  0.2781,  0.7375,  0.0065, -0.1142, -0.4043, -2.8256],
       device='cuda:0')
Solve time for step 4 3.9904201989993453
Current ori: tensor([ 0.0065, -0.1142, -0.4043], device='cuda:0')
Middle force: tensor([0.5465], device='cuda:0')
Thumb force: tensor([0.5981], device='cuda:0')
Storing RECOVERY transition: reward=0.0247 (scaled=0.0041), steps=6
Reward stats updated: mean -0.0029 -> -0.0029, std: 0.1032
Collected 379 transitions for RL
SAC Update 1/5: Actor Loss=-0.0599, Q1 Loss=0.7711, Q2 Loss=0.7711, Entropy=0.0087, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9408
SAC Update 2/5: Actor Loss=-0.0011, Q1 Loss=1.2492, Q2 Loss=1.2492, Entropy=0.3445, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4406
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5196, Q2 Loss=1.5196, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4048
SAC Update 4/5: Actor Loss=-0.2342, Q1 Loss=2.8948, Q2 Loss=2.8948, Entropy=0.3464, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1138
SAC Update 5/5: Actor Loss=-0.0007, Q1 Loss=0.6909, Q2 Loss=0.6909, Entropy=0.1242, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5374

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.3%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (19.5%)
Actor update: 0.09s (39.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.059188
Q1 loss: 1.425141
Q2 loss: 1.425141
Current threshold: -34.7844
Global Scale Offset: 0.0579
Reward stats: mean=-0.0029, std=0.1032, count=379
----------------------------------------------
SAC Update - Actor Loss: -0.0592, Q1 Loss: 1.4251, Q2 Loss: 1.4251, Entropy: 0.1648, Mean TD Error: 1.6875, Threshold: -34.7844
Original likelihood: -47.65727996826172
Adjusted likelihood: -47.65727996826172
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 47.27885437011719
Projection step: 1, Loss: 46.241783142089844
Projection step: 2, Loss: 45.624263763427734
Projection step: 3, Loss: 46.238197326660156
Projection step: 4, Loss: 43.759803771972656
Projection step: 5, Loss: 42.668212890625
Projection step: 6, Loss: 42.25916290283203
Projection step: 7, Loss: 41.87752914428711
Projection step: 8, Loss: 43.923011779785156
Projection step: 9, Loss: 41.155616760253906
Projection step: 10, Loss: 40.401611328125
Projection step: 11, Loss: 40.42923355102539
Projection step: 12, Loss: 39.6680793762207
Projection step: 13, Loss: 39.676658630371094
Projection step: 14, Loss: 40.24800109863281
Projection step: 15, Loss: 39.01727294921875
Projection step: 16, Loss: 37.86835479736328
Projection step: 17, Loss: 38.192626953125
Projection step: 18, Loss: 38.084716796875
Projection step: 19, Loss: 37.20111083984375
Projection step: 20, Loss: 39.086280822753906
Projection step: 21, Loss: 37.876468658447266
Projection step: 22, Loss: 38.246002197265625
Projection step: 23, Loss: 36.579593658447266
Projection step: 24, Loss: 36.664363861083984
Final likelihood: tensor([-35.8321, -35.9571, -35.2045, -35.2419, -35.1658, -35.4307, -34.4118,
        -35.0340, -35.6026, -35.2471, -35.2948, -35.6273, -36.2706, -35.1086,
        -35.3807, -35.5853])
Final projection likelihood: -35.3997
1 mode projection failed, trying anyway
New goal: tensor([-0.1015,  0.5268,  0.5319,  0.6624,  0.0852,  0.7776,  0.6594,  0.9541,
         1.2529,  0.3048,  0.1803,  0.8488, -0.0180, -0.1324, -0.0353],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0154]], device='cuda:0')
Original likelihood: -41.398193359375
Adjusted likelihood: -41.398193359375
Likelihood residual: 0.0
{'index': 41.398193359375, 'thumb_middle': inf}
Current yaw: tensor([-0.0188, -0.1411, -0.4087], device='cuda:0')
10 index
tensor([-0.1581,  0.5785,  0.5298,  0.6451,  0.0927,  0.7654,  0.7218,  0.9423,
         1.2531,  0.2747,  0.2306,  0.7116, -0.0188, -0.1411, -0.4087, -2.7332],
       device='cuda:0')
Solve time for step 1 10.229094257985707
Current ori: tensor([-0.0188, -0.1411, -0.4087], device='cuda:0')
Middle force: tensor([0.5328, 0.6201, 0.5574, 0.5619], device='cuda:0')
Thumb force: tensor([0.5600, 0.5269, 0.5311, 0.6077], device='cuda:0')
tensor([-0.1326,  0.5823,  0.5336,  0.6473,  0.1086,  0.7778,  0.7064,  0.9892,
         1.2493,  0.2637,  0.1919,  0.7797, -0.0055, -0.1549, -0.4337, -2.5031],
       device='cuda:0')
Solve time for step 2 4.170907146995887
Current ori: tensor([-0.0055, -0.1549, -0.4337], device='cuda:0')
Middle force: tensor([0.6183, 0.5531, 0.5550], device='cuda:0')
Thumb force: tensor([0.5221, 0.5298, 0.6109], device='cuda:0')
tensor([-0.1316,  0.5926,  0.5382,  0.6482,  0.1157,  0.8053,  0.6891,  0.9734,
         1.2396,  0.2812,  0.1875,  0.7833, -0.0032, -0.1630, -0.4651, -2.3013],
       device='cuda:0')
Solve time for step 3 4.033111296012066
Current ori: tensor([-0.0032, -0.1630, -0.4651], device='cuda:0')
Middle force: tensor([0.5480, 0.5496], device='cuda:0')
Thumb force: tensor([0.5274, 0.6131], device='cuda:0')
tensor([-1.2526e-01,  6.1459e-01,  5.3371e-01,  6.3873e-01,  1.1988e-01,
         8.3798e-01,  6.7308e-01,  9.4278e-01,  1.2484e+00,  2.6071e-01,
         1.6964e-01,  7.8995e-01, -1.5534e-03, -1.7125e-01, -4.9520e-01,
        -2.2377e+00], device='cuda:0')
Solve time for step 4 3.970388059038669
Current ori: tensor([-0.0016, -0.1713, -0.4952], device='cuda:0')
Middle force: tensor([0.5079], device='cuda:0')
Thumb force: tensor([0.5761], device='cuda:0')
Storing RECOVERY transition: reward=0.0997 (scaled=0.0166), steps=6
Reward stats updated: mean -0.0029 -> -0.0029, std: 0.1031
Collected 380 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2960, Q2 Loss=1.2960, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8770
SAC Update 2/5: Actor Loss=-0.1682, Q1 Loss=0.7670, Q2 Loss=0.7670, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4789
SAC Update 3/5: Actor Loss=-0.0040, Q1 Loss=2.1604, Q2 Loss=2.1604, Entropy=0.3307, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8984
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7986, Q2 Loss=0.7986, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1131
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8034, Q2 Loss=0.8034, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6061

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.1%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.9%)
Actor update: 0.08s (40.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.034440
Q1 loss: 1.165068
Q2 loss: 1.165068
Current threshold: -34.8521
Global Scale Offset: 0.0560
Reward stats: mean=-0.0029, std=0.1031, count=380
----------------------------------------------
SAC Update - Actor Loss: -0.0344, Q1 Loss: 1.1651, Q2 Loss: 1.1651, Entropy: 0.0661, Mean TD Error: 1.1947, Threshold: -34.8521
Original likelihood: -35.77397918701172
Adjusted likelihood: -35.77397918701172
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 34.80673599243164
Projection step: 1, Loss: 36.2294807434082
Projection step: 2, Loss: 35.650146484375
Projection step: 3, Loss: 36.775482177734375
Projection step: 4, Loss: 34.9561767578125
Projection step: 5, Loss: 34.46550750732422
Projection step: 6, Loss: 36.66484832763672
Projection step: 7, Loss: 34.0879020690918
Projection step: 8, Loss: 34.7586669921875
Projection step: 9, Loss: 34.1406364440918
Projection step: 10, Loss: 33.97724914550781
Projection step: 11, Loss: 32.86793899536133
Projection step: 12, Loss: 32.191436767578125
Projection step: 13, Loss: 33.68535614013672
Projection step: 14, Loss: 33.3594970703125
Projection step: 15, Loss: 32.59789276123047
Projection step: 16, Loss: 32.73283386230469
Projection step: 17, Loss: 32.99790954589844
Projection step: 18, Loss: 33.41038513183594
Projection step: 19, Loss: 31.532737731933594
Projection step: 20, Loss: 33.149784088134766
Projection step: 21, Loss: 32.69114303588867
Projection step: 22, Loss: 33.050811767578125
Projection step: 23, Loss: 33.71479034423828
Projection step: 24, Loss: 31.322277069091797
Final likelihood: tensor([-31.5989, -33.1046, -31.2172, -31.9232, -31.9524, -32.3817, -32.1891,
        -32.4201, -32.8819, -34.0221, -47.3237, -30.0663, -32.0381, -33.2518,
        -33.4273, -31.6585])
Final projection likelihood: -33.2161
1 mode projection succeeded
New goal: tensor([-0.0134,  0.6570,  0.5561,  0.6247,  0.1056,  0.8378,  0.7001,  0.9707,
         1.2423,  0.2799,  0.1498,  0.8923,  0.0084, -0.1740, -0.0720],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0031]], device='cuda:0')
Original likelihood: -35.46913146972656
Adjusted likelihood: -35.46913146972656
Likelihood residual: 0.0
Original likelihood: -34.33028793334961
Adjusted likelihood: -34.33028793334961
Likelihood residual: 0.0
{'index': 34.33028793334961, 'thumb_middle': 35.46913146972656}
Current yaw: tensor([ 0.0088, -0.1776, -0.5197], device='cuda:0')
11 index
tensor([-0.0418,  0.7310,  0.5778,  0.6371,  0.1203,  0.8604,  0.6690,  0.9282,
         1.2316,  0.2860,  0.1620,  0.8208,  0.0088, -0.1776, -0.5197, -2.2698],
       device='cuda:0')
Solve time for step 1 10.486820689984597
Current ori: tensor([ 0.0088, -0.1776, -0.5197], device='cuda:0')
Middle force: tensor([0.5954, 0.5624, 0.5189, 0.5799], device='cuda:0')
Thumb force: tensor([0.5148, 0.5878, 0.5899, 0.6133], device='cuda:0')
tensor([-0.0812,  0.6625,  0.5361,  0.6116,  0.1012,  0.8967,  0.6955,  0.9424,
         1.2342,  0.2694,  0.1240,  0.8460,  0.0164, -0.1993, -0.5918, -2.2234],
       device='cuda:0')
Solve time for step 2 4.059421666024718
Current ori: tensor([ 0.0164, -0.1993, -0.5918], device='cuda:0')
Middle force: tensor([0.5586, 0.5163, 0.5765], device='cuda:0')
Thumb force: tensor([0.5822, 0.5896, 0.6115], device='cuda:0')
tensor([-0.0791,  0.6711,  0.5408,  0.6112,  0.1018,  0.9186,  0.6992,  0.9374,
         1.2447,  0.2495,  0.1146,  0.8708,  0.0330, -0.2065, -0.6637, -2.0906],
       device='cuda:0')
Solve time for step 3 4.059720356017351
Current ori: tensor([ 0.0330, -0.2065, -0.6637], device='cuda:0')
Middle force: tensor([0.5131, 0.5714], device='cuda:0')
Thumb force: tensor([0.5889, 0.6125], device='cuda:0')
tensor([-0.0732,  0.6748,  0.5473,  0.6160,  0.0979,  0.9485,  0.7114,  0.9412,
         1.2447,  0.2757,  0.1105,  0.9502,  0.0854, -0.2189, -0.7556, -1.8377],
       device='cuda:0')
Solve time for step 4 3.7940601960290223
Current ori: tensor([ 0.0854, -0.2189, -0.7556], device='cuda:0')
Middle force: tensor([0.5710], device='cuda:0')
Thumb force: tensor([0.6110], device='cuda:0')
Storing RECOVERY transition: reward=-0.0078 (scaled=-0.0013), steps=6
Reward stats updated: mean -0.0029 -> -0.0029, std: 0.1029
Collected 381 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7278, Q2 Loss=0.7278, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1725
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.7095, Q2 Loss=0.7095, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2232
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3299, Q2 Loss=1.3299, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7727
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3333, Q2 Loss=1.3333, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0274
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7200, Q2 Loss=0.7200, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2394

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.8%)
Q1 update: 0.05s (18.9%)
Q2 update: 0.05s (18.8%)
Actor update: 0.09s (39.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 0.964098
Q2 loss: 0.964098
Current threshold: -34.8996
Global Scale Offset: 0.0546
Reward stats: mean=-0.0029, std=0.1029, count=381
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 0.9641, Q2 Loss: 0.9641, Entropy: 0.0000, Mean TD Error: 0.6870, Threshold: -34.8996
Original likelihood: -220.40277099609375
Adjusted likelihood: -220.40277099609375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 214.00439453125
Projection step: 1, Loss: 209.00296020507812
Projection step: 2, Loss: 219.04409790039062
Projection step: 3, Loss: 217.24075317382812
Projection step: 4, Loss: 219.34471130371094
Projection step: 5, Loss: 213.72059631347656
Projection step: 6, Loss: 230.54518127441406
Projection step: 7, Loss: 248.20655822753906
Projection step: 8, Loss: 230.10446166992188
Projection step: 9, Loss: 245.411865234375
Projection step: 10, Loss: 236.3939666748047
Projection step: 11, Loss: 245.33682250976562
Projection step: 12, Loss: 213.93502807617188
Projection step: 13, Loss: 210.55862426757812
Projection step: 14, Loss: 217.25140380859375
Projection step: 15, Loss: 219.7101593017578
Projection step: 16, Loss: 220.61929321289062
Projection step: 17, Loss: 210.338623046875
Projection step: 18, Loss: 228.01077270507812
Projection step: 19, Loss: 238.75498962402344
Projection step: 20, Loss: 229.5524139404297
Projection step: 21, Loss: 246.85618591308594
Projection step: 22, Loss: 211.97019958496094
Projection step: 23, Loss: 238.5174560546875
Projection step: 24, Loss: 231.39212036132812
Final likelihood: tensor([-207.3694, -267.0258, -193.4742, -282.9387, -211.7464, -163.5560,
        -232.0523, -212.3435, -288.9536, -257.9041, -192.2799, -286.0510,
        -198.8732, -297.0073, -276.3851, -262.3286])
Final projection likelihood: -239.3931
1 mode projection failed, trying anyway
New goal: tensor([ 0.0246,  0.7452,  0.6197,  0.6572,  0.0803,  0.9790,  0.7203,  0.9510,
         1.2638,  0.2875,  0.1542,  1.0180,  0.2096, -0.2923, -0.8054],
       device='cuda:0')
tensor([[0.0020]], device='cuda:0') tensor([[0.0194]], device='cuda:0') tensor([[0.0226]], device='cuda:0')
Original likelihood: -63.12616729736328
Adjusted likelihood: -63.12616729736328
Likelihood residual: 0.0
Original likelihood: -148.85467529296875
Adjusted likelihood: -148.85467529296875
Likelihood residual: 0.0
{'index': 148.85467529296875, 'thumb_middle': 63.12616729736328}
Current yaw: tensor([ 0.2100, -0.2919, -0.8399], device='cuda:0')
12 thumb_middle
tensor([ 0.0170,  0.7531,  0.6199,  0.6517,  0.0787,  0.9790,  0.7252,  0.9411,
         1.2803,  0.2888,  0.1553,  1.0143,  0.2100, -0.2919, -0.8399, -1.4410],
       device='cuda:0')
Solve time for step 1 8.500330197974108
Current ori: tensor([ 0.2100, -0.2919, -0.8399], device='cuda:0')
Index force: tensor([0.5846, 0.5654, 0.5752, 0.5517], device='cuda:0')
tensor([-0.0204,  0.9256,  0.6327,  0.6594,  0.0998,  1.0482,  0.7127,  0.9125,
         1.2675,  0.2898,  0.1195,  1.0196,  0.2777, -0.3584, -0.9709, -1.0415],
       device='cuda:0')
Solve time for step 2 3.4963391160126776
Current ori: tensor([ 0.2777, -0.3584, -0.9709], device='cuda:0')
Index force: tensor([0.5618, 0.5674, 0.5779], device='cuda:0')
tensor([-0.0686,  1.1284,  0.5857,  0.6561,  0.0706,  1.1362,  0.7697,  0.9528,
         1.2446,  0.2971,  0.1066,  1.0380,  0.3169, -0.3634, -1.1171, -0.9741],
       device='cuda:0')
Solve time for step 3 3.3414850969566032
Current ori: tensor([ 0.3169, -0.3634, -1.1171], device='cuda:0')
Index force: tensor([0.5591, 0.5710], device='cuda:0')
tensor([-0.1119,  1.1905,  0.5680,  0.6411,  0.0245,  1.1595,  0.8140,  0.9691,
         1.2092,  0.3236,  0.0831,  1.0918,  0.3735, -0.3709, -1.2677, -0.8392],
       device='cuda:0')
Solve time for step 4 3.242741789028514
Current ori: tensor([ 0.3735, -0.3709, -1.2677], device='cuda:0')
Index force: tensor([0.5685], device='cuda:0')
Storing RECOVERY transition: reward=-0.4324 (scaled=-0.0721), steps=6
Reward stats updated: mean -0.0029 -> -0.0030, std: 0.1029
Collected 382 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0424, Q2 Loss=1.0424, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9072
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.7574, Q2 Loss=0.7574, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3098
SAC Update 3/5: Actor Loss=-0.1897, Q1 Loss=1.7309, Q2 Loss=1.7309, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5928
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0866, Q2 Loss=1.0866, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7760
SAC Update 5/5: Actor Loss=-0.2369, Q1 Loss=1.7783, Q2 Loss=1.7783, Entropy=0.2497, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3991

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.4%)
Q1 update: 0.05s (20.4%)
Q2 update: 0.05s (17.6%)
Actor update: 0.10s (40.1%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.131363
Q1 loss: 1.279096
Q2 loss: 1.279096
Current threshold: -34.9289
Global Scale Offset: 0.0538
Reward stats: mean=-0.0030, std=0.1029, count=382
----------------------------------------------
SAC Update - Actor Loss: -0.1314, Q1 Loss: 1.2791, Q2 Loss: 1.2791, Entropy: 0.0499, Mean TD Error: 1.1970, Threshold: -34.9289
Original likelihood: -344.9623107910156
Adjusted likelihood: -344.9623107910156
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 25
Loaded trajectory sampler
Current yaw: tensor([-0.0001,  0.0140, -0.0377], device='cuda:0')
Current yaw: tensor([-0.0001,  0.0140, -0.0377], device='cuda:0')
1 turn
Sampling time 3.639442932966631
tensor([ 1.6866e-01,  5.9480e-01,  6.0649e-01,  6.0871e-01, -1.2663e-01,
         5.7181e-01,  8.6084e-01,  9.3105e-01,  1.2102e+00,  3.3943e-01,
         2.2172e-01,  1.2492e+00, -1.0371e-04,  1.3966e-02, -3.7650e-02,
         2.9952e-01], device='cuda:0')
Original likelihood: -22.761449813842773
Adjusted likelihood: -22.761449813842773
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.03345387201989
Current ori: tensor([-0.0001,  0.0140, -0.0377], device='cuda:0')
Middle force: tensor([1.1122, 1.6636, 0.8273, 0.5049, 0.5741, 0.9129, 1.1698, 0.5112, 0.7017,
        0.9243, 0.5023, 0.5907], device='cuda:0')
Thumb force: tensor([0.9433, 1.5695, 0.5865, 0.5462, 0.5356, 1.2538, 0.6797, 0.5705, 0.6013,
        1.1699, 0.9224, 0.6482], device='cuda:0')
Index force: tensor([0.9255, 1.8268, 0.5646, 0.5975, 0.5943, 0.8124, 0.5429, 0.5766, 0.5911,
        0.5377, 0.6795, 0.5787], device='cuda:0')
Storing NORMAL transition: reward=-0.0030 (scaled=-0.0030), steps=1
Reward stats updated: mean -0.0030 -> -0.0030, std: 0.1027
Collected 383 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5618, Q2 Loss=1.5618, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0248
SAC Update 2/5: Actor Loss=-0.2302, Q1 Loss=0.8014, Q2 Loss=0.8014, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3453
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9226, Q2 Loss=0.9226, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5416
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.7048, Q2 Loss=2.7048, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6846
SAC Update 5/5: Actor Loss=-0.1415, Q1 Loss=1.1059, Q2 Loss=1.1059, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6982

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (16.9%)
Q1 update: 0.05s (19.0%)
Q2 update: 0.06s (20.0%)
Actor update: 0.11s (40.8%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.074356
Q1 loss: 1.419293
Q2 loss: 1.419293
Current threshold: -34.9497
Global Scale Offset: 0.0533
Reward stats: mean=-0.0030, std=0.1027, count=383
----------------------------------------------
SAC Update - Actor Loss: -0.0744, Q1 Loss: 1.4193, Q2 Loss: 1.4193, Entropy: 0.0000, Mean TD Error: 1.2589, Threshold: -34.9497
tensor([ 0.2135,  0.7376,  0.5288,  0.4419, -0.1755,  0.6332,  0.9128,  0.8192,
         1.2187,  0.2817,  0.2137,  1.1514, -0.0359, -0.0237, -0.0362,  0.2572],
       device='cuda:0')
Original likelihood: -37.248600006103516
Adjusted likelihood: -37.248600006103516
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 35.591556549072266
Projection step: 1, Loss: 34.1836051940918
Projection step: 2, Loss: 36.1575927734375
Projection step: 3, Loss: 35.016136169433594
Projection step: 4, Loss: 34.7259521484375
Projection step: 5, Loss: 33.569786071777344
Projection step: 6, Loss: 31.386398315429688
Projection step: 7, Loss: 31.869407653808594
Projection step: 8, Loss: 32.45979309082031
Projection step: 9, Loss: 30.57901382446289
Projection step: 10, Loss: 30.49148178100586
Projection step: 11, Loss: 29.199094772338867
Projection step: 12, Loss: 30.546091079711914
Projection step: 13, Loss: 29.993907928466797
Projection step: 14, Loss: 29.078632354736328
Projection step: 15, Loss: 30.075307846069336
Projection step: 16, Loss: 28.584638595581055
Projection step: 17, Loss: 28.393661499023438
Projection step: 18, Loss: 27.701269149780273
Projection step: 19, Loss: 26.621986389160156
Projection step: 20, Loss: 28.785953521728516
Projection step: 21, Loss: 25.428203582763672
Projection step: 22, Loss: 26.756134033203125
Projection step: 23, Loss: 24.973445892333984
Projection step: 24, Loss: 24.28607940673828
Final likelihood: tensor([-22.0516, -25.8206, -25.9287, -25.2992, -25.8179, -23.7830, -25.6248,
        -22.8123, -22.8031, -26.4868, -24.7791, -22.6164, -24.5912, -24.8313,
        -25.6741, -22.9277])
Final projection likelihood: -24.4905
1 mode projection succeeded
New goal: tensor([ 0.1753,  0.7230,  0.4535,  0.4968, -0.1078,  0.6424,  0.9078,  0.8616,
         1.2852,  0.3159,  0.1511,  1.0939, -0.0364, -0.0188, -2.1763],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0068]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -27.763490676879883
Adjusted likelihood: -27.763490676879883
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 27.763490676879883}
Current yaw: tensor([-0.0359, -0.0237, -0.0362], device='cuda:0')
2 thumb_middle
tensor([ 0.2135,  0.7376,  0.5288,  0.4419, -0.1755,  0.6332,  0.9128,  0.8192,
         1.2187,  0.2817,  0.2137,  1.1514, -0.0359, -0.0237, -0.0362,  0.2572],
       device='cuda:0')
Solve time for step 1 9.040279155015014
Current ori: tensor([-0.0359, -0.0237, -0.0362], device='cuda:0')
Index force: tensor([0.5891, 0.5934, 0.6139, 0.5947], device='cuda:0')
tensor([ 0.2081,  0.7561,  0.4763,  0.4866, -0.2181,  0.6054,  0.8691,  0.8305,
         1.2389,  0.2961,  0.0894,  1.0715, -0.0370, -0.0210, -0.0362,  0.2632],
       device='cuda:0')
Solve time for step 2 3.496646696003154
Current ori: tensor([-0.0370, -0.0210, -0.0362], device='cuda:0')
Index force: tensor([0.5869, 0.5918, 0.5918], device='cuda:0')
tensor([ 0.1934,  0.7530,  0.4612,  0.4918, -0.2292,  0.6090,  0.8634,  0.8333,
         1.2564,  0.3018,  0.0752,  1.0634, -0.0361, -0.0116, -0.0362,  0.2388],
       device='cuda:0')
Solve time for step 3 3.328371307987254
Current ori: tensor([-0.0361, -0.0116, -0.0362], device='cuda:0')
Index force: tensor([0.5849, 0.5866], device='cuda:0')
tensor([ 0.1911,  0.7522,  0.4590,  0.4930, -0.2329,  0.6072,  0.8665,  0.8368,
         1.2592,  0.3021,  0.0746,  1.0609, -0.0359, -0.0100, -0.0362,  0.2358],
       device='cuda:0')
Solve time for step 4 3.436498159018811
Current ori: tensor([-0.0359, -0.0100, -0.0362], device='cuda:0')
Index force: tensor([0.5719], device='cuda:0')
Storing RECOVERY transition: reward=0.0107 (scaled=0.0107), steps=1
Reward stats updated: mean -0.0030 -> -0.0030, std: 0.1026
Collected 384 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2790, Q2 Loss=1.2790, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5614
SAC Update 2/5: Actor Loss=-0.2302, Q1 Loss=0.6792, Q2 Loss=0.6792, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4414
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6925, Q2 Loss=0.6925, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4960
SAC Update 4/5: Actor Loss=-0.0031, Q1 Loss=1.2053, Q2 Loss=1.2053, Entropy=0.2905, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2978
SAC Update 5/5: Actor Loss=-0.0021, Q1 Loss=1.5416, Q2 Loss=1.5416, Entropy=0.2659, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3185

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.8%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (40.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.047088
Q1 loss: 1.079523
Q2 loss: 1.079523
Current threshold: -34.9645
Global Scale Offset: 0.0530
Reward stats: mean=-0.0030, std=0.1026, count=384
----------------------------------------------
SAC Update - Actor Loss: -0.0471, Q1 Loss: 1.0795, Q2 Loss: 1.0795, Entropy: 0.1113, Mean TD Error: 0.8230, Threshold: -34.9645
Original likelihood: -29.90130615234375
Adjusted likelihood: -29.90130615234375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0364, -0.0198, -0.0468], device='cuda:0')
3 turn
Sampling time 3.603425801033154
tensor([ 0.2058,  0.7579,  0.4667,  0.4989, -0.1575,  0.6629,  0.9065,  0.8579,
         1.3139,  0.3191,  0.1180,  1.0901, -0.0364, -0.0198, -0.0468,  0.2673],
       device='cuda:0')
Original likelihood: -32.41511154174805
Adjusted likelihood: -32.41511154174805
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.82372910703998
Current ori: tensor([-0.0364, -0.0198, -0.0468], device='cuda:0')
Middle force: tensor([0.5402, 1.3072, 1.6916, 0.5145, 0.6321, 0.4991, 0.5536, 0.5234, 0.4916,
        0.5868, 0.5078, 0.5342], device='cuda:0')
Thumb force: tensor([0.8842, 1.2841, 0.5597, 0.6235, 0.5806, 0.7248, 0.5288, 0.6381, 0.5242,
        0.6073, 0.6790, 0.6755], device='cuda:0')
Index force: tensor([0.6168, 0.6786, 0.5586, 0.6180, 0.5720, 0.6037, 0.6148, 0.6515, 0.8196,
        0.6284, 0.7031, 0.5243], device='cuda:0')
Storing NORMAL transition: reward=0.2114 (scaled=0.2114), steps=1
Reward stats updated: mean -0.0030 -> -0.0024, std: 0.1031
Collected 385 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.0903, Q2 Loss=1.0903, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8262
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.8744, Q2 Loss=1.8744, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9586
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6865, Q2 Loss=0.6865, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2279
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0680, Q2 Loss=1.0680, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0558
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6112, Q2 Loss=1.6112, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0607

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (20.8%)
Q1 update: 0.05s (20.7%)
Q2 update: 0.04s (18.5%)
Actor update: 0.08s (35.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046050
Q1 loss: 1.266087
Q2 loss: 1.266087
Current threshold: -34.9772
Global Scale Offset: 0.0528
Reward stats: mean=-0.0024, std=0.1031, count=385
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.2661, Q2 Loss: 1.2661, Entropy: 0.0000, Mean TD Error: 1.2258, Threshold: -34.9772
tensor([ 0.1477,  0.7303,  0.4797,  0.3877, -0.1758,  0.5874,  0.9530,  0.9778,
         1.4031,  0.2349,  0.1228,  0.9697, -0.0320, -0.0071, -0.2583,  0.5433],
       device='cuda:0')
Original likelihood: -29.50931167602539
Adjusted likelihood: -29.50931167602539
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.566260255000088
Current ori: tensor([-0.0320, -0.0071, -0.2583], device='cuda:0')
Middle force: tensor([0.5670, 0.6657, 0.5569, 0.8928, 0.5185, 0.7291, 0.5007, 0.5586, 0.5550,
        0.5110, 0.5942], device='cuda:0')
Thumb force: tensor([1.0105, 0.7047, 1.2654, 0.5711, 0.5001, 0.5470, 0.8298, 0.5781, 0.5889,
        0.5544, 1.0440], device='cuda:0')
Index force: tensor([0.5207, 0.8162, 0.7734, 0.6338, 0.5799, 0.6925, 0.5478, 0.6055, 0.5406,
        0.5961, 0.6160], device='cuda:0')
Storing NORMAL transition: reward=0.0563 (scaled=0.0563), steps=1
Reward stats updated: mean -0.0024 -> -0.0023, std: 0.1030
Collected 386 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6826, Q2 Loss=0.6826, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6099
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.0623, Q2 Loss=2.0623, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7939
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0138, Q2 Loss=1.0138, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7148
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6773, Q2 Loss=0.6773, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8877
SAC Update 5/5: Actor Loss=-0.0084, Q1 Loss=2.1719, Q2 Loss=2.1719, Entropy=0.1921, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9045

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.2%)
Q1 update: 0.06s (20.5%)
Q2 update: 0.06s (20.2%)
Actor update: 0.11s (41.7%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.001684
Q1 loss: 1.321584
Q2 loss: 1.321584
Current threshold: -34.9860
Global Scale Offset: 0.0527
Reward stats: mean=-0.0023, std=0.1030, count=386
----------------------------------------------
SAC Update - Actor Loss: -0.0017, Q1 Loss: 1.3216, Q2 Loss: 1.3216, Entropy: 0.0384, Mean TD Error: 1.3822, Threshold: -34.9860
tensor([ 0.1270,  0.6743,  0.5123,  0.4796, -0.1897,  0.5452,  0.9566,  1.0302,
         1.4214,  0.2249,  0.1783,  0.8980, -0.0203,  0.0115, -0.3139,  0.7576],
       device='cuda:0')
Original likelihood: -26.10055160522461
Adjusted likelihood: -26.10055160522461
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.233265658956952
Current ori: tensor([-0.0203,  0.0115, -0.3139], device='cuda:0')
Middle force: tensor([0.6435, 0.5432, 0.8726, 0.5168, 0.7107, 0.5002, 0.5547, 0.5511, 0.5086,
        0.5892], device='cuda:0')
Thumb force: tensor([0.6898, 1.2312, 0.5634, 0.5000, 0.5430, 0.8120, 0.5708, 0.5816, 0.5490,
        1.0251], device='cuda:0')
Index force: tensor([0.8017, 0.7645, 0.6224, 0.5742, 0.6846, 0.5418, 0.5981, 0.5367, 0.5950,
        0.6099], device='cuda:0')
Storing NORMAL transition: reward=0.0389 (scaled=0.0389), steps=1
Reward stats updated: mean -0.0023 -> -0.0022, std: 0.1029
Collected 387 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0174, Q2 Loss=1.0174, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3879
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.9061, Q2 Loss=0.9061, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1870
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6197, Q2 Loss=1.6197, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3457
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2282, Q2 Loss=1.2282, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8515
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.0959, Q2 Loss=1.0959, Entropy=0.0001, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3956

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.7%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (19.7%)
Actor update: 0.11s (42.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.173445
Q2 loss: 1.173445
Current threshold: -34.9954
Global Scale Offset: 0.0527
Reward stats: mean=-0.0022, std=0.1029, count=387
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.1734, Q2 Loss: 1.1734, Entropy: 0.0000, Mean TD Error: 0.8336, Threshold: -34.9954
tensor([ 0.0822,  0.6860,  0.4477,  0.4674, -0.1998,  0.4439,  1.0732,  0.9949,
         1.4304,  0.1677,  0.2499,  0.8477, -0.0211,  0.0365, -0.3558,  0.7347],
       device='cuda:0')
Original likelihood: -26.09369659423828
Adjusted likelihood: -26.09369659423828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.958651983994059
Current ori: tensor([-0.0211,  0.0365, -0.3558], device='cuda:0')
Middle force: tensor([0.5389, 0.8645, 0.5154, 0.6933, 0.5000, 0.5525, 0.5479, 0.5069, 0.5873],
       device='cuda:0')
Thumb force: tensor([1.1860, 0.5505, 0.5001, 0.5394, 0.7970, 0.5631, 0.5747, 0.5458, 1.0011],
       device='cuda:0')
Index force: tensor([0.7493, 0.6173, 0.5695, 0.6792, 0.5371, 0.5914, 0.5333, 0.5896, 0.6049],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0237 (scaled=-0.0237), steps=1
Reward stats updated: mean -0.0022 -> -0.0022, std: 0.1027
Collected 388 transitions for RL
SAC Update 1/5: Actor Loss=-0.1480, Q1 Loss=1.2437, Q2 Loss=1.2437, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6829
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8092, Q2 Loss=0.8092, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6513
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3997, Q2 Loss=1.3997, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1778
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1805, Q2 Loss=1.1805, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5519
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1061, Q2 Loss=1.1061, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6971

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (18.5%)
Actor update: 0.08s (40.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.029606
Q1 loss: 1.147838
Q2 loss: 1.147838
Current threshold: -35.0405
Global Scale Offset: 0.0514
Reward stats: mean=-0.0022, std=0.1027, count=388
----------------------------------------------
SAC Update - Actor Loss: -0.0296, Q1 Loss: 1.1478, Q2 Loss: 1.1478, Entropy: 0.0000, Mean TD Error: 0.7522, Threshold: -35.0405
tensor([ 0.1536,  0.6234,  0.4878,  0.5529, -0.2155,  0.6015,  0.9546,  1.0006,
         1.4106,  0.2527,  0.2197,  0.7929, -0.0829,  0.0309, -0.3448,  2.3994],
       device='cuda:0')
Original likelihood: -28.23063087463379
Adjusted likelihood: -28.23063087463379
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.772419644985348
Current ori: tensor([-0.0829,  0.0309, -0.3448], device='cuda:0')
Middle force: tensor([0.6691, 0.5764, 0.5268, 1.1117, 0.5046, 0.5207, 0.5313, 0.5654],
       device='cuda:0')
Thumb force: tensor([0.9097, 0.5427, 0.9322, 0.6363, 0.5014, 0.9329, 0.5987, 0.5903],
       device='cuda:0')
Index force: tensor([0.7102, 0.6389, 0.5748, 0.5002, 0.7340, 0.5237, 0.5769, 0.5745],
       device='cuda:0')
Storing NORMAL transition: reward=0.0158 (scaled=0.0158), steps=1
Reward stats updated: mean -0.0022 -> -0.0022, std: 0.1026
Collected 389 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2480, Q2 Loss=1.2480, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4264
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8622, Q2 Loss=0.8622, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5301
SAC Update 3/5: Actor Loss=-0.0558, Q1 Loss=0.8346, Q2 Loss=0.8346, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7607
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8596, Q2 Loss=0.8596, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6614
SAC Update 5/5: Actor Loss=-0.0068, Q1 Loss=0.8261, Q2 Loss=0.8261, Entropy=0.1290, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6473

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.7%)
Q1 update: 0.05s (20.8%)
Q2 update: 0.04s (16.7%)
Actor update: 0.09s (39.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.012525
Q1 loss: 0.926087
Q2 loss: 0.926087
Current threshold: -35.0681
Global Scale Offset: 0.0507
Reward stats: mean=-0.0022, std=0.1026, count=389
----------------------------------------------
SAC Update - Actor Loss: -0.0125, Q1 Loss: 0.9261, Q2 Loss: 0.9261, Entropy: 0.0258, Mean TD Error: 0.6052, Threshold: -35.0681
tensor([ 0.1403,  0.5623,  0.6125,  0.6105, -0.2064,  0.6119,  0.9273,  1.0555,
         1.3803,  0.2970,  0.2153,  0.8264, -0.0762,  0.0209, -0.3580,  2.8040],
       device='cuda:0')
Original likelihood: -28.704788208007812
Adjusted likelihood: -28.704788208007812
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.460698631010018
Current ori: tensor([-0.0762,  0.0209, -0.3580], device='cuda:0')
Middle force: tensor([0.5713, 0.5240, 1.0881, 0.5038, 0.5193, 0.5311, 0.5586],
       device='cuda:0')
Thumb force: tensor([0.5386, 0.9161, 0.6283, 0.5011, 0.9131, 0.5894, 0.5894],
       device='cuda:0')
Index force: tensor([0.6329, 0.5729, 0.5000, 0.7315, 0.5217, 0.5725, 0.5704],
       device='cuda:0')
Storing NORMAL transition: reward=0.0195 (scaled=0.0195), steps=1
Reward stats updated: mean -0.0022 -> -0.0021, std: 0.1025
Collected 390 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8174, Q2 Loss=0.8174, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3383
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6368, Q2 Loss=0.6368, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2204
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6895, Q2 Loss=0.6895, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2561
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1091, Q2 Loss=1.1091, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3207
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.1561, Q2 Loss=1.1561, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4696

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.8%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (17.2%)
Actor update: 0.11s (39.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 0.881769
Q2 loss: 0.881769
Current threshold: -35.0875
Global Scale Offset: 0.0502
Reward stats: mean=-0.0021, std=0.1025, count=390
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 0.8818, Q2 Loss: 0.8818, Entropy: 0.0000, Mean TD Error: 0.3210, Threshold: -35.0875
tensor([ 0.1928,  0.5749,  0.6612,  0.5834, -0.2454,  0.6888,  0.9928,  1.1418,
         1.3640,  0.3369,  0.1881,  0.8183, -0.0845, -0.0049, -0.3814,  2.8434],
       device='cuda:0')
Original likelihood: -39.09318542480469
Adjusted likelihood: -39.09318542480469
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 35.06708526611328
Projection step: 1, Loss: 29.188846588134766
Projection step: 2, Loss: 31.132740020751953
Projection step: 3, Loss: 31.380624771118164
Projection step: 4, Loss: 31.859973907470703
Projection step: 5, Loss: 30.074237823486328
Projection step: 6, Loss: 31.46241569519043
Projection step: 7, Loss: 30.33892822265625
Projection step: 8, Loss: 29.8121280670166
Projection step: 9, Loss: 30.654159545898438
Projection step: 10, Loss: 29.22079849243164
Projection step: 11, Loss: 29.893787384033203
Projection step: 12, Loss: 25.315582275390625
Projection step: 13, Loss: 30.44536781311035
Projection step: 14, Loss: 29.883188247680664
Projection step: 15, Loss: 26.56142234802246
Projection step: 16, Loss: 27.185070037841797
Projection step: 17, Loss: 28.830318450927734
Projection step: 18, Loss: 29.23053741455078
Projection step: 19, Loss: 27.7257137298584
Projection step: 20, Loss: 30.52546501159668
Projection step: 21, Loss: 27.655879974365234
Projection step: 22, Loss: 26.763851165771484
Projection step: 23, Loss: 26.828628540039062
Projection step: 24, Loss: 30.447010040283203
Final likelihood: tensor([-27.0170, -30.2264, -30.3115, -25.0562, -17.0907, -28.0931, -32.5512,
        -28.0764, -33.9214, -30.5788, -28.2921, -33.9130, -31.0800, -27.0327,
        -38.1960, -23.1380])
Final projection likelihood: -29.0359
1 mode projection succeeded
New goal: tensor([ 0.1813,  0.5611,  0.5917,  0.6655, -0.1902,  0.6951,  0.9116,  1.0554,
         1.3844,  0.3391,  0.1827,  0.7987, -0.0844, -0.0018,  0.3707],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -27.38573455810547
Adjusted likelihood: -27.38573455810547
Likelihood residual: 0.0
Original likelihood: -30.507972717285156
Adjusted likelihood: -30.507972717285156
Likelihood residual: 0.0
{'index': 30.507972717285156, 'thumb_middle': 27.38573455810547}
Current yaw: tensor([-0.0845, -0.0049, -0.3814], device='cuda:0')
4 thumb_middle
tensor([ 0.1928,  0.5749,  0.6612,  0.5834, -0.2454,  0.6888,  0.9928,  1.1418,
         1.3640,  0.3369,  0.1881,  0.8183, -0.0845, -0.0049, -0.3814,  2.8434],
       device='cuda:0')
Solve time for step 1 8.733800638001412
Current ori: tensor([-0.0845, -0.0049, -0.3814], device='cuda:0')
Index force: tensor([0.5899, 0.5888, 0.5874, 0.6045], device='cuda:0')
tensor([ 0.1993,  0.5974,  0.6114,  0.6410, -0.3003,  0.6616,  0.8932,  1.0608,
         1.3357,  0.3261,  0.1119,  0.7704, -0.0901, -0.0348, -0.3815,  3.1138],
       device='cuda:0')
Solve time for step 2 3.6735389810055494
Current ori: tensor([-0.0901, -0.0348, -0.3815], device='cuda:0')
Index force: tensor([0.5839, 0.5850, 0.6014], device='cuda:0')
tensor([ 0.1842,  0.5878,  0.6000,  0.6572, -0.2997,  0.6757,  0.8850,  1.0437,
         1.3396,  0.3262,  0.0940,  0.7623, -0.0876, -0.0288, -0.3815,  3.1355],
       device='cuda:0')
Solve time for step 3 3.407005871995352
Current ori: tensor([-0.0876, -0.0288, -0.3815], device='cuda:0')
Index force: tensor([0.5788, 0.5408], device='cuda:0')
tensor([ 0.1587,  0.5774,  0.5877,  0.6554, -0.3037,  0.6748,  0.8807,  1.0391,
         1.3448,  0.3264,  0.0979,  0.7643, -0.0865, -0.0093, -0.3815,  3.0947],
       device='cuda:0')
Solve time for step 4 3.195243635971565
Current ori: tensor([-0.0865, -0.0093, -0.3815], device='cuda:0')
Index force: tensor([0.5285], device='cuda:0')
Storing RECOVERY transition: reward=0.0145 (scaled=0.0024), steps=6
Reward stats updated: mean -0.0021 -> -0.0021, std: 0.1023
Collected 391 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8948, Q2 Loss=0.8948, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0313
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2837, Q2 Loss=1.2837, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8439
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0690, Q2 Loss=1.0690, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4364
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.3162, Q2 Loss=2.3162, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1093
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6534, Q2 Loss=1.6534, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7791

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.2%)
Q1 update: 0.04s (18.5%)
Q2 update: 0.04s (17.1%)
Actor update: 0.09s (36.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 1.443407
Q2 loss: 1.443407
Current threshold: -35.0989
Global Scale Offset: 0.0500
Reward stats: mean=-0.0021, std=0.1023, count=391
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.4434, Q2 Loss: 1.4434, Entropy: 0.0000, Mean TD Error: 1.2400, Threshold: -35.0989
Original likelihood: -26.148529052734375
Adjusted likelihood: -26.148529052734375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0836, -0.0212, -0.3982], device='cuda:0')
5 turn
Sampling time 3.929305777011905
tensor([ 0.1554,  0.5669,  0.5906,  0.6715, -0.2227,  0.7305,  0.9142,  1.0515,
         1.3957,  0.3398,  0.1457,  0.7947, -0.0836, -0.0212, -0.3982,  3.2517],
       device='cuda:0')
Original likelihood: -32.077816009521484
Adjusted likelihood: -32.077816009521484
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.21401621698169
Current ori: tensor([-0.0836, -0.0212, -0.3982], device='cuda:0')
Middle force: tensor([1.4107, 1.2368, 0.4896, 0.7706, 0.5112, 0.5782, 1.1810, 0.5731, 0.7095,
        1.0791, 0.5600, 0.5751], device='cuda:0')
Thumb force: tensor([0.7636, 0.9550, 0.5642, 0.6992, 0.5895, 0.5855, 1.0972, 0.5541, 0.9312,
        1.2917, 0.9756, 0.5225], device='cuda:0')
Index force: tensor([1.0735, 0.8590, 0.6572, 0.5632, 0.5408, 0.5959, 0.5497, 0.5127, 0.5663,
        0.5749, 0.5950, 0.5841], device='cuda:0')
Storing NORMAL transition: reward=0.2663 (scaled=0.2663), steps=1
Reward stats updated: mean -0.0021 -> -0.0014, std: 0.1031
Collected 392 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0241, Q2 Loss=1.0241, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1822
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9553, Q2 Loss=0.9553, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4744
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1995, Q2 Loss=1.1995, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8880
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8967, Q2 Loss=0.8967, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3114
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.1636, Q2 Loss=1.1636, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4584

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.7%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (38.7%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.047832
Q2 loss: 1.047832
Current threshold: -35.1057
Global Scale Offset: 0.0498
Reward stats: mean=-0.0014, std=0.1031, count=392
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.0478, Q2 Loss: 1.0478, Entropy: 0.0000, Mean TD Error: 0.6629, Threshold: -35.1057
tensor([ 0.2636,  0.6774,  0.6234,  0.5588, -0.1552,  0.6948,  1.0357,  1.1234,
         1.3940,  0.3608,  0.1002,  0.7726, -0.0743, -0.0730, -0.6788,  2.9434],
       device='cuda:0')
Original likelihood: -27.538589477539062
Adjusted likelihood: -27.538589477539062
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.610889598960057
Current ori: tensor([-0.0743, -0.0730, -0.6788], device='cuda:0')
Middle force: tensor([0.5077, 0.6563, 0.5014, 0.6144, 0.5848, 0.5498, 0.5759, 1.0272, 0.5248,
        0.6330, 0.5002], device='cuda:0')
Thumb force: tensor([0.7232, 0.5430, 0.5751, 0.6570, 0.5881, 0.5068, 0.5921, 0.7752, 0.5911,
        1.7237, 1.3641], device='cuda:0')
Index force: tensor([0.5019, 0.5285, 0.5377, 0.5567, 0.5617, 0.5478, 0.5891, 0.5442, 0.5619,
        0.5638, 0.5438], device='cuda:0')
Storing NORMAL transition: reward=0.0048 (scaled=0.0048), steps=1
Reward stats updated: mean -0.0014 -> -0.0014, std: 0.1030
Collected 393 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1553, Q2 Loss=1.1553, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0508
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.2444, Q2 Loss=1.2444, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5448
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0472, Q2 Loss=1.0472, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5249
SAC Update 4/5: Actor Loss=-0.0062, Q1 Loss=1.8483, Q2 Loss=1.8483, Entropy=0.3451, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9187
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7871, Q2 Loss=0.7871, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7737

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (13.9%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.06s (21.9%)
Actor update: 0.11s (40.8%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.047290
Q1 loss: 1.216459
Q2 loss: 1.216459
Current threshold: -35.1126
Global Scale Offset: 0.0497
Reward stats: mean=-0.0014, std=0.1030, count=393
----------------------------------------------
SAC Update - Actor Loss: -0.0473, Q1 Loss: 1.2165, Q2 Loss: 1.2165, Entropy: 0.0690, Mean TD Error: 0.9626, Threshold: -35.1126
tensor([ 2.9828e-01,  6.9588e-01,  6.0025e-01,  6.6639e-01, -2.5580e-01,
         6.8922e-01,  9.4671e-01,  1.1716e+00,  1.4294e+00,  3.3841e-01,
         2.2596e-01,  6.3718e-01, -6.7055e-02,  1.7396e-04, -6.7106e-01,
         2.2321e+00], device='cuda:0')
Original likelihood: -32.64823913574219
Adjusted likelihood: -32.64823913574219
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.380533743998967
Current ori: tensor([-6.7055e-02,  1.7396e-04, -6.7106e-01], device='cuda:0')
Middle force: tensor([0.5087, 0.5634, 0.5157, 1.5343, 0.5187, 0.5243, 0.5774, 0.5551, 0.5633,
        0.5476], device='cuda:0')
Thumb force: tensor([0.5167, 1.1891, 0.5072, 0.7951, 0.5215, 0.6285, 0.8111, 0.7003, 0.5608,
        0.5548], device='cuda:0')
Index force: tensor([0.6905, 0.5482, 0.6344, 0.7778, 0.5398, 0.5186, 0.5271, 0.7450, 0.5103,
        0.5218], device='cuda:0')
Storing NORMAL transition: reward=-0.0804 (scaled=-0.0804), steps=1
Reward stats updated: mean -0.0014 -> -0.0016, std: 0.1029
Collected 394 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7236, Q2 Loss=0.7236, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2695
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6632, Q2 Loss=1.6632, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6586
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6435, Q2 Loss=0.6435, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2380
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0228, Q2 Loss=1.0228, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3235
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0316, Q2 Loss=1.0316, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7102

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.9%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.6%)
Actor update: 0.09s (39.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.016962
Q2 loss: 1.016962
Current threshold: -35.1199
Global Scale Offset: 0.0497
Reward stats: mean=-0.0016, std=0.1029, count=394
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.0170, Q2 Loss: 1.0170, Entropy: 0.0000, Mean TD Error: 1.0400, Threshold: -35.1199
tensor([ 0.2575,  0.6597,  0.6382,  0.6328, -0.2624,  0.7315,  0.9077,  1.0804,
         1.4645,  0.2530,  0.1394,  0.7525, -0.0922,  0.0074, -0.5947,  2.4747],
       device='cuda:0')
Original likelihood: -29.625293731689453
Adjusted likelihood: -29.625293731689453
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.1709012649953365
Current ori: tensor([-0.0922,  0.0074, -0.5947], device='cuda:0')
Middle force: tensor([0.5610, 0.5151, 1.5139, 0.5175, 0.5227, 0.5733, 0.5511, 0.5644, 0.5462],
       device='cuda:0')
Thumb force: tensor([1.1729, 0.5064, 0.7897, 0.5231, 0.6278, 0.8047, 0.7119, 0.5627, 0.5550],
       device='cuda:0')
Index force: tensor([0.5505, 0.6377, 0.7671, 0.5365, 0.5185, 0.5264, 0.7276, 0.5089, 0.5205],
       device='cuda:0')
Storing NORMAL transition: reward=0.0091 (scaled=0.0091), steps=1
Reward stats updated: mean -0.0016 -> -0.0016, std: 0.1028
Collected 395 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7461, Q2 Loss=0.7461, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6280
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.4259, Q2 Loss=2.4259, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7198
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2640, Q2 Loss=1.2640, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8043
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9228, Q2 Loss=0.9228, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7335
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3920, Q2 Loss=1.3920, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1633

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.1%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (19.8%)
Actor update: 0.11s (40.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 1.350175
Q2 loss: 1.350175
Current threshold: -35.1242
Global Scale Offset: 0.0496
Reward stats: mean=-0.0016, std=0.1028, count=395
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.3502, Q2 Loss: 1.3502, Entropy: 0.0000, Mean TD Error: 1.8098, Threshold: -35.1242
tensor([ 0.2681,  0.6252,  0.6586,  0.5555, -0.2471,  0.7801,  0.8960,  1.1102,
         1.4919,  0.1685,  0.0849,  0.7530, -0.1086, -0.0192, -0.6099,  3.5805],
       device='cuda:0')
Original likelihood: -33.68095016479492
Adjusted likelihood: -33.68095016479492
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.798639282991644
Current ori: tensor([-0.1086, -0.0192, -0.6099], device='cuda:0')
Middle force: tensor([0.5166, 1.4757, 0.5167, 0.5314, 0.5700, 0.5580, 0.5708, 0.5477],
       device='cuda:0')
Thumb force: tensor([0.5049, 0.7906, 0.5192, 0.6040, 0.7864, 0.6982, 0.5576, 0.5505],
       device='cuda:0')
Index force: tensor([0.6563, 0.7688, 0.5420, 0.5165, 0.5308, 0.7069, 0.5078, 0.5190],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0701 (scaled=-0.0701), steps=1
Reward stats updated: mean -0.0016 -> -0.0018, std: 0.1027
Collected 396 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2999, Q2 Loss=1.2999, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9831
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.9708, Q2 Loss=0.9708, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8292
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6613, Q2 Loss=0.6613, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6881
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2967, Q2 Loss=1.2967, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9189
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.0053, Q2 Loss=2.0053, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8533

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.4%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (18.7%)
Actor update: 0.09s (39.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000000
Q1 loss: 1.246814
Q2 loss: 1.246814
Current threshold: -35.1268
Global Scale Offset: 0.0496
Reward stats: mean=-0.0018, std=0.1027, count=396
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.2468, Q2 Loss: 1.2468, Entropy: 0.0000, Mean TD Error: 1.4545, Threshold: -35.1268
tensor([ 0.2534,  0.5664,  0.7039,  0.6263, -0.2850,  0.7176,  0.9865,  1.1901,
         1.4915,  0.1936, -0.0488,  0.9123, -0.1826, -0.0142, -0.5652,  3.7265],
       device='cuda:0')
Original likelihood: -38.94802474975586
Adjusted likelihood: -38.94802474975586
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 39.03177261352539
Projection step: 1, Loss: 37.581947326660156
Projection step: 2, Loss: 35.73748016357422
Projection step: 3, Loss: 38.27119827270508
Projection step: 4, Loss: 37.6295280456543
Projection step: 5, Loss: 39.37720489501953
Projection step: 6, Loss: 36.825538635253906
Projection step: 7, Loss: 36.146629333496094
Projection step: 8, Loss: 36.9899787902832
Projection step: 9, Loss: 37.163291931152344
Projection step: 10, Loss: 36.8648681640625
Projection step: 11, Loss: 36.101036071777344
Projection step: 12, Loss: 36.06018829345703
Projection step: 13, Loss: 36.377403259277344
Projection step: 14, Loss: 36.104549407958984
Projection step: 15, Loss: 38.99312210083008
Projection step: 16, Loss: 34.99128723144531
Projection step: 17, Loss: 37.407257080078125
Projection step: 18, Loss: 34.53684997558594
Projection step: 19, Loss: 35.845848083496094
Projection step: 20, Loss: 37.213172912597656
Projection step: 21, Loss: 35.21266174316406
Projection step: 22, Loss: 34.7540283203125
Projection step: 23, Loss: 37.858009338378906
Projection step: 24, Loss: 34.236297607421875
Final likelihood: tensor([-52.9588, -35.2697, -33.2215, -35.9252, -47.6272, -33.6306, -36.6124,
        -36.9351, -30.7070, -45.2650, -32.2752, -35.3752, -34.3714, -34.9596,
        -35.2948, -37.8437])
Final projection likelihood: -37.3920
1 mode projection failed, trying anyway
New goal: tensor([ 0.2554,  0.5704,  0.6669,  0.6188, -0.2528,  0.7377,  0.9503,  1.2004,
         1.4917,  0.2182,  0.0093,  0.8774, -0.1801, -0.0119, -0.1433],
       device='cuda:0')
tensor([[0.0031]], device='cuda:0') tensor([[0.0132]], device='cuda:0') tensor([[0.0104]], device='cuda:0')
Original likelihood: -30.211273193359375
Adjusted likelihood: -30.211273193359375
Likelihood residual: 0.0
Original likelihood: -31.49339485168457
Adjusted likelihood: -31.49339485168457
Likelihood residual: 0.0
{'index': 31.49339485168457, 'thumb_middle': 30.211273193359375}
Current yaw: tensor([-0.1826, -0.0142, -0.5652], device='cuda:0')
6 thumb_middle
tensor([ 0.2534,  0.5664,  0.7039,  0.6263, -0.2850,  0.7176,  0.9865,  1.1901,
         1.4915,  0.1936, -0.0488,  0.9123, -0.1826, -0.0142, -0.5652,  3.7265],
       device='cuda:0')
Solve time for step 1 9.142343977990095
Current ori: tensor([-0.1826, -0.0142, -0.5652], device='cuda:0')
Index force: tensor([0.5897, 0.5908, 0.5935, 0.6102], device='cuda:0')
tensor([ 0.1743,  0.6723,  0.7246,  0.6336, -0.3164,  0.7376,  0.9454,  1.1989,
         1.4309,  0.1962, -0.0776,  0.8523, -0.4537, -0.0341, -0.5650,  2.9095],
       device='cuda:0')
Solve time for step 2 3.672926331986673
Current ori: tensor([-0.4537, -0.0341, -0.5650], device='cuda:0')
Index force: tensor([0.5704, 0.5707, 0.5881], device='cuda:0')
tensor([ 0.0548,  0.8134,  0.7435,  0.6128, -0.2477,  0.8250,  0.9339,  1.1599,
         1.4146,  0.1916, -0.0912,  0.8384, -1.0614, -0.0685, -0.5650,  1.9935],
       device='cuda:0')
Solve time for step 3 3.485978515993338
Current ori: tensor([-1.0614, -0.0685, -0.5650], device='cuda:0')
Index force: tensor([0.5469, 0.5450], device='cuda:0')
tensor([-0.2723,  1.1563,  0.9646,  0.8826, -0.3325,  0.7791,  0.9241,  1.1648,
         1.4428,  0.1919, -0.1003,  0.8441, -1.8509, -0.0749, -0.5651,  2.2933],
       device='cuda:0')
Solve time for step 4 3.3151765759685077
Current ori: tensor([-1.8509, -0.0749, -0.5651], device='cuda:0')
Index force: tensor([0.5433], device='cuda:0')
Storing RECOVERY transition: reward=-1.7172 (scaled=-0.3434), steps=5
Reward stats updated: mean -0.0018 -> -0.0026, std: 0.1040
Collected 397 transitions for RL
SAC Update 1/5: Actor Loss=-0.0069, Q1 Loss=3.1523, Q2 Loss=3.1523, Entropy=0.3466, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.2522
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4804, Q2 Loss=1.4804, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1713
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0745, Q2 Loss=1.0745, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9376
SAC Update 4/5: Actor Loss=-0.2229, Q1 Loss=1.5023, Q2 Loss=1.5023, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0633
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7017, Q2 Loss=0.7017, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2834

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.4%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (19.8%)
Actor update: 0.11s (41.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092007
Q1 loss: 1.582234
Q2 loss: 1.582234
Current threshold: -35.1349
Global Scale Offset: 0.0496
Reward stats: mean=-0.0026, std=0.1040, count=397
----------------------------------------------
SAC Update - Actor Loss: -0.0920, Q1 Loss: 1.5822, Q2 Loss: 1.5822, Entropy: 0.0693, Mean TD Error: 1.5415, Threshold: -35.1349
Original likelihood: -1299.993408203125
Adjusted likelihood: -1299.993408203125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 26
Loaded trajectory sampler
Current yaw: tensor([-0.0006,  0.0141, -0.0338], device='cuda:0')
Current yaw: tensor([-0.0006,  0.0141, -0.0338], device='cuda:0')
1 turn
Sampling time 3.600860784004908
tensor([ 1.4111e-01,  6.1317e-01,  5.8341e-01,  5.5060e-01, -1.3281e-01,
         5.7112e-01,  9.0362e-01,  8.7191e-01,  1.1987e+00,  3.6139e-01,
         2.4740e-01,  1.1961e+00, -6.0552e-04,  1.4070e-02, -3.3828e-02,
         1.5100e-01], device='cuda:0')
Original likelihood: -22.732158660888672
Adjusted likelihood: -22.732158660888672
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.134154805971775
Current ori: tensor([-0.0006,  0.0141, -0.0338], device='cuda:0')
Middle force: tensor([0.5809, 0.5754, 1.1986, 0.5692, 1.1534, 0.6437, 0.5361, 0.5259, 0.5158,
        0.8331, 0.7331, 0.5899], device='cuda:0')
Thumb force: tensor([0.8819, 0.8508, 0.7622, 1.0893, 1.0096, 0.6392, 0.5250, 0.9255, 0.5359,
        0.5369, 0.6972, 0.6179], device='cuda:0')
Index force: tensor([0.5983, 0.6037, 0.5599, 0.5753, 0.8139, 0.5317, 1.0406, 0.9493, 0.5995,
        0.5923, 0.7694, 0.6254], device='cuda:0')
Storing NORMAL transition: reward=0.0432 (scaled=0.0432), steps=1
Reward stats updated: mean -0.0026 -> -0.0025, std: 0.1039
Collected 398 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9589, Q2 Loss=0.9589, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6511
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.7822, Q2 Loss=1.7822, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5981
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6293, Q2 Loss=1.6293, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4423
SAC Update 4/5: Actor Loss=-0.0343, Q1 Loss=0.8915, Q2 Loss=0.8915, Entropy=0.0712, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8568
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8521, Q2 Loss=0.8521, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6727

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.9%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (38.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.006869
Q1 loss: 1.222791
Q2 loss: 1.222791
Current threshold: -35.1482
Global Scale Offset: 0.0495
Reward stats: mean=-0.0025, std=0.1039, count=398
----------------------------------------------
SAC Update - Actor Loss: -0.0069, Q1 Loss: 1.2228, Q2 Loss: 1.2228, Entropy: 0.0142, Mean TD Error: 1.2442, Threshold: -35.1482
tensor([ 0.1499,  0.6355,  0.5488,  0.5759, -0.1434,  0.6073,  0.8480,  0.9445,
         1.2347,  0.3372,  0.2282,  1.1576, -0.0045,  0.0025, -0.0769,  0.3543],
       device='cuda:0')
Original likelihood: -20.48019027709961
Adjusted likelihood: -20.48019027709961
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.709700424980838
Current ori: tensor([-0.0045,  0.0025, -0.0769], device='cuda:0')
Middle force: tensor([0.5707, 1.1829, 0.5656, 1.1462, 0.6372, 0.5339, 0.5248, 0.5134, 0.8269,
        0.7275, 0.5873], device='cuda:0')
Thumb force: tensor([0.8222, 0.7402, 1.0651, 0.9711, 0.6311, 0.5210, 0.9008, 0.5335, 0.5325,
        0.6854, 0.6113], device='cuda:0')
Index force: tensor([0.5937, 0.5548, 0.5704, 0.8049, 0.5282, 1.0244, 0.9332, 0.5949, 0.5851,
        0.7583, 0.6182], device='cuda:0')
Storing NORMAL transition: reward=0.0798 (scaled=0.0798), steps=1
Reward stats updated: mean -0.0025 -> -0.0023, std: 0.1039
Collected 399 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8058, Q2 Loss=0.8058, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3119
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.0543, Q2 Loss=1.0543, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0727
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9981, Q2 Loss=0.9981, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0164
SAC Update 4/5: Actor Loss=-0.0957, Q1 Loss=0.7610, Q2 Loss=0.7610, Entropy=0.0441, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2660
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.2224, Q2 Loss=1.2224, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1739

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (19.9%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (38.6%)
Target update: 0.00s (1.8%)
Priority update: 0.00s (0.2%)
Actor loss: -0.111251
Q1 loss: 0.968347
Q2 loss: 0.968347
Current threshold: -35.1753
Global Scale Offset: 0.0492
Reward stats: mean=-0.0023, std=0.1039, count=399
----------------------------------------------
SAC Update - Actor Loss: -0.1113, Q1 Loss: 0.9683, Q2 Loss: 0.9683, Entropy: 0.0088, Mean TD Error: 0.7682, Threshold: -35.1753
tensor([ 0.2147,  0.6162,  0.6218,  0.6209, -0.1164,  0.6006,  0.8739,  0.9385,
         1.2067,  0.3881,  0.2335,  1.1136, -0.0121, -0.0138, -0.1571,  0.5151],
       device='cuda:0')
Original likelihood: -34.83320617675781
Adjusted likelihood: -34.83320617675781
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9494)
Solve time for step 3 5.169129185960628
Current ori: tensor([-0.0121, -0.0138, -0.1571], device='cuda:0')
Middle force: tensor([0.5100, 1.0192, 0.5213, 0.5134, 0.5502, 0.5264, 0.5422, 0.5022, 0.5116,
        0.5026], device='cuda:0')
Thumb force: tensor([1.4526, 1.1150, 0.7011, 1.0470, 0.5588, 0.5548, 1.6004, 0.7935, 0.5016,
        0.7344], device='cuda:0')
Index force: tensor([0.5517, 0.5003, 0.5374, 0.5109, 0.5743, 0.6650, 0.8179, 0.8793, 0.6563,
        0.8211], device='cuda:0')
Storing NORMAL transition: reward=-0.0045 (scaled=-0.0045), steps=1
Reward stats updated: mean -0.0023 -> -0.0023, std: 0.1037
Collected 400 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7110, Q2 Loss=0.7110, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7219
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.0431, Q2 Loss=2.0431, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.9192
SAC Update 3/5: Actor Loss=-0.0405, Q1 Loss=0.7962, Q2 Loss=0.7962, Entropy=0.0000, Time=0.10sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9857
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=3.1436, Q2 Loss=3.1436, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.2783
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.1871, Q2 Loss=2.1871, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0693

------ SAC Update Summary (5 iterations) ------
Total time: 0.31s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (16.7%)
Q1 update: 0.05s (16.2%)
Q2 update: 0.05s (16.0%)
Actor update: 0.11s (34.7%)
Target update: 0.05s (14.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.008094
Q1 loss: 1.776175
Q2 loss: 1.776175
Current threshold: -35.2020
Global Scale Offset: 0.0490
Reward stats: mean=-0.0023, std=0.1037, count=400
----------------------------------------------
SAC Update - Actor Loss: -0.0081, Q1 Loss: 1.7762, Q2 Loss: 1.7762, Entropy: 0.0000, Mean TD Error: 2.3949, Threshold: -35.2020
tensor([ 0.0278,  0.5279,  0.5893,  0.5572, -0.1760,  0.5197,  0.7731,  1.0090,
         1.3463,  0.2518,  0.0113,  1.1734,  0.0072,  0.0618, -0.1571,  0.7507],
       device='cuda:0')
Original likelihood: -37.29045867919922
Adjusted likelihood: -37.29045867919922
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 36.80232238769531
Projection step: 1, Loss: 36.85387420654297
Projection step: 2, Loss: 36.30697250366211
Projection step: 3, Loss: 34.697479248046875
Projection step: 4, Loss: 33.87451171875
Projection step: 5, Loss: 33.578041076660156
Projection step: 6, Loss: 34.20975875854492
Projection step: 7, Loss: 32.16124725341797
Projection step: 8, Loss: 31.217662811279297
Projection step: 9, Loss: 31.962148666381836
Projection step: 10, Loss: 31.843738555908203
Projection step: 11, Loss: 29.867204666137695
Projection step: 12, Loss: 29.80789566040039
Projection step: 13, Loss: 30.333751678466797
Projection step: 14, Loss: 30.53142547607422
Projection step: 15, Loss: 27.921527862548828
Projection step: 16, Loss: 27.99374771118164
Projection step: 17, Loss: 27.729644775390625
Projection step: 18, Loss: 29.12653350830078
Projection step: 19, Loss: 28.318771362304688
Projection step: 20, Loss: 27.686321258544922
Projection step: 21, Loss: 27.568866729736328
Projection step: 22, Loss: 26.315441131591797
Projection step: 23, Loss: 27.912675857543945
Projection step: 24, Loss: 26.84579086303711
Final likelihood: tensor([-30.9321, -33.5434, -23.8780, -24.4927, -30.2246, -27.2089, -25.8415,
        -22.0195, -23.1047, -28.1884, -24.6588, -26.8309, -26.0173, -24.8433,
        -25.9908, -30.0473])
Final projection likelihood: -26.7389
1 mode projection succeeded
New goal: tensor([ 0.0096,  0.5422,  0.5462,  0.6487, -0.1547,  0.5063,  0.7468,  0.9091,
         1.3373,  0.2492,  0.0847,  1.2742,  0.0015,  0.0499, -0.8046],
       device='cuda:0')
tensor([[0.0179]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -30.326278686523438
Adjusted likelihood: -30.326278686523438
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 30.326278686523438}
Current yaw: tensor([ 0.0072,  0.0618, -0.1571], device='cuda:0')
2 thumb_middle
tensor([ 0.0278,  0.5279,  0.5893,  0.5572, -0.1760,  0.5197,  0.7731,  1.0090,
         1.3463,  0.2518,  0.0113,  1.1734,  0.0072,  0.0618, -0.1571,  0.7507],
       device='cuda:0')
Solve time for step 1 8.96397441305453
Current ori: tensor([ 0.0072,  0.0618, -0.1571], device='cuda:0')
Index force: tensor([0.5946, 0.5834, 0.6029, 0.5890], device='cuda:0')
tensor([ 0.0214,  0.5448,  0.5336,  0.6063, -0.2303,  0.5092,  0.7406,  0.9147,
         1.3402,  0.2450,  0.0428,  1.2469,  0.0072,  0.0677, -0.1567,  0.7067],
       device='cuda:0')
Solve time for step 2 3.7036420459626243
Current ori: tensor([ 0.0072,  0.0677, -0.1567], device='cuda:0')
Index force: tensor([0.5755, 0.5955, 0.5831], device='cuda:0')
tensor([ 0.0292,  0.5633,  0.5174,  0.6023, -0.2290,  0.5163,  0.7460,  0.9044,
         1.3344,  0.2371,  0.0387,  1.2600,  0.0023,  0.0631, -0.1567,  0.7136],
       device='cuda:0')
Solve time for step 3 3.4711200520396233
Current ori: tensor([ 0.0023,  0.0631, -0.1567], device='cuda:0')
Index force: tensor([0.5848, 0.5751], device='cuda:0')
tensor([ 0.0367,  0.5463,  0.5304,  0.6364, -0.2272,  0.5165,  0.7473,  0.9030,
         1.3323,  0.2389,  0.0368,  1.2611,  0.0086,  0.0595, -0.1567,  0.7348],
       device='cuda:0')
Solve time for step 4 3.338973595004063
Current ori: tensor([ 0.0086,  0.0595, -0.1567], device='cuda:0')
Index force: tensor([0.5000], device='cuda:0')
Storing RECOVERY transition: reward=0.0074 (scaled=0.0025), steps=3
Reward stats updated: mean -0.0023 -> -0.0023, std: 0.1036
Collected 401 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.1717, Q2 Loss=2.1717, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0309
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0578, Q2 Loss=1.0578, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0695
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.6691, Q2 Loss=0.6691, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9504
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7531, Q2 Loss=0.7531, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7920
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0889, Q2 Loss=1.0889, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3501

------ SAC Update Summary (5 iterations) ------
Total time: 0.30s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (16.1%)
Q1 update: 0.05s (16.1%)
Q2 update: 0.09s (31.6%)
Actor update: 0.10s (32.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.148107
Q2 loss: 1.148107
Current threshold: -35.2179
Global Scale Offset: 0.0488
Reward stats: mean=-0.0023, std=0.1036, count=401
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1481, Q2 Loss: 1.1481, Entropy: 0.0000, Mean TD Error: 1.0386, Threshold: -35.2179
Original likelihood: -32.449974060058594
Adjusted likelihood: -32.449974060058594
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0069,  0.0579, -0.1640], device='cuda:0')
3 turn
Sampling time 3.60721269500209
tensor([ 0.0379,  0.5536,  0.5227,  0.6332, -0.1657,  0.5531,  0.7689,  0.9160,
         1.4042,  0.2714,  0.0816,  1.2808,  0.0069,  0.0579, -0.1640,  0.7631],
       device='cuda:0')
Original likelihood: -32.082271575927734
Adjusted likelihood: -32.082271575927734
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.977135957975406
Current ori: tensor([ 0.0069,  0.0579, -0.1640], device='cuda:0')
Middle force: tensor([1.2254, 1.7508, 0.8238, 0.5298, 0.5634, 1.0135, 1.1610, 0.5169, 0.8733,
        0.5416, 0.5850, 0.5827], device='cuda:0')
Thumb force: tensor([1.0728, 1.5675, 0.5931, 0.5290, 0.4854, 1.2064, 0.6940, 0.5591, 0.8355,
        0.6320, 1.0336, 0.6222], device='cuda:0')
Index force: tensor([0.9308, 1.8125, 0.5628, 0.5912, 0.7214, 0.8015, 0.5545, 0.5759, 0.5355,
        0.6278, 0.6078, 0.5993], device='cuda:0')
Storing NORMAL transition: reward=0.0738 (scaled=0.0738), steps=1
Reward stats updated: mean -0.0023 -> -0.0021, std: 0.1035
Collected 402 transitions for RL
SAC Update 1/5: Actor Loss=-0.1095, Q1 Loss=0.9146, Q2 Loss=0.9146, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2336
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1137, Q2 Loss=1.1137, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7379
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1283, Q2 Loss=1.1283, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9436
SAC Update 4/5: Actor Loss=-0.0031, Q1 Loss=0.7747, Q2 Loss=0.7747, Entropy=0.2898, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4110
SAC Update 5/5: Actor Loss=-0.1327, Q1 Loss=1.3104, Q2 Loss=1.3104, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3131

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.5%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.5%)
Actor update: 0.08s (40.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.095103
Q1 loss: 1.048345
Q2 loss: 1.048345
Current threshold: -35.2284
Global Scale Offset: 0.0487
Reward stats: mean=-0.0021, std=0.1035, count=402
----------------------------------------------
SAC Update - Actor Loss: -0.0951, Q1 Loss: 1.0483, Q2 Loss: 1.0483, Entropy: 0.0580, Mean TD Error: 0.7278, Threshold: -35.2284
tensor([ 0.1560,  0.7399,  0.4065,  0.5705, -0.1234,  0.5842,  0.9321,  0.9327,
         1.3902,  0.3326,  0.0182,  1.0992, -0.0382, -0.0189, -0.2366,  0.9366],
       device='cuda:0')
Original likelihood: -25.10755157470703
Adjusted likelihood: -25.10755157470703
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.623457895999309
Current ori: tensor([-0.0382, -0.0189, -0.2366], device='cuda:0')
Middle force: tensor([0.5040, 0.5018, 0.5052, 0.5703, 0.5805, 1.0870, 0.8193, 0.7949, 0.5779,
        0.5718, 0.5087], device='cuda:0')
Thumb force: tensor([1.9655, 1.3967, 0.5915, 1.0821, 0.8057, 1.5060, 0.5726, 0.7347, 0.6461,
        0.5150, 0.5712], device='cuda:0')
Index force: tensor([0.8103, 0.8602, 0.6221, 0.5493, 0.5313, 0.5620, 0.5206, 0.5559, 0.5570,
        0.5017, 0.7960], device='cuda:0')
Storing NORMAL transition: reward=-0.0231 (scaled=-0.0231), steps=1
Reward stats updated: mean -0.0021 -> -0.0022, std: 0.1034
Collected 403 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9767, Q2 Loss=0.9767, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0348
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6923, Q2 Loss=1.6923, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4960
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9983, Q2 Loss=0.9983, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7174
SAC Update 4/5: Actor Loss=-0.1940, Q1 Loss=0.7801, Q2 Loss=0.7801, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2988
SAC Update 5/5: Actor Loss=-0.0508, Q1 Loss=0.8065, Q2 Loss=0.8065, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7700

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (19.0%)
Q1 update: 0.04s (20.0%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (39.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.048963
Q1 loss: 1.050748
Q2 loss: 1.050748
Current threshold: -35.2565
Global Scale Offset: 0.0479
Reward stats: mean=-0.0022, std=0.1034, count=403
----------------------------------------------
SAC Update - Actor Loss: -0.0490, Q1 Loss: 1.0507, Q2 Loss: 1.0507, Entropy: 0.0000, Mean TD Error: 1.2634, Threshold: -35.2565
tensor([ 0.1359,  0.7280,  0.3316,  0.7169, -0.1223,  0.6177,  0.8196,  1.0811,
         1.3505,  0.4326, -0.0202,  1.2317, -0.0244, -0.0202, -0.2125,  1.2932],
       device='cuda:0')
Original likelihood: -26.79642105102539
Adjusted likelihood: -26.79642105102539
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.218303433037363
Current ori: tensor([-0.0244, -0.0202, -0.2125], device='cuda:0')
Middle force: tensor([0.5027, 0.5045, 0.5705, 0.5841, 1.0688, 0.8175, 0.7939, 0.5791, 0.5708,
        0.5050], device='cuda:0')
Thumb force: tensor([1.3415, 0.5847, 1.0601, 0.7887, 1.4834, 0.5672, 0.7193, 0.6374, 0.5139,
        0.5742], device='cuda:0')
Index force: tensor([0.8161, 0.6192, 0.5469, 0.5288, 0.5595, 0.5193, 0.5529, 0.5542, 0.5016,
        0.8196], device='cuda:0')
Storing NORMAL transition: reward=0.0587 (scaled=0.0587), steps=1
Reward stats updated: mean -0.0022 -> -0.0020, std: 0.1033
Collected 404 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.7045, Q2 Loss=0.7045, Entropy=0.0475, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2158
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9946, Q2 Loss=0.9946, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2200
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2081, Q2 Loss=1.2081, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9013
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.6285, Q2 Loss=0.6285, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3844
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8543, Q2 Loss=0.8543, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2948

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.7%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.8%)
Actor update: 0.08s (38.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046056
Q1 loss: 0.877999
Q2 loss: 0.877999
Current threshold: -35.2969
Global Scale Offset: 0.0468
Reward stats: mean=-0.0020, std=0.1033, count=404
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 0.8780, Q2 Loss: 0.8780, Entropy: 0.0095, Mean TD Error: 0.8033, Threshold: -35.2969
tensor([ 0.1314,  0.5914,  0.4562,  0.8294, -0.0508,  0.7158,  0.7077,  1.1993,
         1.3072,  0.5199, -0.0256,  1.1681, -0.0303, -0.0760, -0.2782,  1.2437],
       device='cuda:0')
Original likelihood: -30.87969207763672
Adjusted likelihood: -30.87969207763672
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.942160447011702
Current ori: tensor([-0.0303, -0.0760, -0.2782], device='cuda:0')
Middle force: tensor([0.8915, 0.5141, 0.5369, 0.6052, 0.5433, 0.8372, 0.5760, 0.5424, 0.5117],
       device='cuda:0')
Thumb force: tensor([1.1274, 0.8031, 0.5054, 0.5252, 0.5227, 0.5683, 0.5103, 0.5163, 0.7431],
       device='cuda:0')
Index force: tensor([0.5103, 0.5167, 0.5897, 0.5048, 0.6185, 0.7185, 0.5409, 0.5757, 0.5121],
       device='cuda:0')
Storing NORMAL transition: reward=0.0392 (scaled=0.0392), steps=1
Reward stats updated: mean -0.0020 -> -0.0019, std: 0.1032
Collected 405 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9481, Q2 Loss=0.9481, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5015
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4238, Q2 Loss=1.4238, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6545
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3018, Q2 Loss=1.3018, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2407
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0312, Q2 Loss=1.0312, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5069
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0528, Q2 Loss=1.0528, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7387

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.2%)
Q1 update: 0.05s (18.7%)
Q2 update: 0.05s (18.8%)
Actor update: 0.10s (40.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 1.151527
Q2 loss: 1.151527
Current threshold: -35.3209
Global Scale Offset: 0.0461
Reward stats: mean=-0.0019, std=0.1032, count=405
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.1515, Q2 Loss: 1.1515, Entropy: 0.0000, Mean TD Error: 1.3285, Threshold: -35.3209
tensor([ 0.2061,  0.6202,  0.4644,  0.9078, -0.0728,  0.5811,  0.8374,  1.0918,
         1.4399,  0.2778, -0.1098,  1.3002,  0.0331, -0.0363, -0.3125,  2.8618],
       device='cuda:0')
Original likelihood: -42.147300720214844
Adjusted likelihood: -42.147300720214844
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 34.939918518066406
Projection step: 1, Loss: 35.89778518676758
Projection step: 2, Loss: 30.76113510131836
Projection step: 3, Loss: 31.054683685302734
Projection step: 4, Loss: 33.65857696533203
Projection step: 5, Loss: 37.162078857421875
Projection step: 6, Loss: 35.514373779296875
Projection step: 7, Loss: 36.44866180419922
Projection step: 8, Loss: 31.922935485839844
Projection step: 9, Loss: 33.66082000732422
Projection step: 10, Loss: 32.119956970214844
Projection step: 11, Loss: 31.116920471191406
Projection step: 12, Loss: 29.217960357666016
Projection step: 13, Loss: 31.576702117919922
Projection step: 14, Loss: 32.02204132080078
Projection step: 15, Loss: 30.616085052490234
Projection step: 16, Loss: 27.916704177856445
Projection step: 17, Loss: 27.48870849609375
Projection step: 18, Loss: 27.164188385009766
Projection step: 19, Loss: 29.208045959472656
Projection step: 20, Loss: 27.49834442138672
Projection step: 21, Loss: 25.913591384887695
Projection step: 22, Loss: 27.054214477539062
Projection step: 23, Loss: 25.643543243408203
Projection step: 24, Loss: 26.134384155273438
Final likelihood: tensor([-24.6898, -26.7244, -25.0214, -26.7621, -23.2656, -26.8041, -25.0596,
        -27.0740, -28.2828, -25.5252, -24.1994, -26.2507, -29.5348, -23.8800,
        -25.6049, -25.9875])
Final projection likelihood: -25.9166
1 mode projection succeeded
New goal: tensor([ 0.1504,  0.5694,  0.5681,  0.8800, -0.0492,  0.5860,  0.8812,  1.0124,
         1.4057,  0.2894, -0.0367,  1.2134,  0.0299, -0.0304,  0.4288],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -27.385360717773438
Adjusted likelihood: -27.385360717773438
Likelihood residual: 0.0
Original likelihood: -27.91317367553711
Adjusted likelihood: -27.91317367553711
Likelihood residual: 0.0
{'index': 27.91317367553711, 'thumb_middle': 27.385360717773438}
Current yaw: tensor([ 0.0331, -0.0363, -0.3125], device='cuda:0')
4 thumb_middle
tensor([ 0.2061,  0.6202,  0.4644,  0.9078, -0.0728,  0.5811,  0.8374,  1.0918,
         1.4399,  0.2778, -0.1098,  1.3002,  0.0331, -0.0363, -0.3125,  2.8618],
       device='cuda:0')
Solve time for step 1 8.92070664500352
Current ori: tensor([ 0.0331, -0.0363, -0.3125], device='cuda:0')
Index force: tensor([0.5637, 0.5640, 0.5651, 0.5645], device='cuda:0')
tensor([ 0.2078,  0.5944,  0.5262,  0.8591, -0.1562,  0.5485,  0.8228,  0.9863,
         1.3708,  0.2729, -0.1091,  1.2007,  0.0339, -0.0393, -0.3125,  2.8599],
       device='cuda:0')
Solve time for step 2 3.646153253968805
Current ori: tensor([ 0.0339, -0.0393, -0.3125], device='cuda:0')
Index force: tensor([0.5559, 0.5588, 0.5588], device='cuda:0')
tensor([ 0.2150,  0.5758,  0.5503,  0.8792, -0.1584,  0.5636,  0.8375,  0.9714,
         1.3881,  0.2732, -0.1278,  1.1806,  0.0410, -0.0413, -0.3125,  2.8811],
       device='cuda:0')
Solve time for step 3 3.535234665032476
Current ori: tensor([ 0.0410, -0.0413, -0.3125], device='cuda:0')
Index force: tensor([0.5474, 0.5493], device='cuda:0')
tensor([ 0.1905,  0.5613,  0.5490,  0.8663, -0.1681,  0.5439,  0.8238,  0.9793,
         1.3759,  0.2807, -0.0982,  1.1839,  0.0411, -0.0280, -0.3125,  2.8456],
       device='cuda:0')
Solve time for step 4 3.4168149149627425
Current ori: tensor([ 0.0411, -0.0280, -0.3125], device='cuda:0')
Index force: tensor([0.5413], device='cuda:0')
Storing RECOVERY transition: reward=-0.0012 (scaled=-0.0003), steps=4
Reward stats updated: mean -0.0019 -> -0.0019, std: 0.1031
Collected 406 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8554, Q2 Loss=0.8554, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5784
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9796, Q2 Loss=0.9796, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2271
SAC Update 3/5: Actor Loss=-0.2284, Q1 Loss=1.5167, Q2 Loss=1.5167, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0575
SAC Update 4/5: Actor Loss=-0.1054, Q1 Loss=1.1438, Q2 Loss=1.1438, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2125
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2674, Q2 Loss=1.2674, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4638

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.6%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (19.0%)
Actor update: 0.11s (41.1%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.112795
Q1 loss: 1.152603
Q2 loss: 1.152603
Current threshold: -35.3351
Global Scale Offset: 0.0457
Reward stats: mean=-0.0019, std=0.1031, count=406
----------------------------------------------
SAC Update - Actor Loss: -0.1128, Q1 Loss: 1.1526, Q2 Loss: 1.1526, Entropy: 0.0000, Mean TD Error: 0.9079, Threshold: -35.3351
Original likelihood: -28.89961814880371
Adjusted likelihood: -28.89961814880371
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0435, -0.0220, -0.3113], device='cuda:0')
5 turn
Sampling time 3.64092493703356
tensor([ 0.1751,  0.5566,  0.5350,  0.8736, -0.1226,  0.5998,  0.8847,  0.9929,
         1.4487,  0.2915, -0.0545,  1.2371,  0.0435, -0.0220, -0.3113,  2.7871],
       device='cuda:0')
Original likelihood: -29.18239974975586
Adjusted likelihood: -29.18239974975586
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.011341191013344
Current ori: tensor([ 0.0435, -0.0220, -0.3113], device='cuda:0')
Middle force: tensor([1.1896, 0.5175, 0.4991, 0.5103, 0.5669, 0.5943, 1.0850, 0.7870, 0.7916,
        0.5737, 0.5788, 0.5720], device='cuda:0')
Thumb force: tensor([1.9212, 2.0026, 1.4038, 0.5391, 1.0757, 0.8130, 1.4391, 0.5951, 0.7229,
        0.6636, 0.5278, 0.5386], device='cuda:0')
Index force: tensor([0.5971, 0.7913, 0.7759, 0.6301, 0.5527, 0.5521, 0.6014, 0.5255, 0.5650,
        0.5731, 0.5109, 0.5371], device='cuda:0')
Storing NORMAL transition: reward=-0.1128 (scaled=-0.1128), steps=1
Reward stats updated: mean -0.0019 -> -0.0022, std: 0.1031
Collected 407 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6455, Q2 Loss=1.6455, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1933
SAC Update 2/5: Actor Loss=-0.2373, Q1 Loss=1.0012, Q2 Loss=1.0012, Entropy=0.0075, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3432
SAC Update 3/5: Actor Loss=-0.0195, Q1 Loss=1.0986, Q2 Loss=1.0986, Entropy=0.2040, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9399
SAC Update 4/5: Actor Loss=-0.0200, Q1 Loss=1.7400, Q2 Loss=1.7400, Entropy=0.1981, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5818
SAC Update 5/5: Actor Loss=-0.0055, Q1 Loss=0.8687, Q2 Loss=0.8687, Entropy=0.0033, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2745

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.1%)
Q1 update: 0.06s (21.7%)
Q2 update: 0.05s (19.8%)
Actor update: 0.10s (39.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.056487
Q1 loss: 1.270797
Q2 loss: 1.270797
Current threshold: -35.3610
Global Scale Offset: 0.0453
Reward stats: mean=-0.0022, std=0.1031, count=407
----------------------------------------------
SAC Update - Actor Loss: -0.0565, Q1 Loss: 1.2708, Q2 Loss: 1.2708, Entropy: 0.0826, Mean TD Error: 1.6666, Threshold: -35.3610
tensor([ 0.2059,  0.5514,  0.6118,  0.8034, -0.0659,  0.7086,  0.8392,  0.8891,
         1.3298,  0.4480,  0.0412,  1.1367,  0.0314, -0.0687, -0.2007,  2.5206],
       device='cuda:0')
Original likelihood: -32.23268127441406
Adjusted likelihood: -32.23268127441406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.640319833997637
Current ori: tensor([ 0.0314, -0.0687, -0.2007], device='cuda:0')
Middle force: tensor([0.5159, 0.5017, 0.5070, 0.5769, 0.6008, 1.0675, 0.7870, 0.7915, 0.5766,
        0.5829, 0.5793], device='cuda:0')
Thumb force: tensor([1.9378, 1.3162, 0.5348, 1.0377, 0.7807, 1.4074, 0.5831, 0.7034, 0.6514,
        0.5244, 0.5291], device='cuda:0')
Index force: tensor([0.7786, 0.7043, 0.6409, 0.5479, 0.5495, 0.6022, 0.5240, 0.5616, 0.5685,
        0.5094, 0.5376], device='cuda:0')
Storing NORMAL transition: reward=0.0334 (scaled=0.0334), steps=1
Reward stats updated: mean -0.0022 -> -0.0021, std: 0.1030
Collected 408 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5232, Q2 Loss=1.5232, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2996
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1809, Q2 Loss=1.1809, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3534
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6857, Q2 Loss=0.6857, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1999
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0345, Q2 Loss=1.0345, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7489
SAC Update 5/5: Actor Loss=-0.0191, Q1 Loss=0.6652, Q2 Loss=0.6652, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5442

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.9%)
Q1 update: 0.06s (21.3%)
Q2 update: 0.06s (20.5%)
Actor update: 0.11s (40.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.049867
Q1 loss: 1.017909
Q2 loss: 1.017909
Current threshold: -35.3899
Global Scale Offset: 0.0450
Reward stats: mean=-0.0021, std=0.1030, count=408
----------------------------------------------
SAC Update - Actor Loss: -0.0499, Q1 Loss: 1.0179, Q2 Loss: 1.0179, Entropy: 0.0000, Mean TD Error: 0.6292, Threshold: -35.3899
tensor([ 0.1798,  0.5558,  0.5862,  0.7834, -0.0849,  0.7172,  0.7950,  0.9265,
         1.3734,  0.4361,  0.0294,  1.1044,  0.0245, -0.0543, -0.2326,  2.4904],
       device='cuda:0')
Original likelihood: -32.38348388671875
Adjusted likelihood: -32.38348388671875
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.2283329970086925
Current ori: tensor([ 0.0245, -0.0543, -0.2326], device='cuda:0')
Middle force: tensor([0.5019, 0.5052, 0.5778, 0.5991, 1.0415, 0.7720, 0.7762, 0.5740, 0.5823,
        0.5771], device='cuda:0')
Thumb force: tensor([1.2757, 0.5346, 1.0246, 0.7660, 1.3862, 0.5813, 0.6960, 0.6466, 0.5229,
        0.5268], device='cuda:0')
Index force: tensor([0.6746, 0.6472, 0.5433, 0.5464, 0.6020, 0.5223, 0.5595, 0.5648, 0.5080,
        0.5355], device='cuda:0')
Storing NORMAL transition: reward=0.0081 (scaled=0.0081), steps=1
Reward stats updated: mean -0.0021 -> -0.0021, std: 0.1029
Collected 409 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.5055, Q2 Loss=1.5055, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4610
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.2714, Q2 Loss=1.2714, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9524
SAC Update 3/5: Actor Loss=-0.0456, Q1 Loss=3.5403, Q2 Loss=3.5403, Entropy=0.0028, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.4226
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9486, Q2 Loss=0.9486, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6095
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2436, Q2 Loss=1.2436, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8717

------ SAC Update Summary (5 iterations) ------
Total time: 0.33s, Avg iteration: 0.07s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (15.4%)
Q1 update: 0.06s (18.3%)
Q2 update: 0.06s (19.7%)
Actor update: 0.14s (43.7%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.101218
Q1 loss: 1.701883
Q2 loss: 1.701883
Current threshold: -35.4185
Global Scale Offset: 0.0446
Reward stats: mean=-0.0021, std=0.1029, count=409
----------------------------------------------
SAC Update - Actor Loss: -0.1012, Q1 Loss: 1.7019, Q2 Loss: 1.7019, Entropy: 0.0006, Mean TD Error: 1.6635, Threshold: -35.4185
tensor([ 0.1509,  0.5467,  0.5689,  0.7809, -0.1058,  0.7159,  0.7747,  0.9398,
         1.3635,  0.5212,  0.0830,  1.0402,  0.0232, -0.0392, -0.2395,  2.4433],
       device='cuda:0')
Original likelihood: -29.06450653076172
Adjusted likelihood: -29.06450653076172
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.053731458960101
Current ori: tensor([ 0.0232, -0.0392, -0.2395], device='cuda:0')
Middle force: tensor([0.5049, 0.5764, 0.5960, 1.0191, 0.7612, 0.7654, 0.5717, 0.5822, 0.5732],
       device='cuda:0')
Thumb force: tensor([0.5320, 1.0081, 0.7542, 1.3627, 0.5780, 0.6873, 0.6420, 0.5213, 0.5252],
       device='cuda:0')
Index force: tensor([0.6370, 0.5401, 0.5437, 0.5974, 0.5206, 0.5560, 0.5599, 0.5069, 0.5335],
       device='cuda:0')
Storing NORMAL transition: reward=0.0327 (scaled=0.0327), steps=1
Reward stats updated: mean -0.0021 -> -0.0020, std: 0.1028
Collected 410 transitions for RL
SAC Update 1/5: Actor Loss=-0.0270, Q1 Loss=1.0902, Q2 Loss=1.0902, Entropy=0.1231, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7605
SAC Update 2/5: Actor Loss=-0.0316, Q1 Loss=0.9364, Q2 Loss=0.9364, Entropy=0.0878, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6387
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6524, Q2 Loss=1.6524, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2900
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.9948, Q2 Loss=0.9948, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6682
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.6699, Q2 Loss=0.6699, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2657

------ SAC Update Summary (5 iterations) ------
Total time: 0.31s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (16.8%)
Q1 update: 0.06s (19.1%)
Q2 update: 0.06s (20.5%)
Actor update: 0.12s (39.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.057773
Q1 loss: 1.068714
Q2 loss: 1.068714
Current threshold: -35.4712
Global Scale Offset: 0.0441
Reward stats: mean=-0.0020, std=0.1028, count=410
----------------------------------------------
SAC Update - Actor Loss: -0.0578, Q1 Loss: 1.0687, Q2 Loss: 1.0687, Entropy: 0.0422, Mean TD Error: 1.3246, Threshold: -35.4712
tensor([ 0.1271,  0.5646,  0.5369,  0.7466, -0.1195,  0.7182,  0.7558,  0.9404,
         1.3832,  0.5037,  0.0947,  0.9873,  0.0134, -0.0286, -0.2714,  2.4053],
       device='cuda:0')
Original likelihood: -29.212543487548828
Adjusted likelihood: -29.212543487548828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.903435184969567
Current ori: tensor([ 0.0134, -0.0286, -0.2714], device='cuda:0')
Middle force: tensor([0.5716, 0.5933, 1.0014, 0.7539, 0.7572, 0.5687, 0.5818, 0.5706],
       device='cuda:0')
Thumb force: tensor([0.9875, 0.7430, 1.3329, 0.5737, 0.6776, 0.6366, 0.5196, 0.5230],
       device='cuda:0')
Index force: tensor([0.5368, 0.5408, 0.5914, 0.5189, 0.5517, 0.5556, 0.5058, 0.5313],
       device='cuda:0')
Storing NORMAL transition: reward=0.0617 (scaled=0.0617), steps=1
Reward stats updated: mean -0.0020 -> -0.0018, std: 0.1027
Collected 411 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0748, Q2 Loss=1.0748, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5794
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0773, Q2 Loss=1.0773, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3994
SAC Update 3/5: Actor Loss=-0.0001, Q1 Loss=1.0727, Q2 Loss=1.0727, Entropy=0.0256, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4809
SAC Update 4/5: Actor Loss=-0.0374, Q1 Loss=1.7202, Q2 Loss=1.7202, Entropy=0.0002, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4245
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7561, Q2 Loss=0.7561, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6050

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (14.2%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (19.6%)
Actor update: 0.12s (43.0%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.007485
Q1 loss: 1.140234
Q2 loss: 1.140234
Current threshold: -35.5109
Global Scale Offset: 0.0436
Reward stats: mean=-0.0018, std=0.1027, count=411
----------------------------------------------
SAC Update - Actor Loss: -0.0075, Q1 Loss: 1.1402, Q2 Loss: 1.1402, Entropy: 0.0052, Mean TD Error: 0.8978, Threshold: -35.5109
tensor([ 0.0852,  0.5381,  0.4915,  0.7542, -0.0422,  0.6677,  0.7431,  0.9126,
         1.4161,  0.4516,  0.1366,  0.8831,  0.0296, -0.0332, -0.3342,  0.5059],
       device='cuda:0')
Original likelihood: -26.70816421508789
Adjusted likelihood: -26.70816421508789
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.457281936018262
Current ori: tensor([ 0.0296, -0.0332, -0.3342], device='cuda:0')
Middle force: tensor([0.5992, 0.9803, 0.7589, 0.7599, 0.5710, 0.5881, 0.5805],
       device='cuda:0')
Thumb force: tensor([0.7199, 1.3118, 0.5643, 0.6585, 0.6250, 0.5171, 0.5180],
       device='cuda:0')
Index force: tensor([0.5360, 0.5870, 0.5174, 0.5491, 0.5516, 0.5048, 0.5283],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0145 (scaled=-0.0145), steps=1
Reward stats updated: mean -0.0018 -> -0.0019, std: 0.1026
Collected 412 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9423, Q2 Loss=0.9423, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7142
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4839, Q2 Loss=1.4839, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7103
SAC Update 3/5: Actor Loss=-0.2177, Q1 Loss=1.3221, Q2 Loss=1.3221, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7327
SAC Update 4/5: Actor Loss=-0.0853, Q1 Loss=1.0302, Q2 Loss=1.0302, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1434
SAC Update 5/5: Actor Loss=-0.2207, Q1 Loss=1.9420, Q2 Loss=1.9420, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6625

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.5%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (19.2%)
Actor update: 0.09s (38.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.104750
Q1 loss: 1.344106
Q2 loss: 1.344106
Current threshold: -35.5411
Global Scale Offset: 0.0432
Reward stats: mean=-0.0019, std=0.1026, count=412
----------------------------------------------
SAC Update - Actor Loss: -0.1048, Q1 Loss: 1.3441, Q2 Loss: 1.3441, Entropy: 0.0000, Mean TD Error: 1.1926, Threshold: -35.5411
tensor([ 0.0293,  0.5527,  0.4914,  0.6684, -0.0298,  0.6622,  0.6586,  0.9240,
         1.4136,  0.6381,  0.1355,  0.8861,  0.0282, -0.0134, -0.3186,  0.4656],
       device='cuda:0')
Original likelihood: -27.303428649902344
Adjusted likelihood: -27.303428649902344
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.383377099991776
Current ori: tensor([ 0.0282, -0.0134, -0.3186], device='cuda:0')
Middle force: tensor([1.1286, 0.5872, 0.5901, 0.5567, 0.6083, 0.5498], device='cuda:0')
Thumb force: tensor([0.7160, 0.9621, 0.5743, 0.8711, 0.5515, 0.5371], device='cuda:0')
Index force: tensor([1.1685, 0.5849, 0.5615, 0.5407, 0.5950, 0.5697], device='cuda:0')
Storing NORMAL transition: reward=0.0353 (scaled=0.0353), steps=1
Reward stats updated: mean -0.0019 -> -0.0018, std: 0.1025
Collected 413 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9734, Q2 Loss=0.9734, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4789
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5707, Q2 Loss=1.5707, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4848
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4657, Q2 Loss=1.4657, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3174
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.4061, Q2 Loss=1.4061, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3882
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.8290, Q2 Loss=0.8290, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4959

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.6%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (41.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 1.248981
Q2 loss: 1.248981
Current threshold: -35.5590
Global Scale Offset: 0.0430
Reward stats: mean=-0.0018, std=0.1025, count=413
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.2490, Q2 Loss: 1.2490, Entropy: 0.0000, Mean TD Error: 1.2330, Threshold: -35.5590
tensor([ 0.0287,  0.5524,  0.4829,  0.6827, -0.0325,  0.6104,  0.7458,  0.8969,
         1.4647,  0.5669,  0.1113,  0.8569,  0.0368, -0.0134, -0.3546,  0.6005],
       device='cuda:0')
Original likelihood: -27.181400299072266
Adjusted likelihood: -27.181400299072266
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 3.832116000005044
Current ori: tensor([ 0.0368, -0.0134, -0.3546], device='cuda:0')
Middle force: tensor([0.5829, 0.5855, 0.5531, 0.6066, 0.5480], device='cuda:0')
Thumb force: tensor([0.9475, 0.5735, 0.8683, 0.5490, 0.5334], device='cuda:0')
Index force: tensor([0.5776, 0.5583, 0.5388, 0.5900, 0.5673], device='cuda:0')
Storing NORMAL transition: reward=-0.0098 (scaled=-0.0098), steps=1
Reward stats updated: mean -0.0018 -> -0.0018, std: 0.1023
Collected 414 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8253, Q2 Loss=0.8253, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7160
SAC Update 2/5: Actor Loss=-0.0002, Q1 Loss=1.1001, Q2 Loss=1.1001, Entropy=0.0533, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7831
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0682, Q2 Loss=1.0682, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8405
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5527, Q2 Loss=1.5527, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1251
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.6732, Q2 Loss=0.6732, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1932

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.4%)
Q1 update: 0.05s (20.4%)
Q2 update: 0.05s (18.8%)
Actor update: 0.09s (38.3%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000045
Q1 loss: 1.043924
Q2 loss: 1.043924
Current threshold: -35.5700
Global Scale Offset: 0.0429
Reward stats: mean=-0.0018, std=0.1023, count=414
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.0439, Q2 Loss: 1.0439, Entropy: 0.0107, Mean TD Error: 0.9316, Threshold: -35.5700
tensor([ 0.0142,  0.5404,  0.4804,  0.6935, -0.0429,  0.5792,  0.7837,  0.8937,
         1.5000,  0.5099,  0.1092,  0.8289,  0.0463, -0.0062, -0.3454,  0.6917],
       device='cuda:0')
Original likelihood: -25.70913314819336
Adjusted likelihood: -25.70913314819336
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 4.112411317997612
Current ori: tensor([ 0.0463, -0.0062, -0.3454], device='cuda:0')
Middle force: tensor([0.5814, 0.5504, 0.6017, 0.5429], device='cuda:0')
Thumb force: tensor([0.5713, 0.8717, 0.5479, 0.5293], device='cuda:0')
Index force: tensor([0.5532, 0.5359, 0.5847, 0.5682], device='cuda:0')
Storing NORMAL transition: reward=-0.0004 (scaled=-0.0004), steps=1
Reward stats updated: mean -0.0018 -> -0.0018, std: 0.1022
Collected 415 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7752, Q2 Loss=0.7752, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4392
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7927, Q2 Loss=0.7927, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5595
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2516, Q2 Loss=1.2516, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9256
SAC Update 4/5: Actor Loss=-0.1180, Q1 Loss=1.0111, Q2 Loss=1.0111, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7261
SAC Update 5/5: Actor Loss=-0.0970, Q1 Loss=4.7590, Q2 Loss=4.7590, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.8511

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (18.7%)
Actor update: 0.08s (39.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.042996
Q1 loss: 1.717905
Q2 loss: 1.717905
Current threshold: -35.5837
Global Scale Offset: 0.0426
Reward stats: mean=-0.0018, std=0.1022, count=415
----------------------------------------------
SAC Update - Actor Loss: -0.0430, Q1 Loss: 1.7179, Q2 Loss: 1.7179, Entropy: 0.0000, Mean TD Error: 1.3003, Threshold: -35.5837
tensor([ 0.0423,  0.5665,  0.4486,  0.7323, -0.1921,  0.5710,  0.8758,  0.8190,
         1.5000,  0.3580, -0.0366,  0.7452,  0.0468, -0.0193, -0.3454,  0.7320],
       device='cuda:0')
Original likelihood: -40.83061218261719
Adjusted likelihood: -40.83061218261719
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 39.55139923095703
Projection step: 1, Loss: 40.132957458496094
Projection step: 2, Loss: 39.205406188964844
Projection step: 3, Loss: 38.57312774658203
Projection step: 4, Loss: 38.4039306640625
Projection step: 5, Loss: 36.01892852783203
Projection step: 6, Loss: 36.36353302001953
Projection step: 7, Loss: 35.05805206298828
Projection step: 8, Loss: 34.96062469482422
Projection step: 9, Loss: 33.56902313232422
Projection step: 10, Loss: 33.39894104003906
Projection step: 11, Loss: 31.565608978271484
Projection step: 12, Loss: 30.996631622314453
Projection step: 13, Loss: 31.8835391998291
Projection step: 14, Loss: 31.237234115600586
Projection step: 15, Loss: 28.301259994506836
Projection step: 16, Loss: 29.4814453125
Projection step: 17, Loss: 28.99569320678711
Projection step: 18, Loss: 28.5137882232666
Projection step: 19, Loss: 27.8216552734375
Projection step: 20, Loss: 26.777751922607422
Projection step: 21, Loss: 27.122636795043945
Projection step: 22, Loss: 26.82394790649414
Projection step: 23, Loss: 26.143430709838867
Projection step: 24, Loss: 26.21938705444336
Final likelihood: tensor([-23.3279, -26.6111, -28.9292, -24.3959, -21.2023, -22.9829, -20.2248,
        -25.8972, -24.4909, -28.7831, -28.1561, -25.1356, -29.6020, -30.1586,
        -22.3473, -23.4883])
Final projection likelihood: -25.3583
1 mode projection succeeded
New goal: tensor([ 0.0524,  0.5394,  0.5410,  0.7225, -0.1086,  0.5634,  0.8241,  0.8349,
         1.5084,  0.3128,  0.0725,  0.9331,  0.0439, -0.0144, -1.2365],
       device='cuda:0')
tensor([[0.0168]], device='cuda:0') tensor([[0.0131]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -30.335132598876953
Adjusted likelihood: -30.335132598876953
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 30.335132598876953}
Current yaw: tensor([ 0.0468, -0.0193, -0.3454], device='cuda:0')
6 thumb_middle
tensor([ 0.0423,  0.5665,  0.4486,  0.7323, -0.1921,  0.5710,  0.8758,  0.8190,
         1.5000,  0.3580, -0.0366,  0.7452,  0.0468, -0.0193, -0.3454,  0.7320],
       device='cuda:0')
Solve time for step 1 9.152742257982027
Current ori: tensor([ 0.0468, -0.0193, -0.3454], device='cuda:0')
Index force: tensor([0.5257, 0.5003, 0.5674, 0.5787], device='cuda:0')
tensor([ 0.0405,  0.5524,  0.4750,  0.7153, -0.1660,  0.5756,  0.8198,  0.8280,
         1.4724,  0.3079, -0.0201,  0.8762,  0.0488, -0.0190, -0.3454,  0.7494],
       device='cuda:0')
Solve time for step 2 3.489546632976271
Current ori: tensor([ 0.0488, -0.0190, -0.3454], device='cuda:0')
Index force: tensor([0.5002, 0.5594, 0.5722], device='cuda:0')
tensor([ 0.0379,  0.5338,  0.5076,  0.6980, -0.1647,  0.5907,  0.8107,  0.8198,
         1.4643,  0.3049, -0.0124,  0.8866,  0.0515, -0.0172, -0.3454,  0.7349],
       device='cuda:0')
Solve time for step 3 3.5217162590124644
Current ori: tensor([ 0.0515, -0.0172, -0.3454], device='cuda:0')
Index force: tensor([0.5512, 0.5657], device='cuda:0')
tensor([ 0.0368,  0.5061,  0.5257,  0.7381, -0.1730,  0.5985,  0.8185,  0.8313,
         1.4650,  0.3040, -0.0078,  0.8908,  0.0681, -0.0092, -0.3454,  0.8287],
       device='cuda:0')
Solve time for step 4 3.4049422319512814
Current ori: tensor([ 0.0681, -0.0092, -0.3454], device='cuda:0')
Index force: tensor([0.5519], device='cuda:0')
Storing RECOVERY transition: reward=0.0380 (scaled=0.0042), steps=9
Reward stats updated: mean -0.0018 -> -0.0018, std: 0.1021
Collected 416 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.9159, Q2 Loss=0.9159, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0653
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6985, Q2 Loss=0.6985, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3902
SAC Update 3/5: Actor Loss=-0.2808, Q1 Loss=1.2729, Q2 Loss=1.2729, Entropy=0.0114, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7090
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6419, Q2 Loss=1.6419, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0784
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5432, Q2 Loss=1.5432, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1463

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (18.2%)
Actor update: 0.09s (40.1%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.102210
Q1 loss: 1.214480
Q2 loss: 1.214480
Current threshold: -35.6283
Global Scale Offset: 0.0419
Reward stats: mean=-0.0018, std=0.1021, count=416
----------------------------------------------
SAC Update - Actor Loss: -0.1022, Q1 Loss: 1.2145, Q2 Loss: 1.2145, Entropy: 0.0023, Mean TD Error: 1.0778, Threshold: -35.6283
Original likelihood: -31.47968292236328
Adjusted likelihood: -31.47968292236328
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0587, -0.0132, -0.3848], device='cuda:0')
7 turn
Sampling time 3.602127655991353
tensor([ 0.0273,  0.5146,  0.5286,  0.6900, -0.1088,  0.6453,  0.8608,  0.8474,
         1.5000,  0.3219,  0.0690,  0.9447,  0.0587, -0.0132, -0.3848,  0.8075],
       device='cuda:0')
Original likelihood: -32.083091735839844
Adjusted likelihood: -32.083091735839844
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.150217012967914
Current ori: tensor([ 0.0587, -0.0132, -0.3848], device='cuda:0')
Middle force: tensor([0.8583, 2.2885, 0.5464, 1.3871, 0.6822, 1.0334, 0.5620, 0.8642, 0.5027,
        0.6039, 0.5898, 0.6777], device='cuda:0')
Thumb force: tensor([0.7114, 1.0317, 1.5850, 0.5051, 0.5419, 0.6198, 0.5135, 0.5725, 1.1397,
        0.5949, 0.5676, 0.5815], device='cuda:0')
Index force: tensor([0.5939, 1.5974, 0.6387, 0.7107, 0.6387, 0.6771, 0.5853, 0.7457, 0.5073,
        0.6187, 0.6304, 0.5341], device='cuda:0')
Storing NORMAL transition: reward=-0.0273 (scaled=-0.0273), steps=1
Reward stats updated: mean -0.0018 -> -0.0018, std: 0.1020
Collected 417 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9140, Q2 Loss=0.9140, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7124
SAC Update 2/5: Actor Loss=-0.0166, Q1 Loss=0.8552, Q2 Loss=0.8552, Entropy=0.0099, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7725
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8772, Q2 Loss=0.8772, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8669
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3101, Q2 Loss=1.3101, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8100
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7988, Q2 Loss=0.7988, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8890

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (16.1%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (41.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.049374
Q1 loss: 0.951049
Q2 loss: 0.951049
Current threshold: -35.6678
Global Scale Offset: 0.0412
Reward stats: mean=-0.0018, std=0.1020, count=417
----------------------------------------------
SAC Update - Actor Loss: -0.0494, Q1 Loss: 0.9510, Q2 Loss: 0.9510, Entropy: 0.0020, Mean TD Error: 0.8102, Threshold: -35.6678
tensor([ 0.0039,  0.5188,  0.5143,  0.6610, -0.1104,  0.6702,  0.8102,  0.8734,
         1.5000,  0.2933,  0.0710,  0.9445,  0.0572, -0.0098, -0.3571,  0.7273],
       device='cuda:0')
Original likelihood: -32.98925018310547
Adjusted likelihood: -32.98925018310547
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.587162312993314
Current ori: tensor([ 0.0572, -0.0098, -0.3571], device='cuda:0')
Middle force: tensor([0.5293, 0.6928, 0.7365, 2.0572, 0.6006, 0.5915, 0.6535, 0.5708, 0.6514,
        0.5624, 0.5482], device='cuda:0')
Thumb force: tensor([1.6843, 1.2734, 1.6889, 0.7800, 0.5608, 0.5399, 0.7169, 0.5491, 0.5405,
        0.8676, 0.5601], device='cuda:0')
Index force: tensor([0.8312, 0.6007, 0.7585, 0.7142, 0.5492, 0.5623, 0.5283, 0.5219, 0.5550,
        0.5222, 0.5212], device='cuda:0')
Storing NORMAL transition: reward=-0.0134 (scaled=-0.0134), steps=1
Reward stats updated: mean -0.0018 -> -0.0019, std: 0.1019
Collected 418 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3268, Q2 Loss=1.3268, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3161
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0246, Q2 Loss=1.0246, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7343
SAC Update 3/5: Actor Loss=-0.2907, Q1 Loss=1.7768, Q2 Loss=1.7768, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4637
SAC Update 4/5: Actor Loss=-0.0932, Q1 Loss=1.0724, Q2 Loss=1.0724, Entropy=0.1422, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1767
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.2567, Q2 Loss=2.2567, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6777

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.6%)
Q1 update: 0.05s (18.7%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (41.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.076774
Q1 loss: 1.491469
Q2 loss: 1.491469
Current threshold: -35.7038
Global Scale Offset: 0.0406
Reward stats: mean=-0.0019, std=0.1019, count=418
----------------------------------------------
SAC Update - Actor Loss: -0.0768, Q1 Loss: 1.4915, Q2 Loss: 1.4915, Entropy: 0.0284, Mean TD Error: 1.6737, Threshold: -35.7038
tensor([ 0.0317,  0.5239,  0.5437,  0.6445, -0.0931,  0.6933,  0.8125,  0.9484,
         1.5000,  0.2300,  0.1264,  0.8399,  0.0887, -0.0306, -0.3485,  1.0595],
       device='cuda:0')
Original likelihood: -38.32356262207031
Adjusted likelihood: -38.32356262207031
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.74094772338867
Projection step: 1, Loss: 37.8046989440918
Projection step: 2, Loss: 36.794921875
Projection step: 3, Loss: 36.64082336425781
Projection step: 4, Loss: 35.326881408691406
Projection step: 5, Loss: 35.677520751953125
Projection step: 6, Loss: 34.27878189086914
Projection step: 7, Loss: 33.94999694824219
Projection step: 8, Loss: 33.81930923461914
Projection step: 9, Loss: 32.66417694091797
Projection step: 10, Loss: 31.88266372680664
Projection step: 11, Loss: 32.22572326660156
Projection step: 12, Loss: 31.66656494140625
Projection step: 13, Loss: 30.367202758789062
Projection step: 14, Loss: 32.4388542175293
Projection step: 15, Loss: 31.283998489379883
Projection step: 16, Loss: 30.22555160522461
Projection step: 17, Loss: 30.436094284057617
Projection step: 18, Loss: 29.463665008544922
Projection step: 19, Loss: 30.210678100585938
Projection step: 20, Loss: 28.82596778869629
Projection step: 21, Loss: 29.365367889404297
Projection step: 22, Loss: 28.741058349609375
Projection step: 23, Loss: 28.050920486450195
Projection step: 24, Loss: 28.19265365600586
Final likelihood: tensor([-27.7219, -26.6688, -29.4073, -27.2266, -25.9492, -32.3133, -32.3092,
        -31.1481, -30.1623, -27.6442, -27.2322, -31.6680, -32.4492, -27.2128,
        -26.9806, -25.7868])
Final projection likelihood: -28.8675
1 mode projection succeeded
New goal: tensor([ 0.0495,  0.5186,  0.5974,  0.6777, -0.0689,  0.5826,  0.7262,  0.9080,
         1.4647,  0.1796,  0.1678,  0.9965,  0.0843, -0.0287, -1.0874],
       device='cuda:0')
tensor([[0.0059]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0024]], device='cuda:0')
Original likelihood: -35.97178649902344
Adjusted likelihood: -35.97178649902344
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 35.97178649902344}
Current yaw: tensor([ 0.0887, -0.0306, -0.3485], device='cuda:0')
8 thumb_middle
tensor([ 0.0317,  0.5239,  0.5437,  0.6445, -0.0931,  0.6933,  0.8125,  0.9484,
         1.5000,  0.2300,  0.1264,  0.8399,  0.0887, -0.0306, -0.3485,  1.0595],
       device='cuda:0')
Solve time for step 1 8.964248156989925
Current ori: tensor([ 0.0887, -0.0306, -0.3485], device='cuda:0')
Index force: tensor([0.5845, 0.5881, 0.6056, 0.6111], device='cuda:0')
tensor([ 3.3566e-05,  5.2253e-01,  5.9149e-01,  6.7365e-01, -1.2027e-01,
         6.1658e-01,  7.2842e-01,  9.0315e-01,  1.4253e+00,  1.7157e-01,
         8.3779e-02,  9.3350e-01,  1.8467e-01, -2.8663e-02, -3.4845e-01,
         1.8454e+00], device='cuda:0')
Solve time for step 2 3.582105301960837
Current ori: tensor([ 0.1847, -0.0287, -0.3484], device='cuda:0')
Index force: tensor([0.5839, 0.6016, 0.6076], device='cuda:0')
tensor([-0.0158,  0.5400,  0.5914,  0.6659, -0.0941,  0.6299,  0.7421,  0.9087,
         1.4540,  0.1719,  0.1057,  0.9683,  0.2368, -0.0425, -0.3486,  2.2870],
       device='cuda:0')
Solve time for step 3 3.449299753003288
Current ori: tensor([ 0.2368, -0.0425, -0.3486], device='cuda:0')
Index force: tensor([0.5562, 0.5001], device='cuda:0')
tensor([-0.0206,  0.5534,  0.6339,  0.6798, -0.0396,  0.6492,  0.7946,  0.9388,
         1.4691,  0.1827,  0.1385,  0.9961,  0.2358, -0.0555, -0.3355,  2.5987],
       device='cuda:0')
Solve time for step 4 3.4628774180309847
Current ori: tensor([ 0.2358, -0.0555, -0.3355], device='cuda:0')
Index force: tensor([0.5001], device='cuda:0')
Storing RECOVERY transition: reward=-0.0888 (scaled=-0.0444), steps=2
Reward stats updated: mean -0.0019 -> -0.0020, std: 0.1018
Collected 419 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6919, Q2 Loss=0.6919, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3194
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8530, Q2 Loss=0.8530, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6678
SAC Update 3/5: Actor Loss=-0.0505, Q1 Loss=1.0349, Q2 Loss=1.0349, Entropy=0.0012, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5222
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9568, Q2 Loss=0.9568, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7055
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2094, Q2 Loss=1.2094, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9498

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.1%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (42.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.010106
Q1 loss: 0.949209
Q2 loss: 0.949209
Current threshold: -35.7437
Global Scale Offset: 0.0400
Reward stats: mean=-0.0020, std=0.1018, count=419
----------------------------------------------
SAC Update - Actor Loss: -0.0101, Q1 Loss: 0.9492, Q2 Loss: 0.9492, Entropy: 0.0002, Mean TD Error: 0.6329, Threshold: -35.7437
Original likelihood: -48.781036376953125
Adjusted likelihood: -48.781036376953125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 48.59663009643555
Projection step: 1, Loss: 47.23592758178711
Projection step: 2, Loss: 47.74725341796875
Projection step: 3, Loss: 46.47311019897461
Projection step: 4, Loss: 44.547035217285156
Projection step: 5, Loss: 44.29362487792969
Projection step: 6, Loss: 44.70928192138672
Projection step: 7, Loss: 44.76593017578125
Projection step: 8, Loss: 45.35166549682617
Projection step: 9, Loss: 47.60166549682617
Projection step: 10, Loss: 44.85434341430664
Projection step: 11, Loss: 45.84392547607422
Projection step: 12, Loss: 45.526145935058594
Projection step: 13, Loss: 44.07792663574219
Projection step: 14, Loss: 44.65629577636719
Projection step: 15, Loss: 45.25541687011719
Projection step: 16, Loss: 44.880889892578125
Projection step: 17, Loss: 43.43470001220703
Projection step: 18, Loss: 45.054500579833984
Projection step: 19, Loss: 45.220279693603516
Projection step: 20, Loss: 45.510589599609375
Projection step: 21, Loss: 46.60628890991211
Projection step: 22, Loss: 44.73094940185547
Projection step: 23, Loss: 43.081512451171875
Projection step: 24, Loss: 43.889320373535156
Final likelihood: tensor([-34.7303, -50.9238, -39.9117, -47.0279, -47.5759, -51.3847, -46.8068,
        -45.1975, -40.5550, -43.6768, -36.1061, -45.6446, -41.6592, -40.4716,
        -48.5751, -46.2096])
Final projection likelihood: -44.1535
1 mode projection failed, trying anyway
New goal: tensor([ 0.0652,  0.5892,  0.6631,  0.6809,  0.1460,  0.7647,  0.8261,  0.9236,
         1.4511,  0.1541,  0.2432,  1.0773,  0.2283, -0.0818, -0.1117],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0009]], device='cuda:0')
Original likelihood: -47.30293273925781
Adjusted likelihood: -47.30293273925781
Likelihood residual: 0.0
Original likelihood: -50.16623306274414
Adjusted likelihood: -50.16623306274414
Likelihood residual: 0.0
{'index': 50.16623306274414, 'thumb_middle': 47.30293273925781}
Current yaw: tensor([ 0.2323, -0.0837, -0.3001], device='cuda:0')
9 thumb_middle
tensor([ 0.0553,  0.6077,  0.6166,  0.6407,  0.1701,  0.8047,  0.8521,  0.9432,
         1.5000,  0.1939,  0.2182,  1.0416,  0.2323, -0.0837, -0.3001,  2.6808],
       device='cuda:0')
Solve time for step 1 8.99479046498891
Current ori: tensor([ 0.2323, -0.0837, -0.3001], device='cuda:0')
Index force: tensor([0.5853, 0.5989, 0.5915, 0.5930], device='cuda:0')
tensor([ 0.0400,  0.6501,  0.6656,  0.6724,  0.0761,  0.7544,  0.7765,  0.8939,
         1.4131,  0.1431,  0.1533,  1.0283,  0.2348, -0.1184, -0.2336,  2.9602],
       device='cuda:0')
Solve time for step 2 3.527849057049025
Current ori: tensor([ 0.2348, -0.1184, -0.2336], device='cuda:0')
Index force: tensor([0.5911, 0.5828, 0.5850], device='cuda:0')
tensor([ 1.6427e-03,  6.9778e-01,  6.7098e-01,  6.7989e-01,  8.8567e-02,
         7.7135e-01,  7.9560e-01,  8.9765e-01,  1.4025e+00,  1.3887e-01,
         1.3132e-01,  1.0259e+00,  3.5499e-01, -2.7143e-01, -1.6919e-01,
         4.6704e+00], device='cuda:0')
Solve time for step 3 3.3931236409698613
Current ori: tensor([ 0.3550, -0.2714, -0.1692], device='cuda:0')
Index force: tensor([0.5155, 0.5216], device='cuda:0')
tensor([-0.0114,  0.7600,  0.6376,  0.6849,  0.1835,  0.8680,  0.9013,  0.9527,
         1.3148,  0.2017,  0.1378,  1.0884,  0.3669, -0.3140,  0.0491,  4.5367],
       device='cuda:0')
Solve time for step 4 3.324299061030615
Current ori: tensor([ 0.3669, -0.3140,  0.0491], device='cuda:0')
Index force: tensor([0.5450], device='cuda:0')
Storing RECOVERY transition: reward=-0.4569 (scaled=-0.2285), steps=2
Reward stats updated: mean -0.0020 -> -0.0025, std: 0.1022
Collected 420 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1714, Q2 Loss=1.1714, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3287
SAC Update 2/5: Actor Loss=-0.0946, Q1 Loss=2.7463, Q2 Loss=2.7463, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1294
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6196, Q2 Loss=1.6196, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8776
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6298, Q2 Loss=0.6298, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0633
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.9507, Q2 Loss=0.9507, Entropy=0.0003, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5862

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.0%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (19.0%)
Actor update: 0.10s (39.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.018929
Q1 loss: 1.423560
Q2 loss: 1.423560
Current threshold: -35.7929
Global Scale Offset: 0.0390
Reward stats: mean=-0.0025, std=0.1022, count=420
----------------------------------------------
SAC Update - Actor Loss: -0.0189, Q1 Loss: 1.4236, Q2 Loss: 1.4236, Entropy: 0.0001, Mean TD Error: 1.3970, Threshold: -35.7929
Original likelihood: -235.6896209716797
Adjusted likelihood: -235.6896209716797
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 240.75546264648438
Projection step: 1, Loss: 243.76608276367188
Projection step: 2, Loss: 257.39501953125
Projection step: 3, Loss: 255.97698974609375
Projection step: 4, Loss: 246.1018829345703
Projection step: 5, Loss: 232.65272521972656
Projection step: 6, Loss: 248.9204864501953
Projection step: 7, Loss: 241.50709533691406
Projection step: 8, Loss: 217.74429321289062
Projection step: 9, Loss: 242.25070190429688
Projection step: 10, Loss: 247.27694702148438
Projection step: 11, Loss: 239.0946044921875
Projection step: 12, Loss: 238.439697265625
Projection step: 13, Loss: 251.9920654296875
Projection step: 14, Loss: 251.56497192382812
Projection step: 15, Loss: 253.91131591796875
Projection step: 16, Loss: 233.04632568359375
Projection step: 17, Loss: 247.30874633789062
Projection step: 18, Loss: 241.13076782226562
Projection step: 19, Loss: 212.4922637939453
Projection step: 20, Loss: 240.74073791503906
Projection step: 21, Loss: 233.20533752441406
Projection step: 22, Loss: 231.68121337890625
Projection step: 23, Loss: 238.3611297607422
Projection step: 24, Loss: 247.02188110351562
Final likelihood: tensor([-273.5824, -190.5960, -323.8307, -308.2458, -265.8200, -262.5701,
        -287.3440, -219.3779, -189.1977, -151.8839, -294.5070, -218.0402,
        -226.7737, -263.0456, -301.9030, -247.7279])
Final projection likelihood: -251.5279
1 mode projection failed, trying anyway
New goal: tensor([ 0.0169,  0.8869,  0.6844,  0.7084,  0.1972,  0.8391,  0.9729,  1.0543,
         1.3205,  0.1781,  0.1784,  1.1948,  0.3617, -0.2995,  0.0126],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0208]], device='cuda:0') tensor([[0.0049]], device='cuda:0')
Original likelihood: -227.580322265625
Adjusted likelihood: -227.580322265625
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 227.580322265625}
Current yaw: tensor([ 0.3615, -0.2989,  0.0026], device='cuda:0')
10 thumb_middle
tensor([ 6.2099e-03,  8.8921e-01,  6.7860e-01,  7.0285e-01,  1.9932e-01,
         8.3993e-01,  9.7767e-01,  1.0422e+00,  1.3316e+00,  1.7122e-01,
         1.7690e-01,  1.1886e+00,  3.6155e-01, -2.9887e-01,  2.5889e-03,
         4.2100e+00], device='cuda:0')
Solve time for step 1 8.85332098294748
Current ori: tensor([ 0.3615, -0.2989,  0.0026], device='cuda:0')
Index force: tensor([0.5521, 0.5782, 0.5840, 0.5926], device='cuda:0')
tensor([-0.0155,  0.9398,  0.6842,  0.7052,  0.1749,  0.8712,  1.0088,  1.0589,
         1.2737,  0.2083,  0.0347,  1.2095,  0.3777, -0.3427,  0.1370,  4.1113],
       device='cuda:0')
Solve time for step 2 3.5816976540372707
Current ori: tensor([ 0.3777, -0.3427,  0.1370], device='cuda:0')
Index force: tensor([0.5741, 0.5789, 0.5723], device='cuda:0')
tensor([-0.0305,  0.9268,  0.6835,  0.7015,  0.1616,  0.8600,  1.0117,  1.0688,
         1.2621,  0.2098,  0.0305,  1.1977,  0.3794, -0.3467,  0.1582,  4.2559],
       device='cuda:0')
Solve time for step 3 3.432408422988374
Current ori: tensor([ 0.3794, -0.3467,  0.1582], device='cuda:0')
Index force: tensor([0.5814, 0.5911], device='cuda:0')
tensor([-0.0329,  0.9673,  0.6820,  0.6990,  0.1550,  0.8958,  1.0187,  1.0573,
         1.2486,  0.2093,  0.0580,  1.1980,  0.3800, -0.3480,  0.1665,  4.1662],
       device='cuda:0')
Solve time for step 4 3.3087962479912676
Current ori: tensor([ 0.3800, -0.3480,  0.1665], device='cuda:0')
Index force: tensor([0.5629], device='cuda:0')
Storing RECOVERY transition: reward=-0.5362 (scaled=-0.2681), steps=2
Reward stats updated: mean -0.0025 -> -0.0031, std: 0.1029
Collected 421 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0805, Q2 Loss=1.0805, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0811
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5215, Q2 Loss=1.5215, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2275
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4492, Q2 Loss=1.4492, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0119
SAC Update 4/5: Actor Loss=-0.1103, Q1 Loss=1.3680, Q2 Loss=1.3680, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5533
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.1823, Q2 Loss=1.1823, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0122

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.3%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.05s (19.8%)
Actor update: 0.10s (41.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.022053
Q1 loss: 1.320312
Q2 loss: 1.320312
Current threshold: -35.8340
Global Scale Offset: 0.0381
Reward stats: mean=-0.0031, std=0.1029, count=421
----------------------------------------------
SAC Update - Actor Loss: -0.0221, Q1 Loss: 1.3203, Q2 Loss: 1.3203, Entropy: 0.0000, Mean TD Error: 1.3772, Threshold: -35.8340
Original likelihood: -287.8575439453125
Adjusted likelihood: -287.8575439453125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 289.48272705078125
Projection step: 1, Loss: 295.6173400878906
Projection step: 2, Loss: 293.553466796875
Projection step: 3, Loss: 289.62774658203125
Projection step: 4, Loss: 282.27838134765625
Projection step: 5, Loss: 272.67657470703125
Projection step: 6, Loss: 289.62060546875
Projection step: 7, Loss: 276.36846923828125
Projection step: 8, Loss: 277.2778625488281
Projection step: 9, Loss: 273.5736999511719
Projection step: 10, Loss: 307.3172607421875
Projection step: 11, Loss: 286.4966735839844
Projection step: 12, Loss: 273.24444580078125
Projection step: 13, Loss: 277.86676025390625
Projection step: 14, Loss: 278.3233642578125
Projection step: 15, Loss: 282.1366882324219
Projection step: 16, Loss: 289.6170654296875
Projection step: 17, Loss: 276.4900207519531
Projection step: 18, Loss: 299.71759033203125
Projection step: 19, Loss: 274.0015563964844
Projection step: 20, Loss: 278.87200927734375
Projection step: 21, Loss: 271.58392333984375
Projection step: 22, Loss: 284.10455322265625
Projection step: 23, Loss: 290.0639953613281
Projection step: 24, Loss: 285.2076721191406
Final likelihood: tensor([-281.1534, -274.2506, -300.9247, -326.3192, -311.7260, -314.6089,
        -262.8313, -276.0729, -310.8445, -340.7428, -276.6590, -308.0522,
        -303.9946, -312.0587, -251.7920, -293.1234])
Final projection likelihood: -296.5721
1 mode projection failed, trying anyway
New goal: tensor([-0.0232,  0.9897,  0.7153,  0.7130,  0.1613,  0.9142,  1.0564,  1.1031,
         1.2883,  0.1566,  0.1076,  1.1848,  0.3700, -0.3228,  0.0958],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0121]], device='cuda:0') tensor([[0.0053]], device='cuda:0')
Original likelihood: -212.38629150390625
Adjusted likelihood: -212.38629150390625
Likelihood residual: 0.0
Original likelihood: -284.32611083984375
Adjusted likelihood: -284.32611083984375
Likelihood residual: 0.0
{'index': 284.32611083984375, 'thumb_middle': 212.38629150390625}
Current yaw: tensor([ 0.3700, -0.3223,  0.0805], device='cuda:0')
11 thumb_middle
tensor([-0.0287,  0.9903,  0.7099,  0.7103,  0.1614,  0.9129,  1.0608,  1.0975,
         1.2963,  0.1532,  0.1081,  1.1816,  0.3700, -0.3223,  0.0805,  4.1903],
       device='cuda:0')
Solve time for step 1 8.960199024993926
Current ori: tensor([ 0.3700, -0.3223,  0.0805], device='cuda:0')
Index force: tensor([0.5914, 0.5956, 0.5944, 0.5960], device='cuda:0')
tensor([-0.0665,  1.0096,  0.7044,  0.7067,  0.1178,  0.9198,  1.0681,  1.0952,
         1.2320,  0.1799, -0.0083,  1.1940,  0.3835, -0.3575,  0.1897,  4.1608],
       device='cuda:0')
Solve time for step 2 3.5606793359620497
Current ori: tensor([ 0.3835, -0.3575,  0.1897], device='cuda:0')
Index force: tensor([0.5917, 0.5888, 0.5905], device='cuda:0')
tensor([-0.0874,  1.0333,  0.6879,  0.7051,  0.0959,  0.9253,  1.0812,  1.1055,
         1.2306,  0.1863, -0.0201,  1.1996,  0.3844, -0.3595,  0.1805,  4.1421],
       device='cuda:0')
Solve time for step 3 3.3191812870209105
Current ori: tensor([ 0.3844, -0.3595,  0.1805], device='cuda:0')
Index force: tensor([0.5724, 0.5751], device='cuda:0')
tensor([-0.0918,  1.0364,  0.6882,  0.7066,  0.0916,  0.9249,  1.0855,  1.1134,
         1.2321,  0.1782,  0.0058,  1.1891,  0.3833, -0.3565,  0.1705,  4.1134],
       device='cuda:0')
Solve time for step 4 3.3517202240182087
Current ori: tensor([ 0.3833, -0.3565,  0.1705], device='cuda:0')
Index force: tensor([0.5784], device='cuda:0')
Storing RECOVERY transition: reward=-0.5001 (scaled=-0.2501), steps=2
Reward stats updated: mean -0.0031 -> -0.0037, std: 0.1035
Collected 422 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.5481, Q2 Loss=2.5481, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5307
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4962, Q2 Loss=1.4962, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6224
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8243, Q2 Loss=0.8243, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9684
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2412, Q2 Loss=1.2412, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7666
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.0745, Q2 Loss=1.0745, Entropy=0.0008, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7837

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (20.0%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (18.8%)
Actor update: 0.09s (37.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.436862
Q2 loss: 1.436862
Current threshold: -35.8680
Global Scale Offset: 0.0374
Reward stats: mean=-0.0037, std=0.1035, count=422
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.4369, Q2 Loss: 1.4369, Entropy: 0.0002, Mean TD Error: 1.3344, Threshold: -35.8680
Original likelihood: -284.65777587890625
Adjusted likelihood: -284.65777587890625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 296.73687744140625
Projection step: 1, Loss: 297.4755859375
Projection step: 2, Loss: 301.2491149902344
Projection step: 3, Loss: 292.30657958984375
Projection step: 4, Loss: 301.2035217285156
Projection step: 5, Loss: 283.1685791015625
Projection step: 6, Loss: 290.7257080078125
Projection step: 7, Loss: 297.272705078125
Projection step: 8, Loss: 311.02447509765625
Projection step: 9, Loss: 318.13720703125
Projection step: 10, Loss: 297.3308410644531
Projection step: 11, Loss: 302.97454833984375
Projection step: 12, Loss: 299.54266357421875
Projection step: 13, Loss: 294.8641357421875
Projection step: 14, Loss: 302.0016174316406
Projection step: 15, Loss: 308.38018798828125
Projection step: 16, Loss: 313.820556640625
Projection step: 17, Loss: 322.919189453125
Projection step: 18, Loss: 301.69915771484375
Projection step: 19, Loss: 309.11474609375
Projection step: 20, Loss: 302.8219909667969
Projection step: 21, Loss: 299.10650634765625
Projection step: 22, Loss: 317.5416564941406
Projection step: 23, Loss: 303.853515625
Projection step: 24, Loss: 312.3739929199219
Final likelihood: tensor([-301.6976, -247.2991, -264.4368, -271.5714, -258.4909, -340.6538,
        -298.4348, -315.7011, -393.5821, -300.8990, -298.0768, -368.3369,
        -319.8212, -310.8988, -324.0763, -278.6899])
Final projection likelihood: -305.7917
1 mode projection failed, trying anyway
New goal: tensor([-0.1078,  1.0687,  0.7115,  0.6966,  0.0716,  0.9485,  1.1254,  1.1312,
         1.2755,  0.1267,  0.0595,  1.1728,  0.3727, -0.3313,  0.0518],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0077]], device='cuda:0') tensor([[0.0054]], device='cuda:0')
Original likelihood: -238.99429321289062
Adjusted likelihood: -238.99429321289062
Likelihood residual: 0.0
Original likelihood: -254.35641479492188
Adjusted likelihood: -254.35641479492188
Likelihood residual: 0.0
{'index': 254.35641479492188, 'thumb_middle': 238.99429321289062}
Current yaw: tensor([ 0.3727, -0.3308,  0.0361], device='cuda:0')
12 thumb_middle
tensor([-0.1120,  1.0694,  0.7069,  0.6947,  0.0712,  0.9475,  1.1291,  1.1272,
         1.2826,  0.1239,  0.0604,  1.1703,  0.3727, -0.3308,  0.0361,  4.2925],
       device='cuda:0')
Solve time for step 1 9.216633067990188
Current ori: tensor([ 0.3727, -0.3308,  0.0361], device='cuda:0')
Index force: tensor([0.6030, 0.5866, 0.5974, 0.5926], device='cuda:0')
tensor([-0.1402,  1.0745,  0.6852,  0.6776,  0.0412,  0.9338,  1.1408,  1.1179,
         1.2445,  0.1642, -0.0553,  1.1876,  0.3815, -0.3548,  0.0949,  4.2557],
       device='cuda:0')
Solve time for step 2 3.689431895967573
Current ori: tensor([ 0.3815, -0.3548,  0.0949], device='cuda:0')
Index force: tensor([0.5837, 0.5924, 0.5878], device='cuda:0')
tensor([-0.1495,  1.0961,  0.6620,  0.6687,  0.0299,  0.9482,  1.1293,  1.1256,
         1.2390,  0.1797, -0.0843,  1.1788,  0.3830, -0.3595,  0.0752,  4.2117],
       device='cuda:0')
Solve time for step 3 3.5302414119942114
Current ori: tensor([ 0.3830, -0.3595,  0.0752], device='cuda:0')
Index force: tensor([0.5795, 0.5761], device='cuda:0')
tensor([-0.1647,  1.1051,  0.6664,  0.6800,  0.0145,  0.9443,  1.1568,  1.1342,
         1.2434,  0.1506, -0.0349,  1.1629,  0.3805, -0.3527,  0.0481,  4.2466],
       device='cuda:0')
Solve time for step 4 3.320566563983448
Current ori: tensor([ 0.3805, -0.3527,  0.0481], device='cuda:0')
Index force: tensor([0.5631], device='cuda:0')
Storing RECOVERY transition: reward=-0.3825 (scaled=-0.1912), steps=2
Reward stats updated: mean -0.0037 -> -0.0042, std: 0.1038
Collected 423 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3093, Q2 Loss=1.3093, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2327
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.8620, Q2 Loss=1.8620, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7471
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1165, Q2 Loss=1.1165, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6730
SAC Update 4/5: Actor Loss=-0.1942, Q1 Loss=1.2375, Q2 Loss=1.2375, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6797
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5530, Q2 Loss=1.5530, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8208

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.4%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (40.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.038833
Q1 loss: 1.415675
Q2 loss: 1.415675
Current threshold: -35.8898
Global Scale Offset: 0.0369
Reward stats: mean=-0.0042, std=0.1038, count=423
----------------------------------------------
SAC Update - Actor Loss: -0.0388, Q1 Loss: 1.4157, Q2 Loss: 1.4157, Entropy: 0.0000, Mean TD Error: 1.4307, Threshold: -35.8898
Original likelihood: -298.9073486328125
Adjusted likelihood: -298.9073486328125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 292.7859191894531
Projection step: 1, Loss: 303.23419189453125
Projection step: 2, Loss: 301.7265625
Projection step: 3, Loss: 303.8575439453125
Projection step: 4, Loss: 300.58941650390625
Projection step: 5, Loss: 285.4677429199219
Projection step: 6, Loss: 295.23236083984375
Projection step: 7, Loss: 307.86328125
Projection step: 8, Loss: 299.31378173828125
Projection step: 9, Loss: 311.376220703125
Projection step: 10, Loss: 317.7708740234375
Projection step: 11, Loss: 304.31658935546875
Projection step: 12, Loss: 304.849365234375
Projection step: 13, Loss: 311.65667724609375
Projection step: 14, Loss: 301.1208801269531
Projection step: 15, Loss: 294.378173828125
Projection step: 16, Loss: 305.2794494628906
Projection step: 17, Loss: 283.5794677734375
Projection step: 18, Loss: 302.01690673828125
Projection step: 19, Loss: 320.00811767578125
Projection step: 20, Loss: 330.52716064453125
Projection step: 21, Loss: 308.55035400390625
Projection step: 22, Loss: 289.97589111328125
Projection step: 23, Loss: 289.24920654296875
Projection step: 24, Loss: 300.1402282714844
Final likelihood: tensor([-324.0410, -281.8701, -297.6470, -298.5748, -273.7813, -280.5140,
        -304.2070, -281.3255, -247.4303, -285.3328, -320.8180, -293.0569,
        -311.2639, -327.0637, -295.0411, -327.6528])
Final projection likelihood: -296.8513
1 mode projection failed, trying anyway
New goal: tensor([-0.1673,  1.1461,  0.7024,  0.6885,  0.0056,  0.9858,  1.1867,  1.1597,
         1.2881,  0.1206,  0.0374,  1.1500,  0.3723, -0.3282, -0.0844],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0048]], device='cuda:0')
Original likelihood: -236.97967529296875
Adjusted likelihood: -236.97967529296875
Likelihood residual: 0.0
Original likelihood: -273.839111328125
Adjusted likelihood: -273.839111328125
Likelihood residual: 0.0
{'index': 273.839111328125, 'thumb_middle': 236.97967529296875}
Current yaw: tensor([ 0.3722, -0.3277, -0.0979], device='cuda:0')
13 thumb_middle
tensor([-0.1722,  1.1468,  0.6975,  0.6859,  0.0050,  0.9844,  1.1909,  1.1552,
         1.2956,  0.1178,  0.0381,  1.1470,  0.3722, -0.3277, -0.0979,  4.4486],
       device='cuda:0')
Solve time for step 1 8.888195927022025
Current ori: tensor([ 0.3722, -0.3277, -0.0979], device='cuda:0')
Index force: tensor([0.5828, 0.5901, 0.5785, 0.5863], device='cuda:0')
tensor([-0.1924,  1.1389,  0.6519,  0.6744, -0.0189,  0.9621,  1.1733,  1.1425,
         1.2560,  0.1439, -0.0893,  1.1617,  0.3812, -0.3539, -0.0693,  4.4125],
       device='cuda:0')
Solve time for step 2 3.7067125709727407
Current ori: tensor([ 0.3812, -0.3539, -0.0693], device='cuda:0')
Index force: tensor([0.5698, 0.5696, 0.5796], device='cuda:0')
tensor([-0.2090,  1.1448,  0.6460,  0.6648, -0.0363,  0.9561,  1.1882,  1.1423,
         1.2510,  0.1560, -0.0770,  1.1514,  0.3815, -0.3550, -0.0612,  4.3421],
       device='cuda:0')
Solve time for step 3 3.5159560089814477
Current ori: tensor([ 0.3815, -0.3550, -0.0612], device='cuda:0')
Index force: tensor([0.5813, 0.5744], device='cuda:0')
tensor([-0.2139,  1.1635,  0.6500,  0.6611, -0.0430,  0.9725,  1.1992,  1.1363,
         1.2502,  0.1371, -0.0308,  1.1685,  0.3800, -0.3504, -0.0862,  4.3941],
       device='cuda:0')
Solve time for step 4 3.42535166797461
Current ori: tensor([ 0.3800, -0.3504, -0.0862], device='cuda:0')
Index force: tensor([0.5554], device='cuda:0')
Storing RECOVERY transition: reward=-0.3175 (scaled=-0.1587), steps=2
Reward stats updated: mean -0.0042 -> -0.0045, std: 0.1039
Collected 424 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2185, Q2 Loss=1.2185, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0409
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.2734, Q2 Loss=2.2734, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.9754
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0364, Q2 Loss=1.0364, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9701
SAC Update 4/5: Actor Loss=-0.1564, Q1 Loss=1.1378, Q2 Loss=1.1378, Entropy=0.3376, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7718
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.8832, Q2 Loss=0.8832, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4640

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.6%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.05s (20.3%)
Actor update: 0.09s (39.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.169425
Q1 loss: 1.309850
Q2 loss: 1.309850
Current threshold: -35.9073
Global Scale Offset: 0.0366
Reward stats: mean=-0.0045, std=0.1039, count=424
----------------------------------------------
SAC Update - Actor Loss: -0.1694, Q1 Loss: 1.3098, Q2 Loss: 1.3098, Entropy: 0.0675, Mean TD Error: 1.8444, Threshold: -35.9073
Original likelihood: -306.05145263671875
Adjusted likelihood: -306.05145263671875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 27
Loaded trajectory sampler
Current yaw: tensor([-0.0012,  0.0145, -0.0305], device='cuda:0')
Current yaw: tensor([-0.0012,  0.0145, -0.0305], device='cuda:0')
1 turn
Sampling time 3.672706270008348
tensor([ 1.6019e-01,  5.6942e-01,  6.4369e-01,  5.8610e-01, -1.3492e-01,
         5.2673e-01,  9.5196e-01,  8.8319e-01,  1.2108e+00,  3.0576e-01,
         2.3208e-01,  1.2729e+00, -1.1562e-03,  1.4488e-02, -3.0475e-02,
         3.8849e-02], device='cuda:0')
Original likelihood: -24.187049865722656
Adjusted likelihood: -24.187049865722656
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.028508591989521
Current ori: tensor([-0.0012,  0.0145, -0.0305], device='cuda:0')
Middle force: tensor([0.5791, 0.5731, 1.1797, 0.5662, 1.1204, 0.6426, 0.5376, 0.5268, 0.5148,
        0.5427, 0.6041, 0.4881], device='cuda:0')
Thumb force: tensor([0.8986, 0.8832, 0.7612, 1.0846, 1.0884, 0.6330, 0.5326, 0.8895, 0.5359,
        0.5121, 0.5808, 0.6458], device='cuda:0')
Index force: tensor([0.5985, 0.6057, 0.5627, 0.5741, 0.8164, 0.5400, 1.0156, 0.9163, 0.6094,
        0.5662, 0.5910, 0.8261], device='cuda:0')
Storing NORMAL transition: reward=-0.0776 (scaled=-0.0776), steps=1
Reward stats updated: mean -0.0045 -> -0.0047, std: 0.1039
Collected 425 transitions for RL
SAC Update 1/5: Actor Loss=-0.0790, Q1 Loss=1.8867, Q2 Loss=1.8867, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7241
SAC Update 2/5: Actor Loss=-0.2196, Q1 Loss=1.4939, Q2 Loss=1.4939, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0648
SAC Update 3/5: Actor Loss=-0.0418, Q1 Loss=1.2335, Q2 Loss=1.2335, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4349
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7350, Q2 Loss=0.7350, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3211
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0470, Q2 Loss=1.0470, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5083

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.0%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (19.1%)
Actor update: 0.09s (40.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.068058
Q1 loss: 1.279223
Q2 loss: 1.279223
Current threshold: -35.9218
Global Scale Offset: 0.0364
Reward stats: mean=-0.0047, std=0.1039, count=425
----------------------------------------------
SAC Update - Actor Loss: -0.0681, Q1 Loss: 1.2792, Q2 Loss: 1.2792, Entropy: 0.0000, Mean TD Error: 1.6107, Threshold: -35.9218
tensor([ 0.0891,  0.5273,  0.5931,  0.6611, -0.1202,  0.5096,  0.9121,  0.9416,
         1.2132,  0.2811,  0.2425,  1.3185,  0.0147,  0.0108,  0.0471,  0.3783],
       device='cuda:0')
Original likelihood: -18.875812530517578
Adjusted likelihood: -18.875812530517578
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.5708073420100845
Current ori: tensor([0.0147, 0.0108, 0.0471], device='cuda:0')
Middle force: tensor([0.5725, 1.1643, 0.5672, 1.1469, 0.6377, 0.5449, 0.5673, 0.5146, 0.5495,
        0.6031, 0.5179], device='cuda:0')
Thumb force: tensor([0.8636, 0.7425, 1.0527, 1.0021, 0.6226, 0.5231, 0.7503, 0.5271, 0.5085,
        0.5706, 0.5808], device='cuda:0')
Index force: tensor([0.5971, 0.5593, 0.5702, 0.8252, 0.5377, 0.9983, 0.8889, 0.6157, 0.5675,
        0.5900, 0.8298], device='cuda:0')
Storing NORMAL transition: reward=-0.0620 (scaled=-0.0620), steps=1
Reward stats updated: mean -0.0047 -> -0.0048, std: 0.1038
Collected 426 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8187, Q2 Loss=0.8187, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3804
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6728, Q2 Loss=1.6728, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5402
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.2552, Q2 Loss=1.2552, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3767
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6866, Q2 Loss=0.6866, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1947
SAC Update 5/5: Actor Loss=-0.1697, Q1 Loss=1.2552, Q2 Loss=1.2552, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9180

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.8%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (19.0%)
Actor update: 0.10s (39.2%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.079993
Q1 loss: 1.137688
Q2 loss: 1.137688
Current threshold: -35.9306
Global Scale Offset: 0.0363
Reward stats: mean=-0.0048, std=0.1038, count=426
----------------------------------------------
SAC Update - Actor Loss: -0.0800, Q1 Loss: 1.1377, Q2 Loss: 1.1377, Entropy: 0.0000, Mean TD Error: 0.8820, Threshold: -35.9306
tensor([ 0.1354,  0.5967,  0.5410,  0.6689, -0.0552,  0.5702,  0.8169,  0.9278,
         1.2138,  0.2613,  0.1791,  1.3786,  0.0033, -0.0134,  0.1093,  0.3386],
       device='cuda:0')
Original likelihood: -22.21615219116211
Adjusted likelihood: -22.21615219116211
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.152582515962422
Current ori: tensor([ 0.0033, -0.0134,  0.1093], device='cuda:0')
Middle force: tensor([1.1398, 0.5639, 1.1313, 0.6291, 0.5427, 0.5656, 0.5131, 0.5476, 0.5978,
        0.5182], device='cuda:0')
Thumb force: tensor([0.7282, 1.0337, 0.9791, 0.6164, 0.5192, 0.7343, 0.5236, 0.5076, 0.5670,
        0.5692], device='cuda:0')
Index force: tensor([0.5552, 0.5654, 0.8169, 0.5359, 0.9872, 0.8785, 0.6169, 0.5648, 0.5860,
        0.8226], device='cuda:0')
Storing NORMAL transition: reward=0.0458 (scaled=0.0458), steps=1
Reward stats updated: mean -0.0048 -> -0.0047, std: 0.1037
Collected 427 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0078, Q2 Loss=1.0078, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0544
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7916, Q2 Loss=0.7916, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8644
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4225, Q2 Loss=1.4225, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3843
SAC Update 4/5: Actor Loss=-0.0884, Q1 Loss=2.0823, Q2 Loss=2.0823, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8237
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0019, Q2 Loss=1.0019, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4020

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.6%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (18.5%)
Actor update: 0.11s (41.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.017687
Q1 loss: 1.261195
Q2 loss: 1.261195
Current threshold: -35.9360
Global Scale Offset: 0.0362
Reward stats: mean=-0.0047, std=0.1037, count=427
----------------------------------------------
SAC Update - Actor Loss: -0.0177, Q1 Loss: 1.2612, Q2 Loss: 1.2612, Entropy: 0.0000, Mean TD Error: 1.3057, Threshold: -35.9360
tensor([ 9.8705e-02,  5.8288e-01,  5.3587e-01,  6.4354e-01, -8.0554e-02,
         5.5840e-01,  8.0319e-01,  9.4796e-01,  1.2253e+00,  2.6800e-01,
         2.1730e-01,  1.3115e+00, -8.3368e-04,  3.9108e-03,  6.3604e-02,
         2.8933e-01], device='cuda:0')
Original likelihood: -19.535476684570312
Adjusted likelihood: -19.535476684570312
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.976039493049029
Current ori: tensor([-0.0008,  0.0039,  0.0636], device='cuda:0')
Middle force: tensor([0.5617, 1.1241, 0.6234, 0.5407, 0.5602, 0.5122, 0.5470, 0.5927, 0.5172],
       device='cuda:0')
Thumb force: tensor([1.0088, 0.9537, 0.6101, 0.5161, 0.7253, 0.5207, 0.5067, 0.5635, 0.5635],
       device='cuda:0')
Index force: tensor([0.5600, 0.8057, 0.5333, 0.9723, 0.8689, 0.6134, 0.5611, 0.5827, 0.8147],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0288 (scaled=-0.0288), steps=1
Reward stats updated: mean -0.0047 -> -0.0048, std: 0.1036
Collected 428 transitions for RL
SAC Update 1/5: Actor Loss=-0.0690, Q1 Loss=0.8440, Q2 Loss=0.8440, Entropy=0.0026, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6322
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5924, Q2 Loss=1.5924, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6240
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6756, Q2 Loss=0.6756, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3848
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=2.0094, Q2 Loss=2.0094, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.9026
SAC Update 5/5: Actor Loss=-0.1182, Q1 Loss=1.1880, Q2 Loss=1.1880, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1900

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.2%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (19.5%)
Actor update: 0.12s (42.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.037431
Q1 loss: 1.261878
Q2 loss: 1.261878
Current threshold: -35.9394
Global Scale Offset: 0.0361
Reward stats: mean=-0.0048, std=0.1036, count=428
----------------------------------------------
SAC Update - Actor Loss: -0.0374, Q1 Loss: 1.2619, Q2 Loss: 1.2619, Entropy: 0.0005, Mean TD Error: 1.5467, Threshold: -35.9394
tensor([-0.0055,  0.5557,  0.4833,  0.6225, -0.0801,  0.6321,  0.6684,  0.9553,
         1.3152,  0.3769,  0.0618,  1.2904, -0.0183,  0.0116,  0.0920,  0.4912],
       device='cuda:0')
Original likelihood: -22.177690505981445
Adjusted likelihood: -22.177690505981445
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.5861233759787865
Current ori: tensor([-0.0183,  0.0116,  0.0920], device='cuda:0')
Middle force: tensor([0.5099, 0.5033, 0.8263, 0.5160, 0.5057, 0.5487, 0.5058, 0.5285],
       device='cuda:0')
Thumb force: tensor([0.5054, 0.6373, 0.9101, 0.5057, 0.7431, 0.5071, 0.5038, 0.5014],
       device='cuda:0')
Index force: tensor([0.5013, 0.7867, 0.6625, 0.5100, 0.5094, 0.6164, 0.5456, 0.5059],
       device='cuda:0')
Storing NORMAL transition: reward=0.0544 (scaled=0.0544), steps=1
Reward stats updated: mean -0.0048 -> -0.0046, std: 0.1035
Collected 429 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.9563, Q2 Loss=0.9563, Entropy=0.0001, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5083
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.7713, Q2 Loss=0.7713, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3691
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9883, Q2 Loss=0.9883, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2897
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8806, Q2 Loss=0.8806, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7639
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1070, Q2 Loss=1.1070, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1019

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.4%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.04s (18.9%)
Actor update: 0.09s (40.1%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 0.940714
Q2 loss: 0.940714
Current threshold: -35.9414
Global Scale Offset: 0.0361
Reward stats: mean=-0.0046, std=0.1035, count=429
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 0.9407, Q2 Loss: 0.9407, Entropy: 0.0000, Mean TD Error: 1.2066, Threshold: -35.9414
tensor([-0.0129,  0.5853,  0.3716,  0.7480, -0.0953,  0.5538,  0.7825,  0.9547,
         1.3758,  0.2865,  0.0277,  1.2970, -0.0092,  0.0175,  0.0377,  0.6237],
       device='cuda:0')
Original likelihood: -21.301605224609375
Adjusted likelihood: -21.301605224609375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.587062072998378
Current ori: tensor([-0.0092,  0.0175,  0.0377], device='cuda:0')
Middle force: tensor([1.3266, 0.8459, 0.5732, 0.5389, 0.5672, 0.5285, 0.5047],
       device='cuda:0')
Thumb force: tensor([0.6321, 0.7527, 0.5483, 0.5060, 0.5496, 0.5530, 0.5660],
       device='cuda:0')
Index force: tensor([1.0827, 0.5937, 0.5338, 0.5489, 0.5795, 0.5639, 0.6006],
       device='cuda:0')
Storing NORMAL transition: reward=0.0427 (scaled=0.0427), steps=1
Reward stats updated: mean -0.0046 -> -0.0045, std: 0.1034
Collected 430 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.7426, Q2 Loss=1.7426, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.8278
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5062, Q2 Loss=1.5062, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0853
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0507, Q2 Loss=1.0507, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1632
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7531, Q2 Loss=0.7531, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4160
SAC Update 5/5: Actor Loss=-0.1472, Q1 Loss=1.6439, Q2 Loss=1.6439, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7324

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (13.5%)
Q1 update: 0.06s (20.0%)
Q2 update: 0.06s (21.1%)
Actor update: 0.12s (42.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.121540
Q1 loss: 1.339292
Q2 loss: 1.339292
Current threshold: -35.9427
Global Scale Offset: 0.0361
Reward stats: mean=-0.0045, std=0.1034, count=430
----------------------------------------------
SAC Update - Actor Loss: -0.1215, Q1 Loss: 1.3393, Q2 Loss: 1.3393, Entropy: 0.0000, Mean TD Error: 1.8449, Threshold: -35.9427
tensor([-0.1500,  0.4942,  0.4543,  0.6309, -0.1878,  0.4609,  0.8304,  0.9852,
         1.4282,  0.3988,  0.1135,  1.0913,  0.0065,  0.0773, -0.0102,  0.5718],
       device='cuda:0')
Original likelihood: -30.016189575195312
Adjusted likelihood: -30.016189575195312
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.683882037003059
Current ori: tensor([ 0.0065,  0.0773, -0.0102], device='cuda:0')
Middle force: tensor([0.8318, 0.5750, 0.5385, 0.5653, 0.5270, 0.5046], device='cuda:0')
Thumb force: tensor([0.7328, 0.5412, 0.5046, 0.5468, 0.5489, 0.5582], device='cuda:0')
Index force: tensor([0.6073, 0.5333, 0.5508, 0.5786, 0.5597, 0.5894], device='cuda:0')
Storing NORMAL transition: reward=0.0207 (scaled=0.0207), steps=1
Reward stats updated: mean -0.0045 -> -0.0045, std: 0.1033
Collected 431 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3091, Q2 Loss=1.3091, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7201
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9421, Q2 Loss=0.9421, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6317
SAC Update 3/5: Actor Loss=-0.0323, Q1 Loss=0.7588, Q2 Loss=0.7588, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9574
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7746, Q2 Loss=0.7746, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5606
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.6334, Q2 Loss=0.6334, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2004

------ SAC Update Summary (5 iterations) ------
Total time: 0.33s, Avg iteration: 0.07s
Sampling: 0.00s (0.7%)
Target Q: 0.06s (18.2%)
Q1 update: 0.06s (19.4%)
Q2 update: 0.06s (18.9%)
Actor update: 0.13s (39.7%)
Target update: 0.01s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.006460
Q1 loss: 0.883601
Q2 loss: 0.883601
Current threshold: -35.9434
Global Scale Offset: 0.0361
Reward stats: mean=-0.0045, std=0.1033, count=431
----------------------------------------------
SAC Update - Actor Loss: -0.0065, Q1 Loss: 0.8836, Q2 Loss: 0.8836, Entropy: 0.0000, Mean TD Error: 1.2141, Threshold: -35.9434
tensor([-0.1237,  0.4579,  0.5009,  0.6756, -0.1665,  0.4300,  0.8823,  1.0058,
         1.4544,  0.3796,  0.1003,  1.0469,  0.0181,  0.0599, -0.0291,  0.6171],
       device='cuda:0')
Original likelihood: -39.222965240478516
Adjusted likelihood: -39.222965240478516
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 38.89905548095703
Projection step: 1, Loss: 38.91526794433594
Projection step: 2, Loss: 37.49233627319336
Projection step: 3, Loss: 35.89374923706055
Projection step: 4, Loss: 37.210205078125
Projection step: 5, Loss: 35.23709487915039
Projection step: 6, Loss: 35.004791259765625
Projection step: 7, Loss: 35.56340408325195
Projection step: 8, Loss: 35.37776184082031
Projection step: 9, Loss: 33.633880615234375
Projection step: 10, Loss: 33.38737869262695
Projection step: 11, Loss: 34.3510856628418
Projection step: 12, Loss: 33.788841247558594
Projection step: 13, Loss: 33.003623962402344
Projection step: 14, Loss: 31.846269607543945
Projection step: 15, Loss: 32.42395782470703
Projection step: 16, Loss: 31.289833068847656
Projection step: 17, Loss: 31.33544921875
Projection step: 18, Loss: 31.22018814086914
Projection step: 19, Loss: 29.881038665771484
Projection step: 20, Loss: 28.928627014160156
Projection step: 21, Loss: 28.639507293701172
Projection step: 22, Loss: 28.92941665649414
Projection step: 23, Loss: 28.964384078979492
Projection step: 24, Loss: 29.602262496948242
Final likelihood: tensor([-26.1594, -28.7990, -30.8524, -28.2043, -27.7537, -25.9826, -32.2053,
        -35.7402, -24.5747, -28.2199, -30.1346, -29.9848, -30.5590, -26.3539,
        -27.1197, -29.0816])
Final projection likelihood: -28.8578
1 mode projection succeeded
New goal: tensor([-0.0672,  0.4739,  0.6156,  0.6633, -0.1542,  0.4517,  0.8064,  0.8913,
         1.4236,  0.3705,  0.0895,  1.1315,  0.0234,  0.0484, -0.5095],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -32.63397979736328
Adjusted likelihood: -32.63397979736328
Likelihood residual: 0.0
Original likelihood: -28.863346099853516
Adjusted likelihood: -28.863346099853516
Likelihood residual: 0.0
{'index': 28.863346099853516, 'thumb_middle': 32.63397979736328}
Current yaw: tensor([ 0.0181,  0.0599, -0.0291], device='cuda:0')
2 index
tensor([-0.1237,  0.4579,  0.5009,  0.6756, -0.1665,  0.4300,  0.8823,  1.0058,
         1.4544,  0.3796,  0.1003,  1.0469,  0.0181,  0.0599, -0.0291,  0.6171],
       device='cuda:0')
Solve time for step 1 10.401829165988602
Current ori: tensor([ 0.0181,  0.0599, -0.0291], device='cuda:0')
Middle force: tensor([0.5712, 0.5650, 0.5561, 0.5313], device='cuda:0')
Thumb force: tensor([0.5717, 0.5456, 0.5508, 0.6419], device='cuda:0')
tensor([-0.0239,  0.3923,  0.5273,  0.6304, -0.1582,  0.4398,  0.8841,  0.9776,
         1.4561,  0.3707,  0.0845,  1.0640,  0.0157,  0.0561, -0.0055, -1.0404],
       device='cuda:0')
Solve time for step 2 4.202821115963161
Current ori: tensor([ 0.0157,  0.0561, -0.0055], device='cuda:0')
Middle force: tensor([0.5629, 0.5525, 0.5288], device='cuda:0')
Thumb force: tensor([0.5400, 0.5481, 0.6369], device='cuda:0')
tensor([-0.0111,  0.3893,  0.5371,  0.6248, -0.1358,  0.4660,  0.8713,  0.9517,
         1.4653,  0.3498,  0.0616,  1.0538,  0.0031,  0.0441, -0.0178, -1.8353],
       device='cuda:0')
Solve time for step 3 3.9876264929771423
Current ori: tensor([ 0.0031,  0.0441, -0.0178], device='cuda:0')
Middle force: tensor([0.5226, 0.5842], device='cuda:0')
Thumb force: tensor([0.5229, 0.5343], device='cuda:0')
tensor([-0.0053,  0.4003,  0.5362,  0.6085, -0.1431,  0.4593,  0.8816,  0.9608,
         1.4622,  0.3647,  0.0573,  1.0660,  0.0052,  0.0459, -0.0197, -2.1129],
       device='cuda:0')
Solve time for step 4 3.9556297549861483
Current ori: tensor([ 0.0052,  0.0459, -0.0197], device='cuda:0')
Middle force: tensor([0.5370], device='cuda:0')
Thumb force: tensor([0.5361], device='cuda:0')
Storing RECOVERY transition: reward=-0.0221 (scaled=-0.0032), steps=7
Reward stats updated: mean -0.0045 -> -0.0045, std: 0.1032
Collected 432 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.0957, Q2 Loss=1.0957, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5360
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.1234, Q2 Loss=1.1234, Entropy=0.0110, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7790
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2735, Q2 Loss=1.2735, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8056
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0415, Q2 Loss=1.0415, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9114
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0817, Q2 Loss=1.0817, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1476

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.1%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.04s (17.0%)
Actor update: 0.10s (40.4%)
Target update: 0.00s (1.8%)
Priority update: 0.00s (0.3%)
Actor loss: -0.000007
Q1 loss: 1.123150
Q2 loss: 1.123150
Current threshold: -35.9439
Global Scale Offset: 0.0361
Reward stats: mean=-0.0045, std=0.1032, count=432
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.1231, Q2 Loss: 1.1231, Entropy: 0.0022, Mean TD Error: 0.6359, Threshold: -35.9439
Original likelihood: -27.354774475097656
Adjusted likelihood: -27.354774475097656
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0034,  0.0424, -0.0049], device='cuda:0')
3 turn
Sampling time 3.5792392150033265
tensor([-0.0446,  0.4535,  0.5850,  0.6426, -0.1363,  0.4696,  0.8746,  0.9507,
         1.4585,  0.3669,  0.0508,  1.0741,  0.0034,  0.0424, -0.0049, -2.1693],
       device='cuda:0')
Original likelihood: -27.594867706298828
Adjusted likelihood: -27.594867706298828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.785688597999979
Current ori: tensor([ 0.0034,  0.0424, -0.0049], device='cuda:0')
Middle force: tensor([1.4139, 1.9287, 0.7401, 0.5031, 0.6004, 0.7300, 0.8454, 0.7639, 0.5619,
        0.5836, 0.5411, 0.6277], device='cuda:0')
Thumb force: tensor([1.4406, 1.0200, 0.5794, 0.5775, 0.6497, 1.5144, 0.5182, 0.5363, 1.0655,
        0.5887, 0.5559, 0.5082], device='cuda:0')
Index force: tensor([0.7535, 1.2046, 0.6164, 0.7382, 0.6320, 0.5370, 0.5224, 0.5551, 0.5391,
        0.5637, 0.6476, 0.7202], device='cuda:0')
Storing NORMAL transition: reward=-0.0019 (scaled=-0.0019), steps=1
Reward stats updated: mean -0.0045 -> -0.0045, std: 0.1030
Collected 433 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2296, Q2 Loss=1.2296, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5728
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3131, Q2 Loss=1.3131, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1838
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.6316, Q2 Loss=1.6316, Entropy=0.0001, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7645
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3288, Q2 Loss=1.3288, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5116
SAC Update 5/5: Actor Loss=-0.0366, Q1 Loss=1.1559, Q2 Loss=1.1559, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3922

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.2%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.5%)
Actor update: 0.10s (39.0%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.007324
Q1 loss: 1.331795
Q2 loss: 1.331795
Current threshold: -35.9443
Global Scale Offset: 0.0361
Reward stats: mean=-0.0045, std=0.1030, count=433
----------------------------------------------
SAC Update - Actor Loss: -0.0073, Q1 Loss: 1.3318, Q2 Loss: 1.3318, Entropy: 0.0000, Mean TD Error: 1.6850, Threshold: -35.9443
tensor([ 0.0318,  0.5607,  0.5106,  0.6267, -0.2819,  0.4258,  0.9490,  1.0094,
         1.4213,  0.3528,  0.1876,  0.8725, -0.0236,  0.0393, -0.0033, -2.5486],
       device='cuda:0')
Original likelihood: -34.33999252319336
Adjusted likelihood: -34.33999252319336
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.454875082010403
Current ori: tensor([-0.0236,  0.0393, -0.0033], device='cuda:0')
Middle force: tensor([1.0944, 0.8685, 0.9586, 0.5106, 0.5074, 0.5781, 0.5699, 0.5385, 0.5421,
        0.7024, 0.5565], device='cuda:0')
Thumb force: tensor([0.6885, 0.9579, 0.5451, 0.7862, 1.6729, 0.5426, 0.5587, 0.5816, 0.6429,
        0.5784, 0.5919], device='cuda:0')
Index force: tensor([0.7458, 0.5567, 0.7280, 0.6343, 0.8554, 0.6787, 0.6336, 0.6632, 0.6495,
        0.5001, 0.6641], device='cuda:0')
Storing NORMAL transition: reward=0.0061 (scaled=0.0061), steps=1
Reward stats updated: mean -0.0045 -> -0.0044, std: 0.1029
Collected 434 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5281, Q2 Loss=1.5281, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0784
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6735, Q2 Loss=0.6735, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2236
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9550, Q2 Loss=0.9550, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3101
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6087, Q2 Loss=1.6087, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2098
SAC Update 5/5: Actor Loss=-0.1601, Q1 Loss=1.1977, Q2 Loss=1.1977, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8684

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.1%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.8%)
Actor update: 0.08s (40.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.032014
Q1 loss: 1.192631
Q2 loss: 1.192631
Current threshold: -35.9445
Global Scale Offset: 0.0361
Reward stats: mean=-0.0044, std=0.1029, count=434
----------------------------------------------
SAC Update - Actor Loss: -0.0320, Q1 Loss: 1.1926, Q2 Loss: 1.1926, Entropy: 0.0000, Mean TD Error: 0.7380, Threshold: -35.9445
tensor([ 0.0459,  0.5200,  0.5711,  0.6455, -0.1822,  0.5057,  0.9325,  1.0123,
         1.4471,  0.3025,  0.1260,  0.9503, -0.0130,  0.0307, -0.0084, -2.5015],
       device='cuda:0')
Original likelihood: -25.39044761657715
Adjusted likelihood: -25.39044761657715
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.1611134330159985
Current ori: tensor([-0.0130,  0.0307, -0.0084], device='cuda:0')
Middle force: tensor([0.8833, 0.9660, 0.5082, 0.5058, 0.5708, 0.5312, 0.5730, 0.5508, 0.9470,
        0.5024], device='cuda:0')
Thumb force: tensor([0.9496, 0.5395, 0.7638, 1.6302, 0.5558, 0.5804, 0.5899, 0.6325, 0.5812,
        0.5593], device='cuda:0')
Index force: tensor([0.5476, 0.7317, 0.6291, 0.8715, 0.6935, 0.6497, 0.6237, 0.6329, 0.6218,
        0.6264], device='cuda:0')
Storing NORMAL transition: reward=0.1220 (scaled=0.1220), steps=1
Reward stats updated: mean -0.0044 -> -0.0041, std: 0.1030
Collected 435 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4656, Q2 Loss=1.4656, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3441
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9453, Q2 Loss=0.9453, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2456
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5268, Q2 Loss=1.5268, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1137
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.5663, Q2 Loss=1.5663, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.8017
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1484, Q2 Loss=1.1484, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3082

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.4%)
Q1 update: 0.04s (19.4%)
Q2 update: 0.04s (19.1%)
Actor update: 0.09s (40.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000000
Q1 loss: 1.330478
Q2 loss: 1.330478
Current threshold: -35.9446
Global Scale Offset: 0.0361
Reward stats: mean=-0.0041, std=0.1030, count=435
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.3305, Q2 Loss: 1.3305, Entropy: 0.0000, Mean TD Error: 1.3627, Threshold: -35.9446
tensor([ 3.1689e-02,  4.7101e-01,  6.0742e-01,  6.7801e-01, -1.4014e-01,
         4.7387e-01,  9.8283e-01,  1.0853e+00,  1.4588e+00,  2.7212e-01,
         9.7158e-02,  9.5809e-01,  2.1212e-03,  2.5403e-03, -1.2937e-01,
        -2.1526e+00], device='cuda:0')
Original likelihood: -18.613985061645508
Adjusted likelihood: -18.613985061645508
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.026515369012486
Current ori: tensor([ 0.0021,  0.0025, -0.1294], device='cuda:0')
Middle force: tensor([0.5020, 0.6056, 0.7289, 0.8471, 0.7450, 0.5642, 0.5947, 0.5348, 0.5759],
       device='cuda:0')
Thumb force: tensor([0.5844, 0.6385, 1.4370, 0.5160, 0.5344, 1.0166, 0.5760, 0.5496, 0.5191],
       device='cuda:0')
Index force: tensor([0.7762, 0.6231, 0.5310, 0.5182, 0.5492, 0.5349, 0.5530, 0.6410, 0.6660],
       device='cuda:0')
Storing NORMAL transition: reward=0.0825 (scaled=0.0825), steps=1
Reward stats updated: mean -0.0041 -> -0.0039, std: 0.1030
Collected 436 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.7274, Q2 Loss=0.7274, Entropy=0.0112, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3871
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.6050, Q2 Loss=2.6050, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6330
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8436, Q2 Loss=0.8436, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3086
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1714, Q2 Loss=1.1714, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8199
SAC Update 5/5: Actor Loss=-0.0641, Q1 Loss=1.4157, Q2 Loss=1.4157, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2676

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (17.8%)
Q1 update: 0.04s (20.0%)
Q2 update: 0.04s (19.6%)
Actor update: 0.08s (38.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.012829
Q1 loss: 1.352638
Q2 loss: 1.352638
Current threshold: -35.9448
Global Scale Offset: 0.0361
Reward stats: mean=-0.0039, std=0.1030, count=436
----------------------------------------------
SAC Update - Actor Loss: -0.0128, Q1 Loss: 1.3526, Q2 Loss: 1.3526, Entropy: 0.0022, Mean TD Error: 1.4833, Threshold: -35.9448
tensor([ 0.1115,  0.5381,  0.6316,  0.6004, -0.2849,  0.6400,  1.0002,  1.0696,
         1.5000,  0.2150,  0.1612,  0.7695, -0.0164,  0.0118, -0.2125, -2.3142],
       device='cuda:0')
Original likelihood: -35.626136779785156
Adjusted likelihood: -35.626136779785156
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9740)
Solve time for step 5 4.679968203010503
Current ori: tensor([-0.0164,  0.0118, -0.2125], device='cuda:0')
Middle force: tensor([0.6023, 0.7141, 0.8453, 0.7345, 0.5563, 0.5735, 0.5194, 0.5245],
       device='cuda:0')
Thumb force: tensor([0.6311, 1.4298, 0.5165, 0.5368, 1.0283, 0.5962, 0.5901, 0.5619],
       device='cuda:0')
Index force: tensor([0.6172, 0.5283, 0.5161, 0.5451, 0.5320, 0.5501, 0.6232, 0.6558],
       device='cuda:0')
Storing NORMAL transition: reward=0.0050 (scaled=0.0050), steps=1
Reward stats updated: mean -0.0039 -> -0.0039, std: 0.1028
Collected 437 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9616, Q2 Loss=0.9616, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5760
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1858, Q2 Loss=1.1858, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8381
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2209, Q2 Loss=1.2209, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5440
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7236, Q2 Loss=0.7236, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4939
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3120, Q2 Loss=1.3120, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6541

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.9%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.9%)
Actor update: 0.09s (39.5%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.080787
Q2 loss: 1.080787
Current threshold: -35.9449
Global Scale Offset: 0.0361
Reward stats: mean=-0.0039, std=0.1028, count=437
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0808, Q2 Loss: 1.0808, Entropy: 0.0000, Mean TD Error: 0.8212, Threshold: -35.9449
tensor([ 1.5006e-01,  5.2362e-01,  5.7094e-01,  7.5070e-01, -2.7074e-01,
         5.9645e-01,  1.1530e+00,  9.6121e-01,  1.2860e+00,  2.4638e-01,
         1.3764e-01,  7.2125e-01, -3.8790e-02,  1.7503e-03, -2.1930e-01,
        -2.0948e+00], device='cuda:0')
Original likelihood: -37.91575622558594
Adjusted likelihood: -37.91575622558594
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.695518493652344
Projection step: 1, Loss: 36.88019943237305
Projection step: 2, Loss: 36.23848342895508
Projection step: 3, Loss: 34.530059814453125
Projection step: 4, Loss: 34.991485595703125
Projection step: 5, Loss: 33.84994125366211
Projection step: 6, Loss: 34.945762634277344
Projection step: 7, Loss: 33.184547424316406
Projection step: 8, Loss: 32.685279846191406
Projection step: 9, Loss: 33.06756591796875
Projection step: 10, Loss: 33.96919631958008
Projection step: 11, Loss: 31.184242248535156
Projection step: 12, Loss: 31.684730529785156
Projection step: 13, Loss: 31.17698860168457
Projection step: 14, Loss: 30.549087524414062
Projection step: 15, Loss: 28.63202476501465
Projection step: 16, Loss: 28.770095825195312
Projection step: 17, Loss: 29.728498458862305
Projection step: 18, Loss: 29.56464385986328
Projection step: 19, Loss: 27.89636993408203
Projection step: 20, Loss: 28.454126358032227
Projection step: 21, Loss: 26.942893981933594
Projection step: 22, Loss: 27.32608413696289
Projection step: 23, Loss: 26.222000122070312
Projection step: 24, Loss: 25.974191665649414
Final likelihood: tensor([-23.0145, -28.3782, -30.9911, -23.5169, -24.3304, -29.5303, -26.4633,
        -31.8205, -24.7209, -28.4335, -25.9068, -23.9821, -25.0262, -26.2160,
        -26.3573, -33.9691])
Final projection likelihood: -27.0411
1 mode projection succeeded
New goal: tensor([ 0.1331,  0.5115,  0.5706,  0.8149, -0.1708,  0.6445,  1.0758,  0.9098,
         1.3345,  0.2444,  0.1476,  0.8256, -0.0394,  0.0056,  0.0146],
       device='cuda:0')
tensor([[0.0238]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0051]], device='cuda:0')
Original likelihood: -26.659481048583984
Adjusted likelihood: -26.659481048583984
Likelihood residual: 0.0
Original likelihood: -32.28107833862305
Adjusted likelihood: -32.28107833862305
Likelihood residual: 0.0
{'index': 32.28107833862305, 'thumb_middle': 26.659481048583984}
Current yaw: tensor([-0.0388,  0.0018, -0.2193], device='cuda:0')
4 thumb_middle
tensor([ 1.5006e-01,  5.2362e-01,  5.7094e-01,  7.5070e-01, -2.7074e-01,
         5.9645e-01,  1.1530e+00,  9.6121e-01,  1.2860e+00,  2.4638e-01,
         1.3764e-01,  7.2125e-01, -3.8790e-02,  1.7503e-03, -2.1930e-01,
        -2.0948e+00], device='cuda:0')
Solve time for step 1 8.455703597981483
Current ori: tensor([-0.0388,  0.0018, -0.2193], device='cuda:0')
Index force: tensor([0.5724, 0.5895, 0.5753, 0.5908], device='cuda:0')
tensor([ 0.1584,  0.5482,  0.5703,  0.7805, -0.3105,  0.5757,  1.0395,  0.8908,
         1.3408,  0.2235,  0.1232,  0.8074, -0.0419, -0.0088, -0.2193, -2.0442],
       device='cuda:0')
Solve time for step 2 3.564018579025287
Current ori: tensor([-0.0419, -0.0088, -0.2193], device='cuda:0')
Index force: tensor([0.5781, 0.5664, 0.5816], device='cuda:0')
tensor([ 0.1646,  0.5391,  0.5748,  0.8096, -0.3206,  0.5924,  1.0308,  0.8859,
         1.3410,  0.2465,  0.1221,  0.8105, -0.0367, -0.0129, -0.2193, -2.0198],
       device='cuda:0')
Solve time for step 3 3.3701680909725837
Current ori: tensor([-0.0367, -0.0129, -0.2193], device='cuda:0')
Index force: tensor([0.5564, 0.5720], device='cuda:0')
tensor([ 0.1530,  0.5334,  0.5635,  0.8234, -0.3231,  0.5933,  1.0189,  0.8833,
         1.3570,  0.2366,  0.1112,  0.8214, -0.0346, -0.0052, -0.2193, -2.0313],
       device='cuda:0')
Solve time for step 4 3.3147884629433975
Current ori: tensor([-0.0346, -0.0052, -0.2193], device='cuda:0')
Index force: tensor([0.5558], device='cuda:0')
Storing RECOVERY transition: reward=0.0241 (scaled=0.0048), steps=5
Reward stats updated: mean -0.0039 -> -0.0039, std: 0.1027
Collected 438 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0348, Q2 Loss=1.0348, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6925
SAC Update 2/5: Actor Loss=-0.2312, Q1 Loss=0.6893, Q2 Loss=0.6893, Entropy=0.3464, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2758
SAC Update 3/5: Actor Loss=-0.1495, Q1 Loss=0.8992, Q2 Loss=0.8992, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4205
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.8313, Q2 Loss=1.8313, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9827
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9368, Q2 Loss=0.9368, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6174

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (19.6%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (39.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.076137
Q1 loss: 1.078277
Q2 loss: 1.078277
Current threshold: -35.9751
Global Scale Offset: 0.0355
Reward stats: mean=-0.0039, std=0.1027, count=438
----------------------------------------------
SAC Update - Actor Loss: -0.0761, Q1 Loss: 1.0783, Q2 Loss: 1.0783, Entropy: 0.0693, Mean TD Error: 0.9978, Threshold: -35.9751
Original likelihood: -29.885494232177734
Adjusted likelihood: -29.885494232177734
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0372, -0.0095, -0.2436], device='cuda:0')
5 turn
Sampling time 3.5964009200106375
tensor([ 0.1395,  0.5276,  0.5717,  0.7953, -0.2450,  0.6383,  1.0619,  0.8979,
         1.4146,  0.2699,  0.1619,  0.8439, -0.0372, -0.0095, -0.2436, -1.9551],
       device='cuda:0')
Original likelihood: -31.35832977294922
Adjusted likelihood: -31.35832977294922
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.085647157975473
Current ori: tensor([-0.0372, -0.0095, -0.2436], device='cuda:0')
Middle force: tensor([0.8984, 0.5127, 0.5273, 0.5519, 0.6392, 1.4153, 0.5068, 0.8326, 0.9129,
        0.7241, 0.5005, 0.6353], device='cuda:0')
Thumb force: tensor([0.5419, 1.2551, 0.6168, 0.5333, 0.5211, 0.6453, 0.6002, 0.9316, 0.8268,
        1.8723, 0.5003, 0.6380], device='cuda:0')
Index force: tensor([0.7937, 0.5116, 0.5560, 0.5133, 0.5982, 0.5394, 0.6884, 0.9607, 0.5354,
        0.6108, 0.5434, 0.5722], device='cuda:0')
Storing NORMAL transition: reward=0.0057 (scaled=0.0057), steps=1
Reward stats updated: mean -0.0039 -> -0.0039, std: 0.1026
Collected 439 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1846, Q2 Loss=1.1846, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1007
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9195, Q2 Loss=0.9195, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9485
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9900, Q2 Loss=0.9900, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1252
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.3845, Q2 Loss=1.3845, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2263
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1373, Q2 Loss=1.1373, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3949

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (17.0%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (19.6%)
Actor update: 0.11s (41.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.123199
Q2 loss: 1.123199
Current threshold: -36.0081
Global Scale Offset: 0.0349
Reward stats: mean=-0.0039, std=0.1026, count=439
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1232, Q2 Loss: 1.1232, Entropy: 0.0000, Mean TD Error: 1.1591, Threshold: -36.0081
tensor([ 0.1038,  0.4201,  0.6654,  0.8292, -0.3206,  0.6186,  1.1147,  0.8287,
         1.4351,  0.3281,  0.2208,  0.7577, -0.0088,  0.0176, -0.2483, -1.9779],
       device='cuda:0')
Original likelihood: -41.96986770629883
Adjusted likelihood: -41.96986770629883
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 41.45127868652344
Projection step: 1, Loss: 38.595577239990234
Projection step: 2, Loss: 38.355201721191406
Projection step: 3, Loss: 40.19213104248047
Projection step: 4, Loss: 38.47663879394531
Projection step: 5, Loss: 37.87730407714844
Projection step: 6, Loss: 37.9607048034668
Projection step: 7, Loss: 37.30877685546875
Projection step: 8, Loss: 34.7015266418457
Projection step: 9, Loss: 34.555171966552734
Projection step: 10, Loss: 35.928733825683594
Projection step: 11, Loss: 33.27781677246094
Projection step: 12, Loss: 33.394107818603516
Projection step: 13, Loss: 32.07759094238281
Projection step: 14, Loss: 30.41474151611328
Projection step: 15, Loss: 32.09303283691406
Projection step: 16, Loss: 30.587465286254883
Projection step: 17, Loss: 30.179039001464844
Projection step: 18, Loss: 29.974288940429688
Projection step: 19, Loss: 29.433242797851562
Projection step: 20, Loss: 28.58371353149414
Projection step: 21, Loss: 27.612560272216797
Projection step: 22, Loss: 28.44540786743164
Projection step: 23, Loss: 25.880693435668945
Projection step: 24, Loss: 31.614994049072266
Final likelihood: tensor([-26.8387, -28.6933, -27.3960, -27.5094, -30.6240, -25.4241, -24.3826,
        -25.6599, -25.4864, -23.1316, -27.4998, -24.6400, -27.1722, -29.3389,
        -31.6053, -25.2385])
Final projection likelihood: -26.9150
1 mode projection succeeded
New goal: tensor([ 0.1078,  0.4215,  0.6817,  0.8207, -0.2147,  0.6743,  1.0701,  0.8298,
         1.4083,  0.2915,  0.1933,  0.8720, -0.0079,  0.0164, -0.0111],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0038]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -32.16252517700195
Adjusted likelihood: -32.16252517700195
Likelihood residual: 0.0
Original likelihood: -34.23094940185547
Adjusted likelihood: -34.23094940185547
Likelihood residual: 0.0
{'index': 34.23094940185547, 'thumb_middle': 32.16252517700195}
Current yaw: tensor([-0.0088,  0.0176, -0.2483], device='cuda:0')
6 thumb_middle
tensor([ 0.1038,  0.4201,  0.6654,  0.8292, -0.3206,  0.6186,  1.1147,  0.8287,
         1.4351,  0.3281,  0.2208,  0.7577, -0.0088,  0.0176, -0.2483, -1.9779],
       device='cuda:0')
Solve time for step 1 8.638457542052492
Current ori: tensor([-0.0088,  0.0176, -0.2483], device='cuda:0')
Index force: tensor([0.5706, 0.6041, 0.5944, 0.5982], device='cuda:0')
tensor([ 0.1146,  0.4274,  0.6743,  0.8105, -0.3408,  0.6268,  1.0219,  0.7994,
         1.3729,  0.2853,  0.1247,  0.8211, -0.0123,  0.0114, -0.2482, -1.9746],
       device='cuda:0')
Solve time for step 2 3.548372459015809
Current ori: tensor([-0.0123,  0.0114, -0.2482], device='cuda:0')
Index force: tensor([0.5913, 0.5832, 0.5880], device='cuda:0')
tensor([ 0.1188,  0.4263,  0.6796,  0.8108, -0.3380,  0.6313,  1.0146,  0.7939,
         1.3743,  0.2798,  0.1111,  0.8320, -0.0119,  0.0090, -0.2482, -1.9684],
       device='cuda:0')
Solve time for step 3 3.357789357949514
Current ori: tensor([-0.0119,  0.0090, -0.2482], device='cuda:0')
Index force: tensor([0.5692, 0.5759], device='cuda:0')
tensor([ 0.1111,  0.4162,  0.6792,  0.8244, -0.3397,  0.6322,  1.0082,  0.7961,
         1.3787,  0.2806,  0.1111,  0.8350, -0.0082,  0.0136, -0.2482, -1.9707],
       device='cuda:0')
Solve time for step 4 3.3609901050222106
Current ori: tensor([-0.0082,  0.0136, -0.2482], device='cuda:0')
Index force: tensor([0.5599], device='cuda:0')
Storing RECOVERY transition: reward=0.0067 (scaled=0.0067), steps=1
Reward stats updated: mean -0.0039 -> -0.0039, std: 0.1025
Collected 440 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2451, Q2 Loss=1.2451, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8753
SAC Update 2/5: Actor Loss=-0.1288, Q1 Loss=2.5573, Q2 Loss=2.5573, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8076
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3048, Q2 Loss=1.3048, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8976
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.6939, Q2 Loss=0.6939, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8007
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=2.7003, Q2 Loss=2.7003, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.1613

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (16.8%)
Q1 update: 0.05s (20.5%)
Q2 update: 0.05s (19.7%)
Actor update: 0.10s (39.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.117859
Q1 loss: 1.700287
Q2 loss: 1.700287
Current threshold: -36.0277
Global Scale Offset: 0.0346
Reward stats: mean=-0.0039, std=0.1025, count=440
----------------------------------------------
SAC Update - Actor Loss: -0.1179, Q1 Loss: 1.7003, Q2 Loss: 1.7003, Entropy: 0.0000, Mean TD Error: 2.1085, Threshold: -36.0277
Original likelihood: -34.837379455566406
Adjusted likelihood: -34.837379455566406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0067,  0.0070, -0.2547], device='cuda:0')
7 turn
Sampling time 3.6541599089978263
tensor([ 0.1220,  0.4152,  0.6867,  0.8336, -0.2862,  0.6713,  1.0442,  0.8145,
         1.4338,  0.2941,  0.1630,  0.8703, -0.0067,  0.0070, -0.2547, -1.9310],
       device='cuda:0')
Original likelihood: -33.75531005859375
Adjusted likelihood: -33.75531005859375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.093816717970185
Current ori: tensor([-0.0067,  0.0070, -0.2547], device='cuda:0')
Middle force: tensor([0.6027, 0.5730, 0.5776, 1.8826, 0.5886, 1.2375, 0.5286, 1.1728, 0.6959,
        0.6195, 0.5770, 0.5688], device='cuda:0')
Thumb force: tensor([0.5882, 0.7617, 0.5814, 1.4213, 0.5405, 0.8668, 0.5193, 1.2863, 1.8489,
        0.5995, 0.5599, 0.6341], device='cuda:0')
Index force: tensor([0.6886, 0.5103, 0.5063, 0.5079, 0.5718, 0.5864, 0.6900, 0.5903, 0.5683,
        0.5954, 0.5513, 0.6148], device='cuda:0')
Storing NORMAL transition: reward=0.0410 (scaled=0.0410), steps=1
Reward stats updated: mean -0.0039 -> -0.0038, std: 0.1024
Collected 441 transitions for RL
SAC Update 1/5: Actor Loss=-0.1214, Q1 Loss=2.4009, Q2 Loss=2.4009, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7456
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.2697, Q2 Loss=2.2697, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5996
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.9381, Q2 Loss=1.9381, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2548
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0433, Q2 Loss=1.0433, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5448
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9948, Q2 Loss=0.9948, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7311

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.2%)
Q1 update: 0.04s (19.8%)
Q2 update: 0.04s (18.6%)
Actor update: 0.08s (39.6%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.024272
Q1 loss: 1.729357
Q2 loss: 1.729357
Current threshold: -36.0393
Global Scale Offset: 0.0344
Reward stats: mean=-0.0038, std=0.1024, count=441
----------------------------------------------
SAC Update - Actor Loss: -0.0243, Q1 Loss: 1.7294, Q2 Loss: 1.7294, Entropy: 0.0000, Mean TD Error: 1.5752, Threshold: -36.0393
tensor([ 0.0096,  0.4496,  0.6093,  0.6910, -0.3197,  0.7289,  0.8533,  0.9911,
         1.3265,  0.2048,  0.0986,  0.9488, -0.0257,  0.0401, -0.2979, -1.7156],
       device='cuda:0')
Original likelihood: -35.32926940917969
Adjusted likelihood: -35.32926940917969
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.426603342988528
Current ori: tensor([-0.0257,  0.0401, -0.2979], device='cuda:0')
Middle force: tensor([0.5653, 0.5948, 1.8830, 0.6145, 1.2315, 0.5309, 1.1562, 0.6976, 0.6140,
        0.5751, 0.5724], device='cuda:0')
Thumb force: tensor([0.7411, 0.5595, 1.3673, 0.5238, 0.8289, 0.5141, 1.2461, 1.7920, 0.5932,
        0.5535, 0.6201], device='cuda:0')
Index force: tensor([0.5101, 0.5049, 0.5056, 0.5696, 0.5790, 0.6767, 0.5865, 0.5668, 0.5907,
        0.5483, 0.6076], device='cuda:0')
Storing NORMAL transition: reward=-0.0422 (scaled=-0.0422), steps=1
Reward stats updated: mean -0.0038 -> -0.0038, std: 0.1023
Collected 442 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.1146, Q2 Loss=1.1146, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4013
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.0791, Q2 Loss=1.0791, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5342
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3500, Q2 Loss=1.3500, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3658
SAC Update 4/5: Actor Loss=-0.1898, Q1 Loss=1.2983, Q2 Loss=1.2983, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8621
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.1049, Q2 Loss=1.1049, Entropy=0.0129, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7975

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.3%)
Q1 update: 0.06s (20.7%)
Q2 update: 0.06s (20.7%)
Actor update: 0.12s (41.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.130062
Q1 loss: 1.189391
Q2 loss: 1.189391
Current threshold: -36.0461
Global Scale Offset: 0.0343
Reward stats: mean=-0.0038, std=0.1023, count=442
----------------------------------------------
SAC Update - Actor Loss: -0.1301, Q1 Loss: 1.1894, Q2 Loss: 1.1894, Entropy: 0.0026, Mean TD Error: 0.9922, Threshold: -36.0461
tensor([-0.0052,  0.4229,  0.5816,  0.7995, -0.3149,  0.7116,  0.8745,  1.0508,
         1.3943,  0.1630,  0.2133,  1.0191,  0.0042,  0.0262, -0.2540, -1.5599],
       device='cuda:0')
Original likelihood: -37.760032653808594
Adjusted likelihood: -37.760032653808594
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 39.04351806640625
Projection step: 1, Loss: 38.71685028076172
Projection step: 2, Loss: 38.82939910888672
Projection step: 3, Loss: 38.38945770263672
Projection step: 4, Loss: 36.66554260253906
Projection step: 5, Loss: 35.575050354003906
Projection step: 6, Loss: 36.0324592590332
Projection step: 7, Loss: 35.21626281738281
Projection step: 8, Loss: 34.48188400268555
Projection step: 9, Loss: 33.58279037475586
Projection step: 10, Loss: 34.50531768798828
Projection step: 11, Loss: 31.46524429321289
Projection step: 12, Loss: 32.74384307861328
Projection step: 13, Loss: 30.791141510009766
Projection step: 14, Loss: 31.592220306396484
Projection step: 15, Loss: 32.18223190307617
Projection step: 16, Loss: 30.337369918823242
Projection step: 17, Loss: 27.736526489257812
Projection step: 18, Loss: 29.626995086669922
Projection step: 19, Loss: 27.483642578125
Projection step: 20, Loss: 27.33941650390625
Projection step: 21, Loss: 27.325294494628906
Projection step: 22, Loss: 27.476276397705078
Projection step: 23, Loss: 26.85238265991211
Projection step: 24, Loss: 25.978580474853516
Final likelihood: tensor([-27.2828, -22.5369, -28.8436, -30.7866, -22.2113, -26.9267, -27.3706,
        -26.8210, -28.2027, -22.1237, -23.7547, -28.7484, -23.0739, -21.7433,
        -28.9241, -24.6643])
Final projection likelihood: -25.8759
1 mode projection succeeded
New goal: tensor([ 0.0016,  0.4217,  0.6834,  0.7759, -0.2298,  0.6964,  0.8124,  0.9165,
         1.3290,  0.1375,  0.1594,  1.0690,  0.0029,  0.0224, -0.4176],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -26.4081974029541
Adjusted likelihood: -26.4081974029541
Likelihood residual: 0.0
Original likelihood: -34.814300537109375
Adjusted likelihood: -34.814300537109375
Likelihood residual: 0.0
{'index': 34.814300537109375, 'thumb_middle': 26.4081974029541}
Current yaw: tensor([ 0.0042,  0.0262, -0.2540], device='cuda:0')
8 thumb_middle
tensor([-0.0052,  0.4229,  0.5816,  0.7995, -0.3149,  0.7116,  0.8745,  1.0508,
         1.3943,  0.1630,  0.2133,  1.0191,  0.0042,  0.0262, -0.2540, -1.5599],
       device='cuda:0')
Solve time for step 1 8.93868477200158
Current ori: tensor([ 0.0042,  0.0262, -0.2540], device='cuda:0')
Index force: tensor([0.5701, 0.5821, 0.5931, 0.5937], device='cuda:0')
tensor([-0.0074,  0.4032,  0.6218,  0.7655, -0.3251,  0.7107,  0.8027,  0.9091,
         1.3305,  0.1389,  0.1383,  1.0611,  0.0048,  0.0276, -0.2540, -1.5800],
       device='cuda:0')
Solve time for step 2 3.571292938955594
Current ori: tensor([ 0.0048,  0.0276, -0.2540], device='cuda:0')
Index force: tensor([0.5000, 0.5900, 0.5637], device='cuda:0')
tensor([-0.0080,  0.4008,  0.6306,  0.7532, -0.3263,  0.7206,  0.8013,  0.8962,
         1.3355,  0.1315,  0.1287,  1.0643,  0.0040,  0.0281, -0.2540, -1.5841],
       device='cuda:0')
Solve time for step 3 3.478711832955014
Current ori: tensor([ 0.0040,  0.0281, -0.2540], device='cuda:0')
Index force: tensor([0.5785, 0.5565], device='cuda:0')
tensor([ 0.0118,  0.3905,  0.6549,  0.7658, -0.3230,  0.7247,  0.8028,  0.8985,
         1.3296,  0.1319,  0.1156,  1.0612,  0.0089,  0.0170, -0.2540, -1.5435],
       device='cuda:0')
Solve time for step 4 3.338694272970315
Current ori: tensor([ 0.0089,  0.0170, -0.2540], device='cuda:0')
Index force: tensor([0.5446], device='cuda:0')
Storing RECOVERY transition: reward=-0.0010 (scaled=-0.0005), steps=2
Reward stats updated: mean -0.0038 -> -0.0038, std: 0.1022
Collected 443 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=2.5625, Q2 Loss=2.5625, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.1460
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5446, Q2 Loss=1.5446, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0724
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8554, Q2 Loss=0.8554, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4192
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.7795, Q2 Loss=1.7795, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8428
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.2523, Q2 Loss=1.2523, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0744

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.4%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (18.8%)
Actor update: 0.10s (38.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.598857
Q2 loss: 1.598857
Current threshold: -36.0502
Global Scale Offset: 0.0342
Reward stats: mean=-0.0038, std=0.1022, count=443
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.5989, Q2 Loss: 1.5989, Entropy: 0.0000, Mean TD Error: 2.1110, Threshold: -36.0502
Original likelihood: -33.33425521850586
Adjusted likelihood: -33.33425521850586
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0115,  0.0120, -0.2525], device='cuda:0')
9 turn
Sampling time 3.758822985982988
tensor([ 0.0205,  0.3866,  0.6625,  0.7761, -0.2589,  0.7506,  0.8240,  0.9158,
         1.3916,  0.1584,  0.1671,  1.0850,  0.0115,  0.0120, -0.2525, -1.5319],
       device='cuda:0')
Original likelihood: -32.850284576416016
Adjusted likelihood: -32.850284576416016
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.058197040983941
Current ori: tensor([ 0.0115,  0.0120, -0.2525], device='cuda:0')
Middle force: tensor([1.1502, 1.1400, 0.5446, 1.1045, 1.7410, 1.0365, 0.5808, 0.5006, 0.7316,
        0.5449, 0.7120, 0.5027], device='cuda:0')
Thumb force: tensor([1.0286, 0.5975, 0.5613, 1.3117, 1.6457, 0.9151, 0.5305, 0.5314, 0.5688,
        0.5668, 0.5604, 0.5668], device='cuda:0')
Index force: tensor([0.5360, 0.6016, 0.5311, 0.5731, 0.5065, 0.5408, 0.5596, 0.7654, 0.5301,
        0.6672, 0.5393, 0.7096], device='cuda:0')
Storing NORMAL transition: reward=0.0061 (scaled=0.0061), steps=1
Reward stats updated: mean -0.0038 -> -0.0038, std: 0.1021
Collected 444 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.8322, Q2 Loss=0.8322, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4742
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9413, Q2 Loss=0.9413, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2097
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.7304, Q2 Loss=0.7304, Entropy=0.0133, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5099
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7270, Q2 Loss=0.7270, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4162
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9969, Q2 Loss=0.9969, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3000

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.9%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (19.5%)
Actor update: 0.09s (38.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.138156
Q1 loss: 0.845581
Q2 loss: 0.845581
Current threshold: -36.0526
Global Scale Offset: 0.0342
Reward stats: mean=-0.0038, std=0.1021, count=444
----------------------------------------------
SAC Update - Actor Loss: -0.1382, Q1 Loss: 0.8456, Q2 Loss: 0.8456, Entropy: 0.0027, Mean TD Error: 0.7820, Threshold: -36.0526
tensor([-0.1468,  0.4070,  0.6304,  0.6118, -0.1998,  0.6638,  0.8994,  1.0220,
         1.3307,  0.1438,  0.1998,  1.0867,  0.0423, -0.0219, -0.2606, -1.0793],
       device='cuda:0')
Original likelihood: -32.10462188720703
Adjusted likelihood: -32.10462188720703
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.522209557995666
Current ori: tensor([ 0.0423, -0.0219, -0.2606], device='cuda:0')
Middle force: tensor([0.8588, 0.8485, 1.0576, 0.6120, 0.5311, 0.5599, 0.6937, 1.2080, 0.5612,
        0.8787, 0.6624], device='cuda:0')
Thumb force: tensor([1.9684, 0.5429, 0.6248, 0.5791, 2.2752, 0.5164, 0.5657, 1.0963, 0.5165,
        0.5425, 0.5734], device='cuda:0')
Index force: tensor([0.6194, 0.5729, 0.5086, 0.5994, 0.5050, 0.5978, 0.9007, 0.6440, 0.5802,
        0.5901, 0.5736], device='cuda:0')
Storing NORMAL transition: reward=0.1577 (scaled=0.1577), steps=1
Reward stats updated: mean -0.0038 -> -0.0034, std: 0.1022
Collected 445 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6901, Q2 Loss=0.6901, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1778
SAC Update 2/5: Actor Loss=-0.2172, Q1 Loss=1.6331, Q2 Loss=1.6331, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2958
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7854, Q2 Loss=0.7854, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9532
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.7402, Q2 Loss=1.7402, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4374
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.9648, Q2 Loss=0.9648, Entropy=0.0138, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6425

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.4%)
Q1 update: 0.04s (20.0%)
Q2 update: 0.04s (19.2%)
Actor update: 0.09s (39.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.043437
Q1 loss: 1.162710
Q2 loss: 1.162710
Current threshold: -36.0540
Global Scale Offset: 0.0341
Reward stats: mean=-0.0034, std=0.1022, count=445
----------------------------------------------
SAC Update - Actor Loss: -0.0434, Q1 Loss: 1.1627, Q2 Loss: 1.1627, Entropy: 0.0028, Mean TD Error: 1.1013, Threshold: -36.0540
tensor([-0.1430,  0.4445,  0.5942,  0.6373, -0.1880,  0.6772,  0.8800,  1.0675,
         1.3031,  0.2665,  0.2813,  0.8694,  0.0186, -0.0321, -0.4180, -1.1442],
       device='cuda:0')
Original likelihood: -30.974149703979492
Adjusted likelihood: -30.974149703979492
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.255366494995542
Current ori: tensor([ 0.0186, -0.0321, -0.4180], device='cuda:0')
Middle force: tensor([0.5788, 1.1011, 1.7210, 1.0404, 0.5747, 0.5002, 0.7191, 0.5597, 0.7666,
        0.5015], device='cuda:0')
Thumb force: tensor([0.5427, 1.2357, 1.5133, 0.8530, 0.5266, 0.5212, 0.5598, 0.5497, 0.5408,
        0.5593], device='cuda:0')
Index force: tensor([0.5160, 0.5557, 0.5043, 0.5332, 0.5514, 0.7321, 0.5263, 0.6365, 0.5282,
        0.6886], device='cuda:0')
Storing NORMAL transition: reward=0.0361 (scaled=0.0361), steps=1
Reward stats updated: mean -0.0034 -> -0.0034, std: 0.1021
Collected 446 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7070, Q2 Loss=0.7070, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9221
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0304, Q2 Loss=1.0304, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5325
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4755, Q2 Loss=1.4755, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0878
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.7939, Q2 Loss=1.7939, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8077
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7291, Q2 Loss=0.7291, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3631

------ SAC Update Summary (5 iterations) ------
Total time: 0.19s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.8%)
Q1 update: 0.03s (18.7%)
Q2 update: 0.03s (18.5%)
Actor update: 0.07s (40.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.147177
Q2 loss: 1.147177
Current threshold: -36.0549
Global Scale Offset: 0.0341
Reward stats: mean=-0.0034, std=0.1021, count=446
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1472, Q2 Loss: 1.1472, Entropy: 0.0000, Mean TD Error: 1.1426, Threshold: -36.0549
tensor([-0.1202,  0.4375,  0.6566,  0.6609, -0.1248,  0.6852,  0.9183,  0.9720,
         1.2989,  0.2355,  0.2301,  0.9406,  0.0400, -0.0653, -0.4614, -0.7681],
       device='cuda:0')
Original likelihood: -42.54399871826172
Adjusted likelihood: -42.54399871826172
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 36.710670471191406
Projection step: 1, Loss: 33.51641082763672
Projection step: 2, Loss: 32.22383117675781
Projection step: 3, Loss: 32.441619873046875
Projection step: 4, Loss: 34.16773986816406
Projection step: 5, Loss: 33.25579833984375
Projection step: 6, Loss: 31.61235237121582
Projection step: 7, Loss: 32.37124252319336
Projection step: 8, Loss: 30.38253402709961
Projection step: 9, Loss: 30.841167449951172
Projection step: 10, Loss: 30.298004150390625
Projection step: 11, Loss: 30.473926544189453
Projection step: 12, Loss: 29.92941665649414
Projection step: 13, Loss: 29.505329132080078
Projection step: 14, Loss: 30.205913543701172
Projection step: 15, Loss: 29.075998306274414
Projection step: 16, Loss: 30.006301879882812
Projection step: 17, Loss: 31.126707077026367
Projection step: 18, Loss: 28.79401397705078
Projection step: 19, Loss: 29.835411071777344
Projection step: 20, Loss: 29.731542587280273
Projection step: 21, Loss: 29.084087371826172
Projection step: 22, Loss: 28.03704071044922
Projection step: 23, Loss: 28.288894653320312
Projection step: 24, Loss: 27.73371124267578
Final likelihood: tensor([-26.0456, -25.7018, -25.4670, -26.2678, -27.2983, -27.2584, -26.0439,
        -26.4493, -31.0519, -26.0455, -28.7458, -32.5522, -28.3153, -24.5677,
        -24.2286, -28.8232])
Final projection likelihood: -27.1789
1 mode projection succeeded
New goal: tensor([-0.0822,  0.4210,  0.7112,  0.7209, -0.0954,  0.6291,  0.8506,  0.9964,
         1.2967,  0.1775,  0.2136,  0.8861,  0.0364, -0.0588, -0.5779],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0019]], device='cuda:0')
Original likelihood: -30.284320831298828
Adjusted likelihood: -30.284320831298828
Likelihood residual: 0.0
Original likelihood: -31.727933883666992
Adjusted likelihood: -31.727933883666992
Likelihood residual: 0.0
{'index': 31.727933883666992, 'thumb_middle': 30.284320831298828}
Current yaw: tensor([ 0.0400, -0.0653, -0.4614], device='cuda:0')
10 thumb_middle
tensor([-0.1202,  0.4375,  0.6566,  0.6609, -0.1248,  0.6852,  0.9183,  0.9720,
         1.2989,  0.2355,  0.2301,  0.9406,  0.0400, -0.0653, -0.4614, -0.7681],
       device='cuda:0')
Solve time for step 1 8.688740151992533
Current ori: tensor([ 0.0400, -0.0653, -0.4614], device='cuda:0')
Index force: tensor([0.5872, 0.5591, 0.5863, 0.6023], device='cuda:0')
tensor([-0.1463,  0.4439,  0.7207,  0.7248, -0.1666,  0.6328,  0.8257,  0.9658,
         1.2814,  0.1777,  0.1735,  0.8823,  0.1011, -0.1770, -0.4614, -1.6793],
       device='cuda:0')
Solve time for step 2 3.4077425799914636
Current ori: tensor([ 0.1011, -0.1770, -0.4614], device='cuda:0')
Index force: tensor([0.5538, 0.5795, 0.5953], device='cuda:0')
tensor([-0.1397,  0.5424,  0.7643,  0.7250, -0.1266,  0.6636,  0.8468,  0.9856,
         1.2680,  0.1743,  0.1333,  0.8610,  0.2038, -0.3533, -0.4909, -2.2645],
       device='cuda:0')
Solve time for step 3 3.4868609799887054
Current ori: tensor([ 0.2038, -0.3533, -0.4909], device='cuda:0')
Index force: tensor([0.5631, 0.5045], device='cuda:0')
tensor([-0.1486,  0.7872,  1.0416,  0.8872, -0.0134,  0.7542,  0.8845,  0.9976,
         1.2411,  0.1748,  0.0639,  0.8222,  0.2273, -0.3814, -0.6062, -1.7672],
       device='cuda:0')
Solve time for step 4 3.3697349180001765
Current ori: tensor([ 0.2273, -0.3814, -0.6062], device='cuda:0')
Index force: tensor([0.5616], device='cuda:0')
Storing RECOVERY transition: reward=-0.1509 (scaled=-0.0503), steps=3
Reward stats updated: mean -0.0034 -> -0.0035, std: 0.1020
Collected 447 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8714, Q2 Loss=0.8714, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9086
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=2.4808, Q2 Loss=2.4808, Entropy=0.0001, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.1215
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7986, Q2 Loss=0.7986, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2815
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1463, Q2 Loss=1.1463, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5708
SAC Update 5/5: Actor Loss=-0.1148, Q1 Loss=2.6916, Q2 Loss=2.6916, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1377

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.4%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.06s (20.4%)
Actor update: 0.12s (42.8%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.022969
Q1 loss: 1.597723
Q2 loss: 1.597723
Current threshold: -36.0554
Global Scale Offset: 0.0341
Reward stats: mean=-0.0035, std=0.1020, count=447
----------------------------------------------
SAC Update - Actor Loss: -0.0230, Q1 Loss: 1.5977, Q2 Loss: 1.5977, Entropy: 0.0000, Mean TD Error: 2.0040, Threshold: -36.0554
Original likelihood: -334.8550720214844
Adjusted likelihood: -334.8550720214844
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 28
Loaded trajectory sampler
Current yaw: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
Current yaw: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
1 turn
Sampling time 3.6428183059906587
tensor([ 1.8764e-01,  6.0009e-01,  6.3699e-01,  5.6973e-01, -1.0762e-01,
         5.1622e-01,  8.9546e-01,  9.3307e-01,  1.1988e+00,  2.9498e-01,
         2.9227e-01,  1.1746e+00, -9.3357e-04,  1.4822e-02, -2.8876e-02,
         2.6114e-01], device='cuda:0')
Original likelihood: -20.873458862304688
Adjusted likelihood: -20.873458862304688
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.610962675011251
Current ori: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
Middle force: tensor([1.7774, 0.5758, 0.5805, 0.5043, 0.5223, 0.5198, 0.5211, 0.5786, 0.5611,
        0.5652, 0.5538, 0.6348], device='cuda:0')
Thumb force: tensor([0.7381, 0.6215, 0.7433, 0.9271, 0.6751, 0.5631, 0.7128, 1.0077, 0.9061,
        0.6499, 0.8060, 0.6635], device='cuda:0')
Index force: tensor([0.9346, 0.6960, 0.5492, 0.7622, 0.5495, 0.5322, 0.5664, 0.5376, 0.5402,
        0.5827, 0.5614, 0.5587], device='cuda:0')
Storing NORMAL transition: reward=0.0511 (scaled=0.0511), steps=1
Reward stats updated: mean -0.0035 -> -0.0033, std: 0.1020
Collected 448 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5932, Q2 Loss=1.5932, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5190
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1841, Q2 Loss=1.1841, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9216
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.7398, Q2 Loss=0.7398, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4485
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.1939, Q2 Loss=2.1939, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1258
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5305, Q2 Loss=1.5305, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7643

------ SAC Update Summary (5 iterations) ------
Total time: 0.30s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.2%)
Q1 update: 0.05s (17.5%)
Q2 update: 0.06s (19.0%)
Actor update: 0.11s (36.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.138155
Q1 loss: 1.448294
Q2 loss: 1.448294
Current threshold: -36.0557
Global Scale Offset: 0.0341
Reward stats: mean=-0.0033, std=0.1020, count=448
----------------------------------------------
SAC Update - Actor Loss: -0.1382, Q1 Loss: 1.4483, Q2 Loss: 1.4483, Entropy: 0.0000, Mean TD Error: 1.3558, Threshold: -36.0557
tensor([ 0.2245,  0.5728,  0.7202,  0.5588, -0.0705,  0.5920,  0.7882,  0.9948,
         1.2137,  0.2749,  0.2369,  1.1644, -0.0209, -0.0154, -0.0805,  0.9735],
       device='cuda:0')
Original likelihood: -28.71381950378418
Adjusted likelihood: -28.71381950378418
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.57979589496972
Current ori: tensor([-0.0209, -0.0154, -0.0805], device='cuda:0')
Middle force: tensor([0.5713, 0.5744, 0.5040, 0.5208, 0.5179, 0.5197, 0.5734, 0.5567, 0.5610,
        0.5514, 0.6321], device='cuda:0')
Thumb force: tensor([0.6072, 0.7296, 0.8968, 0.6522, 0.5564, 0.7024, 0.9878, 0.9025, 0.6425,
        0.7890, 0.6507], device='cuda:0')
Index force: tensor([0.6878, 0.5457, 0.7523, 0.5479, 0.5305, 0.5621, 0.5338, 0.5345, 0.5782,
        0.5589, 0.5553], device='cuda:0')
Storing NORMAL transition: reward=0.1348 (scaled=0.1348), steps=1
Reward stats updated: mean -0.0033 -> -0.0030, std: 0.1021
Collected 449 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5466, Q2 Loss=1.5466, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3386
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9486, Q2 Loss=0.9486, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3763
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0517, Q2 Loss=1.0517, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1664
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=2.9003, Q2 Loss=2.9003, Entropy=0.0006, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.3677
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7555, Q2 Loss=0.7555, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2483

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.06s (22.1%)
Q1 update: 0.05s (18.4%)
Q2 update: 0.05s (17.9%)
Actor update: 0.10s (37.7%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.440563
Q2 loss: 1.440563
Current threshold: -36.0559
Global Scale Offset: 0.0341
Reward stats: mean=-0.0030, std=0.1021, count=449
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.4406, Q2 Loss: 1.4406, Entropy: 0.0001, Mean TD Error: 1.6995, Threshold: -36.0559
tensor([ 0.1589,  0.6124,  0.5930,  0.5648, -0.1071,  0.5217,  0.8258,  1.0073,
         1.1795,  0.4740,  0.2556,  1.1737, -0.0326,  0.0189, -0.2167,  1.0891],
       device='cuda:0')
Original likelihood: -21.483427047729492
Adjusted likelihood: -21.483427047729492
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.128976556006819
Current ori: tensor([-0.0326,  0.0189, -0.2167], device='cuda:0')
Middle force: tensor([0.5671, 0.5043, 0.5184, 0.5169, 0.5184, 0.5681, 0.5529, 0.5568, 0.5484,
        0.6260], device='cuda:0')
Thumb force: tensor([0.7192, 0.8638, 0.6480, 0.5499, 0.6968, 0.9750, 0.9070, 0.6374, 0.7852,
        0.6468], device='cuda:0')
Index force: tensor([0.5417, 0.7366, 0.5417, 0.5284, 0.5563, 0.5301, 0.5292, 0.5737, 0.5534,
        0.5514], device='cuda:0')
Storing NORMAL transition: reward=-0.0099 (scaled=-0.0099), steps=1
Reward stats updated: mean -0.0030 -> -0.0031, std: 0.1019
Collected 450 transitions for RL
SAC Update 1/5: Actor Loss=-0.0002, Q1 Loss=0.6931, Q2 Loss=0.6931, Entropy=0.0428, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1956
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2187, Q2 Loss=1.2187, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0050
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7597, Q2 Loss=0.7597, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3677
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.5111, Q2 Loss=1.5111, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3174
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7516, Q2 Loss=0.7516, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9849

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.7%)
Q1 update: 0.05s (18.8%)
Q2 update: 0.05s (19.7%)
Actor update: 0.10s (38.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.3%)
Actor loss: -0.046086
Q1 loss: 0.986844
Q2 loss: 0.986844
Current threshold: -36.0564
Global Scale Offset: 0.0341
Reward stats: mean=-0.0031, std=0.1019, count=450
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 0.9868, Q2 Loss: 0.9868, Entropy: 0.0086, Mean TD Error: 0.7741, Threshold: -36.0564
tensor([ 0.0669,  0.4853,  0.6113,  0.4828, -0.2217,  0.5558,  1.0248,  0.9037,
         1.2172,  0.3593,  0.2575,  1.2120, -0.0550,  0.0296, -0.2103,  2.1970],
       device='cuda:0')
Original likelihood: -30.656757354736328
Adjusted likelihood: -30.656757354736328
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.954900332028046
Current ori: tensor([-0.0550,  0.0296, -0.2103], device='cuda:0')
Middle force: tensor([0.5090, 0.5366, 0.5266, 0.5212, 0.5701, 0.5577, 0.5567, 0.5556, 0.6510],
       device='cuda:0')
Thumb force: tensor([0.7585, 0.5741, 0.5273, 0.6668, 0.9389, 0.8567, 0.6250, 0.7473, 0.6164],
       device='cuda:0')
Index force: tensor([0.6899, 0.5413, 0.5282, 0.5546, 0.5290, 0.5313, 0.5698, 0.5508, 0.5460],
       device='cuda:0')
Storing NORMAL transition: reward=0.0698 (scaled=0.0698), steps=1
Reward stats updated: mean -0.0031 -> -0.0029, std: 0.1019
Collected 451 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.9035, Q2 Loss=0.9035, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4285
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2674, Q2 Loss=1.2674, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8682
SAC Update 3/5: Actor Loss=-0.1683, Q1 Loss=1.1733, Q2 Loss=1.1733, Entropy=0.2684, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7986
SAC Update 4/5: Actor Loss=-0.1993, Q1 Loss=4.1743, Q2 Loss=4.1743, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.5365
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.3518, Q2 Loss=1.3518, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2459

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.1%)
Q1 update: 0.05s (18.9%)
Q2 update: 0.06s (20.5%)
Actor update: 0.12s (41.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.165632
Q1 loss: 1.774045
Q2 loss: 1.774045
Current threshold: -36.0649
Global Scale Offset: 0.0341
Reward stats: mean=-0.0029, std=0.1019, count=451
----------------------------------------------
SAC Update - Actor Loss: -0.1656, Q1 Loss: 1.7740, Q2 Loss: 1.7740, Entropy: 0.0537, Mean TD Error: 1.7755, Threshold: -36.0649
tensor([ 0.0789,  0.4707,  0.6987,  0.5721, -0.2122,  0.5644,  0.9938,  0.9827,
         1.2524,  0.3195,  0.2197,  1.2186, -0.0519,  0.0241, -0.2804,  2.3048],
       device='cuda:0')
Original likelihood: -30.998828887939453
Adjusted likelihood: -30.998828887939453
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.708219344029203
Current ori: tensor([-0.0519,  0.0241, -0.2804], device='cuda:0')
Middle force: tensor([0.5303, 0.5228, 0.5202, 0.5678, 0.5561, 0.5542, 0.5533, 0.6478],
       device='cuda:0')
Thumb force: tensor([0.5688, 0.5260, 0.6598, 0.9174, 0.8481, 0.6199, 0.7381, 0.6114],
       device='cuda:0')
Index force: tensor([0.5394, 0.5276, 0.5497, 0.5265, 0.5285, 0.5660, 0.5471, 0.5416],
       device='cuda:0')
Storing NORMAL transition: reward=0.0072 (scaled=0.0072), steps=1
Reward stats updated: mean -0.0029 -> -0.0029, std: 0.1018
Collected 452 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.8375, Q2 Loss=0.8375, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2082
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7373, Q2 Loss=0.7373, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4917
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2905, Q2 Loss=1.2905, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9641
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9130, Q2 Loss=0.9130, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8481
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2385, Q2 Loss=1.2385, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2726

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.3%)
Q1 update: 0.06s (20.2%)
Q2 update: 0.05s (18.9%)
Actor update: 0.11s (39.8%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.003355
Q2 loss: 1.003355
Current threshold: -36.0740
Global Scale Offset: 0.0341
Reward stats: mean=-0.0029, std=0.1018, count=452
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0034, Q2 Loss: 1.0034, Entropy: 0.0000, Mean TD Error: 0.7569, Threshold: -36.0740
tensor([ 0.0811,  0.4399,  0.7212,  0.6105, -0.2179,  0.5661,  0.9727,  1.0244,
         1.2669,  0.2999,  0.2129,  1.2294, -0.0462,  0.0272, -0.2870,  2.4264],
       device='cuda:0')
Original likelihood: -31.808670043945312
Adjusted likelihood: -31.808670043945312
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.500321984000038
Current ori: tensor([-0.0462,  0.0272, -0.2870], device='cuda:0')
Middle force: tensor([0.9011, 0.5019, 1.1604, 0.5047, 0.6483, 0.5011, 0.5861],
       device='cuda:0')
Thumb force: tensor([0.8726, 1.0607, 0.7324, 0.5647, 0.5603, 0.5952, 0.5027],
       device='cuda:0')
Index force: tensor([0.6065, 0.6357, 0.5131, 0.5378, 0.5499, 0.6132, 0.5225],
       device='cuda:0')
Storing NORMAL transition: reward=0.0482 (scaled=0.0482), steps=1
Reward stats updated: mean -0.0029 -> -0.0028, std: 0.1017
Collected 453 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.4494, Q2 Loss=2.4494, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3511
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.9376, Q2 Loss=0.9376, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0059
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8002, Q2 Loss=0.8002, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9658
SAC Update 4/5: Actor Loss=-0.0397, Q1 Loss=1.2056, Q2 Loss=1.2056, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4634
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.8005, Q2 Loss=0.8005, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6984

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.2%)
Q1 update: 0.04s (17.4%)
Q2 update: 0.05s (20.1%)
Actor update: 0.11s (43.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.053991
Q1 loss: 1.238647
Q2 loss: 1.238647
Current threshold: -36.0794
Global Scale Offset: 0.0340
Reward stats: mean=-0.0028, std=0.1017, count=453
----------------------------------------------
SAC Update - Actor Loss: -0.0540, Q1 Loss: 1.2386, Q2 Loss: 1.2386, Entropy: 0.0000, Mean TD Error: 1.4969, Threshold: -36.0794
tensor([ 0.0028,  0.4131,  0.7091,  0.5728, -0.2568,  0.5768,  0.8847,  1.2193,
         1.3300,  0.2390,  0.2220,  1.2154, -0.0406,  0.0531, -0.3401,  2.6015],
       device='cuda:0')
Original likelihood: -32.31998825073242
Adjusted likelihood: -32.31998825073242
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.402296814951114
Current ori: tensor([-0.0406,  0.0531, -0.3401], device='cuda:0')
Middle force: tensor([0.5176, 0.5653, 0.5530, 0.5509, 0.5500, 0.6415], device='cuda:0')
Thumb force: tensor([0.6409, 0.8711, 0.8434, 0.6086, 0.7181, 0.6000], device='cuda:0')
Index force: tensor([0.5437, 0.5232, 0.5235, 0.5594, 0.5391, 0.5346], device='cuda:0')
Storing NORMAL transition: reward=0.0401 (scaled=0.0401), steps=1
Reward stats updated: mean -0.0028 -> -0.0027, std: 0.1016
Collected 454 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7687, Q2 Loss=0.7687, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5727
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.7216, Q2 Loss=0.7216, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4495
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.9972, Q2 Loss=1.9972, Entropy=0.0001, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.0054
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6886, Q2 Loss=0.6886, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1456
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3320, Q2 Loss=1.3320, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9486

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.6%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (19.5%)
Actor update: 0.09s (41.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.101620
Q2 loss: 1.101620
Current threshold: -36.0826
Global Scale Offset: 0.0340
Reward stats: mean=-0.0027, std=0.1016, count=454
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1016, Q2 Loss: 1.1016, Entropy: 0.0000, Mean TD Error: 1.2244, Threshold: -36.0826
tensor([ 0.0550,  0.4042,  0.7091,  0.6896, -0.3397,  0.4632,  1.0246,  1.2914,
         1.3264,  0.2645,  0.3086,  1.2195,  0.0076,  0.0932, -0.3957,  2.3067],
       device='cuda:0')
Original likelihood: -58.82987594604492
Adjusted likelihood: -58.82987594604492
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 59.05901336669922
Projection step: 1, Loss: 58.8013801574707
Projection step: 2, Loss: 56.203311920166016
Projection step: 3, Loss: 57.65937042236328
Projection step: 4, Loss: 56.561614990234375
Projection step: 5, Loss: 55.30615997314453
Projection step: 6, Loss: 58.80258560180664
Projection step: 7, Loss: 52.11543273925781
Projection step: 8, Loss: 53.574440002441406
Projection step: 9, Loss: 56.05650329589844
Projection step: 10, Loss: 54.627830505371094
Projection step: 11, Loss: 53.93225860595703
Projection step: 12, Loss: 50.79753112792969
Projection step: 13, Loss: 54.468170166015625
Projection step: 14, Loss: 49.06687545776367
Projection step: 15, Loss: 50.566314697265625
Projection step: 16, Loss: 50.71250915527344
Projection step: 17, Loss: 50.91313171386719
Projection step: 18, Loss: 49.56649398803711
Projection step: 19, Loss: 49.348236083984375
Projection step: 20, Loss: 52.503257751464844
Projection step: 21, Loss: 47.733787536621094
Projection step: 22, Loss: 50.210243225097656
Projection step: 23, Loss: 50.57159423828125
Projection step: 24, Loss: 48.632171630859375
Final likelihood: tensor([-49.7536, -49.3375, -41.0210, -51.6250, -40.9356, -49.9202, -48.8777,
        -40.1626, -52.2529, -40.0483, -49.5683, -50.0962, -51.3130, -49.0399,
        -52.1798, -49.8947])
Final projection likelihood: -47.8766
1 mode projection failed, trying anyway
New goal: tensor([ 0.0398,  0.4021,  0.6618,  0.6871, -0.2887,  0.4799,  0.8524,  1.1713,
         1.3122,  0.3144,  0.2832,  1.1740,  0.0028,  0.0887,  0.0070],
       device='cuda:0')
tensor([[0.0021]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -44.66271209716797
Adjusted likelihood: -44.66271209716797
Likelihood residual: 0.0
Original likelihood: -38.94182586669922
Adjusted likelihood: -38.94182586669922
Likelihood residual: 0.0
{'index': 38.94182586669922, 'thumb_middle': 44.66271209716797}
Current yaw: tensor([ 0.0076,  0.0932, -0.3957], device='cuda:0')
2 index
tensor([ 0.0550,  0.4042,  0.7091,  0.6896, -0.3397,  0.4632,  1.0246,  1.2914,
         1.3264,  0.2645,  0.3086,  1.2195,  0.0076,  0.0932, -0.3957,  2.3067],
       device='cuda:0')
Solve time for step 1 10.322760662005749
Current ori: tensor([ 0.0076,  0.0932, -0.3957], device='cuda:0')
Middle force: tensor([0.5589, 0.5904, 0.5536, 0.5846], device='cuda:0')
Thumb force: tensor([0.5753, 0.5792, 0.5883, 0.5860], device='cuda:0')
tensor([ 0.0709,  0.3550,  0.6262,  0.6632, -0.3164,  0.5034,  0.9765,  1.2698,
         1.3214,  0.2842,  0.3029,  1.1691, -0.0243,  0.0903, -0.4318,  5.0498],
       device='cuda:0')
Solve time for step 2 4.202292456990108
Current ori: tensor([-0.0243,  0.0903, -0.4318], device='cuda:0')
Middle force: tensor([0.5874, 0.5510, 0.5817], device='cuda:0')
Thumb force: tensor([0.5742, 0.5841, 0.5822], device='cuda:0')
tensor([ 0.0702,  0.3608,  0.6173,  0.6607, -0.3079,  0.5211,  0.9581,  1.2556,
         1.3290,  0.2879,  0.2870,  1.1721, -0.0421,  0.0916, -0.4683, -5.4733],
       device='cuda:0')
Solve time for step 3 4.1081664669909514
Current ori: tensor([-0.0421,  0.0916, -0.4683], device='cuda:0')
Middle force: tensor([0.5051, 0.5628], device='cuda:0')
Thumb force: tensor([0.5608, 0.5940], device='cuda:0')
tensor([ 0.0690,  0.3613,  0.6158,  0.6603, -0.3040,  0.5357,  0.9401,  1.2374,
         1.3285,  0.3088,  0.2925,  1.1761, -0.0603,  0.0975, -0.5082, -4.0640],
       device='cuda:0')
Solve time for step 4 4.027452283015009
Current ori: tensor([-0.0603,  0.0975, -0.5082], device='cuda:0')
Middle force: tensor([0.5128], device='cuda:0')
Thumb force: tensor([0.5763], device='cuda:0')
Storing RECOVERY transition: reward=0.0497 (scaled=0.0071), steps=7
Reward stats updated: mean -0.0027 -> -0.0026, std: 0.1015
Collected 455 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1333, Q2 Loss=1.1333, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5879
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0207, Q2 Loss=1.0207, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3220
SAC Update 3/5: Actor Loss=-0.1343, Q1 Loss=1.2606, Q2 Loss=1.2606, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0813
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6943, Q2 Loss=1.6943, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9543
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2696, Q2 Loss=1.2696, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5725

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.2%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.04s (18.6%)
Actor update: 0.09s (38.9%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.026851
Q1 loss: 1.275685
Q2 loss: 1.275685
Current threshold: -36.1048
Global Scale Offset: 0.0336
Reward stats: mean=-0.0026, std=0.1015, count=455
----------------------------------------------
SAC Update - Actor Loss: -0.0269, Q1 Loss: 1.2757, Q2 Loss: 1.2757, Entropy: 0.0000, Mean TD Error: 1.1036, Threshold: -36.1048
Original likelihood: -44.370094299316406
Adjusted likelihood: -44.370094299316406
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 43.394317626953125
Projection step: 1, Loss: 40.54018783569336
Projection step: 2, Loss: 39.781005859375
Projection step: 3, Loss: 40.776275634765625
Projection step: 4, Loss: 38.239471435546875
Projection step: 5, Loss: 40.71871566772461
Projection step: 6, Loss: 36.17250061035156
Projection step: 7, Loss: 38.64195251464844
Projection step: 8, Loss: 37.604454040527344
Projection step: 9, Loss: 39.92351531982422
Projection step: 10, Loss: 39.64856719970703
Projection step: 11, Loss: 38.89437484741211
Projection step: 12, Loss: 39.15332794189453
Projection step: 13, Loss: 37.16716766357422
Projection step: 14, Loss: 37.52459716796875
Projection step: 15, Loss: 38.1640510559082
Projection step: 16, Loss: 36.860595703125
Projection step: 17, Loss: 35.86249542236328
Projection step: 18, Loss: 37.034000396728516
Projection step: 19, Loss: 35.994380950927734
Projection step: 20, Loss: 36.23747634887695
Projection step: 21, Loss: 35.935691833496094
Projection step: 22, Loss: 35.691009521484375
Projection step: 23, Loss: 33.972633361816406
Projection step: 24, Loss: 32.573143005371094
Final likelihood: tensor([-44.2046, -34.6097, -39.1923, -31.1626, -52.6237, -31.5061, -32.6144,
        -38.6201, -33.4880, -33.7793, -37.8473, -30.0006, -47.7109, -29.8688,
        -34.2161, -37.8428])
Final projection likelihood: -36.8305
1 mode projection failed, trying anyway
New goal: tensor([ 0.0119,  0.4483,  0.6288,  0.6849, -0.2771,  0.5132,  0.7978,  1.0728,
         1.3390,  0.2972,  0.2625,  1.1266, -0.0755,  0.1033, -0.2422],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0016]], device='cuda:0')
Original likelihood: -34.57175064086914
Adjusted likelihood: -34.57175064086914
Likelihood residual: 0.0
Original likelihood: -40.34747314453125
Adjusted likelihood: -40.34747314453125
Likelihood residual: 0.0
{'index': 40.34747314453125, 'thumb_middle': 34.57175064086914}
Current yaw: tensor([-0.0764,  0.1077, -0.5384], device='cuda:0')
3 thumb_middle
tensor([ 0.0165,  0.4266,  0.6617,  0.6856, -0.3061,  0.5456,  0.9272,  1.2194,
         1.3471,  0.3046,  0.2873,  1.1991, -0.0764,  0.1077, -0.5384, -3.7391],
       device='cuda:0')
Solve time for step 1 8.84035774297081
Current ori: tensor([-0.0764,  0.1077, -0.5384], device='cuda:0')
Index force: tensor([0.5017, 0.5019, 0.6018, 0.5994], device='cuda:0')
tensor([-0.0141,  0.4860,  0.6405,  0.7191, -0.3290,  0.5544,  0.8260,  1.0909,
         1.2966,  0.2673,  0.2359,  1.1368, -0.1417,  0.1853, -0.5585, -3.4463],
       device='cuda:0')
Solve time for step 2 3.6875329129979946
Current ori: tensor([-0.1417,  0.1853, -0.5585], device='cuda:0')
Index force: tensor([0.5017, 0.5943, 0.5943], device='cuda:0')
tensor([-0.0263,  0.5087,  0.6878,  0.6739, -0.3355,  0.5579,  0.8123,  1.0744,
         1.3447,  0.2850,  0.2553,  1.1405, -0.3449,  0.4383, -0.5621, -4.4656],
       device='cuda:0')
Solve time for step 3 3.376667758973781
Current ori: tensor([-0.3449,  0.4383, -0.5621], device='cuda:0')
Index force: tensor([0.5322, 0.5000], device='cuda:0')
tensor([-0.2339,  0.6640,  0.7578,  0.7411, -0.3418,  0.5470,  0.8146,  1.0675,
         1.4132,  0.3148,  0.2978,  1.1449, -0.6993,  0.9332, -0.5620, -5.0735],
       device='cuda:0')
Solve time for step 4 3.449904140958097
Current ori: tensor([-0.6993,  0.9332, -0.5620], device='cuda:0')
Index force: tensor([0.5003], device='cuda:0')
Storing RECOVERY transition: reward=-1.5971 (scaled=-0.2282), steps=7
Reward stats updated: mean -0.0026 -> -0.0031, std: 0.1019
Collected 456 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3267, Q2 Loss=1.3267, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3216
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.0702, Q2 Loss=2.0702, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3463
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1261, Q2 Loss=1.1261, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9979
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.4062, Q2 Loss=1.4062, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4110
SAC Update 5/5: Actor Loss=-0.3446, Q1 Loss=2.6376, Q2 Loss=2.6376, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1551

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.9%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (19.1%)
Actor update: 0.10s (40.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.114968
Q1 loss: 1.713363
Q2 loss: 1.713363
Current threshold: -36.1283
Global Scale Offset: 0.0332
Reward stats: mean=-0.0031, std=0.1019, count=456
----------------------------------------------
SAC Update - Actor Loss: -0.1150, Q1 Loss: 1.7134, Q2 Loss: 1.7134, Entropy: 0.0000, Mean TD Error: 1.8464, Threshold: -36.1283
Original likelihood: -1111.643798828125
Adjusted likelihood: -1111.643798828125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 29
Loaded trajectory sampler
Current yaw: tensor([ 9.2809e-05,  1.3551e-02, -4.8036e-02], device='cuda:0')
Current yaw: tensor([ 9.2809e-05,  1.3551e-02, -4.8036e-02], device='cuda:0')
1 turn
Sampling time 3.6653192149824463
tensor([ 1.4393e-01,  5.8951e-01,  5.7280e-01,  6.4078e-01, -1.5087e-01,
         5.7930e-01,  8.8586e-01,  9.2268e-01,  1.2514e+00,  2.6514e-01,
         2.4362e-01,  1.1728e+00,  9.2809e-05,  1.3551e-02, -4.8036e-02,
         2.5971e-01], device='cuda:0')
Original likelihood: -21.2497615814209
Adjusted likelihood: -21.2497615814209
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.386299896985292
Current ori: tensor([ 9.2809e-05,  1.3551e-02, -4.8036e-02], device='cuda:0')
Middle force: tensor([1.8158, 0.5916, 0.5161, 1.0667, 0.5221, 0.5282, 0.5581, 0.5479, 0.5577,
        0.5296, 0.4946, 0.5202], device='cuda:0')
Thumb force: tensor([1.0332, 0.6588, 1.5229, 1.1287, 0.6877, 1.0841, 0.5744, 0.5818, 1.6343,
        0.6663, 0.6151, 0.6778], device='cuda:0')
Index force: tensor([0.6345, 0.5243, 0.5613, 0.5045, 0.5445, 0.5318, 0.5943, 0.6676, 0.8359,
        0.5719, 0.6982, 0.6461], device='cuda:0')
Storing NORMAL transition: reward=0.1240 (scaled=0.1240), steps=1
Reward stats updated: mean -0.0031 -> -0.0029, std: 0.1020
Collected 457 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8969, Q2 Loss=0.8969, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4484
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4065, Q2 Loss=1.4065, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9219
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.1815, Q2 Loss=2.1815, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6040
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6537, Q2 Loss=0.6537, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2477
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=3.1864, Q2 Loss=3.1864, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1568

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.3%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (18.2%)
Actor update: 0.08s (40.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.664990
Q2 loss: 1.664990
Current threshold: -36.1422
Global Scale Offset: 0.0329
Reward stats: mean=-0.0029, std=0.1020, count=457
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.6650, Q2 Loss: 1.6650, Entropy: 0.0000, Mean TD Error: 1.2758, Threshold: -36.1422
tensor([ 0.2094,  0.6087,  0.6316,  0.6092, -0.0738,  0.5980,  0.9493,  1.0993,
         1.1994,  0.1817,  0.2167,  1.1272,  0.0326, -0.0764, -0.1806,  1.0441],
       device='cuda:0')
Original likelihood: -35.40715026855469
Adjusted likelihood: -35.40715026855469
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.632389403996058
Current ori: tensor([ 0.0326, -0.0764, -0.1806], device='cuda:0')
Middle force: tensor([0.6983, 0.5359, 0.5184, 0.6360, 0.9795, 1.0206, 0.6039, 0.5016, 0.5062,
        0.5215, 0.5195], device='cuda:0')
Thumb force: tensor([2.3761, 0.6019, 1.4941, 1.0410, 0.8782, 1.9326, 0.5710, 0.6537, 0.5904,
        0.5729, 0.5917], device='cuda:0')
Index force: tensor([0.5002, 0.5921, 0.5570, 0.5642, 0.5121, 0.5677, 0.6024, 0.6618, 0.6013,
        0.6661, 0.6846], device='cuda:0')
Storing NORMAL transition: reward=-0.0269 (scaled=-0.0269), steps=1
Reward stats updated: mean -0.0029 -> -0.0029, std: 0.1019
Collected 458 transitions for RL
SAC Update 1/5: Actor Loss=-0.0001, Q1 Loss=1.1454, Q2 Loss=1.1454, Entropy=0.0423, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7574
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6317, Q2 Loss=1.6317, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3932
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7418, Q2 Loss=0.7418, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4364
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2543, Q2 Loss=1.2543, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9452
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1461, Q2 Loss=1.1461, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3914

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.1%)
Q1 update: 0.04s (18.5%)
Q2 update: 0.04s (19.1%)
Actor update: 0.09s (41.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000021
Q1 loss: 1.183882
Q2 loss: 1.183882
Current threshold: -36.1507
Global Scale Offset: 0.0327
Reward stats: mean=-0.0029, std=0.1019, count=458
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.1839, Q2 Loss: 1.1839, Entropy: 0.0085, Mean TD Error: 0.7847, Threshold: -36.1507
tensor([ 0.3097,  0.5156,  0.4983,  0.5985, -0.1184,  0.5663,  0.9879,  0.9488,
         1.3682, -0.0253,  0.2553,  1.0004,  0.0042, -0.0352, -0.1464,  0.3610],
       device='cuda:0')
Original likelihood: -35.90769958496094
Adjusted likelihood: -35.90769958496094
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9449)
Solve time for step 3 5.246842872991692
Current ori: tensor([ 0.0042, -0.0352, -0.1464], device='cuda:0')
Middle force: tensor([0.5195, 1.0414, 0.5251, 0.5286, 0.5537, 0.5477, 0.5685, 0.5367, 0.5074,
        0.5229], device='cuda:0')
Thumb force: tensor([1.4507, 1.0690, 0.6485, 1.0162, 0.5664, 0.5587, 1.5340, 0.6141, 0.5720,
        0.6370], device='cuda:0')
Index force: tensor([0.5446, 0.5041, 0.5363, 0.5287, 0.5854, 0.6654, 0.8299, 0.5687, 0.6976,
        0.6404], device='cuda:0')
Storing NORMAL transition: reward=0.0053 (scaled=0.0053), steps=1
Reward stats updated: mean -0.0029 -> -0.0029, std: 0.1018
Collected 459 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6193, Q2 Loss=1.6193, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7750
SAC Update 2/5: Actor Loss=-0.2278, Q1 Loss=1.2925, Q2 Loss=1.2925, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3559
SAC Update 3/5: Actor Loss=-0.2658, Q1 Loss=1.0338, Q2 Loss=1.0338, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1026
SAC Update 4/5: Actor Loss=-0.0154, Q1 Loss=1.0495, Q2 Loss=1.0495, Entropy=0.1381, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4034
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2337, Q2 Loss=1.2337, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6948

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.0%)
Q1 update: 0.04s (18.4%)
Q2 update: 0.04s (18.2%)
Actor update: 0.08s (40.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.101795
Q1 loss: 1.245738
Q2 loss: 1.245738
Current threshold: -36.1607
Global Scale Offset: 0.0326
Reward stats: mean=-0.0029, std=0.1018, count=459
----------------------------------------------
SAC Update - Actor Loss: -0.1018, Q1 Loss: 1.2457, Q2 Loss: 1.2457, Entropy: 0.0276, Mean TD Error: 1.0663, Threshold: -36.1607
tensor([ 0.1827,  0.5693,  0.6116,  0.7002, -0.1232,  0.6093,  0.9283,  0.9216,
         1.3029,  0.1259,  0.2624,  0.9658, -0.0235, -0.0288, -0.1519,  0.1211],
       device='cuda:0')
Original likelihood: -30.669986724853516
Adjusted likelihood: -30.669986724853516
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.7522165550035425
Current ori: tensor([-0.0235, -0.0288, -0.1519], device='cuda:0')
Middle force: tensor([1.0150, 0.5218, 0.5266, 0.5509, 0.5455, 0.5573, 0.5323, 0.5055, 0.5208],
       device='cuda:0')
Thumb force: tensor([1.0569, 0.6473, 0.9969, 0.5625, 0.5550, 1.5314, 0.6144, 0.5813, 0.6356],
       device='cuda:0')
Index force: tensor([0.5036, 0.5338, 0.5270, 0.5811, 0.6586, 0.8129, 0.5652, 0.6939, 0.6344],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0528 (scaled=-0.0528), steps=1
Reward stats updated: mean -0.0029 -> -0.0030, std: 0.1017
Collected 460 transitions for RL
SAC Update 1/5: Actor Loss=-0.0002, Q1 Loss=0.9400, Q2 Loss=0.9400, Entropy=0.0560, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5271
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0535, Q2 Loss=1.0535, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9304
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.5371, Q2 Loss=2.5371, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3279
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4745, Q2 Loss=1.4745, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8052
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.6568, Q2 Loss=1.6568, Entropy=0.0045, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8515

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.4%)
Q1 update: 0.05s (18.6%)
Q2 update: 0.05s (17.9%)
Actor update: 0.10s (37.2%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000045
Q1 loss: 1.532387
Q2 loss: 1.532387
Current threshold: -36.1729
Global Scale Offset: 0.0325
Reward stats: mean=-0.0030, std=0.1017, count=460
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.5324, Q2 Loss: 1.5324, Entropy: 0.0121, Mean TD Error: 2.0884, Threshold: -36.1729
tensor([ 0.1610,  0.5402,  0.6171,  0.7212, -0.1279,  0.6260,  0.8978,  0.9159,
         1.2888,  0.1471,  0.2754,  0.9754, -0.0232, -0.0244, -0.0986,  0.0959],
       device='cuda:0')
Original likelihood: -26.86968994140625
Adjusted likelihood: -26.86968994140625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.623159795009997
Current ori: tensor([-0.0232, -0.0244, -0.0986], device='cuda:0')
Middle force: tensor([0.5198, 0.5267, 0.5489, 0.5447, 0.5540, 0.5322, 0.5052, 0.5213],
       device='cuda:0')
Thumb force: tensor([0.6362, 0.9689, 0.5586, 0.5503, 1.5076, 0.6049, 0.5750, 0.6247],
       device='cuda:0')
Index force: tensor([0.5314, 0.5249, 0.5765, 0.6509, 0.8006, 0.5618, 0.6888, 0.6280],
       device='cuda:0')
Storing NORMAL transition: reward=0.0118 (scaled=0.0118), steps=1
Reward stats updated: mean -0.0030 -> -0.0030, std: 0.1016
Collected 461 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.1590, Q2 Loss=1.1590, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6734
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4774, Q2 Loss=1.4774, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4380
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.9417, Q2 Loss=1.9417, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3470
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2292, Q2 Loss=1.2292, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9668
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5568, Q2 Loss=1.5568, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0960

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.4%)
Q1 update: 0.04s (17.8%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (40.3%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.472809
Q2 loss: 1.472809
Current threshold: -36.1801
Global Scale Offset: 0.0325
Reward stats: mean=-0.0030, std=0.1016, count=461
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.4728, Q2 Loss: 1.4728, Entropy: 0.0000, Mean TD Error: 1.5042, Threshold: -36.1801
tensor([ 0.1394,  0.5115,  0.6227,  0.7429, -0.1432,  0.6132,  0.8949,  0.9289,
         1.2939,  0.1467,  0.3156,  0.9337, -0.0155, -0.0116, -0.1096,  0.0705],
       device='cuda:0')
Original likelihood: -27.58210563659668
Adjusted likelihood: -27.58210563659668
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.32373797299806
Current ori: tensor([-0.0155, -0.0116, -0.1096], device='cuda:0')
Middle force: tensor([0.5228, 0.5474, 0.5424, 0.5513, 0.5305, 0.5046, 0.5204],
       device='cuda:0')
Thumb force: tensor([0.9505, 0.5553, 0.5477, 1.4843, 0.6004, 0.5724, 0.6194],
       device='cuda:0')
Index force: tensor([0.5247, 0.5726, 0.6458, 0.7891, 0.5595, 0.6838, 0.6234],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0246 (scaled=-0.0246), steps=1
Reward stats updated: mean -0.0030 -> -0.0030, std: 0.1015
Collected 462 transitions for RL
SAC Update 1/5: Actor Loss=-0.0001, Q1 Loss=0.9387, Q2 Loss=0.9387, Entropy=0.0773, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5957
SAC Update 2/5: Actor Loss=-0.1076, Q1 Loss=1.0769, Q2 Loss=1.0769, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9526
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.9369, Q2 Loss=0.9369, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1184
SAC Update 4/5: Actor Loss=-0.2117, Q1 Loss=1.9267, Q2 Loss=1.9267, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7268
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.8787, Q2 Loss=0.8787, Entropy=0.0056, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4856

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.4%)
Q1 update: 0.04s (19.6%)
Q2 update: 0.04s (18.9%)
Actor update: 0.09s (39.3%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.109917
Q1 loss: 1.151611
Q2 loss: 1.151611
Current threshold: -36.1846
Global Scale Offset: 0.0324
Reward stats: mean=-0.0030, std=0.1015, count=462
----------------------------------------------
SAC Update - Actor Loss: -0.1099, Q1 Loss: 1.1516, Q2 Loss: 1.1516, Entropy: 0.0166, Mean TD Error: 1.3758, Threshold: -36.1846
tensor([ 0.0776,  0.3832,  0.6965,  0.8241, -0.1127,  0.4819,  0.8869,  0.9418,
         1.3286,  0.0520,  0.3268,  0.8868,  0.0219,  0.0225, -0.0856,  0.0166],
       device='cuda:0')
Original likelihood: -25.520992279052734
Adjusted likelihood: -25.520992279052734
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.412980128021445
Current ori: tensor([ 0.0219,  0.0225, -0.0856], device='cuda:0')
Middle force: tensor([0.5433, 0.5400, 0.5476, 0.5271, 0.5039, 0.5188], device='cuda:0')
Thumb force: tensor([0.5522, 0.5418, 1.4658, 0.5993, 0.5663, 0.6146], device='cuda:0')
Index force: tensor([0.5729, 0.6493, 0.7761, 0.5585, 0.6914, 0.6216], device='cuda:0')
Storing NORMAL transition: reward=0.0291 (scaled=0.0291), steps=1
Reward stats updated: mean -0.0030 -> -0.0029, std: 0.1014
Collected 463 transitions for RL
SAC Update 1/5: Actor Loss=-0.2301, Q1 Loss=1.8191, Q2 Loss=1.8191, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5197
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5093, Q2 Loss=1.5093, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3699
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.5722, Q2 Loss=2.5722, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4246
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3166, Q2 Loss=1.3166, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3359
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7511, Q2 Loss=0.7511, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7111

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.1%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (20.1%)
Actor update: 0.11s (41.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046024
Q1 loss: 1.593654
Q2 loss: 1.593654
Current threshold: -36.1883
Global Scale Offset: 0.0324
Reward stats: mean=-0.0029, std=0.1014, count=463
----------------------------------------------
SAC Update - Actor Loss: -0.0460, Q1 Loss: 1.5937, Q2 Loss: 1.5937, Entropy: 0.0000, Mean TD Error: 1.6722, Threshold: -36.1883
tensor([ 0.1054,  0.4112,  0.6849,  0.8233, -0.0887,  0.5031,  0.8765,  0.9454,
         1.3687,  0.0648,  0.3661,  0.8299,  0.0152,  0.0071, -0.1140,  0.0801],
       device='cuda:0')
Original likelihood: -14.376825332641602
Adjusted likelihood: -14.376825332641602
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.19693587499205
Current ori: tensor([ 0.0152,  0.0071, -0.1140], device='cuda:0')
Middle force: tensor([0.5385, 0.5444, 0.5258, 0.5034, 0.5180], device='cuda:0')
Thumb force: tensor([0.5400, 1.4479, 0.5960, 0.5646, 0.6099], device='cuda:0')
Index force: tensor([0.6393, 0.7634, 0.5553, 0.6860, 0.6169], device='cuda:0')
Storing NORMAL transition: reward=-0.0503 (scaled=-0.0503), steps=1
Reward stats updated: mean -0.0029 -> -0.0030, std: 0.1013
Collected 464 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1236, Q2 Loss=1.1236, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3275
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=3.4693, Q2 Loss=3.4693, Entropy=0.0061, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6681
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.8905, Q2 Loss=1.8905, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6184
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8593, Q2 Loss=0.8593, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5926
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8325, Q2 Loss=0.8325, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5708

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.6%)
Q1 update: 0.05s (20.3%)
Q2 update: 0.05s (20.0%)
Actor update: 0.10s (38.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000002
Q1 loss: 1.635029
Q2 loss: 1.635029
Current threshold: -36.1905
Global Scale Offset: 0.0324
Reward stats: mean=-0.0030, std=0.1013, count=464
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.6350, Q2 Loss: 1.6350, Entropy: 0.0012, Mean TD Error: 1.5555, Threshold: -36.1905
tensor([ 0.0708,  0.4905,  0.5735,  0.7666, -0.1591,  0.5358,  0.8223,  0.9717,
         1.3004,  0.1845,  0.4611,  0.7993,  0.0222,  0.0387, -0.0653,  0.2138],
       device='cuda:0')
Original likelihood: -29.228984832763672
Adjusted likelihood: -29.228984832763672
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 4.021307768009137
Current ori: tensor([ 0.0222,  0.0387, -0.0653], device='cuda:0')
Middle force: tensor([0.5414, 0.5204, 0.5026, 0.5148], device='cuda:0')
Thumb force: tensor([1.4287, 0.6087, 0.5755, 0.6194], device='cuda:0')
Index force: tensor([0.7507, 0.5509, 0.6745, 0.6106], device='cuda:0')
Storing NORMAL transition: reward=-0.0223 (scaled=-0.0223), steps=1
Reward stats updated: mean -0.0030 -> -0.0031, std: 0.1012
Collected 465 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.1743, Q2 Loss=1.1743, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1437
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0067, Q2 Loss=1.0067, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7275
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7379, Q2 Loss=0.7379, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2913
SAC Update 4/5: Actor Loss=-0.0215, Q1 Loss=0.7383, Q2 Loss=0.7383, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9485
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8473, Q2 Loss=0.8473, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6595

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.0%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (18.8%)
Actor update: 0.12s (41.4%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.050358
Q1 loss: 0.900890
Q2 loss: 0.900890
Current threshold: -36.1918
Global Scale Offset: 0.0323
Reward stats: mean=-0.0031, std=0.1012, count=465
----------------------------------------------
SAC Update - Actor Loss: -0.0504, Q1 Loss: 0.9009, Q2 Loss: 0.9009, Entropy: 0.0000, Mean TD Error: 0.7541, Threshold: -36.1918
tensor([ 0.0398,  0.4305,  0.6284,  0.7695, -0.1634,  0.5362,  0.7937,  1.0420,
         1.1620,  0.1933,  0.4847,  0.6583,  0.0365,  0.0392, -0.0437, -0.1157],
       device='cuda:0')
Original likelihood: -32.45785140991211
Adjusted likelihood: -32.45785140991211
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 10 3.8181030150153674
Current ori: tensor([ 0.0365,  0.0392, -0.0437], device='cuda:0')
Middle force: tensor([0.5003, 0.6645, 0.5519], device='cuda:0')
Thumb force: tensor([0.7233, 1.0629, 0.5544], device='cuda:0')
Index force: tensor([0.6533, 0.5330, 0.5431], device='cuda:0')
Storing NORMAL transition: reward=-0.0521 (scaled=-0.0521), steps=1
Reward stats updated: mean -0.0031 -> -0.0032, std: 0.1011
Collected 466 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2373, Q2 Loss=1.2373, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5721
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2836, Q2 Loss=1.2836, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7496
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8747, Q2 Loss=0.8747, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9448
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=4.5046, Q2 Loss=4.5046, Entropy=0.0064, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.0700
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.7702, Q2 Loss=1.7702, Entropy=0.0009, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.9604

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.0%)
Q1 update: 0.04s (18.3%)
Q2 update: 0.04s (19.4%)
Actor update: 0.08s (40.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000003
Q1 loss: 1.934076
Q2 loss: 1.934076
Current threshold: -36.1926
Global Scale Offset: 0.0323
Reward stats: mean=-0.0032, std=0.1011, count=466
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.9341, Q2 Loss: 1.9341, Entropy: 0.0015, Mean TD Error: 2.2594, Threshold: -36.1926
tensor([ 0.0719,  0.3251,  0.7500,  0.8547, -0.1118,  0.5503,  0.7564,  1.1549,
         1.3040,  0.0737,  0.5813,  0.5892,  0.0625,  0.0082,  0.0077, -0.5144],
       device='cuda:0')
Original likelihood: -31.59494400024414
Adjusted likelihood: -31.59494400024414
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 11 4.049209034012165
Current ori: tensor([0.0625, 0.0082, 0.0077], device='cuda:0')
Middle force: tensor([0.6622, 0.5540], device='cuda:0')
Thumb force: tensor([1.0333, 0.5507], device='cuda:0')
Index force: tensor([0.5310, 0.5378], device='cuda:0')
Storing NORMAL transition: reward=0.0998 (scaled=0.0998), steps=1
Reward stats updated: mean -0.0032 -> -0.0030, std: 0.1011
Collected 467 transitions for RL
SAC Update 1/5: Actor Loss=-0.0036, Q1 Loss=0.7222, Q2 Loss=0.7222, Entropy=0.0941, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3021
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1563, Q2 Loss=1.1563, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9871
SAC Update 3/5: Actor Loss=-0.2587, Q1 Loss=1.0385, Q2 Loss=1.0385, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4045
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9378, Q2 Loss=0.9378, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4682
SAC Update 5/5: Actor Loss=-0.1142, Q1 Loss=1.0468, Q2 Loss=1.0468, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7223

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (1.1%)
Target Q: 0.05s (20.0%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (38.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.075310
Q1 loss: 0.980332
Q2 loss: 0.980332
Current threshold: -36.1959
Global Scale Offset: 0.0323
Reward stats: mean=-0.0030, std=0.1011, count=467
----------------------------------------------
SAC Update - Actor Loss: -0.0753, Q1 Loss: 0.9803, Q2 Loss: 0.9803, Entropy: 0.0188, Mean TD Error: 1.1769, Threshold: -36.1959
tensor([ 0.0699,  0.3097,  0.7552,  0.8751, -0.0848,  0.5427,  0.7941,  1.1525,
         1.2875,  0.1061,  0.6348,  0.4432,  0.0574, -0.0089, -0.0922, -0.8068],
       device='cuda:0')
Original likelihood: -25.438945770263672
Adjusted likelihood: -25.438945770263672
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 12 3.5327393409679644
Current ori: tensor([ 0.0574, -0.0089, -0.0922], device='cuda:0')
Middle force: tensor([0.5509], device='cuda:0')
Thumb force: tensor([0.5449], device='cuda:0')
Index force: tensor([0.5292], device='cuda:0')
Storing NORMAL transition: reward=0.0376 (scaled=0.0376), steps=1
Reward stats updated: mean -0.0030 -> -0.0029, std: 0.1010
Collected 468 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0284, Q2 Loss=1.0284, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4246
SAC Update 2/5: Actor Loss=-0.1147, Q1 Loss=1.0626, Q2 Loss=1.0626, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7769
SAC Update 3/5: Actor Loss=-0.0003, Q1 Loss=0.8011, Q2 Loss=0.8011, Entropy=0.0628, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3945
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8970, Q2 Loss=0.8970, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4152
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.9228, Q2 Loss=0.9228, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7180

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.3%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (19.1%)
Actor update: 0.09s (39.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.069057
Q1 loss: 0.942382
Q2 loss: 0.942382
Current threshold: -36.1976
Global Scale Offset: 0.0323
Reward stats: mean=-0.0029, std=0.1010, count=468
----------------------------------------------
SAC Update - Actor Loss: -0.0691, Q1 Loss: 0.9424, Q2 Loss: 0.9424, Entropy: 0.0126, Mean TD Error: 0.5458, Threshold: -36.1976
Original likelihood: -26.233617782592773
Adjusted likelihood: -26.233617782592773
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0605, -0.0099, -0.1306], device='cuda:0')
2 turn
Sampling time 3.6307080339756794
tensor([ 0.0732,  0.3042,  0.7563,  0.8913, -0.0878,  0.5276,  0.8115,  1.1694,
         1.3256,  0.0735,  0.6554,  0.3579,  0.0605, -0.0099, -0.1306, -0.7569],
       device='cuda:0')
Original likelihood: -24.43489646911621
Adjusted likelihood: -24.43489646911621
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.909675409027841
Current ori: tensor([ 0.0605, -0.0099, -0.1306], device='cuda:0')
Middle force: tensor([2.3833, 0.5120, 0.6695, 0.6076, 0.6799, 1.4218, 0.5561, 0.8688, 0.8080,
        1.0791, 0.5358, 0.5099], device='cuda:0')
Thumb force: tensor([1.4416, 0.5145, 1.4381, 0.5938, 0.8523, 0.7157, 1.1403, 0.6428, 0.5640,
        0.6054, 0.5219, 0.5377], device='cuda:0')
Index force: tensor([1.4970, 0.6817, 0.8197, 0.5994, 0.5344, 1.0274, 0.6352, 0.5279, 0.9748,
        0.5339, 0.8233, 0.6132], device='cuda:0')
Storing NORMAL transition: reward=0.0171 (scaled=0.0171), steps=1
Reward stats updated: mean -0.0029 -> -0.0028, std: 0.1009
Collected 469 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=1.9492, Q2 Loss=1.9492, Entropy=0.0009, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.0302
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0206, Q2 Loss=1.0206, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5001
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5978, Q2 Loss=1.5978, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2021
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4305, Q2 Loss=1.4305, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6065
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8331, Q2 Loss=0.8331, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4014

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.9%)
Target Q: 0.04s (16.4%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (19.5%)
Actor update: 0.10s (41.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.366231
Q2 loss: 1.366231
Current threshold: -36.1984
Global Scale Offset: 0.0323
Reward stats: mean=-0.0028, std=0.1009, count=469
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.3662, Q2 Loss: 1.3662, Entropy: 0.0002, Mean TD Error: 1.5480, Threshold: -36.1984
tensor([ 0.0535,  0.3634,  0.6959,  0.8288, -0.0735,  0.5495,  0.8259,  1.0480,
         1.2887,  0.1336,  0.7340,  0.1495,  0.0314, -0.0137, -0.1453, -0.8463],
       device='cuda:0')
Original likelihood: -20.23162078857422
Adjusted likelihood: -20.23162078857422
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.464042129984591
Current ori: tensor([ 0.0314, -0.0137, -0.1453], device='cuda:0')
Middle force: tensor([0.5108, 0.6751, 0.6042, 0.6730, 1.4220, 0.5620, 0.8693, 0.8346, 1.0657,
        0.5364, 0.5102], device='cuda:0')
Thumb force: tensor([0.5128, 1.4014, 0.5895, 0.8398, 0.6908, 1.1153, 0.6327, 0.5445, 0.6002,
        0.5183, 0.5335], device='cuda:0')
Index force: tensor([0.6746, 0.8010, 0.5946, 0.5322, 1.0046, 0.6188, 0.5256, 0.9622, 0.5316,
        0.8126, 0.6039], device='cuda:0')
Storing NORMAL transition: reward=-0.0983 (scaled=-0.0983), steps=1
Reward stats updated: mean -0.0028 -> -0.0030, std: 0.1009
Collected 470 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.1135, Q2 Loss=1.1135, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1045
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.9249, Q2 Loss=2.9249, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9010
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1365, Q2 Loss=1.1365, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5907
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.3181, Q2 Loss=1.3181, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7273
SAC Update 5/5: Actor Loss=-0.1278, Q1 Loss=1.2709, Q2 Loss=1.2709, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2498

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (19.1%)
Actor update: 0.10s (39.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.117662
Q1 loss: 1.552775
Q2 loss: 1.552775
Current threshold: -36.1989
Global Scale Offset: 0.0323
Reward stats: mean=-0.0030, std=0.1009, count=470
----------------------------------------------
SAC Update - Actor Loss: -0.1177, Q1 Loss: 1.5528, Q2 Loss: 1.5528, Entropy: 0.0000, Mean TD Error: 1.7147, Threshold: -36.1989
tensor([-8.8613e-04,  3.7100e-01,  6.4292e-01,  8.2773e-01, -7.1592e-02,
         6.1081e-01,  7.1275e-01,  8.8072e-01,  1.4463e+00, -1.0500e-01,
         7.9917e-01,  5.8099e-02,  2.9954e-02,  1.5701e-02, -4.6766e-02,
        -9.8084e-01], device='cuda:0')
Original likelihood: -18.303573608398438
Adjusted likelihood: -18.303573608398438
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.094793291005772
Current ori: tensor([ 0.0300,  0.0157, -0.0468], device='cuda:0')
Middle force: tensor([0.6633, 0.6050, 0.6767, 1.4375, 0.5512, 0.8381, 0.8213, 1.0376, 0.5318,
        0.5103], device='cuda:0')
Thumb force: tensor([1.3739, 0.5907, 0.8217, 0.6615, 1.0957, 0.6362, 0.5364, 0.6035, 0.5150,
        0.5300], device='cuda:0')
Index force: tensor([0.8063, 0.5907, 0.5286, 0.9836, 0.6291, 0.5265, 0.9688, 0.5305, 0.8240,
        0.5993], device='cuda:0')
Storing NORMAL transition: reward=0.0122 (scaled=0.0122), steps=1
Reward stats updated: mean -0.0030 -> -0.0030, std: 0.1008
Collected 471 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9902, Q2 Loss=0.9902, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8375
SAC Update 2/5: Actor Loss=-0.0415, Q1 Loss=0.8050, Q2 Loss=0.8050, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7720
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0359, Q2 Loss=1.0359, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2917
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9059, Q2 Loss=0.9059, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9417
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7179, Q2 Loss=0.7179, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7050

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.6%)
Actor update: 0.08s (40.2%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.054349
Q1 loss: 0.890971
Q2 loss: 0.890971
Current threshold: -36.1992
Global Scale Offset: 0.0323
Reward stats: mean=-0.0030, std=0.1008, count=471
----------------------------------------------
SAC Update - Actor Loss: -0.0543, Q1 Loss: 0.8910, Q2 Loss: 0.8910, Entropy: 0.0000, Mean TD Error: 1.1096, Threshold: -36.1992
tensor([-0.0504,  0.3815,  0.6427,  0.7721,  0.0216,  0.6213,  0.8063,  0.8810,
         1.2476,  0.0420,  0.7796,  0.1056,  0.0486, -0.0551, -0.0626, -0.5454],
       device='cuda:0')
Original likelihood: -28.55959701538086
Adjusted likelihood: -28.55959701538086
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.859506731037982
Current ori: tensor([ 0.0486, -0.0551, -0.0626], device='cuda:0')
Middle force: tensor([0.6026, 0.7836, 0.5554, 0.5039, 0.5035, 0.5010, 0.5750, 0.6352, 0.5005],
       device='cuda:0')
Thumb force: tensor([0.5015, 1.7229, 0.5068, 0.5499, 0.5106, 1.4458, 0.5199, 0.5232, 0.5066],
       device='cuda:0')
Index force: tensor([0.6509, 0.5375, 0.5528, 0.9473, 0.7221, 0.5359, 0.5151, 0.5129, 0.5394],
       device='cuda:0')
Storing NORMAL transition: reward=0.1003 (scaled=0.1003), steps=1
Reward stats updated: mean -0.0030 -> -0.0028, std: 0.1008
Collected 472 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6703, Q2 Loss=0.6703, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2693
SAC Update 2/5: Actor Loss=-0.1491, Q1 Loss=1.4086, Q2 Loss=1.4086, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3571
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1572, Q2 Loss=1.1572, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5855
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5261, Q2 Loss=1.5261, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1480
SAC Update 5/5: Actor Loss=-0.1144, Q1 Loss=2.5979, Q2 Loss=2.5979, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1831

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.5%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (19.0%)
Actor update: 0.09s (38.3%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.052700
Q1 loss: 1.472019
Q2 loss: 1.472019
Current threshold: -36.1994
Global Scale Offset: 0.0323
Reward stats: mean=-0.0028, std=0.1008, count=472
----------------------------------------------
SAC Update - Actor Loss: -0.0527, Q1 Loss: 1.4720, Q2 Loss: 1.4720, Entropy: 0.0000, Mean TD Error: 1.3086, Threshold: -36.1994
tensor([-0.0250,  0.3839,  0.6815,  0.8234,  0.0701,  0.6662,  0.7924,  0.9209,
         1.1473,  0.0967,  0.8887, -0.1045,  0.0592, -0.0918, -0.1705, -0.5627],
       device='cuda:0')
Original likelihood: -42.13567352294922
Adjusted likelihood: -42.13567352294922
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 42.55265808105469
Projection step: 1, Loss: 42.233795166015625
Projection step: 2, Loss: 41.397972106933594
Projection step: 3, Loss: 40.78108215332031
Projection step: 4, Loss: 40.38449478149414
Projection step: 5, Loss: 39.836326599121094
Projection step: 6, Loss: 38.962928771972656
Projection step: 7, Loss: 38.95348358154297
Projection step: 8, Loss: 38.709678649902344
Projection step: 9, Loss: 38.34440612792969
Projection step: 10, Loss: 37.778236389160156
Projection step: 11, Loss: 37.39039611816406
Projection step: 12, Loss: 36.61907196044922
Projection step: 13, Loss: 36.5394401550293
Projection step: 14, Loss: 36.00271224975586
Projection step: 15, Loss: 35.11786651611328
Projection step: 16, Loss: 35.03138732910156
Projection step: 17, Loss: 35.042381286621094
Projection step: 18, Loss: 34.53384017944336
Projection step: 19, Loss: 34.036895751953125
Projection step: 20, Loss: 34.103355407714844
Projection step: 21, Loss: 33.423866271972656
Projection step: 22, Loss: 32.90177536010742
Projection step: 23, Loss: 32.66388702392578
Projection step: 24, Loss: 32.03043746948242
Final likelihood: tensor([-32.4349, -31.7053, -32.4273, -31.0501, -32.4748, -31.4825, -32.5711,
        -32.1003, -32.4725, -31.8439, -31.8278, -32.1291, -32.5948, -31.8662,
        -31.6196, -30.9017])
Final projection likelihood: -31.9689
1 mode projection succeeded
New goal: tensor([-0.0062,  0.3296,  0.6972,  0.8777,  0.0457,  0.6194,  0.7698,  0.8717,
         1.2717,  0.1070,  0.7799, -0.0244,  0.0572, -0.0857,  0.3239],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[0.0021]], device='cuda:0')
Original likelihood: -36.77739334106445
Adjusted likelihood: -36.77739334106445
Likelihood residual: 0.0
Original likelihood: -42.70362091064453
Adjusted likelihood: -42.70362091064453
Likelihood residual: 0.0
{'index': 42.70362091064453, 'thumb_middle': 36.77739334106445}
Current yaw: tensor([ 0.0592, -0.0918, -0.1705], device='cuda:0')
3 thumb_middle
tensor([-0.0250,  0.3839,  0.6815,  0.8234,  0.0701,  0.6662,  0.7924,  0.9209,
         1.1473,  0.0967,  0.8887, -0.1045,  0.0592, -0.0918, -0.1705, -0.5627],
       device='cuda:0')
Solve time for step 1 9.000516350963153
Current ori: tensor([ 0.0592, -0.0918, -0.1705], device='cuda:0')
Index force: tensor([0.5991, 0.5861, 0.6008, 0.6004], device='cuda:0')
tensor([-0.0452,  0.3731,  0.7546,  0.9165,  0.0015,  0.6309,  0.7596,  0.8661,
         1.1920,  0.0815,  0.7039, -0.0712,  0.1186, -0.1857, -0.1979, -0.3793],
       device='cuda:0')
Solve time for step 2 4.011489857046399
Current ori: tensor([ 0.1186, -0.1857, -0.1979], device='cuda:0')
Index force: tensor([0.5764, 0.5939, 0.5939], device='cuda:0')
tensor([-0.0643,  0.4395,  0.8061,  0.9368,  0.0348,  0.6666,  0.7885,  0.8763,
         1.1756,  0.0603,  0.6479, -0.0640,  0.1654, -0.2293, -0.2905, -1.3436],
       device='cuda:0')
Solve time for step 3 3.507972098013852
Current ori: tensor([ 0.1654, -0.2293, -0.2905], device='cuda:0')
Index force: tensor([0.5012, 0.5591], device='cuda:0')
tensor([-0.0795,  0.4899,  0.8296,  0.9580,  0.0757,  0.6916,  0.7924,  0.8629,
         1.1700,  0.0594,  0.6384, -0.0460,  0.1910, -0.2413, -0.4293, -2.4141],
       device='cuda:0')
Solve time for step 4 3.3607910799910314
Current ori: tensor([ 0.1910, -0.2413, -0.4293], device='cuda:0')
Index force: tensor([0.5392], device='cuda:0')
Storing RECOVERY transition: reward=0.1102 (scaled=0.0275), steps=4
Reward stats updated: mean -0.0028 -> -0.0027, std: 0.1007
Collected 473 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1223, Q2 Loss=1.1223, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6753
SAC Update 2/5: Actor Loss=-0.3259, Q1 Loss=1.2318, Q2 Loss=1.2318, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4976
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1591, Q2 Loss=1.1591, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1812
SAC Update 4/5: Actor Loss=-0.0548, Q1 Loss=1.4460, Q2 Loss=1.4460, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6183
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0276, Q2 Loss=1.0276, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3982

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.5%)
Q1 update: 0.05s (16.7%)
Q2 update: 0.05s (16.7%)
Actor update: 0.09s (34.7%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.076131
Q1 loss: 1.197375
Q2 loss: 1.197375
Current threshold: -36.1995
Global Scale Offset: 0.0323
Reward stats: mean=-0.0027, std=0.1007, count=473
----------------------------------------------
SAC Update - Actor Loss: -0.0761, Q1 Loss: 1.1974, Q2 Loss: 1.1974, Entropy: 0.0000, Mean TD Error: 1.0741, Threshold: -36.1995
Original likelihood: -88.18092346191406
Adjusted likelihood: -88.18092346191406
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 74.94451904296875
Projection step: 1, Loss: 71.0623779296875
Projection step: 2, Loss: 70.1443862915039
Projection step: 3, Loss: 72.97201538085938
Projection step: 4, Loss: 72.4747314453125
Projection step: 5, Loss: 72.03900146484375
Projection step: 6, Loss: 73.5813980102539
Projection step: 7, Loss: 72.27142333984375
Projection step: 8, Loss: 72.51956176757812
Projection step: 9, Loss: 73.68923950195312
Projection step: 10, Loss: 73.37300109863281
Projection step: 11, Loss: 69.72694396972656
Projection step: 12, Loss: 67.10417938232422
Projection step: 13, Loss: 75.25835418701172
Projection step: 14, Loss: 69.3079833984375
Projection step: 15, Loss: 67.22947692871094
Projection step: 16, Loss: 67.63946533203125
Projection step: 17, Loss: 68.56160736083984
Projection step: 18, Loss: 71.17160034179688
Projection step: 19, Loss: 70.13645935058594
Projection step: 20, Loss: 74.84919738769531
Projection step: 21, Loss: 68.62388610839844
Projection step: 22, Loss: 68.91799926757812
Projection step: 23, Loss: 68.69891357421875
Projection step: 24, Loss: 66.7642822265625
Final likelihood: tensor([-68.6245, -62.8290, -68.7085, -70.1126, -68.0726, -66.0350, -58.7911,
        -59.0252, -62.2336, -62.6326, -59.3210, -97.6072, -70.6077, -64.1471,
        -63.8999, -63.6129])
Final projection likelihood: -66.6413
1 mode projection failed, trying anyway
New goal: tensor([ 0.0075,  0.5375,  0.8299,  0.9090,  0.1205,  0.7860,  0.8614,  0.9543,
         1.2450,  0.0578,  0.7509,  0.0391,  0.3041, -0.2034, -0.8062],
       device='cuda:0')
tensor([[0.0041]], device='cuda:0') tensor([[0.0250]], device='cuda:0') tensor([[0.0034]], device='cuda:0')
Original likelihood: -84.02210998535156
Adjusted likelihood: -84.02210998535156
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 84.02210998535156}
Current yaw: tensor([ 0.3060, -0.2057, -0.5409], device='cuda:0')
4 thumb_middle
tensor([-0.0144,  0.5833,  0.8997,  0.9772,  0.1361,  0.7953,  0.8529,  0.8915,
         1.2163,  0.0744,  0.7753, -0.0079,  0.3060, -0.2057, -0.5409, -2.0736],
       device='cuda:0')
Solve time for step 1 9.034315261989832
Current ori: tensor([ 0.3060, -0.2057, -0.5409], device='cuda:0')
Index force: tensor([0.5822, 0.5723, 0.5766, 0.5906], device='cuda:0')
tensor([-0.0102,  0.6668,  0.8486,  0.9246,  0.1581,  0.8697,  0.8880,  0.9464,
         1.1636,  0.0108,  0.6616,  0.0145,  0.3568, -0.2781, -0.3641, -1.8620],
       device='cuda:0')
Solve time for step 2 3.721750173950568
Current ori: tensor([ 0.3568, -0.2781, -0.3641], device='cuda:0')
Index force: tensor([0.5777, 0.5577, 0.5606], device='cuda:0')
tensor([ 0.0041,  0.7412,  0.8381,  0.9149,  0.1596,  0.9051,  0.9086,  0.9613,
         1.1191,  0.0536,  0.6547,  0.0183,  0.3607, -0.2949, -0.2692, -1.4265],
       device='cuda:0')
Solve time for step 3 3.50246543699177
Current ori: tensor([ 0.3607, -0.2949, -0.2692], device='cuda:0')
Index force: tensor([0.5518, 0.5536], device='cuda:0')
tensor([-0.0020,  0.7172,  0.8471,  0.9086,  0.1575,  0.8907,  0.9111,  0.9684,
         1.0847,  0.0820,  0.6693,  0.0251,  0.3643, -0.3047, -0.1960, -1.5796],
       device='cuda:0')
Solve time for step 4 3.3819810549612157
Current ori: tensor([ 0.3643, -0.3047, -0.1960], device='cuda:0')
Index force: tensor([0.5435], device='cuda:0')
Storing RECOVERY transition: reward=0.0073 (scaled=0.0018), steps=4
Reward stats updated: mean -0.0027 -> -0.0027, std: 0.1006
Collected 474 transitions for RL
SAC Update 1/5: Actor Loss=-0.0317, Q1 Loss=0.9616, Q2 Loss=0.9616, Entropy=0.0871, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7634
SAC Update 2/5: Actor Loss=-0.0953, Q1 Loss=1.0942, Q2 Loss=1.0942, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1408
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2364, Q2 Loss=1.2364, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1241
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4257, Q2 Loss=1.4257, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4245
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=4.5324, Q2 Loss=4.5324, Entropy=0.0098, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.0950

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.0%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.04s (17.4%)
Actor update: 0.11s (41.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.025408
Q1 loss: 1.850045
Q2 loss: 1.850045
Current threshold: -36.2205
Global Scale Offset: 0.0322
Reward stats: mean=-0.0027, std=0.1006, count=474
----------------------------------------------
SAC Update - Actor Loss: -0.0254, Q1 Loss: 1.8500, Q2 Loss: 1.8500, Entropy: 0.0194, Mean TD Error: 1.7096, Threshold: -36.2205
Original likelihood: -115.40605926513672
Adjusted likelihood: -115.40605926513672
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 142.58213806152344
Projection step: 1, Loss: 124.90847778320312
Projection step: 2, Loss: 145.65472412109375
Projection step: 3, Loss: 130.6805419921875
Projection step: 4, Loss: 131.88034057617188
Projection step: 5, Loss: 130.9537353515625
Projection step: 6, Loss: 133.27088928222656
Projection step: 7, Loss: 124.86170959472656
Projection step: 8, Loss: 135.2897491455078
Projection step: 9, Loss: 128.542236328125
Projection step: 10, Loss: 147.1829833984375
Projection step: 11, Loss: 138.91632080078125
Projection step: 12, Loss: 143.27218627929688
Projection step: 13, Loss: 135.38583374023438
Projection step: 14, Loss: 143.79736328125
Projection step: 15, Loss: 131.38055419921875
Projection step: 16, Loss: 134.81097412109375
Projection step: 17, Loss: 128.39369201660156
Projection step: 18, Loss: 135.25146484375
Projection step: 19, Loss: 123.06008911132812
Projection step: 20, Loss: 158.0207977294922
Projection step: 21, Loss: 130.30223083496094
Projection step: 22, Loss: 123.00302124023438
Projection step: 23, Loss: 135.67730712890625
Projection step: 24, Loss: 143.18685913085938
Final likelihood: tensor([ -89.4092, -149.7085, -118.8902, -189.0676,  -84.3155, -184.4055,
        -107.8887, -152.8907, -155.1246, -135.2626, -169.8139, -133.1539,
        -117.6786, -200.1562, -104.2566, -128.4727])
Final projection likelihood: -138.7809
1 mode projection failed, trying anyway
New goal: tensor([ 0.0636,  0.7382,  0.8712,  0.9239,  0.1988,  0.9285,  0.9854,  1.0827,
         1.1502,  0.0568,  0.7282,  0.0740,  0.3543, -0.2587, -0.5539],
       device='cuda:0')
tensor([[0.0046]], device='cuda:0') tensor([[0.0101]], device='cuda:0') tensor([[0.0045]], device='cuda:0')
Original likelihood: -128.8141632080078
Adjusted likelihood: -128.8141632080078
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 128.8141632080078}
Current yaw: tensor([ 0.3549, -0.2589, -0.4403], device='cuda:0')
5 thumb_middle
tensor([ 0.0454,  0.7562,  0.9170,  0.9346,  0.2079,  0.9474,  0.9834,  1.0214,
         1.1492,  0.0541,  0.7575,  0.0271,  0.3549, -0.2589, -0.4403, -1.6288],
       device='cuda:0')
Solve time for step 1 9.065593000967056
Current ori: tensor([ 0.3549, -0.2589, -0.4403], device='cuda:0')
Index force: tensor([0.5805, 0.5895, 0.5839, 0.5803], device='cuda:0')
tensor([ 0.0177,  0.7976,  0.8823,  0.9174,  0.1775,  0.9678,  0.9590,  1.0562,
         1.0359,  0.0858,  0.6645,  0.0564,  0.3698, -0.3229, -0.2424, -1.6915],
       device='cuda:0')
Solve time for step 2 3.6667570009594783
Current ori: tensor([ 0.3698, -0.3229, -0.2424], device='cuda:0')
Index force: tensor([0.5890, 0.5972, 0.5861], device='cuda:0')
tensor([-0.0063,  0.7982,  0.8710,  0.9139,  0.1554,  0.9527,  0.9751,  1.0655,
         1.0159,  0.0949,  0.6453,  0.0612,  0.3739, -0.3344, -0.2162, -1.6574],
       device='cuda:0')
Solve time for step 3 3.4468040069914423
Current ori: tensor([ 0.3739, -0.3344, -0.2162], device='cuda:0')
Index force: tensor([0.5801, 0.5755], device='cuda:0')
tensor([-0.0166,  0.7968,  0.8651,  0.9121,  0.1461,  0.9444,  0.9815,  1.0674,
         1.0213,  0.0815,  0.6357,  0.0585,  0.3739, -0.3344, -0.2165, -1.7722],
       device='cuda:0')
Solve time for step 4 3.197599830047693
Current ori: tensor([ 0.3739, -0.3344, -0.2165], device='cuda:0')
Index force: tensor([0.5614], device='cuda:0')
Storing RECOVERY transition: reward=-0.0782 (scaled=-0.0195), steps=4
Reward stats updated: mean -0.0027 -> -0.0028, std: 0.1005
Collected 475 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2144, Q2 Loss=1.2144, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4564
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8274, Q2 Loss=0.8274, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6525
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3583, Q2 Loss=1.3583, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2090
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0584, Q2 Loss=1.0584, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3062
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8580, Q2 Loss=0.8580, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8877

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.8%)
Q1 update: 0.04s (19.8%)
Q2 update: 0.04s (18.1%)
Actor update: 0.09s (39.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.063293
Q2 loss: 1.063293
Current threshold: -36.2330
Global Scale Offset: 0.0321
Reward stats: mean=-0.0028, std=0.1005, count=475
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.0633, Q2 Loss: 1.0633, Entropy: 0.0000, Mean TD Error: 0.9024, Threshold: -36.2330
Original likelihood: -214.68463134765625
Adjusted likelihood: -214.68463134765625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 225.0448455810547
Projection step: 1, Loss: 241.4548797607422
Projection step: 2, Loss: 232.89682006835938
Projection step: 3, Loss: 221.57766723632812
Projection step: 4, Loss: 234.83517456054688
Projection step: 5, Loss: 217.45362854003906
Projection step: 6, Loss: 228.11196899414062
Projection step: 7, Loss: 224.9742889404297
Projection step: 8, Loss: 222.31344604492188
Projection step: 9, Loss: 230.92361450195312
Projection step: 10, Loss: 224.4888153076172
Projection step: 11, Loss: 223.12997436523438
Projection step: 12, Loss: 224.4820098876953
Projection step: 13, Loss: 218.05850219726562
Projection step: 14, Loss: 219.67832946777344
Projection step: 15, Loss: 218.2895965576172
Projection step: 16, Loss: 227.62130737304688
Projection step: 17, Loss: 233.46035766601562
Projection step: 18, Loss: 206.86611938476562
Projection step: 19, Loss: 228.1324920654297
Projection step: 20, Loss: 233.75051879882812
Projection step: 21, Loss: 232.83871459960938
Projection step: 22, Loss: 221.3252716064453
Projection step: 23, Loss: 216.01193237304688
Projection step: 24, Loss: 234.37356567382812
Final likelihood: tensor([-240.6127, -247.5648, -262.2148, -314.6347, -152.4761, -260.4695,
        -201.9751, -186.6964, -157.2998, -218.0230, -139.7887, -203.4942,
        -316.6993, -241.2431, -216.5730, -239.9738])
Final projection likelihood: -224.9837
1 mode projection failed, trying anyway
New goal: tensor([-0.0087,  0.8444,  0.8994,  0.9294,  0.1414,  0.9888,  1.0412,  1.1194,
         1.1237,  0.0023,  0.6319,  0.0715,  0.3614, -0.2985, -0.3111],
       device='cuda:0')
tensor([[0.0033]], device='cuda:0') tensor([[0.0047]], device='cuda:0') tensor([[0.0046]], device='cuda:0')
Original likelihood: -218.88108825683594
Adjusted likelihood: -218.88108825683594
Likelihood residual: 0.0
Original likelihood: -238.78189086914062
Adjusted likelihood: -238.78189086914062
Likelihood residual: 0.0
{'index': 238.78189086914062, 'thumb_middle': 218.88108825683594}
Current yaw: tensor([ 0.3614, -0.2981, -0.3223], device='cuda:0')
6 thumb_middle
tensor([-1.8681e-02,  8.4830e-01,  8.9932e-01,  9.2420e-01,  1.4217e-01,
         9.9100e-01,  1.0440e+00,  1.1054e+00,  1.1301e+00,  1.3710e-03,
         6.3302e-01,  5.3241e-02,  3.6136e-01, -2.9811e-01, -3.2234e-01,
        -1.6543e+00], device='cuda:0')
Solve time for step 1 8.927558679017238
Current ori: tensor([ 0.3614, -0.2981, -0.3223], device='cuda:0')
Index force: tensor([0.5536, 0.5878, 0.5927, 0.5784], device='cuda:0')
tensor([-0.0525,  0.8729,  0.8680,  0.9065,  0.1021,  1.0072,  1.0118,  1.0838,
         1.0195,  0.0402,  0.5548,  0.0579,  0.3799, -0.3494, -0.0991, -1.7962],
       device='cuda:0')
Solve time for step 2 3.5426335759693757
Current ori: tensor([ 0.3799, -0.3494, -0.0991], device='cuda:0')
Index force: tensor([0.5810, 0.5831, 0.5698], device='cuda:0')
tensor([-0.0731,  0.8183,  0.8719,  0.9117,  0.0866,  0.9572,  1.0200,  1.0986,
         1.0254,  0.0235,  0.5493,  0.0665,  0.3796, -0.3483, -0.1029, -1.8781],
       device='cuda:0')
Solve time for step 3 3.3765837810351513
Current ori: tensor([ 0.3796, -0.3483, -0.1029], device='cuda:0')
Index force: tensor([0.5392, 0.5416], device='cuda:0')
tensor([-0.0853,  0.8493,  0.8603,  0.9029,  0.0715,  0.9828,  1.0121,  1.1063,
         1.0309,  0.0094,  0.5420,  0.0648,  0.3794, -0.3477, -0.1051, -1.8950],
       device='cuda:0')
Solve time for step 4 3.3704519510501996
Current ori: tensor([ 0.3794, -0.3477, -0.1051], device='cuda:0')
Index force: tensor([0.5513], device='cuda:0')
Storing RECOVERY transition: reward=-0.1317 (scaled=-0.0329), steps=4
Reward stats updated: mean -0.0028 -> -0.0028, std: 0.1004
Collected 476 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.7417, Q2 Loss=1.7417, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2011
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=2.5334, Q2 Loss=2.5334, Entropy=0.0020, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.2524
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.2386, Q2 Loss=1.2386, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8639
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.2410, Q2 Loss=2.2410, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7296
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9917, Q2 Loss=0.9917, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7009

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.2%)
Q1 update: 0.06s (20.7%)
Q2 update: 0.05s (19.7%)
Actor update: 0.11s (41.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.749273
Q2 loss: 1.749273
Current threshold: -36.2404
Global Scale Offset: 0.0320
Reward stats: mean=-0.0028, std=0.1004, count=476
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.7493, Q2 Loss: 1.7493, Entropy: 0.0004, Mean TD Error: 1.9496, Threshold: -36.2404
Original likelihood: -268.6322937011719
Adjusted likelihood: -268.6322937011719
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 255.293212890625
Projection step: 1, Loss: 263.75732421875
Projection step: 2, Loss: 268.7089538574219
Projection step: 3, Loss: 277.8909606933594
Projection step: 4, Loss: 249.10031127929688
Projection step: 5, Loss: 258.24957275390625
Projection step: 6, Loss: 257.46014404296875
Projection step: 7, Loss: 273.4522705078125
Projection step: 8, Loss: 263.13336181640625
Projection step: 9, Loss: 279.1704406738281
Projection step: 10, Loss: 267.331787109375
Projection step: 11, Loss: 264.0800476074219
Projection step: 12, Loss: 259.8721008300781
Projection step: 13, Loss: 272.70562744140625
Projection step: 14, Loss: 261.1278076171875
Projection step: 15, Loss: 252.0864715576172
Projection step: 16, Loss: 273.5828857421875
Projection step: 17, Loss: 261.3003234863281
Projection step: 18, Loss: 269.98968505859375
Projection step: 19, Loss: 266.0392150878906
Projection step: 20, Loss: 261.41888427734375
Projection step: 21, Loss: 256.52978515625
Projection step: 22, Loss: 249.293701171875
Projection step: 23, Loss: 264.548095703125
Projection step: 24, Loss: 260.83319091796875
Final likelihood: tensor([-219.2197, -235.3373, -289.7154, -289.9938, -236.0448, -296.6016,
        -286.6388, -233.2198, -338.8112, -260.3850, -331.6046, -205.1363,
        -278.1702, -203.9956, -253.1207, -288.5825])
Final projection likelihood: -265.4111
1 mode projection failed, trying anyway
New goal: tensor([-0.0797,  0.9015,  0.9110,  0.9174,  0.0670,  1.0338,  1.0720,  1.1291,
         1.1142, -0.0464,  0.5843,  0.0807,  0.3663, -0.3124, -0.2357],
       device='cuda:0')
tensor([[0.0033]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0045]], device='cuda:0')
Original likelihood: -236.97586059570312
Adjusted likelihood: -236.97586059570312
Likelihood residual: 0.0
Original likelihood: -261.72747802734375
Adjusted likelihood: -261.72747802734375
Likelihood residual: 0.0
{'index': 261.72747802734375, 'thumb_middle': 236.97586059570312}
Current yaw: tensor([ 0.3663, -0.3119, -0.2512], device='cuda:0')
7 thumb_middle
tensor([-0.0864,  0.9040,  0.9075,  0.9131,  0.0663,  1.0329,  1.0759,  1.1224,
         1.1211, -0.0458,  0.5843,  0.0726,  0.3663, -0.3119, -0.2512, -1.6730],
       device='cuda:0')
Solve time for step 1 9.0514183419873
Current ori: tensor([ 0.3663, -0.3119, -0.2512], device='cuda:0')
Index force: tensor([0.5675, 0.5724, 0.5587, 0.5480], device='cuda:0')
tensor([-0.1239,  0.8944,  0.8906,  0.9176,  0.0202,  1.0082,  1.0157,  1.0944,
         1.0655, -0.0326,  0.5011,  0.0648,  0.3777, -0.3428, -0.1084, -1.8032],
       device='cuda:0')
Solve time for step 2 3.651005175022874
Current ori: tensor([ 0.3777, -0.3428, -0.1084], device='cuda:0')
Index force: tensor([0.5619, 0.5503, 0.5397], device='cuda:0')
tensor([-0.1506,  0.8959,  0.9015,  0.8961, -0.0071,  1.0033,  1.0345,  1.1034,
         1.0630, -0.0290,  0.4857,  0.0781,  0.3786, -0.3457, -0.1011, -1.8963],
       device='cuda:0')
Solve time for step 3 3.537318866001442
Current ori: tensor([ 0.3786, -0.3457, -0.1011], device='cuda:0')
Index force: tensor([0.5418, 0.5324], device='cuda:0')
tensor([-0.1484,  0.9097,  0.8865,  0.9155, -0.0053,  1.0124,  1.0329,  1.1044,
         1.0625, -0.0274,  0.4877,  0.0681,  0.3786, -0.3456, -0.1013, -1.8792],
       device='cuda:0')
Solve time for step 4 3.2979535380145535
Current ori: tensor([ 0.3786, -0.3456, -0.1013], device='cuda:0')
Index force: tensor([0.5359], device='cuda:0')
Storing RECOVERY transition: reward=-0.1691 (scaled=-0.0423), steps=4
Reward stats updated: mean -0.0028 -> -0.0029, std: 0.1003
Collected 477 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7689, Q2 Loss=0.7689, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7391
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.1085, Q2 Loss=2.1085, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1241
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.4929, Q2 Loss=1.4929, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3247
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.7439, Q2 Loss=1.7439, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6291
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.3722, Q2 Loss=1.3722, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7280

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (18.2%)
Q1 update: 0.05s (20.6%)
Q2 update: 0.05s (18.5%)
Actor update: 0.10s (39.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.138154
Q1 loss: 1.497286
Q2 loss: 1.497286
Current threshold: -36.2448
Global Scale Offset: 0.0320
Reward stats: mean=-0.0029, std=0.1003, count=477
----------------------------------------------
SAC Update - Actor Loss: -0.1382, Q1 Loss: 1.4973, Q2 Loss: 1.4973, Entropy: 0.0000, Mean TD Error: 1.3090, Threshold: -36.2448
Original likelihood: -272.3858337402344
Adjusted likelihood: -272.3858337402344
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 280.6239013671875
Projection step: 1, Loss: 277.5892639160156
Projection step: 2, Loss: 275.3224792480469
Projection step: 3, Loss: 271.8909912109375
Projection step: 4, Loss: 274.5533447265625
Projection step: 5, Loss: 276.28082275390625
Projection step: 6, Loss: 273.5456848144531
Projection step: 7, Loss: 274.0228576660156
Projection step: 8, Loss: 276.7733154296875
Projection step: 9, Loss: 267.9678039550781
Projection step: 10, Loss: 283.2637634277344
Projection step: 11, Loss: 278.000244140625
Projection step: 12, Loss: 277.9058532714844
Projection step: 13, Loss: 259.097900390625
Projection step: 14, Loss: 259.44671630859375
Projection step: 15, Loss: 279.20074462890625
Projection step: 16, Loss: 279.75653076171875
Projection step: 17, Loss: 271.9610595703125
Projection step: 18, Loss: 270.0999755859375
Projection step: 19, Loss: 280.2850036621094
Projection step: 20, Loss: 270.85601806640625
Projection step: 21, Loss: 267.9324645996094
Projection step: 22, Loss: 278.4604187011719
Projection step: 23, Loss: 279.58056640625
Projection step: 24, Loss: 284.8194885253906
Final likelihood: tensor([-317.9437, -275.6425, -278.5568, -255.5921, -209.3635, -279.8958,
        -283.7841, -254.2379, -275.6815, -301.6877, -323.9231, -295.6481,
        -258.6934, -267.3624, -253.7799, -295.5900])
Final projection likelihood: -276.7114
1 mode projection failed, trying anyway
New goal: tensor([-0.1536,  0.9454,  0.9278,  0.9249, -0.0169,  1.0493,  1.0827,  1.1286,
         1.1193, -0.0895,  0.5525,  0.0678,  0.3682, -0.3164, -0.1828],
       device='cuda:0')
tensor([[0.0032]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0042]], device='cuda:0')
Original likelihood: -140.74462890625
Adjusted likelihood: -140.74462890625
Likelihood residual: 0.0
Original likelihood: -256.7301330566406
Adjusted likelihood: -256.7301330566406
Likelihood residual: 0.0
{'index': 256.7301330566406, 'thumb_middle': 140.74462890625}
Current yaw: tensor([ 0.3682, -0.3159, -0.1995], device='cuda:0')
8 thumb_middle
tensor([-0.1596,  0.9476,  0.9239,  0.9211, -0.0180,  1.0480,  1.0864,  1.1231,
         1.1262, -0.0885,  0.5526,  0.0622,  0.3682, -0.3159, -0.1995, -1.7190],
       device='cuda:0')
Solve time for step 1 8.949291916040238
Current ori: tensor([ 0.3682, -0.3159, -0.1995], device='cuda:0')
Index force: tensor([0.5941, 0.5692, 0.5810, 0.5902], device='cuda:0')
tensor([-0.1940,  0.9438,  0.8992,  0.9105, -0.0659,  1.0204,  1.0361,  1.0985,
         1.0337, -0.0708,  0.4600,  0.0545,  0.3826, -0.3575, -0.0679, -1.8697],
       device='cuda:0')
Solve time for step 2 3.5742018910241313
Current ori: tensor([ 0.3826, -0.3575, -0.0679], device='cuda:0')
Index force: tensor([0.5580, 0.5700, 0.5790], device='cuda:0')
tensor([-0.2175,  0.9562,  0.8929,  0.9230, -0.0898,  1.0214,  1.0535,  1.1216,
         1.0335, -0.0701,  0.4628,  0.0620,  0.3825, -0.3574, -0.0685, -1.9484],
       device='cuda:0')
Solve time for step 3 3.459073926031124
Current ori: tensor([ 0.3825, -0.3574, -0.0685], device='cuda:0')
Index force: tensor([0.5629, 0.5614], device='cuda:0')
tensor([-0.2272,  0.9773,  0.8969,  0.9088, -0.1021,  1.0385,  1.0668,  1.1066,
         1.0322, -0.0667,  0.4759,  0.0674,  0.3821, -0.3562, -0.0746, -1.9017],
       device='cuda:0')
Solve time for step 4 3.3620022810064256
Current ori: tensor([ 0.3821, -0.3562, -0.0746], device='cuda:0')
Index force: tensor([0.5663], device='cuda:0')
Storing RECOVERY transition: reward=-0.1667 (scaled=-0.0417), steps=4
Reward stats updated: mean -0.0029 -> -0.0030, std: 0.1002
Collected 478 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.2030, Q2 Loss=1.2030, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3576
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3718, Q2 Loss=1.3718, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9718
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3396, Q2 Loss=1.3396, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4595
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8994, Q2 Loss=0.8994, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6866
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6283, Q2 Loss=1.6283, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2464

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (18.8%)
Q1 update: 0.06s (20.6%)
Q2 update: 0.05s (18.0%)
Actor update: 0.11s (38.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.288419
Q2 loss: 1.288419
Current threshold: -36.2474
Global Scale Offset: 0.0320
Reward stats: mean=-0.0030, std=0.1002, count=478
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.2884, Q2 Loss: 1.2884, Entropy: 0.0000, Mean TD Error: 1.3444, Threshold: -36.2474
Original likelihood: -273.10748291015625
Adjusted likelihood: -273.10748291015625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 272.47955322265625
Projection step: 1, Loss: 296.01898193359375
Projection step: 2, Loss: 286.01458740234375
Projection step: 3, Loss: 290.4876708984375
Projection step: 4, Loss: 290.051025390625
Projection step: 5, Loss: 293.2406921386719
Projection step: 6, Loss: 292.8409423828125
Projection step: 7, Loss: 290.34600830078125
Projection step: 8, Loss: 298.9058837890625
Projection step: 9, Loss: 300.4406433105469
Projection step: 10, Loss: 293.35662841796875
Projection step: 11, Loss: 285.56884765625
Projection step: 12, Loss: 308.2393798828125
Projection step: 13, Loss: 285.42437744140625
Projection step: 14, Loss: 296.2256164550781
Projection step: 15, Loss: 297.3184509277344
Projection step: 16, Loss: 291.17364501953125
Projection step: 17, Loss: 279.49169921875
Projection step: 18, Loss: 293.6175231933594
Projection step: 19, Loss: 282.20703125
Projection step: 20, Loss: 288.8560791015625
Projection step: 21, Loss: 303.6484069824219
Projection step: 22, Loss: 281.8997802734375
Projection step: 23, Loss: 309.033935546875
Projection step: 24, Loss: 295.12835693359375
Final likelihood: tensor([-288.0697, -310.6679, -250.1126, -357.8422, -291.6618, -288.2597,
        -325.5185, -273.6509, -303.5139, -262.7753, -286.9352, -269.9483,
        -271.5344, -297.8938, -282.4990, -248.1520])
Final projection likelihood: -288.0647
1 mode projection failed, trying anyway
New goal: tensor([-0.2319,  1.0077,  0.9336,  0.9220, -0.1109,  1.0748,  1.1008,  1.1358,
         1.1070, -0.1051,  0.5314,  0.0617,  0.3709, -0.3249, -0.1940],
       device='cuda:0')
tensor([[0.0029]], device='cuda:0') tensor([[0.0021]], device='cuda:0') tensor([[0.0045]], device='cuda:0')
Original likelihood: -262.2076416015625
Adjusted likelihood: -262.2076416015625
Likelihood residual: 0.0
Original likelihood: -298.54498291015625
Adjusted likelihood: -298.54498291015625
Likelihood residual: 0.0
{'index': 298.54498291015625, 'thumb_middle': 262.2076416015625}
Current yaw: tensor([ 0.3709, -0.3244, -0.2113], device='cuda:0')
9 thumb_middle
tensor([-0.2362,  1.0088,  0.9291,  0.9186, -0.1122,  1.0741,  1.1049,  1.1317,
         1.1136, -0.1050,  0.5320,  0.0597,  0.3709, -0.3244, -0.2113, -1.7142],
       device='cuda:0')
Solve time for step 1 9.134373057982884
Current ori: tensor([ 0.3709, -0.3244, -0.2113], device='cuda:0')
Index force: tensor([0.5922, 0.5901, 0.5813, 0.5849], device='cuda:0')
tensor([-0.2643,  0.9983,  0.9064,  0.9110, -0.1536,  1.0436,  1.0575,  1.1095,
         1.0493, -0.0783,  0.4474,  0.0551,  0.3828, -0.3558, -0.1206, -1.8262],
       device='cuda:0')
Solve time for step 2 3.5997116710059345
Current ori: tensor([ 0.3828, -0.3558, -0.1206], device='cuda:0')
Index force: tensor([0.5844, 0.5780, 0.5814], device='cuda:0')
tensor([-0.2913,  1.0067,  0.8934,  0.9028, -0.1831,  1.0405,  1.0635,  1.1132,
         1.0388, -0.0789,  0.4361,  0.0551,  0.3845, -0.3601, -0.1119, -1.8937],
       device='cuda:0')
Solve time for step 3 3.4261360010132194
Current ori: tensor([ 0.3845, -0.3601, -0.1119], device='cuda:0')
Index force: tensor([0.5574, 0.5595], device='cuda:0')
tensor([-0.2977,  1.0127,  0.8943,  0.9020, -0.1902,  1.0423,  1.0751,  1.1072,
         1.0481, -0.1050,  0.4119,  0.0473,  0.3841, -0.3593, -0.1161, -1.8598],
       device='cuda:0')
Solve time for step 4 3.4130047170328908
Current ori: tensor([ 0.3841, -0.3593, -0.1161], device='cuda:0')
Index force: tensor([0.5527], device='cuda:0')
Storing RECOVERY transition: reward=-0.1613 (scaled=-0.0403), steps=4
Reward stats updated: mean -0.0030 -> -0.0031, std: 0.1001
Collected 479 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.7664, Q2 Loss=0.7664, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4008
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5757, Q2 Loss=1.5757, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2737
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7047, Q2 Loss=0.7047, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4789
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0889, Q2 Loss=1.0889, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9179
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.9252, Q2 Loss=0.9252, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6430

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.7%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (18.7%)
Actor update: 0.11s (41.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.012163
Q2 loss: 1.012163
Current threshold: -36.2489
Global Scale Offset: 0.0320
Reward stats: mean=-0.0031, std=0.1001, count=479
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0122, Q2 Loss: 1.0122, Entropy: 0.0000, Mean TD Error: 0.7429, Threshold: -36.2489
Original likelihood: -276.19708251953125
Adjusted likelihood: -276.19708251953125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 295.132568359375
Projection step: 1, Loss: 306.0594482421875
Projection step: 2, Loss: 299.190673828125
Projection step: 3, Loss: 303.6756591796875
Projection step: 4, Loss: 300.1200256347656
Projection step: 5, Loss: 290.95428466796875
Projection step: 6, Loss: 310.45843505859375
Projection step: 7, Loss: 289.9117431640625
Projection step: 8, Loss: 303.3836975097656
Projection step: 9, Loss: 287.1263427734375
Projection step: 10, Loss: 294.647705078125
Projection step: 11, Loss: 310.980712890625
Projection step: 12, Loss: 315.3970947265625
Projection step: 13, Loss: 308.1202697753906
Projection step: 14, Loss: 311.4143371582031
Projection step: 15, Loss: 309.1611633300781
Projection step: 16, Loss: 319.3019104003906
Projection step: 17, Loss: 303.0142517089844
Projection step: 18, Loss: 293.77056884765625
Projection step: 19, Loss: 303.8669738769531
Projection step: 20, Loss: 306.3730163574219
Projection step: 21, Loss: 314.4871520996094
Projection step: 22, Loss: 310.56219482421875
Projection step: 23, Loss: 319.3382568359375
Projection step: 24, Loss: 303.2060241699219
Final likelihood: tensor([-333.3177, -262.8851, -290.3183, -292.4432, -330.2965, -339.9867,
        -295.8520, -320.0999, -267.1849, -347.2574, -294.2627, -284.6172,
        -343.5753, -289.7202, -308.9555, -331.3095])
Final projection likelihood: -308.2551
1 mode projection failed, trying anyway
New goal: tensor([-0.3000,  1.0518,  0.9390,  0.9268, -0.1956,  1.0990,  1.1119,  1.1354,
         1.0994, -0.1046,  0.5128,  0.0546,  0.3727, -0.3300, -0.2100],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0048]], device='cuda:0')
Original likelihood: -206.83990478515625
Adjusted likelihood: -206.83990478515625
Likelihood residual: 0.0
Original likelihood: -274.783447265625
Adjusted likelihood: -274.783447265625
Likelihood residual: 0.0
{'index': 274.783447265625, 'thumb_middle': 206.83990478515625}
Current yaw: tensor([ 0.3726, -0.3295, -0.2246], device='cuda:0')
10 thumb_middle
tensor([-0.3036,  1.0521,  0.9338,  0.9229, -0.1969,  1.0988,  1.1168,  1.1320,
         1.1053, -0.1050,  0.5135,  0.0543,  0.3726, -0.3295, -0.2246, -1.7115],
       device='cuda:0')
Solve time for step 1 9.061940839048475
Current ori: tensor([ 0.3726, -0.3295, -0.2246], device='cuda:0')
Index force: tensor([0.5926, 0.5977, 0.5983, 0.5804], device='cuda:0')
tensor([-0.3456,  1.0525,  0.9250,  0.9199, -0.2518,  1.0440,  1.0510,  1.1056,
         1.0408, -0.0821,  0.4350,  0.0476,  0.3845, -0.3593, -0.1238, -1.8267],
       device='cuda:0')
Solve time for step 2 3.802862321957946
Current ori: tensor([ 0.3845, -0.3593, -0.1238], device='cuda:0')
Index force: tensor([0.5861, 0.5893, 0.5727], device='cuda:0')
tensor([-0.3711,  1.0676,  0.9199,  0.9162, -0.2819,  1.0520,  1.0625,  1.1118,
         1.0449, -0.0953,  0.4288,  0.0493,  0.3841, -0.3585, -0.1266, -1.8747],
       device='cuda:0')
Solve time for step 3 3.4298691999865696
Current ori: tensor([ 0.3841, -0.3585, -0.1266], device='cuda:0')
Index force: tensor([0.5662, 0.5534], device='cuda:0')
tensor([-0.3843,  1.0751,  0.9236,  0.9088, -0.2979,  1.0570,  1.0726,  1.1132,
         1.0483, -0.1050,  0.4395,  0.0461,  0.3833, -0.3564, -0.1315, -1.8404],
       device='cuda:0')
Solve time for step 4 3.224961814994458
Current ori: tensor([ 0.3833, -0.3564, -0.1315], device='cuda:0')
Index force: tensor([0.5576], device='cuda:0')
Storing RECOVERY transition: reward=-0.1632 (scaled=-0.0408), steps=4
Reward stats updated: mean -0.0031 -> -0.0031, std: 0.1000
Collected 480 transitions for RL
SAC Update 1/5: Actor Loss=-0.2234, Q1 Loss=1.3373, Q2 Loss=1.3373, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5056
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3242, Q2 Loss=1.3242, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9068
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.7598, Q2 Loss=1.7598, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8550
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.6753, Q2 Loss=0.6753, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2553
SAC Update 5/5: Actor Loss=-0.2673, Q1 Loss=1.3233, Q2 Loss=1.3233, Entropy=0.0470, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5579

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.2%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (17.5%)
Actor update: 0.11s (41.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.190244
Q1 loss: 1.284009
Q2 loss: 1.284009
Current threshold: -36.2554
Global Scale Offset: 0.0320
Reward stats: mean=-0.0031, std=0.1000, count=480
----------------------------------------------
SAC Update - Actor Loss: -0.1902, Q1 Loss: 1.2840, Q2 Loss: 1.2840, Entropy: 0.0094, Mean TD Error: 0.8161, Threshold: -36.2554
Original likelihood: -267.77032470703125
Adjusted likelihood: -267.77032470703125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 289.46868896484375
Projection step: 1, Loss: 282.1800231933594
Projection step: 2, Loss: 280.32171630859375
Projection step: 3, Loss: 311.13739013671875
Projection step: 4, Loss: 303.182861328125
Projection step: 5, Loss: 274.1207580566406
Projection step: 6, Loss: 302.6256103515625
Projection step: 7, Loss: 291.46356201171875
Projection step: 8, Loss: 299.73162841796875
Projection step: 9, Loss: 308.172607421875
Projection step: 10, Loss: 297.51568603515625
Projection step: 11, Loss: 293.44757080078125
Projection step: 12, Loss: 303.6978454589844
Projection step: 13, Loss: 292.6603088378906
Projection step: 14, Loss: 305.3742980957031
Projection step: 15, Loss: 288.23236083984375
Projection step: 16, Loss: 289.9255676269531
Projection step: 17, Loss: 276.22381591796875
Projection step: 18, Loss: 303.105224609375
Projection step: 19, Loss: 296.261474609375
Projection step: 20, Loss: 307.0145568847656
Projection step: 21, Loss: 310.1280822753906
Projection step: 22, Loss: 310.84307861328125
Projection step: 23, Loss: 297.3394775390625
Projection step: 24, Loss: 291.03369140625
Final likelihood: tensor([-308.2471, -335.8477, -277.4425, -299.1206, -303.8489, -344.4791,
        -336.6490, -291.2333, -254.4960, -244.8482, -226.5662, -270.6617,
        -305.7935, -320.9081, -241.3597, -354.1139])
Final projection likelihood: -294.7260
1 mode projection failed, trying anyway
New goal: tensor([-0.3762,  1.1186,  0.9788,  0.9340, -0.2942,  1.1268,  1.1045,  1.1330,
         1.1151, -0.1040,  0.5162,  0.0441,  0.3709, -0.3245, -0.2116],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0038]], device='cuda:0') tensor([[0.0041]], device='cuda:0')
Original likelihood: -329.68896484375
Adjusted likelihood: -329.68896484375
Likelihood residual: 0.0
Original likelihood: -340.408935546875
Adjusted likelihood: -340.408935546875
Likelihood residual: 0.0
{'index': 340.408935546875, 'thumb_middle': 329.68896484375}
Current yaw: tensor([ 0.3707, -0.3239, -0.2160], device='cuda:0')
11 thumb_middle
tensor([-0.3805,  1.1193,  0.9723,  0.9278, -0.2960,  1.1275,  1.1098,  1.1297,
         1.1207, -0.1050,  0.5173,  0.0451,  0.3707, -0.3239, -0.2160, -1.7208],
       device='cuda:0')
Solve time for step 1 8.874201122962404
Current ori: tensor([ 0.3707, -0.3239, -0.2160], device='cuda:0')
Index force: tensor([0.5574, 0.5199, 0.5415, 0.5833], device='cuda:0')
tensor([-0.4039,  1.1638,  0.9786,  0.9060, -0.3542,  1.0558,  1.0263,  1.0982,
         0.9924, -0.0677,  0.4209,  0.0406,  0.3901, -0.3745, -0.0898, -1.8680],
       device='cuda:0')
Solve time for step 2 3.5664948219782673
Current ori: tensor([ 0.3901, -0.3745, -0.0898], device='cuda:0')
Index force: tensor([0.5202, 0.5308, 0.5718], device='cuda:0')
tensor([-0.4228,  1.1983,  0.9829,  0.9084, -0.3441,  1.0878,  1.0450,  1.1058,
         0.9333, -0.0489,  0.3669,  0.0380,  0.3979, -0.3945, -0.0453, -1.9860],
       device='cuda:0')
Solve time for step 3 3.7082386419642717
Current ori: tensor([ 0.3979, -0.3945, -0.0453], device='cuda:0')
Index force: tensor([0.5392, 0.5471], device='cuda:0')
tensor([-0.4240,  1.2030,  0.9872,  0.9088, -0.3456,  1.0939,  1.0482,  1.1091,
         0.9357, -0.0575,  0.3703,  0.0321,  0.3974, -0.3935, -0.0491, -1.9567],
       device='cuda:0')
Solve time for step 4 3.381402657018043
Current ori: tensor([ 0.3974, -0.3935, -0.0491], device='cuda:0')
Index force: tensor([0.5567], device='cuda:0')
Storing RECOVERY transition: reward=-0.3268 (scaled=-0.0817), steps=4
Reward stats updated: mean -0.0031 -> -0.0033, std: 0.1000
Collected 481 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9969, Q2 Loss=0.9969, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8009
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2974, Q2 Loss=1.2974, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9392
SAC Update 3/5: Actor Loss=-0.1990, Q1 Loss=1.2077, Q2 Loss=1.2077, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1257
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8071, Q2 Loss=0.8071, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7674
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0172, Q2 Loss=1.0172, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7672

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.9%)
Q1 update: 0.05s (18.5%)
Q2 update: 0.05s (18.0%)
Actor update: 0.10s (38.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.039798
Q1 loss: 1.065262
Q2 loss: 1.065262
Current threshold: -36.2762
Global Scale Offset: 0.0318
Reward stats: mean=-0.0033, std=0.1000, count=481
----------------------------------------------
SAC Update - Actor Loss: -0.0398, Q1 Loss: 1.0653, Q2 Loss: 1.0653, Entropy: 0.0000, Mean TD Error: 0.8801, Threshold: -36.2762
Original likelihood: -383.0470275878906
Adjusted likelihood: -383.0470275878906
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 30
Loaded trajectory sampler
Current yaw: tensor([-0.0027,  0.0149, -0.0447], device='cuda:0')
Current yaw: tensor([-0.0027,  0.0149, -0.0447], device='cuda:0')
1 turn
Sampling time 3.6118321319809183
tensor([ 0.1482,  0.6010,  0.5537,  0.6560, -0.1036,  0.5160,  0.8975,  0.9140,
         1.2162,  0.3217,  0.2870,  1.1139, -0.0027,  0.0149, -0.0447,  0.1274],
       device='cuda:0')
Original likelihood: -19.395992279052734
Adjusted likelihood: -19.395992279052734
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.257694027968682
Current ori: tensor([-0.0027,  0.0149, -0.0447], device='cuda:0')
Middle force: tensor([1.3164, 1.7177, 1.8277, 0.5753, 0.5784, 0.5200, 0.5514, 0.5112, 0.5617,
        0.5698, 0.5742, 0.5274], device='cuda:0')
Thumb force: tensor([0.5250, 1.8353, 0.5666, 0.6673, 0.5738, 0.5236, 0.5501, 0.8241, 0.8375,
        0.6354, 0.6528, 0.6337], device='cuda:0')
Index force: tensor([0.6410, 0.7542, 0.5889, 0.5162, 0.5907, 0.6519, 0.5752, 0.6259, 0.5770,
        0.5383, 0.6183, 0.6060], device='cuda:0')
Storing NORMAL transition: reward=0.0845 (scaled=0.0845), steps=1
Reward stats updated: mean -0.0033 -> -0.0031, std: 0.1000
Collected 482 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3593, Q2 Loss=1.3593, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8060
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7087, Q2 Loss=0.7087, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3237
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.1603, Q2 Loss=2.1603, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2494
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3124, Q2 Loss=1.3124, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9358
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7811, Q2 Loss=0.7811, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7758

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.2%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (19.5%)
Actor update: 0.09s (40.7%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.264381
Q2 loss: 1.264381
Current threshold: -36.2885
Global Scale Offset: 0.0317
Reward stats: mean=-0.0031, std=0.1000, count=482
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.2644, Q2 Loss: 1.2644, Entropy: 0.0000, Mean TD Error: 1.0182, Threshold: -36.2885
tensor([ 0.1303,  0.6282,  0.5332,  0.5861, -0.1187,  0.5203,  0.8614,  0.9720,
         1.3042,  0.2300,  0.2770,  1.0218, -0.0129,  0.0175, -0.1295,  0.2668],
       device='cuda:0')
Original likelihood: -20.218338012695312
Adjusted likelihood: -20.218338012695312
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.457635757979006
Current ori: tensor([-0.0129,  0.0175, -0.1295], device='cuda:0')
Middle force: tensor([1.6857, 1.7902, 0.5733, 0.5742, 0.5142, 0.5501, 0.5110, 0.5598, 0.5675,
        0.5712, 0.5265], device='cuda:0')
Thumb force: tensor([1.7944, 0.5625, 0.6594, 0.5676, 0.5222, 0.5473, 0.8085, 0.8260, 0.6280,
        0.6454, 0.6262], device='cuda:0')
Index force: tensor([0.7392, 0.5869, 0.5150, 0.5880, 0.6606, 0.5710, 0.6193, 0.5732, 0.5357,
        0.6135, 0.6021], device='cuda:0')
Storing NORMAL transition: reward=0.2795 (scaled=0.2795), steps=1
Reward stats updated: mean -0.0031 -> -0.0025, std: 0.1007
Collected 483 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6531, Q2 Loss=0.6531, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0748
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.0666, Q2 Loss=1.0666, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9372
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7142, Q2 Loss=0.7142, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3057
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.2700, Q2 Loss=1.2700, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0745
SAC Update 5/5: Actor Loss=-0.1765, Q1 Loss=1.3150, Q2 Loss=1.3150, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9342

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.4%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (18.5%)
Actor update: 0.08s (40.2%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.127399
Q1 loss: 1.003799
Q2 loss: 1.003799
Current threshold: -36.2958
Global Scale Offset: 0.0317
Reward stats: mean=-0.0025, std=0.1007, count=483
----------------------------------------------
SAC Update - Actor Loss: -0.1274, Q1 Loss: 1.0038, Q2 Loss: 1.0038, Entropy: 0.0000, Mean TD Error: 0.6653, Threshold: -36.2958
tensor([ 0.1988,  0.6610,  0.5787,  0.5586, -0.0410,  0.5301,  0.8769,  1.0899,
         1.3011,  0.2338,  0.2401,  0.8957, -0.0265, -0.0326, -0.4140,  0.7605],
       device='cuda:0')
Original likelihood: -27.626543045043945
Adjusted likelihood: -27.626543045043945
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.237111445982009
Current ori: tensor([-0.0265, -0.0326, -0.4140], device='cuda:0')
Middle force: tensor([1.7319, 0.5633, 0.5710, 0.5105, 0.5457, 0.5121, 0.5589, 0.5660, 0.5663,
        0.5274], device='cuda:0')
Thumb force: tensor([0.5572, 0.6532, 0.5574, 0.5196, 0.5444, 0.7819, 0.8090, 0.6149, 0.6376,
        0.6087], device='cuda:0')
Index force: tensor([0.5830, 0.5158, 0.5813, 0.6565, 0.5680, 0.6107, 0.5693, 0.5334, 0.6108,
        0.5995], device='cuda:0')
Storing NORMAL transition: reward=0.0261 (scaled=0.0261), steps=1
Reward stats updated: mean -0.0025 -> -0.0025, std: 0.1006
Collected 484 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6167, Q2 Loss=1.6167, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2901
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.6982, Q2 Loss=0.6982, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3687
SAC Update 3/5: Actor Loss=-0.0014, Q1 Loss=1.4889, Q2 Loss=1.4889, Entropy=0.1899, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0741
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2669, Q2 Loss=1.2669, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6821
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1877, Q2 Loss=1.1877, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6332

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.2%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.7%)
Actor update: 0.08s (40.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046322
Q1 loss: 1.251692
Q2 loss: 1.251692
Current threshold: -36.3017
Global Scale Offset: 0.0317
Reward stats: mean=-0.0025, std=0.1006, count=484
----------------------------------------------
SAC Update - Actor Loss: -0.0463, Q1 Loss: 1.2517, Q2 Loss: 1.2517, Entropy: 0.0380, Mean TD Error: 1.2096, Threshold: -36.3017
tensor([ 0.1843,  0.6510,  0.5745,  0.5622, -0.0499,  0.5200,  0.8842,  1.0919,
         1.3334,  0.1898,  0.2425,  0.8722, -0.0269, -0.0259, -0.4398,  0.7780],
       device='cuda:0')
Original likelihood: -25.448793411254883
Adjusted likelihood: -25.448793411254883
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.106739618000574
Current ori: tensor([-0.0269, -0.0259, -0.4398], device='cuda:0')
Middle force: tensor([1.1303, 0.5760, 0.5028, 0.6042, 0.5380, 0.5538, 0.5517, 1.1605, 0.5593],
       device='cuda:0')
Thumb force: tensor([0.5570, 0.6827, 0.6167, 0.5545, 0.5752, 0.5881, 0.5449, 0.8823, 0.5625],
       device='cuda:0')
Index force: tensor([0.5480, 0.8053, 0.8402, 0.6035, 0.5591, 0.5830, 0.6423, 0.5587, 0.5576],
       device='cuda:0')
Storing NORMAL transition: reward=0.0345 (scaled=0.0345), steps=1
Reward stats updated: mean -0.0025 -> -0.0024, std: 0.1005
Collected 485 transitions for RL
SAC Update 1/5: Actor Loss=-0.0713, Q1 Loss=0.8780, Q2 Loss=0.8780, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6035
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9802, Q2 Loss=0.9802, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5902
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.3521, Q2 Loss=1.3521, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7266
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2288, Q2 Loss=1.2288, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9001
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8103, Q2 Loss=0.8103, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8719

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (21.4%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (17.5%)
Actor update: 0.08s (38.5%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.060311
Q1 loss: 1.049881
Q2 loss: 1.049881
Current threshold: -36.3060
Global Scale Offset: 0.0317
Reward stats: mean=-0.0024, std=0.1005, count=485
----------------------------------------------
SAC Update - Actor Loss: -0.0603, Q1 Loss: 1.0499, Q2 Loss: 1.0499, Entropy: 0.0000, Mean TD Error: 0.7385, Threshold: -36.3060
tensor([ 0.1673,  0.6362,  0.5534,  0.5928,  0.0361,  0.5802,  0.8973,  1.0600,
         1.3415,  0.2751,  0.1388,  0.8284, -0.0481, -0.0859, -0.5552,  0.8490],
       device='cuda:0')
Original likelihood: -27.41921615600586
Adjusted likelihood: -27.41921615600586
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.831673295993824
Current ori: tensor([-0.0481, -0.0859, -0.5552], device='cuda:0')
Middle force: tensor([0.9348, 0.5654, 0.5746, 0.5433, 0.5189, 0.6190, 0.5170, 0.7544],
       device='cuda:0')
Thumb force: tensor([0.8806, 0.6897, 0.5037, 0.5189, 0.5945, 0.5624, 1.0715, 0.5962],
       device='cuda:0')
Index force: tensor([1.2219, 0.5936, 0.5014, 0.5904, 0.6674, 0.5988, 0.5936, 0.5514],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0292 (scaled=-0.0292), steps=1
Reward stats updated: mean -0.0024 -> -0.0025, std: 0.1004
Collected 486 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9777, Q2 Loss=0.9777, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3029
SAC Update 2/5: Actor Loss=-0.0501, Q1 Loss=1.0817, Q2 Loss=1.0817, Entropy=0.0200, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2835
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2746, Q2 Loss=1.2746, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8663
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7158, Q2 Loss=0.7158, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3617
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9973, Q2 Loss=0.9973, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0478

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.7%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (20.1%)
Actor update: 0.10s (41.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.010028
Q1 loss: 1.009443
Q2 loss: 1.009443
Current threshold: -36.3316
Global Scale Offset: 0.0315
Reward stats: mean=-0.0025, std=0.1004, count=486
----------------------------------------------
SAC Update - Actor Loss: -0.0100, Q1 Loss: 1.0094, Q2 Loss: 1.0094, Entropy: 0.0040, Mean TD Error: 0.9724, Threshold: -36.3316
tensor([ 0.0826,  0.5166,  0.6270,  0.4950, -0.2341,  0.5222,  0.8810,  1.1277,
         1.3409,  0.4211,  0.1549,  0.6884, -0.0894, -0.0923, -0.5713,  2.7090],
       device='cuda:0')
Original likelihood: -39.47303009033203
Adjusted likelihood: -39.47303009033203
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 39.13152313232422
Projection step: 1, Loss: 36.596031188964844
Projection step: 2, Loss: 36.9295654296875
Projection step: 3, Loss: 35.96435546875
Projection step: 4, Loss: 34.135433197021484
Projection step: 5, Loss: 34.21448516845703
Projection step: 6, Loss: 33.15736770629883
Projection step: 7, Loss: 35.074790954589844
Projection step: 8, Loss: 32.316436767578125
Projection step: 9, Loss: 32.33357238769531
Projection step: 10, Loss: 30.64925193786621
Projection step: 11, Loss: 29.13234519958496
Projection step: 12, Loss: 27.778865814208984
Projection step: 13, Loss: 27.62331771850586
Projection step: 14, Loss: 30.264585494995117
Projection step: 15, Loss: 26.238758087158203
Projection step: 16, Loss: 26.71957778930664
Projection step: 17, Loss: 29.087825775146484
Projection step: 18, Loss: 25.761058807373047
Projection step: 19, Loss: 28.08414649963379
Projection step: 20, Loss: 27.59332275390625
Projection step: 21, Loss: 25.261333465576172
Projection step: 22, Loss: 26.18557357788086
Projection step: 23, Loss: 27.590778350830078
Projection step: 24, Loss: 24.705318450927734
Final likelihood: tensor([-44.9836, -24.4066, -34.0540, -23.2654, -24.0525, -24.9618, -24.0047,
        -24.0581, -23.6686, -24.5244, -23.7531, -24.4394, -29.5110, -22.6051,
        -24.3534, -24.8166])
Final projection likelihood: -26.3412
1 mode projection succeeded
New goal: tensor([ 0.0841,  0.5311,  0.6409,  0.5213, -0.1927,  0.5574,  0.9861,  1.2397,
         1.3482,  0.4042,  0.1798,  0.6376, -0.0880, -0.0860, -0.3720],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0256]], device='cuda:0') tensor([[0.0077]], device='cuda:0')
Original likelihood: -29.561925888061523
Adjusted likelihood: -29.561925888061523
Likelihood residual: 0.0
Original likelihood: -25.542003631591797
Adjusted likelihood: -25.542003631591797
Likelihood residual: 0.0
{'index': 25.542003631591797, 'thumb_middle': 29.561925888061523}
Current yaw: tensor([-0.0894, -0.0923, -0.5713], device='cuda:0')
2 index
tensor([ 0.0826,  0.5166,  0.6270,  0.4950, -0.2341,  0.5222,  0.8810,  1.1277,
         1.3409,  0.4211,  0.1549,  0.6884, -0.0894, -0.0923, -0.5713,  2.7090],
       device='cuda:0')
Solve time for step 1 10.557514254003763
Current ori: tensor([-0.0894, -0.0923, -0.5713], device='cuda:0')
Middle force: tensor([0.5539, 0.5793, 0.5730, 0.5441], device='cuda:0')
Thumb force: tensor([0.5845, 0.6194, 0.5704, 0.5860], device='cuda:0')
tensor([ 0.0852,  0.4677,  0.5843,  0.4893, -0.1285,  0.6514,  0.9570,  1.1623,
         1.3513,  0.3993,  0.1805,  0.7073, -0.0724, -0.0648, -0.5467,  4.0638],
       device='cuda:0')
Solve time for step 2 4.210702480981126
Current ori: tensor([-0.0724, -0.0648, -0.5467], device='cuda:0')
Middle force: tensor([0.5720, 0.5684, 0.5414], device='cuda:0')
Thumb force: tensor([0.6134, 0.5675, 0.5820], device='cuda:0')
tensor([ 0.0850,  0.4725,  0.5858,  0.4901, -0.1274,  0.6661,  0.9596,  1.1674,
         1.3514,  0.4026,  0.1911,  0.6938, -0.0722, -0.0615, -0.5535,  4.5483],
       device='cuda:0')
Solve time for step 3 4.128847002983093
Current ori: tensor([-0.0722, -0.0615, -0.5535], device='cuda:0')
Middle force: tensor([0.5368, 0.5230], device='cuda:0')
Thumb force: tensor([0.5336, 0.6078], device='cuda:0')
tensor([ 0.0816,  0.4757,  0.5890,  0.4912, -0.1284,  0.6816,  0.9629,  1.1663,
         1.3579,  0.3923,  0.1838,  0.6760, -0.0814, -0.0656, -0.5508,  4.3352],
       device='cuda:0')
Solve time for step 4 3.828471635992173
Current ori: tensor([-0.0814, -0.0656, -0.5508], device='cuda:0')
Middle force: tensor([0.5195], device='cuda:0')
Thumb force: tensor([0.5955], device='cuda:0')
Storing RECOVERY transition: reward=0.0167 (scaled=0.0033), steps=5
Reward stats updated: mean -0.0025 -> -0.0024, std: 0.1003
Collected 487 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8616, Q2 Loss=0.8616, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6369
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0814, Q2 Loss=1.0814, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3813
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0558, Q2 Loss=1.0558, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6783
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9435, Q2 Loss=0.9435, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7735
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8710, Q2 Loss=0.8710, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6747

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.3%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (39.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 0.962688
Q2 loss: 0.962688
Current threshold: -36.3512
Global Scale Offset: 0.0313
Reward stats: mean=-0.0024, std=0.1003, count=487
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 0.9627, Q2 Loss: 0.9627, Entropy: 0.0000, Mean TD Error: 0.8289, Threshold: -36.3512
Original likelihood: -29.289260864257812
Adjusted likelihood: -29.289260864257812
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0827, -0.0753, -0.5677], device='cuda:0')
3 turn
Sampling time 3.7222773960093036
tensor([ 0.0617,  0.5378,  0.6293,  0.5156, -0.1161,  0.6893,  0.9656,  1.1669,
         1.3524,  0.4016,  0.1739,  0.6804, -0.0827, -0.0753, -0.5677,  4.2018],
       device='cuda:0')
Original likelihood: -28.210147857666016
Adjusted likelihood: -28.210147857666016
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.834410555951763
Current ori: tensor([-0.0827, -0.0753, -0.5677], device='cuda:0')
Middle force: tensor([0.6067, 0.5803, 1.5980, 0.7131, 0.6745, 0.8762, 0.6396, 0.7644, 0.6875,
        0.6567, 0.6089, 0.6277], device='cuda:0')
Thumb force: tensor([0.5847, 1.1565, 0.8960, 1.0461, 0.5360, 0.5723, 0.5638, 0.5304, 0.5565,
        0.5365, 0.6272, 0.5978], device='cuda:0')
Index force: tensor([0.5957, 0.5052, 1.2993, 0.5933, 0.5812, 0.5916, 0.5968, 0.5046, 0.5479,
        0.5319, 0.5858, 0.5922], device='cuda:0')
Storing NORMAL transition: reward=-0.0415 (scaled=-0.0415), steps=1
Reward stats updated: mean -0.0024 -> -0.0025, std: 0.1002
Collected 488 transitions for RL
SAC Update 1/5: Actor Loss=-0.0858, Q1 Loss=2.0519, Q2 Loss=2.0519, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9053
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6708, Q2 Loss=0.6708, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1858
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.9150, Q2 Loss=0.9150, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7216
SAC Update 4/5: Actor Loss=-0.0026, Q1 Loss=1.4810, Q2 Loss=1.4810, Entropy=0.2697, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7459
SAC Update 5/5: Actor Loss=-0.1583, Q1 Loss=1.3206, Q2 Loss=1.3206, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1193

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (39.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.095393
Q1 loss: 1.287882
Q2 loss: 1.287882
Current threshold: -36.3647
Global Scale Offset: 0.0313
Reward stats: mean=-0.0025, std=0.1002, count=488
----------------------------------------------
SAC Update - Actor Loss: -0.0954, Q1 Loss: 1.2879, Q2 Loss: 1.2879, Entropy: 0.0539, Mean TD Error: 1.3356, Threshold: -36.3647
tensor([-0.1414,  0.6161,  0.6119,  0.6689, -0.1591,  0.7759,  0.9729,  1.1518,
         1.4743,  0.2549, -0.0392,  0.6364, -0.1569, -0.1421, -0.5547,  5.2826],
       device='cuda:0')
Original likelihood: -45.896202087402344
Adjusted likelihood: -45.896202087402344
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 48.4592399597168
Projection step: 1, Loss: 44.14101028442383
Projection step: 2, Loss: 48.05680847167969
Projection step: 3, Loss: 44.14508056640625
Projection step: 4, Loss: 46.56987380981445
Projection step: 5, Loss: 44.66094970703125
Projection step: 6, Loss: 44.01820373535156
Projection step: 7, Loss: 43.89214324951172
Projection step: 8, Loss: 45.708396911621094
Projection step: 9, Loss: 43.059326171875
Projection step: 10, Loss: 44.32533264160156
Projection step: 11, Loss: 42.937644958496094
Projection step: 12, Loss: 43.873390197753906
Projection step: 13, Loss: 43.02318572998047
Projection step: 14, Loss: 44.42082977294922
Projection step: 15, Loss: 42.58084487915039
Projection step: 16, Loss: 45.666656494140625
Projection step: 17, Loss: 41.94720458984375
Projection step: 18, Loss: 39.79513931274414
Projection step: 19, Loss: 41.710052490234375
Projection step: 20, Loss: 40.434776306152344
Projection step: 21, Loss: 46.2625617980957
Projection step: 22, Loss: 40.012718200683594
Projection step: 23, Loss: 39.331932067871094
Projection step: 24, Loss: 41.33509826660156
Final likelihood: tensor([-42.5195, -41.2111, -42.1518, -44.7123, -39.2667, -36.9821, -35.1263,
        -49.1764, -41.1574, -41.3149, -39.7641, -40.4097, -39.4839, -34.8967,
        -41.4788, -41.2689])
Final projection likelihood: -40.6825
1 mode projection failed, trying anyway
New goal: tensor([-1.2124e-01,  5.8244e-01,  5.7865e-01,  6.0316e-01, -1.4994e-01,
         7.7317e-01,  9.5897e-01,  1.3137e+00,  1.4823e+00,  2.7917e-01,
         5.9164e-04,  5.4551e-01, -1.5476e-01, -1.3766e-01, -2.8171e-01],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0122]], device='cuda:0') tensor([[0.0138]], device='cuda:0')
Original likelihood: -45.292724609375
Adjusted likelihood: -45.292724609375
Likelihood residual: 0.0
Original likelihood: -44.24037551879883
Adjusted likelihood: -44.24037551879883
Likelihood residual: 0.0
{'index': 44.24037551879883, 'thumb_middle': 45.292724609375}
Current yaw: tensor([-0.1569, -0.1421, -0.5547], device='cuda:0')
4 index
tensor([-0.1414,  0.6161,  0.6119,  0.6689, -0.1591,  0.7759,  0.9729,  1.1518,
         1.4743,  0.2549, -0.0392,  0.6364, -0.1569, -0.1421, -0.5547,  5.2826],
       device='cuda:0')
Solve time for step 1 10.485424098034855
Current ori: tensor([-0.1569, -0.1421, -0.5547], device='cuda:0')
Middle force: tensor([0.5640, 0.5166, 0.5189, 0.5361], device='cuda:0')
Thumb force: tensor([0.5558, 0.5974, 0.5819, 0.5901], device='cuda:0')
tensor([-0.1900,  0.5982,  0.5780,  0.6071, -0.1440,  0.8987,  0.8968,  1.1880,
         1.4684,  0.2778, -0.0284,  0.6290, -0.1692, -0.1328, -0.5018,  4.7283],
       device='cuda:0')
Solve time for step 2 4.230575462977868
Current ori: tensor([-0.1692, -0.1328, -0.5018], device='cuda:0')
Middle force: tensor([0.5119, 0.5161, 0.5290], device='cuda:0')
Thumb force: tensor([0.5960, 0.5801, 0.5888], device='cuda:0')
tensor([-0.1949,  0.6236,  0.5774,  0.6052, -0.1491,  0.9687,  0.9048,  1.1738,
         1.4685,  0.2838, -0.0094,  0.6202, -0.1838, -0.1128, -0.4253,  3.3035],
       device='cuda:0')
Solve time for step 3 4.033999659994151
Current ori: tensor([-0.1838, -0.1128, -0.4253], device='cuda:0')
Middle force: tensor([0.5112, 0.5232], device='cuda:0')
Thumb force: tensor([0.5802, 0.5857], device='cuda:0')
tensor([-0.1921,  0.6169,  0.5792,  0.5943, -0.1626,  1.0223,  0.9061,  1.1650,
         1.4769,  0.2781,  0.0280,  0.5538, -0.2140, -0.0898, -0.3231,  1.9852],
       device='cuda:0')
Solve time for step 4 4.259381959040184
Current ori: tensor([-0.2140, -0.0898, -0.3231], device='cuda:0')
Middle force: tensor([0.5511], device='cuda:0')
Thumb force: tensor([0.5917], device='cuda:0')
Storing RECOVERY transition: reward=-0.3608 (scaled=-0.3608), steps=1
Reward stats updated: mean -0.0025 -> -0.0033, std: 0.1014
Collected 489 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1229, Q2 Loss=1.1229, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8034
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8580, Q2 Loss=0.8580, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6227
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1680, Q2 Loss=1.1680, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4922
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8609, Q2 Loss=0.8609, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8580
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.4990, Q2 Loss=1.4990, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7325

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (21.5%)
Q1 update: 0.04s (19.5%)
Q2 update: 0.04s (17.2%)
Actor update: 0.09s (38.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.101747
Q2 loss: 1.101747
Current threshold: -36.3747
Global Scale Offset: 0.0312
Reward stats: mean=-0.0033, std=0.1014, count=489
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.1017, Q2 Loss: 1.1017, Entropy: 0.0000, Mean TD Error: 0.9017, Threshold: -36.3747
Original likelihood: -140.26583862304688
Adjusted likelihood: -140.26583862304688
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 138.7538604736328
Projection step: 1, Loss: 150.78041076660156
Projection step: 2, Loss: 146.07305908203125
Projection step: 3, Loss: 148.57733154296875
Projection step: 4, Loss: 138.80477905273438
Projection step: 5, Loss: 141.30905151367188
Projection step: 6, Loss: 136.70130920410156
Projection step: 7, Loss: 133.709228515625
Projection step: 8, Loss: 135.42066955566406
Projection step: 9, Loss: 140.28431701660156
Projection step: 10, Loss: 143.3994140625
Projection step: 11, Loss: 140.18121337890625
Projection step: 12, Loss: 135.11602783203125
Projection step: 13, Loss: 143.19683837890625
Projection step: 14, Loss: 135.620849609375
Projection step: 15, Loss: 147.8409423828125
Projection step: 16, Loss: 138.88131713867188
Projection step: 17, Loss: 131.60684204101562
Projection step: 18, Loss: 130.62130737304688
Projection step: 19, Loss: 138.952392578125
Projection step: 20, Loss: 129.45225524902344
Projection step: 21, Loss: 130.0723114013672
Projection step: 22, Loss: 138.35984802246094
Projection step: 23, Loss: 141.09718322753906
Projection step: 24, Loss: 134.1829833984375
Final likelihood: tensor([-147.2658, -158.7617,  -98.7289, -148.2779, -144.0768, -170.7870,
        -120.9551, -169.6039, -119.4694, -135.0541, -122.0149, -165.6086,
        -124.4979, -146.9845, -170.1862, -172.3558])
Final projection likelihood: -144.6643
1 mode projection failed, trying anyway
New goal: tensor([-0.1415,  0.6685,  0.6183,  0.6217, -0.1766,  1.0380,  0.8908,  1.1827,
         1.4897,  0.2553,  0.1046,  0.4651, -0.2537, -0.0384, -0.1326],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0079]], device='cuda:0') tensor([[0.0080]], device='cuda:0')
Original likelihood: -186.2112579345703
Adjusted likelihood: -186.2112579345703
Likelihood residual: 0.0
Original likelihood: -133.6342010498047
Adjusted likelihood: -133.6342010498047
Likelihood residual: 0.0
{'index': 133.6342010498047, 'thumb_middle': 186.2112579345703}
Current yaw: tensor([-0.2551, -0.0394, -0.1899], device='cuda:0')
5 index
tensor([-0.1549,  0.6748,  0.6158,  0.6198, -0.1796,  1.0590,  0.9101,  1.1642,
         1.5000,  0.2437,  0.0975,  0.4624, -0.2551, -0.0394, -0.1899,  1.6967],
       device='cuda:0')
Solve time for step 1 9.759394872991834
Current ori: tensor([-0.2551, -0.0394, -0.1899], device='cuda:0')
Middle force: tensor([0.6110, 0.5160, 0.5020, 0.5244], device='cuda:0')
Thumb force: tensor([0.5345, 0.5479, 0.5014, 0.5969], device='cuda:0')
tensor([-0.2555,  0.5952,  0.5730,  0.6005, -0.1915,  1.1154,  0.8757,  1.1352,
         1.5000,  0.2671,  0.1050,  0.5457, -0.3942,  0.0108, -0.0688,  2.0427],
       device='cuda:0')
Solve time for step 2 4.1078749560401775
Current ori: tensor([-0.3942,  0.0108, -0.0688], device='cuda:0')
Middle force: tensor([0.5205, 0.5402, 0.5152], device='cuda:0')
Thumb force: tensor([0.5244, 0.5512, 0.5962], device='cuda:0')
tensor([-0.2795,  0.5577,  0.5567,  0.5932, -0.2347,  1.2041,  0.8359,  1.0816,
         1.5000,  0.2433,  0.1402,  0.6903, -0.9343,  0.0348, -0.0639,  2.0705],
       device='cuda:0')
Solve time for step 3 3.934961053018924
Current ori: tensor([-0.9343,  0.0348, -0.0639], device='cuda:0')
Middle force: tensor([0.5011, 0.5224], device='cuda:0')
Thumb force: tensor([0.5016, 0.6075], device='cuda:0')
tensor([-0.2803,  0.4658,  0.5082,  0.5690, -0.3259,  1.5414,  0.7626,  0.9790,
         1.5000,  0.0317,  0.1111,  0.6015, -1.7533,  0.0428, -0.0639,  1.9592],
       device='cuda:0')
Solve time for step 4 3.8698201589868404
Current ori: tensor([-1.7533,  0.0428, -0.0639], device='cuda:0')
Middle force: tensor([0.5048], device='cuda:0')
Thumb force: tensor([0.5053], device='cuda:0')
Storing RECOVERY transition: reward=-1.7369 (scaled=-1.7369), steps=1
Reward stats updated: mean -0.0033 -> -0.0068, std: 0.1280
Collected 490 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1159, Q2 Loss=1.1159, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1993
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6877, Q2 Loss=0.6877, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4400
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4304, Q2 Loss=1.4304, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2221
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1842, Q2 Loss=1.1842, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7175
SAC Update 5/5: Actor Loss=-0.0032, Q1 Loss=1.1960, Q2 Loss=1.1960, Entropy=0.2941, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4341

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.8%)
Q1 update: 0.06s (20.1%)
Q2 update: 0.05s (18.6%)
Actor update: 0.11s (38.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000643
Q1 loss: 1.122831
Q2 loss: 1.122831
Current threshold: -36.3818
Global Scale Offset: 0.0312
Reward stats: mean=-0.0068, std=0.1280, count=490
----------------------------------------------
SAC Update - Actor Loss: -0.0006, Q1 Loss: 1.1228, Q2 Loss: 1.1228, Entropy: 0.0588, Mean TD Error: 0.8026, Threshold: -36.3818
Original likelihood: -1274.0150146484375
Adjusted likelihood: -1274.0150146484375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 31
Loaded trajectory sampler
Current yaw: tensor([-0.0022,  0.0149, -0.0410], device='cuda:0')
Current yaw: tensor([-0.0022,  0.0149, -0.0410], device='cuda:0')
1 turn
Sampling time 3.65396171703469
tensor([ 0.1434,  0.5811,  0.6222,  0.5648, -0.1427,  0.5610,  0.8785,  0.9661,
         1.2100,  0.2965,  0.2700,  1.1913, -0.0022,  0.0149, -0.0410, -0.1203],
       device='cuda:0')
Original likelihood: -23.258344650268555
Adjusted likelihood: -23.258344650268555
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.21467852499336
Current ori: tensor([-0.0022,  0.0149, -0.0410], device='cuda:0')
Middle force: tensor([0.6814, 0.7257, 0.5272, 0.5189, 0.6387, 0.9675, 1.0156, 0.5800, 0.5409,
        0.5630, 0.5895, 0.6560], device='cuda:0')
Thumb force: tensor([0.5582, 2.3887, 0.6445, 1.5333, 1.0556, 0.8795, 2.0000, 0.6127, 0.5148,
        0.6524, 0.5821, 0.5911], device='cuda:0')
Index force: tensor([0.5846, 0.5017, 0.5974, 0.5569, 0.5747, 0.5064, 0.5833, 0.6067, 0.5505,
        0.6369, 0.6314, 0.5738], device='cuda:0')
Storing NORMAL transition: reward=-0.0102 (scaled=-0.0102), steps=1
Reward stats updated: mean -0.0068 -> -0.0068, std: 0.1279
Collected 491 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.7068, Q2 Loss=0.7068, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2823
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0987, Q2 Loss=1.0987, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8163
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1998, Q2 Loss=1.1998, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2138
SAC Update 4/5: Actor Loss=-0.0570, Q1 Loss=0.8132, Q2 Loss=0.8132, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5964
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.1026, Q2 Loss=1.1026, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8363

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.9%)
Q1 update: 0.04s (18.4%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (40.8%)
Target update: 0.00s (1.8%)
Priority update: 0.00s (0.2%)
Actor loss: -0.057456
Q1 loss: 0.984211
Q2 loss: 0.984211
Current threshold: -36.3895
Global Scale Offset: 0.0312
Reward stats: mean=-0.0068, std=0.1279, count=491
----------------------------------------------
SAC Update - Actor Loss: -0.0575, Q1 Loss: 0.9842, Q2 Loss: 0.9842, Entropy: 0.0000, Mean TD Error: 0.9490, Threshold: -36.3895
tensor([ 0.1986,  0.6821,  0.5678,  0.5219, -0.2887,  0.6592,  0.9934,  1.0700,
         1.1677,  0.2396,  0.3295,  1.0991, -0.0256, -0.0239, -0.0318, -0.0903],
       device='cuda:0')
Original likelihood: -48.46879577636719
Adjusted likelihood: -48.46879577636719
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 47.416351318359375
Projection step: 1, Loss: 49.29972839355469
Projection step: 2, Loss: 46.62615966796875
Projection step: 3, Loss: 45.62678146362305
Projection step: 4, Loss: 45.69129180908203
Projection step: 5, Loss: 44.62493133544922
Projection step: 6, Loss: 43.480674743652344
Projection step: 7, Loss: 43.58872985839844
Projection step: 8, Loss: 43.02674102783203
Projection step: 9, Loss: 42.37146759033203
Projection step: 10, Loss: 41.34119415283203
Projection step: 11, Loss: 41.46124267578125
Projection step: 12, Loss: 41.60997009277344
Projection step: 13, Loss: 40.67985534667969
Projection step: 14, Loss: 39.37788772583008
Projection step: 15, Loss: 39.74510192871094
Projection step: 16, Loss: 38.8114013671875
Projection step: 17, Loss: 38.70868682861328
Projection step: 18, Loss: 37.34640884399414
Projection step: 19, Loss: 37.70099639892578
Projection step: 20, Loss: 36.92365264892578
Projection step: 21, Loss: 36.37916564941406
Projection step: 22, Loss: 36.781288146972656
Projection step: 23, Loss: 35.14643478393555
Projection step: 24, Loss: 36.08168029785156
Final likelihood: tensor([-33.2078, -32.4693, -37.3130, -31.5263, -33.1075, -34.2584, -29.8470,
        -35.1707, -33.4594, -34.5648, -25.6926, -33.2212, -36.7719, -36.9360,
        -36.8623, -36.4933])
Final projection likelihood: -33.8064
1 mode projection succeeded
New goal: tensor([ 0.1940,  0.6649,  0.5234,  0.5178, -0.2070,  0.6630,  0.9715,  1.0300,
         1.2387,  0.2798,  0.2243,  1.0188, -0.0273, -0.0160,  0.3314],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0063]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -34.57905578613281
Adjusted likelihood: -34.57905578613281
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 34.57905578613281}
Current yaw: tensor([-0.0256, -0.0239, -0.0318], device='cuda:0')
2 thumb_middle
tensor([ 0.1986,  0.6821,  0.5678,  0.5219, -0.2887,  0.6592,  0.9934,  1.0700,
         1.1677,  0.2396,  0.3295,  1.0991, -0.0256, -0.0239, -0.0318, -0.0903],
       device='cuda:0')
Solve time for step 1 8.834308996971231
Current ori: tensor([-0.0256, -0.0239, -0.0318], device='cuda:0')
Index force: tensor([0.5870, 0.5914, 0.5807, 0.5983], device='cuda:0')
tensor([ 0.1978,  0.6902,  0.5531,  0.5248, -0.3159,  0.6330,  0.9412,  1.0211,
         1.2020,  0.2601,  0.1774,  1.0051, -0.0273, -0.0238, -0.0318, -0.0898],
       device='cuda:0')
Solve time for step 2 3.580368167022243
Current ori: tensor([-0.0273, -0.0238, -0.0318], device='cuda:0')
Index force: tensor([0.5860, 0.5779, 0.5948], device='cuda:0')
tensor([ 0.1794,  0.6995,  0.5241,  0.5018, -0.3218,  0.6362,  0.9385,  1.0132,
         1.2230,  0.2678,  0.1592,  0.9957, -0.0310, -0.0124, -0.0318, -0.1369],
       device='cuda:0')
Solve time for step 3 3.387031455989927
Current ori: tensor([-0.0310, -0.0124, -0.0318], device='cuda:0')
Index force: tensor([0.5720, 0.5894], device='cuda:0')
tensor([ 0.1942,  0.6996,  0.5353,  0.5213, -0.3199,  0.6391,  0.9386,  1.0126,
         1.2205,  0.2679,  0.1516,  0.9898, -0.0292, -0.0219, -0.0318, -0.1091],
       device='cuda:0')
Solve time for step 4 3.3462900950107723
Current ori: tensor([-0.0292, -0.0219, -0.0318], device='cuda:0')
Index force: tensor([0.5757], device='cuda:0')
Storing RECOVERY transition: reward=0.0058 (scaled=0.0058), steps=1
Reward stats updated: mean -0.0068 -> -0.0068, std: 0.1278
Collected 492 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9629, Q2 Loss=0.9629, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9142
SAC Update 2/5: Actor Loss=-0.1278, Q1 Loss=1.1554, Q2 Loss=1.1554, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0107
SAC Update 3/5: Actor Loss=-0.3045, Q1 Loss=1.2757, Q2 Loss=1.2757, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8602
SAC Update 4/5: Actor Loss=-0.0692, Q1 Loss=1.2545, Q2 Loss=1.2545, Entropy=0.0070, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4809
SAC Update 5/5: Actor Loss=-0.0714, Q1 Loss=1.1205, Q2 Loss=1.1205, Entropy=0.0032, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6353

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.1%)
Q1 update: 0.05s (20.3%)
Q2 update: 0.05s (19.0%)
Actor update: 0.11s (41.1%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.114577
Q1 loss: 1.153818
Q2 loss: 1.153818
Current threshold: -36.4169
Global Scale Offset: 0.0310
Reward stats: mean=-0.0068, std=0.1278, count=492
----------------------------------------------
SAC Update - Actor Loss: -0.1146, Q1 Loss: 1.1538, Q2 Loss: 1.1538, Entropy: 0.0020, Mean TD Error: 0.9803, Threshold: -36.4169
Original likelihood: -38.24637985229492
Adjusted likelihood: -38.24637985229492
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 38.33707046508789
Projection step: 1, Loss: 38.10036087036133
Projection step: 2, Loss: 37.192626953125
Projection step: 3, Loss: 36.316162109375
Projection step: 4, Loss: 36.41257095336914
Projection step: 5, Loss: 37.86836242675781
Projection step: 6, Loss: 35.314762115478516
Projection step: 7, Loss: 36.66526794433594
Projection step: 8, Loss: 33.77508544921875
Projection step: 9, Loss: 33.699501037597656
Projection step: 10, Loss: 34.026878356933594
Projection step: 11, Loss: 33.86055374145508
Projection step: 12, Loss: 33.94627380371094
Projection step: 13, Loss: 33.593284606933594
Projection step: 14, Loss: 32.88678741455078
Projection step: 15, Loss: 33.10575485229492
Projection step: 16, Loss: 32.19757080078125
Projection step: 17, Loss: 32.59736633300781
Projection step: 18, Loss: 31.27593421936035
Projection step: 19, Loss: 32.41731262207031
Projection step: 20, Loss: 32.02204513549805
Projection step: 21, Loss: 31.144695281982422
Projection step: 22, Loss: 30.91971778869629
Projection step: 23, Loss: 30.549205780029297
Projection step: 24, Loss: 30.19124984741211
Final likelihood: tensor([-35.1639, -28.8877, -31.0710, -29.4800, -30.8024, -28.1064, -28.6839,
        -39.1594, -31.3340, -29.2956, -32.6175, -31.2297, -31.9132, -31.6881,
        -29.9320, -28.2024])
Final projection likelihood: -31.0980
1 mode projection succeeded
New goal: tensor([ 0.1779,  0.6632,  0.5077,  0.5526, -0.1719,  0.6806,  0.9623,  0.9781,
         1.3284,  0.2775,  0.1596,  1.0074, -0.0272, -0.0125,  0.6964],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -32.09822463989258
Adjusted likelihood: -32.09822463989258
Likelihood residual: 0.0
Original likelihood: -35.373897552490234
Adjusted likelihood: -35.373897552490234
Likelihood residual: 0.0
{'index': 35.373897552490234, 'thumb_middle': 32.09822463989258}
Current yaw: tensor([-0.0263, -0.0183, -0.0374], device='cuda:0')
3 thumb_middle
tensor([ 0.1886,  0.6880,  0.5416,  0.5337, -0.2524,  0.6875,  0.9729,  1.0262,
         1.2816,  0.2882,  0.2088,  1.0197, -0.0263, -0.0183, -0.0374, -0.1049],
       device='cuda:0')
Solve time for step 1 8.993402263033204
Current ori: tensor([-0.0263, -0.0183, -0.0374], device='cuda:0')
Index force: tensor([0.5843, 0.6045, 0.5828, 0.5028], device='cuda:0')
tensor([ 0.1775,  0.6844,  0.5225,  0.5565, -0.2997,  0.6356,  0.9174,  0.9627,
         1.2922,  0.2671,  0.1002,  0.9765, -0.0245, -0.0106, -0.0375, -0.1022],
       device='cuda:0')
Solve time for step 2 3.5839865540037863
Current ori: tensor([-0.0245, -0.0106, -0.0375], device='cuda:0')
Index force: tensor([0.5020, 0.5806, 0.5772], device='cuda:0')
tensor([ 0.1775,  0.6952,  0.5095,  0.5517, -0.3021,  0.6390,  0.9133,  0.9507,
         1.3052,  0.2664,  0.0857,  0.9766, -0.0271, -0.0111, -0.0375, -0.1074],
       device='cuda:0')
Solve time for step 3 3.448396356019657
Current ori: tensor([-0.0271, -0.0111, -0.0375], device='cuda:0')
Index force: tensor([0.5648, 0.5634], device='cuda:0')
tensor([ 0.1934,  0.6966,  0.5211,  0.5618, -0.2987,  0.6419,  0.9188,  0.9564,
         1.2981,  0.2618,  0.0786,  0.9714, -0.0261, -0.0209, -0.0375, -0.0812],
       device='cuda:0')
Solve time for step 4 3.3278157510212623
Current ori: tensor([-0.0261, -0.0209, -0.0375], device='cuda:0')
Index force: tensor([0.5391], device='cuda:0')
Storing RECOVERY transition: reward=0.0170 (scaled=0.0170), steps=1
Reward stats updated: mean -0.0068 -> -0.0067, std: 0.1276
Collected 493 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.7309, Q2 Loss=0.7309, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8570
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8912, Q2 Loss=0.8912, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4903
SAC Update 3/5: Actor Loss=-0.0042, Q1 Loss=1.1612, Q2 Loss=1.1612, Entropy=0.3423, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6245
SAC Update 4/5: Actor Loss=-0.0577, Q1 Loss=0.7945, Q2 Loss=0.7945, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4783
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8974, Q2 Loss=0.8974, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0059

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.9%)
Target Q: 0.04s (17.9%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (19.4%)
Actor update: 0.09s (39.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.058437
Q1 loss: 0.895040
Q2 loss: 0.895040
Current threshold: -36.4775
Global Scale Offset: 0.0304
Reward stats: mean=-0.0067, std=0.1276, count=493
----------------------------------------------
SAC Update - Actor Loss: -0.0584, Q1 Loss: 0.8950, Q2 Loss: 0.8950, Entropy: 0.0685, Mean TD Error: 0.6912, Threshold: -36.4775
Original likelihood: -34.65058135986328
Adjusted likelihood: -34.65058135986328
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0229, -0.0105, -0.0482], device='cuda:0')
4 turn
Sampling time 3.6794667540234514
tensor([ 0.1747,  0.6798,  0.5208,  0.5661, -0.2415,  0.6716,  0.9593,  0.9772,
         1.3818,  0.2976,  0.1118,  1.0062, -0.0229, -0.0105, -0.0482, -0.0633],
       device='cuda:0')
Original likelihood: -35.5093994140625
Adjusted likelihood: -35.5093994140625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.45585895999102
Current ori: tensor([-0.0229, -0.0105, -0.0482], device='cuda:0')
Middle force: tensor([0.7603, 1.9509, 0.5636, 1.4580, 0.5803, 0.9205, 0.5002, 0.5183, 0.5595,
        0.5070, 0.4895, 0.5750], device='cuda:0')
Thumb force: tensor([0.7415, 1.3825, 1.5272, 0.5212, 0.6433, 0.7175, 0.5396, 0.6926, 0.5335,
        0.6561, 0.5051, 0.6218], device='cuda:0')
Index force: tensor([0.5295, 1.5033, 0.5827, 0.6488, 0.7042, 0.6695, 0.6321, 0.5009, 0.5843,
        0.6351, 0.7214, 0.5913], device='cuda:0')
Storing NORMAL transition: reward=-0.0037 (scaled=-0.0037), steps=1
Reward stats updated: mean -0.0067 -> -0.0067, std: 0.1275
Collected 494 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6775, Q2 Loss=0.6775, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1952
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6038, Q2 Loss=1.6038, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2308
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5376, Q2 Loss=1.5376, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2302
SAC Update 4/5: Actor Loss=-0.0962, Q1 Loss=1.2480, Q2 Loss=1.2480, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4565
SAC Update 5/5: Actor Loss=-0.1282, Q1 Loss=1.0362, Q2 Loss=1.0362, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6646

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.7%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (40.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.044882
Q1 loss: 1.220615
Q2 loss: 1.220615
Current threshold: -36.5314
Global Scale Offset: 0.0299
Reward stats: mean=-0.0067, std=0.1275, count=494
----------------------------------------------
SAC Update - Actor Loss: -0.0449, Q1 Loss: 1.2206, Q2 Loss: 1.2206, Entropy: 0.0000, Mean TD Error: 0.7555, Threshold: -36.5314
tensor([ 0.1517,  0.6580,  0.5212,  0.5741, -0.2571,  0.6771,  0.9279,  0.9792,
         1.4101,  0.2778,  0.1152,  0.9914, -0.0217,  0.0045, -0.0444, -0.0161],
       device='cuda:0')
Original likelihood: -33.909950256347656
Adjusted likelihood: -33.909950256347656
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.621776125044562
Current ori: tensor([-0.0217,  0.0045, -0.0444], device='cuda:0')
Middle force: tensor([1.8685, 0.5604, 1.4186, 0.5708, 0.8945, 0.5001, 0.5154, 0.5563, 0.5051,
        0.5044, 0.5713], device='cuda:0')
Thumb force: tensor([1.3559, 1.4884, 0.5173, 0.6432, 0.7143, 0.5382, 0.6848, 0.5281, 0.6543,
        0.5039, 0.6150], device='cuda:0')
Index force: tensor([1.4383, 0.5716, 0.6369, 0.6872, 0.6576, 0.6226, 0.5004, 0.5797, 0.6265,
        0.7332, 0.5851], device='cuda:0')
Storing NORMAL transition: reward=0.0384 (scaled=0.0384), steps=1
Reward stats updated: mean -0.0067 -> -0.0066, std: 0.1274
Collected 495 transitions for RL
SAC Update 1/5: Actor Loss=-0.0432, Q1 Loss=1.0691, Q2 Loss=1.0691, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9597
SAC Update 2/5: Actor Loss=-0.0162, Q1 Loss=0.6713, Q2 Loss=0.6713, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7336
SAC Update 3/5: Actor Loss=-0.0336, Q1 Loss=0.7026, Q2 Loss=0.7026, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2933
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9390, Q2 Loss=0.9390, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6852
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9367, Q2 Loss=0.9367, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7429

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.4%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (18.7%)
Actor update: 0.09s (39.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.018600
Q1 loss: 0.863745
Q2 loss: 0.863745
Current threshold: -36.5824
Global Scale Offset: 0.0293
Reward stats: mean=-0.0066, std=0.1274, count=495
----------------------------------------------
SAC Update - Actor Loss: -0.0186, Q1 Loss: 0.8637, Q2 Loss: 0.8637, Entropy: 0.0000, Mean TD Error: 0.8829, Threshold: -36.5824
tensor([ 0.1726,  0.7615,  0.3952,  0.5612, -0.2042,  0.6767,  0.9107,  0.9739,
         1.4246,  0.2989,  0.1479,  0.8176, -0.0397, -0.0157, -0.0842,  0.0874],
       device='cuda:0')
Original likelihood: -31.707500457763672
Adjusted likelihood: -31.707500457763672
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.153087175975088
Current ori: tensor([-0.0397, -0.0157, -0.0842], device='cuda:0')
Middle force: tensor([0.5096, 0.5055, 0.6118, 1.5212, 0.5493, 0.5092, 0.5366, 0.5180, 0.5375,
        0.5175], device='cuda:0')
Thumb force: tensor([0.5038, 0.8464, 0.8473, 0.9906, 0.7275, 0.5807, 1.0430, 0.8431, 0.7059,
        0.5539], device='cuda:0')
Index force: tensor([0.7897, 0.6152, 0.5111, 0.5073, 0.5129, 0.5733, 0.5264, 0.5335, 0.5023,
        0.5055], device='cuda:0')
Storing NORMAL transition: reward=-0.0734 (scaled=-0.0734), steps=1
Reward stats updated: mean -0.0066 -> -0.0068, std: 0.1273
Collected 496 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2582, Q2 Loss=1.2582, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5086
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.6354, Q2 Loss=0.6354, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0674
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.8558, Q2 Loss=0.8558, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4814
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=4.3094, Q2 Loss=4.3094, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.0189
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=2.7447, Q2 Loss=2.7447, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8010

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.2%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (39.7%)
Target update: 0.00s (1.8%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 1.960731
Q2 loss: 1.960731
Current threshold: -36.6126
Global Scale Offset: 0.0290
Reward stats: mean=-0.0068, std=0.1273, count=496
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.9607, Q2 Loss: 1.9607, Entropy: 0.0000, Mean TD Error: 2.1755, Threshold: -36.6126
tensor([ 0.1417,  0.6573,  0.5541,  0.4843, -0.2229,  0.7420,  0.7621,  0.9924,
         1.3859,  0.2746,  0.2250,  0.7414, -0.0247,  0.0064, -0.0095, -0.0350],
       device='cuda:0')
Original likelihood: -36.85749053955078
Adjusted likelihood: -36.85749053955078
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0384)
State is out of distribution
Projection step: 0, Loss: 36.08233642578125
Projection step: 1, Loss: 34.73776626586914
Projection step: 2, Loss: 33.31787109375
Projection step: 3, Loss: 34.760986328125
Projection step: 4, Loss: 32.9053955078125
Projection step: 5, Loss: 31.84889030456543
Projection step: 6, Loss: 31.959760665893555
Projection step: 7, Loss: 31.025836944580078
Projection step: 8, Loss: 32.28681945800781
Projection step: 9, Loss: 29.628982543945312
Projection step: 10, Loss: 29.487098693847656
Projection step: 11, Loss: 30.599590301513672
Projection step: 12, Loss: 30.55378532409668
Projection step: 13, Loss: 28.992183685302734
Projection step: 14, Loss: 28.16681671142578
Projection step: 15, Loss: 27.572832107543945
Projection step: 16, Loss: 27.36469078063965
Projection step: 17, Loss: 27.135351181030273
Projection step: 18, Loss: 26.884275436401367
Projection step: 19, Loss: 26.60739517211914
Projection step: 20, Loss: 25.853382110595703
Projection step: 21, Loss: 28.080434799194336
Projection step: 22, Loss: 24.332483291625977
Projection step: 23, Loss: 25.083148956298828
Projection step: 24, Loss: 26.08057403564453
Final likelihood: tensor([-28.0604, -24.4350, -25.1064, -27.9986, -30.4743, -22.6983, -26.2835,
        -28.3093, -24.4214, -23.1950, -21.1206, -24.4816, -23.5746, -25.7921,
        -24.3796, -24.1754])
Final projection likelihood: -25.2816
1 mode projection succeeded
New goal: tensor([ 0.1221,  0.6211,  0.5323,  0.5664, -0.1525,  0.7107,  0.8030,  0.8934,
         1.4346,  0.2539,  0.1988,  0.9765, -0.0266,  0.0072,  1.2424],
       device='cuda:0')
tensor([[0.0069]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0025]], device='cuda:0')
Original likelihood: -26.32928466796875
Adjusted likelihood: -26.32928466796875
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 26.32928466796875}
Current yaw: tensor([-0.0247,  0.0064, -0.0095], device='cuda:0')
5 thumb_middle
tensor([ 0.1417,  0.6573,  0.5541,  0.4843, -0.2229,  0.7420,  0.7621,  0.9924,
         1.3859,  0.2746,  0.2250,  0.7414, -0.0247,  0.0064, -0.0095, -0.0350],
       device='cuda:0')
Solve time for step 1 8.900719274010044
Current ori: tensor([-0.0247,  0.0064, -0.0095], device='cuda:0')
Index force: tensor([0.5789, 0.5941, 0.6050, 0.5710], device='cuda:0')
tensor([ 0.1101,  0.6417,  0.5210,  0.5331, -0.2648,  0.6905,  0.7523,  0.8825,
         1.3736,  0.2361,  0.1123,  0.8912, -0.0184,  0.0265, -0.0095, -0.0557],
       device='cuda:0')
Solve time for step 2 3.648829386977013
Current ori: tensor([-0.0184,  0.0265, -0.0095], device='cuda:0')
Index force: tensor([0.5834, 0.5961, 0.5646], device='cuda:0')
tensor([ 0.1236,  0.6477,  0.5205,  0.5454, -0.2609,  0.6937,  0.7572,  0.8680,
         1.3727,  0.2295,  0.0892,  0.9166, -0.0190,  0.0185, -0.0095, -0.0329],
       device='cuda:0')
Solve time for step 3 3.4619875429780222
Current ori: tensor([-0.0190,  0.0185, -0.0095], device='cuda:0')
Index force: tensor([0.5871, 0.5591], device='cuda:0')
tensor([ 0.1323,  0.6487,  0.5261,  0.5499, -0.2589,  0.6933,  0.7626,  0.8665,
         1.3722,  0.2272,  0.0820,  0.9177, -0.0189,  0.0133, -0.0095, -0.0180],
       device='cuda:0')
Solve time for step 4 3.36394658498466
Current ori: tensor([-0.0189,  0.0133, -0.0095], device='cuda:0')
Index force: tensor([0.5481], device='cuda:0')
Storing RECOVERY transition: reward=0.0014 (scaled=0.0005), steps=3
Reward stats updated: mean -0.0068 -> -0.0067, std: 0.1272
Collected 497 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2984, Q2 Loss=1.2984, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8037
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9255, Q2 Loss=0.9255, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0319
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0261, Q2 Loss=1.0261, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4428
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4108, Q2 Loss=1.4108, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8867
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7852, Q2 Loss=0.7852, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7826

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.4%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (19.3%)
Actor update: 0.11s (41.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.089177
Q2 loss: 1.089177
Current threshold: -36.6304
Global Scale Offset: 0.0288
Reward stats: mean=-0.0067, std=0.1272, count=497
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0892, Q2 Loss: 1.0892, Entropy: 0.0000, Mean TD Error: 0.7896, Threshold: -36.6304
Original likelihood: -30.32290267944336
Adjusted likelihood: -30.32290267944336
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0140,  0.0149, -0.0107], device='cuda:0')
6 turn
Sampling time 3.7208049899782054
tensor([ 0.1293,  0.6340,  0.5343,  0.5671, -0.2134,  0.7212,  0.7928,  0.8857,
         1.4325,  0.2450,  0.1371,  0.9542, -0.0140,  0.0149, -0.0107,  0.0120],
       device='cuda:0')
Original likelihood: -29.60106658935547
Adjusted likelihood: -29.60106658935547
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.651030741049908
Current ori: tensor([-0.0140,  0.0149, -0.0107], device='cuda:0')
Middle force: tensor([1.3316, 0.5132, 0.5002, 0.4950, 0.5805, 0.5723, 1.1243, 0.8088, 0.8159,
        0.5924, 0.5435, 0.5470], device='cuda:0')
Thumb force: tensor([1.9511, 1.9949, 1.4545, 0.6903, 1.1458, 0.8395, 1.4870, 0.5789, 0.7639,
        0.7497, 0.5969, 0.6498], device='cuda:0')
Index force: tensor([0.5720, 0.8336, 0.7944, 0.6566, 0.5606, 0.5471, 0.5782, 0.5247, 0.5735,
        0.5641, 0.5053, 0.5835], device='cuda:0')
Storing NORMAL transition: reward=-0.1222 (scaled=-0.1222), steps=1
Reward stats updated: mean -0.0067 -> -0.0070, std: 0.1271
Collected 498 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1918, Q2 Loss=1.1918, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5956
SAC Update 2/5: Actor Loss=-0.0132, Q1 Loss=1.0467, Q2 Loss=1.0467, Entropy=0.1778, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5580
SAC Update 3/5: Actor Loss=-0.0007, Q1 Loss=0.8770, Q2 Loss=0.8770, Entropy=0.3463, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0428
SAC Update 4/5: Actor Loss=-0.2184, Q1 Loss=1.5695, Q2 Loss=1.5695, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1892
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.3400, Q2 Loss=1.3400, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6686

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.9%)
Q1 update: 0.04s (18.5%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (40.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046473
Q1 loss: 1.204997
Q2 loss: 1.204997
Current threshold: -36.6499
Global Scale Offset: 0.0287
Reward stats: mean=-0.0070, std=0.1271, count=498
----------------------------------------------
SAC Update - Actor Loss: -0.0465, Q1 Loss: 1.2050, Q2 Loss: 1.2050, Entropy: 0.1048, Mean TD Error: 1.0108, Threshold: -36.6499
tensor([ 0.0466,  0.5843,  0.4972,  0.6169, -0.2389,  0.8169,  0.6495,  0.8349,
         1.4504,  0.2177,  0.0914,  1.0992, -0.0073,  0.0441,  0.1104, -0.0356],
       device='cuda:0')
Original likelihood: -34.45396423339844
Adjusted likelihood: -34.45396423339844
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.455875865009148
Current ori: tensor([-0.0073,  0.0441,  0.1104], device='cuda:0')
Middle force: tensor([0.5130, 0.5002, 0.5025, 0.5798, 0.5746, 1.1072, 0.8077, 0.8132, 0.5931,
        0.5460, 0.5500], device='cuda:0')
Thumb force: tensor([1.9521, 1.4255, 0.6807, 1.1228, 0.8252, 1.4606, 0.5732, 0.7525, 0.7364,
        0.5924, 0.6382], device='cuda:0')
Index force: tensor([0.8317, 0.7954, 0.6800, 0.5594, 0.5428, 0.5745, 0.5237, 0.5699, 0.5611,
        0.5046, 0.5787], device='cuda:0')
Storing NORMAL transition: reward=-0.0314 (scaled=-0.0314), steps=1
Reward stats updated: mean -0.0070 -> -0.0070, std: 0.1270
Collected 499 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8738, Q2 Loss=0.8738, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5932
SAC Update 2/5: Actor Loss=-0.0416, Q1 Loss=1.0522, Q2 Loss=1.0522, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9509
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.6594, Q2 Loss=0.6594, Entropy=0.1279, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1823
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0581, Q2 Loss=1.0581, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3353
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7131, Q2 Loss=0.7131, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4876

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.0%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.04s (18.3%)
Actor update: 0.10s (39.8%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.008333
Q1 loss: 0.871322
Q2 loss: 0.871322
Current threshold: -36.6633
Global Scale Offset: 0.0286
Reward stats: mean=-0.0070, std=0.1270, count=499
----------------------------------------------
SAC Update - Actor Loss: -0.0083, Q1 Loss: 0.8713, Q2 Loss: 0.8713, Entropy: 0.0256, Mean TD Error: 0.9099, Threshold: -36.6633
tensor([ 0.0968,  0.5814,  0.5443,  0.6292, -0.1553,  0.8918,  0.5701,  0.6800,
         1.3166,  0.4183,  0.1377,  1.1277, -0.0071,  0.0170,  0.1431, -0.0103],
       device='cuda:0')
Original likelihood: -29.68008041381836
Adjusted likelihood: -29.68008041381836
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.126664756971877
Current ori: tensor([-0.0071,  0.0170,  0.1431], device='cuda:0')
Middle force: tensor([0.5002, 0.5019, 0.5761, 0.5694, 1.0857, 0.7970, 0.7979, 0.5874, 0.5436,
        0.5456], device='cuda:0')
Thumb force: tensor([1.3957, 0.6736, 1.1079, 0.8181, 1.4379, 0.5694, 0.7482, 0.7314, 0.5876,
        0.6377], device='cuda:0')
Index force: tensor([0.7937, 0.6914, 0.5570, 0.5405, 0.5711, 0.5233, 0.5677, 0.5590, 0.5044,
        0.5768], device='cuda:0')
Storing NORMAL transition: reward=-0.0212 (scaled=-0.0212), steps=1
Reward stats updated: mean -0.0070 -> -0.0071, std: 0.1269
Collected 500 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=3.9440, Q2 Loss=3.9440, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.1280
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7541, Q2 Loss=0.7541, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5709
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.8982, Q2 Loss=1.8982, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2213
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.2791, Q2 Loss=1.2791, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8838
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1145, Q2 Loss=1.1145, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3075

------ SAC Update Summary (5 iterations) ------
Total time: 0.31s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.0%)
Q1 update: 0.05s (15.8%)
Q2 update: 0.05s (14.7%)
Actor update: 0.15s (47.2%)
Target update: 0.00s (1.0%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000000
Q1 loss: 1.797958
Q2 loss: 1.797958
Current threshold: -36.6713
Global Scale Offset: 0.0285
Reward stats: mean=-0.0071, std=0.1269, count=500
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.7980, Q2 Loss: 1.7980, Entropy: 0.0000, Mean TD Error: 2.4223, Threshold: -36.6713
tensor([ 0.3037,  0.6008,  0.6336,  0.6730, -0.1877,  0.7845,  0.5446,  0.6490,
         1.2817,  0.4638,  0.1819,  1.2417,  0.0327,  0.0600,  0.1613, -0.9041],
       device='cuda:0')
Original likelihood: -42.778839111328125
Adjusted likelihood: -42.778839111328125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 42.792625427246094
Projection step: 1, Loss: 43.505584716796875
Projection step: 2, Loss: 41.98075866699219
Projection step: 3, Loss: 39.8426513671875
Projection step: 4, Loss: 40.867095947265625
Projection step: 5, Loss: 39.405494689941406
Projection step: 6, Loss: 38.77009582519531
Projection step: 7, Loss: 38.64630889892578
Projection step: 8, Loss: 39.284332275390625
Projection step: 9, Loss: 38.147254943847656
Projection step: 10, Loss: 37.21364212036133
Projection step: 11, Loss: 36.8473014831543
Projection step: 12, Loss: 37.35107421875
Projection step: 13, Loss: 36.80974197387695
Projection step: 14, Loss: 35.57423400878906
Projection step: 15, Loss: 35.7922477722168
Projection step: 16, Loss: 35.51814651489258
Projection step: 17, Loss: 35.03900146484375
Projection step: 18, Loss: 33.67470932006836
Projection step: 19, Loss: 33.88975524902344
Projection step: 20, Loss: 34.92259216308594
Projection step: 21, Loss: 32.73448181152344
Projection step: 22, Loss: 33.51648712158203
Projection step: 23, Loss: 32.91466522216797
Projection step: 24, Loss: 31.838272094726562
Final likelihood: tensor([-28.9439, -28.8904, -31.5430, -29.2208, -34.2836, -30.8457, -35.0535,
        -33.5735, -29.6589, -29.4822, -30.1104, -29.7393, -33.4116, -29.7427,
        -34.0781, -29.4926])
Final projection likelihood: -31.1294
1 mode projection succeeded
New goal: tensor([ 0.2616,  0.5921,  0.6832,  0.6262, -0.1564,  0.7516,  0.6583,  0.7311,
         1.3151,  0.3928,  0.2081,  1.2060,  0.0352,  0.0528,  1.4451],
       device='cuda:0')
tensor([[0.0033]], device='cuda:0') tensor([[0.0069]], device='cuda:0') tensor([[0.0081]], device='cuda:0')
Original likelihood: -33.80792999267578
Adjusted likelihood: -33.80792999267578
Likelihood residual: 0.0
Original likelihood: -31.471317291259766
Adjusted likelihood: -31.471317291259766
Likelihood residual: 0.0
{'index': 31.471317291259766, 'thumb_middle': 33.80792999267578}
Current yaw: tensor([0.0327, 0.0600, 0.1613], device='cuda:0')
7 index
tensor([ 0.3037,  0.6008,  0.6336,  0.6730, -0.1877,  0.7845,  0.5446,  0.6490,
         1.2817,  0.4638,  0.1819,  1.2417,  0.0327,  0.0600,  0.1613, -0.9041],
       device='cuda:0')
Solve time for step 1 10.521976112038828
Current ori: tensor([0.0327, 0.0600, 0.1613], device='cuda:0')
Middle force: tensor([0.5717, 0.5770, 0.5652, 0.5601], device='cuda:0')
Thumb force: tensor([0.5884, 0.6009, 0.5712, 0.5993], device='cuda:0')
tensor([ 0.3349,  0.5644,  0.6442,  0.6199, -0.1873,  0.7462,  0.6227,  0.7100,
         1.3289,  0.4155,  0.1904,  1.2270,  0.0440,  0.0739,  0.1556, -1.2280],
       device='cuda:0')
Solve time for step 2 4.256533736013807
Current ori: tensor([0.0440, 0.0739, 0.1556], device='cuda:0')
Middle force: tensor([0.5090, 0.5197, 0.5429], device='cuda:0')
Thumb force: tensor([0.5302, 0.5525, 0.5830], device='cuda:0')
tensor([ 0.3282,  0.5607,  0.6439,  0.6109, -0.1754,  0.7508,  0.6266,  0.7066,
         1.3352,  0.4081,  0.1772,  1.2161,  0.0393,  0.0662,  0.1186, -1.0695],
       device='cuda:0')
Solve time for step 3 4.006071184994653
Current ori: tensor([0.0393, 0.0662, 0.1186], device='cuda:0')
Middle force: tensor([0.5026, 0.5479], device='cuda:0')
Thumb force: tensor([0.5771, 0.5820], device='cuda:0')
tensor([ 0.3160,  0.5522,  0.6407,  0.6071, -0.1832,  0.7437,  0.6421,  0.7218,
         1.3373,  0.4130,  0.1703,  1.2269,  0.0395,  0.0676,  0.1175, -0.7317],
       device='cuda:0')
Solve time for step 4 3.9557221140130423
Current ori: tensor([0.0395, 0.0676, 0.1175], device='cuda:0')
Middle force: tensor([0.5354], device='cuda:0')
Thumb force: tensor([0.5621], device='cuda:0')
Storing RECOVERY transition: reward=0.0510 (scaled=0.0170), steps=3
Reward stats updated: mean -0.0071 -> -0.0070, std: 0.1268
Collected 501 transitions for RL
SAC Update 1/5: Actor Loss=-0.0948, Q1 Loss=1.7802, Q2 Loss=1.7802, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3877
SAC Update 2/5: Actor Loss=-0.1701, Q1 Loss=1.1413, Q2 Loss=1.1413, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7268
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=5.5615, Q2 Loss=5.5615, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.3989
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6923, Q2 Loss=1.6923, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5101
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7793, Q2 Loss=0.7793, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8157

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (14.8%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (20.1%)
Actor update: 0.11s (42.0%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.099044
Q1 loss: 2.190919
Q2 loss: 2.190919
Current threshold: -36.7173
Global Scale Offset: 0.0278
Reward stats: mean=-0.0070, std=0.1268, count=501
----------------------------------------------
SAC Update - Actor Loss: -0.0990, Q1 Loss: 2.1909, Q2 Loss: 2.1909, Entropy: 0.0000, Mean TD Error: 2.7678, Threshold: -36.7173
Original likelihood: -33.88335418701172
Adjusted likelihood: -33.88335418701172
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([0.0340, 0.0609, 0.1098], device='cuda:0')
8 turn
Sampling time 3.6888880300102755
tensor([ 0.2305,  0.5953,  0.6788,  0.6249, -0.1726,  0.7648,  0.6271,  0.6996,
         1.3349,  0.4184,  0.1582,  1.2267,  0.0340,  0.0609,  0.1098, -0.4930],
       device='cuda:0')
Original likelihood: -34.55821228027344
Adjusted likelihood: -34.55821228027344
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.048052628000733
Current ori: tensor([0.0340, 0.0609, 0.1098], device='cuda:0')
Middle force: tensor([0.9719, 1.7013, 0.7288, 0.5086, 0.5329, 0.4874, 0.9830, 0.5263, 0.5176,
        0.6974, 0.5006, 0.5699], device='cuda:0')
Thumb force: tensor([2.8473, 1.2256, 0.6982, 0.9660, 1.1298, 0.6385, 1.8302, 0.7634, 0.8970,
        0.9493, 0.5515, 0.5944], device='cuda:0')
Index force: tensor([1.0795, 0.5837, 0.5170, 0.5131, 0.5356, 0.5753, 0.9141, 0.6626, 0.5171,
        0.5640, 0.5195, 0.5729], device='cuda:0')
Storing NORMAL transition: reward=0.0154 (scaled=0.0154), steps=1
Reward stats updated: mean -0.0070 -> -0.0070, std: 0.1266
Collected 502 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6723, Q2 Loss=0.6723, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2604
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8510, Q2 Loss=0.8510, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6691
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6221, Q2 Loss=1.6221, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4227
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.3567, Q2 Loss=1.3567, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2506
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0968, Q2 Loss=1.0968, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8766

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (16.9%)
Q1 update: 0.05s (20.3%)
Q2 update: 0.05s (19.3%)
Actor update: 0.10s (40.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.119769
Q2 loss: 1.119769
Current threshold: -36.7524
Global Scale Offset: 0.0273
Reward stats: mean=-0.0070, std=0.1266, count=502
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1198, Q2 Loss: 1.1198, Entropy: 0.0000, Mean TD Error: 0.8959, Threshold: -36.7524
tensor([ 0.2032,  0.5816,  0.7177,  0.4903, -0.1876,  0.6432,  0.8546,  0.7454,
         1.4933,  0.5099,  0.0438,  1.0991,  0.0880,  0.0509,  0.0888, -1.0442],
       device='cuda:0')
Original likelihood: -34.983951568603516
Adjusted likelihood: -34.983951568603516
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.5608990309992805
Current ori: tensor([0.0880, 0.0509, 0.0888], device='cuda:0')
Middle force: tensor([1.6872, 0.7572, 0.5096, 0.5444, 0.5049, 0.9864, 0.5344, 0.5186, 0.6977,
        0.5010, 0.5647], device='cuda:0')
Thumb force: tensor([1.2110, 0.6905, 0.9632, 1.0994, 0.6331, 1.8117, 0.7089, 0.8828, 0.9305,
        0.5283, 0.5976], device='cuda:0')
Index force: tensor([0.6035, 0.5161, 0.5153, 0.5331, 0.5932, 0.9211, 0.6693, 0.5165, 0.5603,
        0.5194, 0.5688], device='cuda:0')
Storing NORMAL transition: reward=0.0148 (scaled=0.0148), steps=1
Reward stats updated: mean -0.0070 -> -0.0069, std: 0.1265
Collected 503 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.8009, Q2 Loss=0.8009, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4905
SAC Update 2/5: Actor Loss=-0.0856, Q1 Loss=1.5991, Q2 Loss=1.5991, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2786
SAC Update 3/5: Actor Loss=-0.1351, Q1 Loss=1.3324, Q2 Loss=1.3324, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3306
SAC Update 4/5: Actor Loss=-0.1505, Q1 Loss=1.1189, Q2 Loss=1.1189, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6422
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4916, Q2 Loss=1.4916, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5317

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (40.0%)
Target update: 0.00s (1.8%)
Priority update: 0.00s (0.2%)
Actor loss: -0.120289
Q1 loss: 1.268550
Q2 loss: 1.268550
Current threshold: -36.7733
Global Scale Offset: 0.0270
Reward stats: mean=-0.0069, std=0.1265, count=503
----------------------------------------------
SAC Update - Actor Loss: -0.1203, Q1 Loss: 1.2686, Q2 Loss: 1.2686, Entropy: 0.0000, Mean TD Error: 1.2548, Threshold: -36.7733
tensor([ 0.2123,  0.5972,  0.6517,  0.6464, -0.1404,  0.5618,  1.0209,  0.9100,
         1.5000,  0.4127,  0.1640,  0.9561,  0.1508, -0.0055,  0.0608, -1.4004],
       device='cuda:0')
Original likelihood: -28.40504264831543
Adjusted likelihood: -28.40504264831543
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.183714986022096
Current ori: tensor([ 0.1508, -0.0055,  0.0608], device='cuda:0')
Middle force: tensor([0.5004, 0.5066, 0.5945, 0.5738, 0.9387, 0.7620, 0.7891, 0.5618, 0.7904,
        0.5128], device='cuda:0')
Thumb force: tensor([1.1828, 0.5833, 0.9848, 0.7670, 1.3271, 0.5706, 0.7430, 0.6972, 0.6269,
        0.5091], device='cuda:0')
Index force: tensor([0.8140, 0.5629, 0.5321, 0.5037, 0.5349, 0.5119, 0.5018, 0.5466, 0.7067,
        0.5412], device='cuda:0')
Storing NORMAL transition: reward=-0.0031 (scaled=-0.0031), steps=1
Reward stats updated: mean -0.0069 -> -0.0069, std: 0.1264
Collected 504 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5095, Q2 Loss=1.5095, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0819
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9144, Q2 Loss=0.9144, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5518
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8484, Q2 Loss=0.8484, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1251
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8577, Q2 Loss=0.8577, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6196
SAC Update 5/5: Actor Loss=-0.2535, Q1 Loss=1.0446, Q2 Loss=1.0446, Entropy=0.0224, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6213

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (20.0%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (18.1%)
Actor update: 0.10s (39.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.050704
Q1 loss: 1.034902
Q2 loss: 1.034902
Current threshold: -36.7937
Global Scale Offset: 0.0267
Reward stats: mean=-0.0069, std=0.1264, count=504
----------------------------------------------
SAC Update - Actor Loss: -0.0507, Q1 Loss: 1.0349, Q2 Loss: 1.0349, Entropy: 0.0045, Mean TD Error: 0.8000, Threshold: -36.7937
tensor([ 0.2039,  0.5881,  0.6451,  0.6634, -0.2075,  0.5959,  0.8408,  1.0141,
         1.3867,  0.6027,  0.2194,  0.9448,  0.1609, -0.0020,  0.0608, -1.5612],
       device='cuda:0')
Original likelihood: -30.997575759887695
Adjusted likelihood: -30.997575759887695
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.056845793034881
Current ori: tensor([ 0.1609, -0.0020,  0.0608], device='cuda:0')
Middle force: tensor([0.5167, 0.6573, 0.6178, 0.9394, 0.7379, 0.8433, 1.3116, 0.5852, 0.5543],
       device='cuda:0')
Thumb force: tensor([0.5975, 1.0133, 0.6424, 1.3776, 0.5819, 0.7955, 0.5775, 0.5764, 1.5113],
       device='cuda:0')
Index force: tensor([0.6026, 0.5609, 0.5045, 0.5639, 0.5260, 0.5198, 0.5020, 0.5763, 0.5374],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0091 (scaled=-0.0091), steps=1
Reward stats updated: mean -0.0069 -> -0.0069, std: 0.1263
Collected 505 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0418, Q2 Loss=1.0418, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7734
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.2539, Q2 Loss=2.2539, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0674
SAC Update 3/5: Actor Loss=-0.3618, Q1 Loss=1.0215, Q2 Loss=1.0215, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4392
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2996, Q2 Loss=1.2996, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1352
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.6753, Q2 Loss=1.6753, Entropy=0.0003, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2811

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.3%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (19.3%)
Actor update: 0.11s (41.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.072365
Q1 loss: 1.458420
Q2 loss: 1.458420
Current threshold: -36.8337
Global Scale Offset: 0.0262
Reward stats: mean=-0.0069, std=0.1263, count=505
----------------------------------------------
SAC Update - Actor Loss: -0.0724, Q1 Loss: 1.4584, Q2 Loss: 1.4584, Entropy: 0.0001, Mean TD Error: 1.1393, Threshold: -36.8337
tensor([ 2.1394e-01,  5.9494e-01,  6.2104e-01,  5.7808e-01, -1.5891e-01,
         6.2720e-01,  8.9760e-01,  1.0756e+00,  1.4925e+00,  5.4051e-01,
         1.9152e-01,  9.2319e-01,  1.6653e-01, -1.4485e-03,  6.8443e-02,
        -1.7542e+00], device='cuda:0')
Original likelihood: -26.71848487854004
Adjusted likelihood: -26.71848487854004
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.767861456028186
Current ori: tensor([ 0.1665, -0.0014,  0.0684], device='cuda:0')
Middle force: tensor([0.6449, 0.5384, 0.9769, 0.7227, 0.5078, 0.5098, 0.5053, 0.5336],
       device='cuda:0')
Thumb force: tensor([1.0593, 0.5947, 1.5211, 0.5744, 0.5758, 0.5146, 0.5760, 0.5136],
       device='cuda:0')
Index force: tensor([0.5009, 0.5003, 0.5030, 0.5269, 0.5017, 0.6660, 0.5050, 0.5074],
       device='cuda:0')
Storing NORMAL transition: reward=0.0110 (scaled=0.0110), steps=1
Reward stats updated: mean -0.0069 -> -0.0069, std: 0.1262
Collected 506 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8362, Q2 Loss=0.8362, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2176
SAC Update 2/5: Actor Loss=-0.0443, Q1 Loss=0.9982, Q2 Loss=0.9982, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7175
SAC Update 3/5: Actor Loss=-0.1055, Q1 Loss=1.8913, Q2 Loss=1.8913, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4401
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3163, Q2 Loss=1.3163, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7497
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.7543, Q2 Loss=1.7543, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4642

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.0%)
Q1 update: 0.04s (18.3%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (41.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.029957
Q1 loss: 1.359234
Q2 loss: 1.359234
Current threshold: -36.8589
Global Scale Offset: 0.0258
Reward stats: mean=-0.0069, std=0.1262, count=506
----------------------------------------------
SAC Update - Actor Loss: -0.0300, Q1 Loss: 1.3592, Q2 Loss: 1.3592, Entropy: 0.0000, Mean TD Error: 1.5178, Threshold: -36.8589
tensor([ 0.2402,  0.5851,  0.6989,  0.5605, -0.1468,  0.5986,  0.9269,  1.1746,
         1.5000,  0.2010,  0.1470,  0.8473,  0.2007, -0.0142,  0.0430, -2.0677],
       device='cuda:0')
Original likelihood: -31.137451171875
Adjusted likelihood: -31.137451171875
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.435703560011461
Current ori: tensor([ 0.2007, -0.0142,  0.0430], device='cuda:0')
Middle force: tensor([0.5908, 0.8663, 0.7185, 0.7550, 0.5418, 0.5419, 0.6025],
       device='cuda:0')
Thumb force: tensor([0.6624, 1.1696, 0.5577, 0.7191, 0.6584, 0.5329, 0.6743],
       device='cuda:0')
Index force: tensor([0.5026, 0.5411, 0.5102, 0.5076, 0.5650, 0.5651, 0.5348],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0024 (scaled=-0.0024), steps=1
Reward stats updated: mean -0.0069 -> -0.0069, std: 0.1260
Collected 507 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.9439, Q2 Loss=0.9439, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7061
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0825, Q2 Loss=1.0825, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1702
SAC Update 3/5: Actor Loss=-0.3939, Q1 Loss=1.1266, Q2 Loss=1.1266, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5188
SAC Update 4/5: Actor Loss=-0.0655, Q1 Loss=0.8868, Q2 Loss=0.8868, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8486
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2249, Q2 Loss=1.2249, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3637

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.4%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (18.6%)
Actor update: 0.09s (38.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.137945
Q1 loss: 1.052958
Q2 loss: 1.052958
Current threshold: -36.8738
Global Scale Offset: 0.0256
Reward stats: mean=-0.0069, std=0.1260, count=507
----------------------------------------------
SAC Update - Actor Loss: -0.1379, Q1 Loss: 1.0530, Q2 Loss: 1.0530, Entropy: 0.0000, Mean TD Error: 0.7215, Threshold: -36.8738
tensor([ 0.3488,  0.6420,  0.7048,  0.6498, -0.2521,  0.6127,  1.0112,  1.1688,
         1.4999,  0.4092,  0.2054,  0.8751,  0.2006, -0.0512,  0.0430, -2.0097],
       device='cuda:0')
Original likelihood: -42.502708435058594
Adjusted likelihood: -42.502708435058594
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 42.06499099731445
Projection step: 1, Loss: 43.08750915527344
Projection step: 2, Loss: 41.637210845947266
Projection step: 3, Loss: 40.94695281982422
Projection step: 4, Loss: 41.65999984741211
Projection step: 5, Loss: 40.976722717285156
Projection step: 6, Loss: 39.957359313964844
Projection step: 7, Loss: 40.38362503051758
Projection step: 8, Loss: 40.55254364013672
Projection step: 9, Loss: 41.0838623046875
Projection step: 10, Loss: 40.992759704589844
Projection step: 11, Loss: 39.892333984375
Projection step: 12, Loss: 38.94239044189453
Projection step: 13, Loss: 39.389892578125
Projection step: 14, Loss: 40.549339294433594
Projection step: 15, Loss: 38.483062744140625
Projection step: 16, Loss: 38.94047164916992
Projection step: 17, Loss: 38.58344268798828
Projection step: 18, Loss: 39.53025436401367
Projection step: 19, Loss: 38.74011993408203
Projection step: 20, Loss: 38.116790771484375
Projection step: 21, Loss: 38.66632843017578
Projection step: 22, Loss: 38.26971435546875
Projection step: 23, Loss: 38.96014404296875
Projection step: 24, Loss: 38.30193328857422
Final likelihood: tensor([-36.8952, -38.9564, -44.7002, -34.8099, -38.9288, -36.8321, -34.0295,
        -35.4010, -38.3873, -38.4119, -34.4342, -41.1812, -36.0708, -34.6446,
        -36.0737, -38.0198])
Final projection likelihood: -37.3610
1 mode projection failed, trying anyway
New goal: tensor([ 0.3290,  0.6301,  0.6990,  0.6413, -0.2077,  0.6160,  1.0488,  1.2348,
         1.5129,  0.3487,  0.2138,  0.9375,  0.1970, -0.0496,  0.4270],
       device='cuda:0')
tensor([[0.0042]], device='cuda:0') tensor([[0.0071]], device='cuda:0') tensor([[0.0007]], device='cuda:0')
Original likelihood: -37.71754455566406
Adjusted likelihood: -37.71754455566406
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 37.71754455566406}
Current yaw: tensor([ 0.2006, -0.0512,  0.0430], device='cuda:0')
9 thumb_middle
tensor([ 0.3488,  0.6420,  0.7048,  0.6498, -0.2521,  0.6127,  1.0112,  1.1688,
         1.4999,  0.4092,  0.2054,  0.8751,  0.2006, -0.0512,  0.0430, -2.0097],
       device='cuda:0')
Solve time for step 1 9.148690308036748
Current ori: tensor([ 0.2006, -0.0512,  0.0430], device='cuda:0')
Index force: tensor([0.5929, 0.5484, 0.5390, 0.5720], device='cuda:0')
tensor([ 0.3477,  0.6409,  0.7174,  0.6339, -0.2971,  0.5767,  0.9853,  1.1830,
         1.4591,  0.3491,  0.1244,  0.8853,  0.2037, -0.0511,  0.0430, -1.9653],
       device='cuda:0')
Solve time for step 2 3.5302899989765137
Current ori: tensor([ 0.2037, -0.0511,  0.0430], device='cuda:0')
Index force: tensor([0.5407, 0.5300, 0.5662], device='cuda:0')
tensor([ 0.3450,  0.6396,  0.7176,  0.6116, -0.2992,  0.5775,  0.9735,  1.2023,
         1.4604,  0.3417,  0.1257,  0.8843,  0.2021, -0.0494,  0.0430, -1.9774],
       device='cuda:0')
Solve time for step 3 3.3875172970001586
Current ori: tensor([ 0.2021, -0.0494,  0.0430], device='cuda:0')
Index force: tensor([0.5412, 0.5333], device='cuda:0')
tensor([ 0.3574,  0.6472,  0.7071,  0.6238, -0.3000,  0.5811,  0.9758,  1.1967,
         1.4655,  0.3355,  0.1159,  0.8851,  0.2015, -0.0529,  0.0430, -1.9306],
       device='cuda:0')
Solve time for step 4 3.344924966979306
Current ori: tensor([ 0.2015, -0.0529,  0.0430], device='cuda:0')
Index force: tensor([0.5283], device='cuda:0')
Storing RECOVERY transition: reward=-0.0062 (scaled=-0.0010), steps=6
Reward stats updated: mean -0.0069 -> -0.0069, std: 0.1259
Collected 508 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4631, Q2 Loss=1.4631, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3900
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0034, Q2 Loss=1.0034, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2023
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2530, Q2 Loss=1.2530, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8005
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1482, Q2 Loss=1.1482, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9057
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5039, Q2 Loss=1.5039, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3608

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.3%)
Q1 update: 0.05s (20.4%)
Q2 update: 0.05s (18.4%)
Actor update: 0.10s (38.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 1.274306
Q2 loss: 1.274306
Current threshold: -36.8827
Global Scale Offset: 0.0255
Reward stats: mean=-0.0069, std=0.1259, count=508
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.2743, Q2 Loss: 1.2743, Entropy: 0.0000, Mean TD Error: 1.1318, Threshold: -36.8827
Original likelihood: -41.886417388916016
Adjusted likelihood: -41.886417388916016
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 41.87169647216797
Projection step: 1, Loss: 41.531455993652344
Projection step: 2, Loss: 41.81559371948242
Projection step: 3, Loss: 41.862144470214844
Projection step: 4, Loss: 42.44165802001953
Projection step: 5, Loss: 42.79814910888672
Projection step: 6, Loss: 41.349700927734375
Projection step: 7, Loss: 42.117645263671875
Projection step: 8, Loss: 41.552555084228516
Projection step: 9, Loss: 39.214473724365234
Projection step: 10, Loss: 41.05683898925781
Projection step: 11, Loss: 39.54844665527344
Projection step: 12, Loss: 39.842742919921875
Projection step: 13, Loss: 41.89600372314453
Projection step: 14, Loss: 39.890586853027344
Projection step: 15, Loss: 40.69773864746094
Projection step: 16, Loss: 40.389137268066406
Projection step: 17, Loss: 38.44063186645508
Projection step: 18, Loss: 38.917762756347656
Projection step: 19, Loss: 38.439857482910156
Projection step: 20, Loss: 41.1007080078125
Projection step: 21, Loss: 39.14910888671875
Projection step: 22, Loss: 39.136077880859375
Projection step: 23, Loss: 40.30342102050781
Projection step: 24, Loss: 37.59505081176758
Final likelihood: tensor([-36.8703, -39.7438, -44.4995, -34.0611, -35.6896, -47.9664, -38.2237,
        -31.6969, -41.6392, -34.5103, -38.6219, -36.1258, -39.3496, -37.0913,
        -35.8219, -43.1485])
Final projection likelihood: -38.4412
1 mode projection failed, trying anyway
New goal: tensor([ 0.3406,  0.6360,  0.7036,  0.6437, -0.2041,  0.6343,  1.0695,  1.2720,
         1.5134,  0.2983,  0.2096,  0.9705,  0.2024, -0.0562,  0.4573],
       device='cuda:0')
tensor([[0.0034]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0007]], device='cuda:0')
Original likelihood: -39.84140396118164
Adjusted likelihood: -39.84140396118164
Likelihood residual: 0.0
Original likelihood: -41.23176193237305
Adjusted likelihood: -41.23176193237305
Likelihood residual: 0.0
{'index': 41.23176193237305, 'thumb_middle': 39.84140396118164}
Current yaw: tensor([ 0.2063, -0.0580,  0.0465], device='cuda:0')
10 thumb_middle
tensor([ 0.3612,  0.6466,  0.7014,  0.6475, -0.2393,  0.6425,  1.0500,  1.2318,
         1.5000,  0.3577,  0.2023,  0.9373,  0.2063, -0.0580,  0.0465, -1.8866],
       device='cuda:0')
Solve time for step 1 9.024894140951801
Current ori: tensor([ 0.2063, -0.0580,  0.0465], device='cuda:0')
Index force: tensor([0.5903, 0.5963, 0.5295, 0.5306], device='cuda:0')
tensor([ 0.3517,  0.6415,  0.7059,  0.6444, -0.3068,  0.5786,  0.9864,  1.2162,
         1.4505,  0.3017,  0.1277,  0.9205,  0.2082, -0.0540,  0.0465, -1.8844],
       device='cuda:0')
Solve time for step 2 3.6380628920160234
Current ori: tensor([ 0.2082, -0.0540,  0.0465], device='cuda:0')
Index force: tensor([0.5833, 0.5320, 0.5944], device='cuda:0')
tensor([ 0.3519,  0.6414,  0.7196,  0.6458, -0.3139,  0.5815,  0.9823,  1.2183,
         1.4624,  0.2875,  0.1157,  0.9189,  0.2095, -0.0543,  0.0465, -1.8615],
       device='cuda:0')
Solve time for step 3 3.5782281570136547
Current ori: tensor([ 0.2095, -0.0543,  0.0465], device='cuda:0')
Index force: tensor([0.5282, 0.5828], device='cuda:0')
tensor([ 0.3599,  0.6459,  0.7210,  0.6462, -0.3174,  0.5796,  0.9861,  1.2224,
         1.4657,  0.2856,  0.1106,  0.9178,  0.2072, -0.0563,  0.0465, -1.8756],
       device='cuda:0')
Solve time for step 4 3.5052923500188626
Current ori: tensor([ 0.2072, -0.0563,  0.0465], device='cuda:0')
Index force: tensor([0.5144], device='cuda:0')
Storing RECOVERY transition: reward=-0.0085 (scaled=-0.0014), steps=6
Reward stats updated: mean -0.0069 -> -0.0069, std: 0.1258
Collected 509 transitions for RL
SAC Update 1/5: Actor Loss=-0.0540, Q1 Loss=1.0850, Q2 Loss=1.0850, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7537
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8561, Q2 Loss=0.8561, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5455
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2137, Q2 Loss=1.2137, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7416
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.1930, Q2 Loss=2.1930, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6378
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1255, Q2 Loss=1.1255, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9472

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.4%)
Q2 update: 0.04s (18.8%)
Actor update: 0.08s (40.2%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.010809
Q1 loss: 1.294650
Q2 loss: 1.294650
Current threshold: -36.8879
Global Scale Offset: 0.0254
Reward stats: mean=-0.0069, std=0.1258, count=509
----------------------------------------------
SAC Update - Actor Loss: -0.0108, Q1 Loss: 1.2946, Q2 Loss: 1.2946, Entropy: 0.0000, Mean TD Error: 1.1252, Threshold: -36.8879
Original likelihood: -41.44913864135742
Adjusted likelihood: -41.44913864135742
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 41.633575439453125
Projection step: 1, Loss: 40.813873291015625
Projection step: 2, Loss: 43.222984313964844
Projection step: 3, Loss: 41.84223937988281
Projection step: 4, Loss: 41.057552337646484
Projection step: 5, Loss: 41.928428649902344
Projection step: 6, Loss: 40.37153625488281
Projection step: 7, Loss: 42.89939498901367
Projection step: 8, Loss: 40.65493392944336
Projection step: 9, Loss: 42.13066101074219
Projection step: 10, Loss: 40.354774475097656
Projection step: 11, Loss: 39.62705993652344
Projection step: 12, Loss: 41.49802780151367
Projection step: 13, Loss: 39.324851989746094
Projection step: 14, Loss: 41.505184173583984
Projection step: 15, Loss: 40.561492919921875
Projection step: 16, Loss: 40.238609313964844
Projection step: 17, Loss: 39.48668670654297
Projection step: 18, Loss: 38.63257598876953
Projection step: 19, Loss: 39.978214263916016
Projection step: 20, Loss: 38.18183517456055
Projection step: 21, Loss: 39.411346435546875
Projection step: 22, Loss: 38.70152282714844
Projection step: 23, Loss: 37.401634216308594
Projection step: 24, Loss: 40.487342834472656
Final likelihood: tensor([-38.5486, -39.6707, -35.5318, -42.7992, -42.7149, -39.7146, -37.1245,
        -37.9790, -37.0096, -36.1572, -39.7027, -38.9564, -34.4416, -36.8632,
        -40.3997, -36.0014])
Final projection likelihood: -38.3509
1 mode projection failed, trying anyway
New goal: tensor([ 0.3459,  0.6386,  0.7210,  0.6461, -0.2104,  0.6370,  1.0653,  1.3056,
         1.5119,  0.2411,  0.2117,  0.9819,  0.2041, -0.0601,  0.4666],
       device='cuda:0')
tensor([[0.0032]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0006]], device='cuda:0')
Original likelihood: -38.736228942871094
Adjusted likelihood: -38.736228942871094
Likelihood residual: 0.0
Original likelihood: -40.80002212524414
Adjusted likelihood: -40.80002212524414
Likelihood residual: 0.0
{'index': 40.80002212524414, 'thumb_middle': 38.736228942871094}
Current yaw: tensor([ 0.2081, -0.0617,  0.0477], device='cuda:0')
11 thumb_middle
tensor([ 0.3668,  0.6508,  0.7276,  0.6574, -0.2440,  0.6426,  1.0471,  1.2711,
         1.5000,  0.2980,  0.2063,  0.9619,  0.2081, -0.0617,  0.0477, -1.8703],
       device='cuda:0')
Solve time for step 1 9.15760473604314
Current ori: tensor([ 0.2081, -0.0617,  0.0477], device='cuda:0')
Index force: tensor([0.5744, 0.5835, 0.5289, 0.5348], device='cuda:0')
tensor([ 0.3618,  0.6459,  0.7404,  0.6468, -0.3132,  0.5741,  0.9903,  1.2448,
         1.4708,  0.2326,  0.1098,  0.9323,  0.2125, -0.0586,  0.0477, -1.8526],
       device='cuda:0')
Solve time for step 2 3.5990575819741935
Current ori: tensor([ 0.2125, -0.0586,  0.0477], device='cuda:0')
Index force: tensor([0.5404, 0.6017, 0.5897], device='cuda:0')
tensor([ 0.3678,  0.6491,  0.7445,  0.6482, -0.3257,  0.5796,  0.9834,  1.2522,
         1.4722,  0.2264,  0.1100,  0.9314,  0.2145, -0.0605,  0.0473, -1.8388],
       device='cuda:0')
Solve time for step 3 3.6212868409929797
Current ori: tensor([ 0.2145, -0.0605,  0.0473], device='cuda:0')
Index force: tensor([0.5170, 0.5785], device='cuda:0')
tensor([ 0.3695,  0.6504,  0.7382,  0.6524, -0.3253,  0.5895,  0.9687,  1.2546,
         1.4617,  0.2365,  0.1170,  0.9433,  0.2142, -0.0603,  0.0514, -1.8457],
       device='cuda:0')
Solve time for step 4 3.375215455016587
Current ori: tensor([ 0.2142, -0.0603,  0.0514], device='cuda:0')
Index force: tensor([0.5267], device='cuda:0')
Storing RECOVERY transition: reward=-0.0108 (scaled=-0.0018), steps=6
Reward stats updated: mean -0.0069 -> -0.0068, std: 0.1257
Collected 510 transitions for RL
SAC Update 1/5: Actor Loss=-0.0787, Q1 Loss=1.5562, Q2 Loss=1.5562, Entropy=0.0017, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=6.9323
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2804, Q2 Loss=1.2804, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8156
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.9270, Q2 Loss=0.9270, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6419
SAC Update 4/5: Actor Loss=-0.0359, Q1 Loss=0.9160, Q2 Loss=0.9160, Entropy=0.0008, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4936
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0219, Q2 Loss=1.0219, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0765

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.6%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (41.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.068957
Q1 loss: 1.140297
Q2 loss: 1.140297
Current threshold: -36.9313
Global Scale Offset: 0.0250
Reward stats: mean=-0.0068, std=0.1257, count=510
----------------------------------------------
SAC Update - Actor Loss: -0.0690, Q1 Loss: 1.1403, Q2 Loss: 1.1403, Entropy: 0.0005, Mean TD Error: 1.9920, Threshold: -36.9313
Original likelihood: -43.886558532714844
Adjusted likelihood: -43.886558532714844
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 44.55851745605469
Projection step: 1, Loss: 42.278297424316406
Projection step: 2, Loss: 43.81385040283203
Projection step: 3, Loss: 43.50396728515625
Projection step: 4, Loss: 43.15452194213867
Projection step: 5, Loss: 43.24002456665039
Projection step: 6, Loss: 44.328758239746094
Projection step: 7, Loss: 42.52389144897461
Projection step: 8, Loss: 43.43677520751953
Projection step: 9, Loss: 43.26799774169922
Projection step: 10, Loss: 42.45172882080078
Projection step: 11, Loss: 43.426918029785156
Projection step: 12, Loss: 42.5985221862793
Projection step: 13, Loss: 42.34699249267578
Projection step: 14, Loss: 41.45698547363281
Projection step: 15, Loss: 42.43177795410156
Projection step: 16, Loss: 42.484066009521484
Projection step: 17, Loss: 42.05154037475586
Projection step: 18, Loss: 41.64121627807617
Projection step: 19, Loss: 39.987640380859375
Projection step: 20, Loss: 41.86867141723633
Projection step: 21, Loss: 42.21311950683594
Projection step: 22, Loss: 41.610511779785156
Projection step: 23, Loss: 40.581886291503906
Projection step: 24, Loss: 39.8852424621582
Final likelihood: tensor([-38.1490, -38.8764, -42.0570, -38.9349, -40.1004, -39.0589, -37.1325,
        -44.0819, -38.9048, -38.6902, -44.1803, -38.4432, -41.4851, -40.5048,
        -40.3352, -42.7075])
Final projection likelihood: -40.2276
1 mode projection failed, trying anyway
New goal: tensor([ 0.3507,  0.6387,  0.7350,  0.6385, -0.2251,  0.6486,  1.0648,  1.3287,
         1.5103,  0.2046,  0.2126,  0.9998,  0.2100, -0.0616,  0.4679],
       device='cuda:0')
tensor([[0.0034]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0008]], device='cuda:0')
Original likelihood: -41.81068420410156
Adjusted likelihood: -41.81068420410156
Likelihood residual: 0.0
Original likelihood: -38.23247528076172
Adjusted likelihood: -38.23247528076172
Likelihood residual: 0.0
{'index': 38.23247528076172, 'thumb_middle': 41.81068420410156}
Current yaw: tensor([ 0.2142, -0.0633,  0.0473], device='cuda:0')
12 index
tensor([ 0.3700,  0.6517,  0.7427,  0.6504, -0.2598,  0.6537,  1.0472,  1.2930,
         1.5000,  0.2574,  0.2056,  0.9914,  0.2142, -0.0633,  0.0473, -1.8420],
       device='cuda:0')
Solve time for step 1 10.572507589007728
Current ori: tensor([ 0.2142, -0.0633,  0.0473], device='cuda:0')
Middle force: tensor([0.5180, 0.5336, 0.5073, 0.5972], device='cuda:0')
Thumb force: tensor([0.5033, 0.5502, 0.5141, 0.5860], device='cuda:0')
tensor([ 0.3429,  0.3692,  0.7341,  0.6518, -0.2472,  0.7223,  1.0420,  1.2737,
         1.5000,  0.1945,  0.1807,  1.0010,  0.3462, -0.1974,  0.0614,  1.0726],
       device='cuda:0')
Solve time for step 2 4.193860483006574
Current ori: tensor([ 0.3462, -0.1974,  0.0614], device='cuda:0')
Middle force: tensor([0.5047, 0.5905, 0.5824], device='cuda:0')
Thumb force: tensor([0.5276, 0.5951, 0.5934], device='cuda:0')
tensor([ 0.3237,  0.3078,  0.7726,  0.6539, -0.1057,  0.9322,  0.9708,  1.0829,
         1.4999,  0.2082,  0.2464,  0.9992,  0.3551, -0.2305,  0.1333,  3.0978],
       device='cuda:0')
Solve time for step 3 4.005138528998941
Current ori: tensor([ 0.3551, -0.2305,  0.1333], device='cuda:0')
Middle force: tensor([0.5870, 0.5734], device='cuda:0')
Thumb force: tensor([0.5001, 0.5081], device='cuda:0')
tensor([ 0.3722,  0.4151,  0.7866,  0.6564, -0.1372,  0.9509,  0.9902,  1.0904,
         1.4974,  0.2510,  0.2065,  1.0848,  0.3548, -0.2450,  0.1848,  5.3836],
       device='cuda:0')
Solve time for step 4 3.894982361001894
Current ori: tensor([ 0.3548, -0.2450,  0.1848], device='cuda:0')
Middle force: tensor([0.5679], device='cuda:0')
Thumb force: tensor([0.5781], device='cuda:0')
Storing RECOVERY transition: reward=-0.2605 (scaled=-0.0434), steps=6
Reward stats updated: mean -0.0068 -> -0.0069, std: 0.1255
Collected 511 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.6885, Q2 Loss=1.6885, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3030
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6482, Q2 Loss=0.6482, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3336
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3300, Q2 Loss=1.3300, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9744
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1999, Q2 Loss=1.1999, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2489
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9833, Q2 Loss=0.9833, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2016

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (20.6%)
Q1 update: 0.04s (19.5%)
Q2 update: 0.04s (17.3%)
Actor update: 0.09s (38.5%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 1.170001
Q2 loss: 1.170001
Current threshold: -36.9648
Global Scale Offset: 0.0247
Reward stats: mean=-0.0069, std=0.1255, count=511
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.1700, Q2 Loss: 1.1700, Entropy: 0.0000, Mean TD Error: 1.0123, Threshold: -36.9648
Original likelihood: -155.98382568359375
Adjusted likelihood: -155.98382568359375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 142.74951171875
Projection step: 1, Loss: 147.19000244140625
Projection step: 2, Loss: 139.55503845214844
Projection step: 3, Loss: 163.1142578125
Projection step: 4, Loss: 154.0166778564453
Projection step: 5, Loss: 142.44473266601562
Projection step: 6, Loss: 125.2193374633789
Projection step: 7, Loss: 134.59193420410156
Projection step: 8, Loss: 143.03131103515625
Projection step: 9, Loss: 149.1260223388672
Projection step: 10, Loss: 149.185791015625
Projection step: 11, Loss: 132.41317749023438
Projection step: 12, Loss: 158.89837646484375
Projection step: 13, Loss: 157.85853576660156
Projection step: 14, Loss: 141.74008178710938
Projection step: 15, Loss: 145.29254150390625
Projection step: 16, Loss: 153.923095703125
Projection step: 17, Loss: 137.60702514648438
Projection step: 18, Loss: 144.00238037109375
Projection step: 19, Loss: 159.64031982421875
Projection step: 20, Loss: 147.43081665039062
Projection step: 21, Loss: 128.09130859375
Projection step: 22, Loss: 160.24032592773438
Projection step: 23, Loss: 128.28619384765625
Projection step: 24, Loss: 123.36338806152344
Final likelihood: tensor([-204.5981, -192.2662, -141.6911, -125.0603,  -98.8510, -102.3973,
         -87.2504, -142.7403, -139.4272, -104.7838, -104.0619, -114.7699,
        -115.2536, -153.8752, -211.0759,  -86.3968])
Final projection likelihood: -132.7812
1 mode projection failed, trying anyway
New goal: tensor([ 0.3813,  0.6141,  0.7752,  0.6674, -0.1546,  0.9155,  1.0243,  1.1646,
         1.4671,  0.2855,  0.2058,  1.1633,  0.3536, -0.2483,  0.2293],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0108]], device='cuda:0') tensor([[0.0048]], device='cuda:0')
Original likelihood: -109.33954620361328
Adjusted likelihood: -109.33954620361328
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 109.33954620361328}
Current yaw: tensor([ 0.3545, -0.2492,  0.2130], device='cuda:0')
13 thumb_middle
tensor([ 0.3845,  0.6130,  0.7853,  0.6520, -0.1640,  0.9320,  1.0372,  1.1333,
         1.4864,  0.2898,  0.2109,  1.1596,  0.3545, -0.2492,  0.2130,  5.1797],
       device='cuda:0')
Solve time for step 1 8.973123027011752
Current ori: tensor([ 0.3545, -0.2492,  0.2130], device='cuda:0')
Index force: tensor([0.5842, 0.5977, 0.5752, 0.5949], device='cuda:0')
tensor([ 0.3249,  0.6875,  0.7868,  0.6676, -0.1929,  0.9163,  1.0170,  1.1529,
         1.3365,  0.3397,  0.1092,  1.1698,  0.3689, -0.3209,  0.3669,  3.4692],
       device='cuda:0')
Solve time for step 2 3.6831569600035436
Current ori: tensor([ 0.3689, -0.3209,  0.3669], device='cuda:0')
Index force: tensor([0.5818, 0.5801, 0.5845], device='cuda:0')
tensor([ 0.1902,  0.6647,  0.7875,  0.6782, -0.1974,  0.9437,  1.0407,  1.1708,
         1.3355,  0.3389,  0.0681,  1.1827,  0.3717, -0.3295,  0.3476,  3.4614],
       device='cuda:0')
Solve time for step 3 3.607534237962682
Current ori: tensor([ 0.3717, -0.3295,  0.3476], device='cuda:0')
Index force: tensor([0.5737, 0.5769], device='cuda:0')
tensor([ 0.0918,  0.6323,  0.7935,  0.6690, -0.1580,  0.9609,  1.0485,  1.1678,
         1.3183,  0.3213,  0.0673,  1.1696,  0.3737, -0.3340,  0.3499,  4.3895],
       device='cuda:0')
Solve time for step 4 3.559743261022959
Current ori: tensor([ 0.3737, -0.3340,  0.3499], device='cuda:0')
Index force: tensor([0.5659], device='cuda:0')
Storing RECOVERY transition: reward=-0.3384 (scaled=-0.0564), steps=6
Reward stats updated: mean -0.0069 -> -0.0070, std: 0.1254
Collected 512 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.9539, Q2 Loss=0.9539, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6077
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8275, Q2 Loss=0.8275, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5011
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2899, Q2 Loss=1.2899, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2929
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.8649, Q2 Loss=1.8649, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7565
SAC Update 5/5: Actor Loss=-0.2391, Q1 Loss=1.8616, Q2 Loss=1.8616, Entropy=0.0487, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.3220

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.1%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (38.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.093866
Q1 loss: 1.359585
Q2 loss: 1.359585
Current threshold: -36.9859
Global Scale Offset: 0.0245
Reward stats: mean=-0.0070, std=0.1254, count=512
----------------------------------------------
SAC Update - Actor Loss: -0.0939, Q1 Loss: 1.3596, Q2 Loss: 1.3596, Entropy: 0.0097, Mean TD Error: 1.4961, Threshold: -36.9859
Original likelihood: -258.1047058105469
Adjusted likelihood: -258.1047058105469
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 280.1805114746094
Projection step: 1, Loss: 270.60723876953125
Projection step: 2, Loss: 282.44964599609375
Projection step: 3, Loss: 284.224609375
Projection step: 4, Loss: 255.5039825439453
Projection step: 5, Loss: 262.27880859375
Projection step: 6, Loss: 277.50933837890625
Projection step: 7, Loss: 272.1498718261719
Projection step: 8, Loss: 273.8569641113281
Projection step: 9, Loss: 290.0229187011719
Projection step: 10, Loss: 264.1053466796875
Projection step: 11, Loss: 269.25830078125
Projection step: 12, Loss: 270.42034912109375
Projection step: 13, Loss: 260.8809509277344
Projection step: 14, Loss: 277.3028564453125
Projection step: 15, Loss: 261.99560546875
Projection step: 16, Loss: 276.7550048828125
Projection step: 17, Loss: 238.86886596679688
Projection step: 18, Loss: 279.6924133300781
Projection step: 19, Loss: 278.8231201171875
Projection step: 20, Loss: 284.067626953125
Projection step: 21, Loss: 284.81365966796875
Projection step: 22, Loss: 280.89215087890625
Projection step: 23, Loss: 278.0875244140625
Projection step: 24, Loss: 284.5543212890625
Final likelihood: tensor([-279.1145, -314.2700, -256.9713, -207.2648, -303.1397, -306.8770,
        -284.2559, -294.3206, -237.4856, -236.4405, -256.1203, -274.9254,
        -287.9375, -219.3012, -275.9246, -211.3905])
Final projection likelihood: -265.3587
1 mode projection failed, trying anyway
New goal: tensor([ 0.0098,  0.6477,  0.8000,  0.6736, -0.0600,  1.0573,  1.0663,  1.1490,
         1.3364,  0.3091,  0.1467,  1.1822,  0.3652, -0.3093,  0.2955],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([[0.0050]], device='cuda:0')
Original likelihood: -265.22027587890625
Adjusted likelihood: -265.22027587890625
Likelihood residual: 0.0
{'index': 265.22027587890625, 'thumb_middle': inf}
Current yaw: tensor([ 0.3651, -0.3088,  0.2768], device='cuda:0')
14 index
tensor([ 3.1483e-03,  6.4779e-01,  7.9393e-01,  6.7124e-01, -5.9893e-02,
         1.0572e+00,  1.0713e+00,  1.1417e+00,  1.3459e+00,  3.0511e-01,
         1.4747e-01,  1.1772e+00,  3.6508e-01, -3.0875e-01,  2.7678e-01,
         4.2841e+00], device='cuda:0')
Solve time for step 1 10.487903984030709
Current ori: tensor([ 0.3651, -0.3088,  0.2768], device='cuda:0')
Middle force: tensor([0.5064, 0.7359, 0.7248, 0.7276], device='cuda:0')
Thumb force: tensor([0.5335, 0.6089, 0.7278, 0.6995], device='cuda:0')
tensor([ 0.0564,  0.6564,  0.8058,  0.6730, -0.0156,  1.0981,  1.0288,  1.0912,
         1.3384,  0.3364,  0.1457,  1.1789,  0.3674, -0.3153,  0.2671,  3.8022],
       device='cuda:0')
Solve time for step 2 4.066595465003047
Current ori: tensor([ 0.3674, -0.3153,  0.2671], device='cuda:0')
Middle force: tensor([0.7108, 0.6352, 0.6719], device='cuda:0')
Thumb force: tensor([0.5978, 0.6543, 0.6811], device='cuda:0')
tensor([ 0.0462,  0.6624,  0.8121,  0.6730,  0.0082,  1.1288,  1.0006,  1.0261,
         1.3389,  0.3340,  0.1532,  1.1798,  0.3668, -0.3136,  0.2662,  4.0246],
       device='cuda:0')
Solve time for step 3 3.7943456770153716
Current ori: tensor([ 0.3668, -0.3136,  0.2662], device='cuda:0')
Middle force: tensor([0.5151, 0.5374], device='cuda:0')
Thumb force: tensor([0.5173, 0.5408], device='cuda:0')
tensor([ 0.0746,  0.6643,  0.8119,  0.6739,  0.0112,  1.1456,  1.0268,  0.9394,
         1.3283,  0.3644,  0.1418,  1.1766,  0.3696, -0.3223,  0.3043,  4.2250],
       device='cuda:0')
Solve time for step 4 3.8515734329703264
Current ori: tensor([ 0.3696, -0.3223,  0.3043], device='cuda:0')
Middle force: tensor([0.5344], device='cuda:0')
Thumb force: tensor([0.5391], device='cuda:0')
Storing RECOVERY transition: reward=-0.3190 (scaled=-0.0532), steps=6
Reward stats updated: mean -0.0070 -> -0.0071, std: 0.1253
Collected 513 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8624, Q2 Loss=0.8624, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0983
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6377, Q2 Loss=1.6377, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4629
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=1.6619, Q2 Loss=1.6619, Entropy=0.0125, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3535
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.5876, Q2 Loss=2.5876, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9993
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.0890, Q2 Loss=1.0890, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7366

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.8%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (17.8%)
Actor update: 0.09s (40.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092111
Q1 loss: 1.567710
Q2 loss: 1.567710
Current threshold: -37.0023
Global Scale Offset: 0.0244
Reward stats: mean=-0.0071, std=0.1253, count=513
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.5677, Q2 Loss: 1.5677, Entropy: 0.0025, Mean TD Error: 1.3301, Threshold: -37.0023
Original likelihood: -255.30084228515625
Adjusted likelihood: -255.30084228515625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 275.931396484375
Projection step: 1, Loss: 288.25848388671875
Projection step: 2, Loss: 274.315185546875
Projection step: 3, Loss: 268.08343505859375
Projection step: 4, Loss: 277.54095458984375
Projection step: 5, Loss: 273.2574157714844
Projection step: 6, Loss: 272.92095947265625
Projection step: 7, Loss: 265.7564697265625
Projection step: 8, Loss: 273.88671875
Projection step: 9, Loss: 264.9664001464844
Projection step: 10, Loss: 271.4134826660156
Projection step: 11, Loss: 268.5104675292969
Projection step: 12, Loss: 300.12384033203125
Projection step: 13, Loss: 274.56158447265625
Projection step: 14, Loss: 280.19073486328125
Projection step: 15, Loss: 274.12353515625
Projection step: 16, Loss: 257.92327880859375
Projection step: 17, Loss: 267.150634765625
Projection step: 18, Loss: 282.6070861816406
Projection step: 19, Loss: 269.3094482421875
Projection step: 20, Loss: 282.4952087402344
Projection step: 21, Loss: 269.5400390625
Projection step: 22, Loss: 271.454833984375
Projection step: 23, Loss: 273.5218505859375
Projection step: 24, Loss: 269.03082275390625
Final likelihood: tensor([-213.5267, -361.0800, -277.7004, -229.9520, -292.2799, -268.3946,
        -236.8513, -333.5640, -258.0845, -250.0443, -318.5766, -233.5004,
        -242.5853, -272.5960, -315.7924, -271.7692])
Final projection likelihood: -273.5186
1 mode projection failed, trying anyway
New goal: tensor([-0.0813,  0.6613,  0.8165,  0.6763, -0.0366,  1.1230,  0.9939,  1.0811,
         1.3234,  0.3171,  0.1767,  1.1825,  0.3658, -0.3107,  0.2682],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0022]], device='cuda:0') tensor([[0.0043]], device='cuda:0')
Original likelihood: -251.08689880371094
Adjusted likelihood: -251.08689880371094
Likelihood residual: 0.0
Original likelihood: -278.24285888671875
Adjusted likelihood: -278.24285888671875
Likelihood residual: 0.0
{'index': 278.24285888671875, 'thumb_middle': 251.08689880371094}
Current yaw: tensor([ 0.3657, -0.3101,  0.2521], device='cuda:0')
15 thumb_middle
tensor([-0.0883,  0.6616,  0.8109,  0.6738, -0.0367,  1.1226,  0.9986,  1.0729,
         1.3325,  0.3132,  0.1777,  1.1783,  0.3657, -0.3101,  0.2521,  4.0766],
       device='cuda:0')
Solve time for step 1 9.100210765958764
Current ori: tensor([ 0.3657, -0.3101,  0.2521], device='cuda:0')
Index force: tensor([0.5609, 0.5957, 0.6026, 0.5908], device='cuda:0')
tensor([-0.1477,  0.6951,  0.8277,  0.6810, -0.0900,  1.0539,  0.9328,  1.0469,
         1.2247,  0.3540,  0.0253,  1.1933,  0.3881, -0.3728,  0.4307,  3.7917],
       device='cuda:0')
Solve time for step 2 3.5619419959839433
Current ori: tensor([ 0.3881, -0.3728,  0.4307], device='cuda:0')
Index force: tensor([0.6028, 0.5513, 0.5827], device='cuda:0')
tensor([-0.2582,  0.7096,  0.8334,  0.6809, -0.0980,  1.0665,  0.9383,  1.0494,
         1.2248,  0.3526,  0.0150,  1.1981,  0.3881, -0.3731,  0.4242,  3.8860],
       device='cuda:0')
Solve time for step 3 3.589614970027469
Current ori: tensor([ 0.3881, -0.3731,  0.4242], device='cuda:0')
Index force: tensor([0.5486, 0.5767], device='cuda:0')
tensor([-0.3154,  0.7135,  0.8231,  0.6746, -0.1188,  1.0645,  0.9423,  1.0538,
         1.2254,  0.3465,  0.0291,  1.1947,  0.3875, -0.3713,  0.4214,  3.9193],
       device='cuda:0')
Solve time for step 4 3.4258554039988667
Current ori: tensor([ 0.3875, -0.3713,  0.4214], device='cuda:0')
Index force: tensor([0.5667], device='cuda:0')
Storing RECOVERY transition: reward=-0.3193 (scaled=-0.0532), steps=6
Reward stats updated: mean -0.0071 -> -0.0072, std: 0.1252
Collected 514 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3590, Q2 Loss=1.3590, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3409
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1977, Q2 Loss=1.1977, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1686
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8590, Q2 Loss=0.8590, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5211
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8138, Q2 Loss=0.8138, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4349
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.6775, Q2 Loss=0.6775, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0915

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.4%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.06s (20.7%)
Actor update: 0.12s (42.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 0.981386
Q2 loss: 0.981386
Current threshold: -37.0121
Global Scale Offset: 0.0243
Reward stats: mean=-0.0072, std=0.1252, count=514
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 0.9814, Q2 Loss: 0.9814, Entropy: 0.0000, Mean TD Error: 1.1114, Threshold: -37.0121
Original likelihood: -286.8696594238281
Adjusted likelihood: -286.8696594238281
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 291.15765380859375
Projection step: 1, Loss: 291.9634704589844
Projection step: 2, Loss: 289.77374267578125
Projection step: 3, Loss: 298.9316101074219
Projection step: 4, Loss: 313.5166015625
Projection step: 5, Loss: 308.7160949707031
Projection step: 6, Loss: 289.1533203125
Projection step: 7, Loss: 299.1410827636719
Projection step: 8, Loss: 318.3512268066406
Projection step: 9, Loss: 306.38555908203125
Projection step: 10, Loss: 298.44793701171875
Projection step: 11, Loss: 307.85552978515625
Projection step: 12, Loss: 293.1698303222656
Projection step: 13, Loss: 306.7813720703125
Projection step: 14, Loss: 301.80645751953125
Projection step: 15, Loss: 290.7286682128906
Projection step: 16, Loss: 294.71771240234375
Projection step: 17, Loss: 299.74774169921875
Projection step: 18, Loss: 281.4660339355469
Projection step: 19, Loss: 304.2232360839844
Projection step: 20, Loss: 310.7806396484375
Projection step: 21, Loss: 311.6353759765625
Projection step: 22, Loss: 301.6753234863281
Projection step: 23, Loss: 307.5545654296875
Projection step: 24, Loss: 300.50341796875
Final likelihood: tensor([-227.2652, -293.9563, -359.3997, -365.9698, -320.6176, -308.4534,
        -328.3072, -290.6097, -272.5545, -336.3894, -247.5683, -291.6612,
        -243.2150, -318.1237, -288.1519, -281.0689])
Final projection likelihood: -298.3320
1 mode projection failed, trying anyway
New goal: tensor([-0.3201,  0.7657,  0.8469,  0.6782, -0.1341,  1.1281,  1.0141,  1.0967,
         1.2804,  0.3167,  0.1666,  1.1931,  0.3728, -0.3304,  0.2553],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0040]], device='cuda:0') tensor([[0.0048]], device='cuda:0')
Original likelihood: -227.11363220214844
Adjusted likelihood: -227.11363220214844
Likelihood residual: 0.0
Original likelihood: -268.6673583984375
Adjusted likelihood: -268.6673583984375
Likelihood residual: 0.0
{'index': 268.6673583984375, 'thumb_middle': 227.11363220214844}
Current yaw: tensor([ 0.3727, -0.3299,  0.2390], device='cuda:0')
16 thumb_middle
tensor([-0.3245,  0.7652,  0.8418,  0.6759, -0.1346,  1.1279,  1.0187,  1.0911,
         1.2868,  0.3135,  0.1681,  1.1927,  0.3727, -0.3299,  0.2390,  4.0533],
       device='cuda:0')
Solve time for step 1 9.053799047018401
Current ori: tensor([ 0.3727, -0.3299,  0.2390], device='cuda:0')
Index force: tensor([0.5571, 0.5962, 0.5802, 0.5760], device='cuda:0')
tensor([-0.3488,  0.7601,  0.8299,  0.6638, -0.1906,  1.1014,  0.9710,  1.0714,
         1.2155,  0.3398,  0.0693,  1.2075,  0.3868, -0.3705,  0.4309,  3.8862],
       device='cuda:0')
Solve time for step 2 3.5591601519845426
Current ori: tensor([ 0.3868, -0.3705,  0.4309], device='cuda:0')
Index force: tensor([0.5915, 0.5760, 0.5880], device='cuda:0')
tensor([-0.3822,  0.7699,  0.8050,  0.6562, -0.2290,  1.0895,  0.9757,  1.0696,
         1.2159,  0.3381,  0.0692,  1.2058,  0.3867, -0.3703,  0.4305,  3.8677],
       device='cuda:0')
Solve time for step 3 3.4966395790106617
Current ori: tensor([ 0.3867, -0.3703,  0.4305], device='cuda:0')
Index force: tensor([0.5807, 0.5798], device='cuda:0')
tensor([-0.3994,  0.7723,  0.7907,  0.6490, -0.2484,  1.0792,  0.9751,  1.0696,
         1.2157,  0.3374,  0.0753,  1.2038,  0.3865, -0.3696,  0.4296,  3.8615],
       device='cuda:0')
Solve time for step 4 3.4170854139956646
Current ori: tensor([ 0.3865, -0.3696,  0.4296], device='cuda:0')
Index force: tensor([0.5808], device='cuda:0')
Storing RECOVERY transition: reward=-0.3195 (scaled=-0.0532), steps=6
Reward stats updated: mean -0.0072 -> -0.0073, std: 0.1251
Collected 515 transitions for RL
SAC Update 1/5: Actor Loss=-0.1537, Q1 Loss=1.1207, Q2 Loss=1.1207, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6136
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0110, Q2 Loss=1.0110, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6561
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6696, Q2 Loss=1.6696, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.0026
SAC Update 4/5: Actor Loss=-0.0001, Q1 Loss=1.3875, Q2 Loss=1.3875, Entropy=0.0338, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7904
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1415, Q2 Loss=1.1415, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4615

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.5%)
Q1 update: 0.05s (20.5%)
Q2 update: 0.05s (18.4%)
Actor update: 0.10s (39.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.030758
Q1 loss: 1.266055
Q2 loss: 1.266055
Current threshold: -37.0181
Global Scale Offset: 0.0243
Reward stats: mean=-0.0073, std=0.1251, count=515
----------------------------------------------
SAC Update - Actor Loss: -0.0308, Q1 Loss: 1.2661, Q2 Loss: 1.2661, Entropy: 0.0068, Mean TD Error: 1.9048, Threshold: -37.0181
Original likelihood: -317.4802551269531
Adjusted likelihood: -317.4802551269531
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 32
Loaded trajectory sampler
Current yaw: tensor([ 0.0004,  0.0145, -0.0449], device='cuda:0')
Current yaw: tensor([ 0.0004,  0.0145, -0.0449], device='cuda:0')
1 turn
Sampling time 3.6193506519775838
tensor([ 1.3451e-01,  6.1230e-01,  5.3274e-01,  6.3932e-01, -1.0271e-01,
         5.4961e-01,  8.7077e-01,  8.7766e-01,  1.2289e+00,  2.5107e-01,
         2.6267e-01,  1.2065e+00,  3.7056e-04,  1.4473e-02, -4.4853e-02,
         3.0192e-02], device='cuda:0')
Original likelihood: -18.295795440673828
Adjusted likelihood: -18.295795440673828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.447258559986949
Current ori: tensor([ 0.0004,  0.0145, -0.0449], device='cuda:0')
Middle force: tensor([0.5540, 0.5712, 1.1528, 0.5641, 1.1100, 0.6351, 0.5319, 0.5242, 0.5139,
        0.9162, 0.5771, 0.6329], device='cuda:0')
Thumb force: tensor([0.8444, 0.8298, 0.7576, 0.9854, 0.9320, 0.6475, 0.5208, 0.8557, 0.5315,
        0.6037, 0.5569, 0.5071], device='cuda:0')
Index force: tensor([0.5984, 0.5999, 0.5506, 0.5672, 0.7683, 0.5345, 0.9891, 0.9299, 0.5592,
        0.5398, 0.5815, 0.5677], device='cuda:0')
Storing NORMAL transition: reward=0.0292 (scaled=0.0292), steps=1
Reward stats updated: mean -0.0073 -> -0.0072, std: 0.1250
Collected 516 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1925, Q2 Loss=1.1925, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3411
SAC Update 2/5: Actor Loss=-0.0552, Q1 Loss=1.2168, Q2 Loss=1.2168, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0608
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0032, Q2 Loss=1.0032, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6975
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9257, Q2 Loss=0.9257, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7974
SAC Update 5/5: Actor Loss=-0.0101, Q1 Loss=1.7783, Q2 Loss=1.7783, Entropy=0.0234, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.3164

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.8%)
Q1 update: 0.04s (18.5%)
Q2 update: 0.04s (18.8%)
Actor update: 0.09s (42.3%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.059122
Q1 loss: 1.223299
Q2 loss: 1.223299
Current threshold: -37.0231
Global Scale Offset: 0.0243
Reward stats: mean=-0.0072, std=0.1250, count=516
----------------------------------------------
SAC Update - Actor Loss: -0.0591, Q1 Loss: 1.2233, Q2 Loss: 1.2233, Entropy: 0.0047, Mean TD Error: 1.4427, Threshold: -37.0231
tensor([ 0.1556,  0.6363,  0.5321,  0.6120, -0.1445,  0.6009,  0.7667,  1.0078,
         1.2339,  0.2767,  0.2872,  1.1536, -0.0128,  0.0207, -0.0745, -0.0420],
       device='cuda:0')
Original likelihood: -24.30044937133789
Adjusted likelihood: -24.30044937133789
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.609621373994742
Current ori: tensor([-0.0128,  0.0207, -0.0745], device='cuda:0')
Middle force: tensor([0.5590, 1.0609, 0.5499, 1.0123, 0.6156, 0.5244, 0.5149, 0.5092, 0.7832,
        0.6854, 0.5228], device='cuda:0')
Thumb force: tensor([0.8035, 0.7060, 0.9539, 0.9298, 0.6353, 0.5188, 0.8855, 0.5353, 0.5276,
        0.6629, 0.6269], device='cuda:0')
Index force: tensor([0.5826, 0.5432, 0.5570, 0.7393, 0.5203, 0.9607, 0.8604, 0.5617, 0.5770,
        0.7208, 0.5859], device='cuda:0')
Storing NORMAL transition: reward=0.1155 (scaled=0.1155), steps=1
Reward stats updated: mean -0.0072 -> -0.0070, std: 0.1250
Collected 517 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6676, Q2 Loss=0.6676, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2812
SAC Update 2/5: Actor Loss=-0.1319, Q1 Loss=3.7032, Q2 Loss=3.7032, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.3089
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.8204, Q2 Loss=0.8204, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4871
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1633, Q2 Loss=1.1633, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6734
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.5003, Q2 Loss=1.5003, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2465

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.5%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (18.4%)
Actor update: 0.10s (39.4%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.118480
Q1 loss: 1.570952
Q2 loss: 1.570952
Current threshold: -37.0668
Global Scale Offset: 0.0238
Reward stats: mean=-0.0070, std=0.1250, count=517
----------------------------------------------
SAC Update - Actor Loss: -0.1185, Q1 Loss: 1.5710, Q2 Loss: 1.5710, Entropy: 0.0000, Mean TD Error: 2.1994, Threshold: -37.0668
tensor([ 0.1650,  0.6445,  0.5335,  0.6095, -0.1348,  0.5697,  0.8104,  1.0324,
         1.2422,  0.2788,  0.2815,  1.1036, -0.0155,  0.0141, -0.1899,  0.1150],
       device='cuda:0')
Original likelihood: -24.18643569946289
Adjusted likelihood: -24.18643569946289
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.210354526992887
Current ori: tensor([-0.0155,  0.0141, -0.1899], device='cuda:0')
Middle force: tensor([1.0754, 0.5491, 1.0144, 0.6114, 0.5230, 0.5100, 0.5078, 0.8678, 0.5592,
        0.6194], device='cuda:0')
Thumb force: tensor([0.7052, 0.9170, 0.8895, 0.6284, 0.5156, 0.8740, 0.5290, 0.5941, 0.5479,
        0.5036], device='cuda:0')
Index force: tensor([0.5357, 0.5508, 0.7031, 0.5266, 0.9256, 0.8548, 0.5424, 0.5280, 0.5587,
        0.5531], device='cuda:0')
Storing NORMAL transition: reward=0.1736 (scaled=0.1736), steps=1
Reward stats updated: mean -0.0070 -> -0.0066, std: 0.1251
Collected 518 transitions for RL
SAC Update 1/5: Actor Loss=-0.2863, Q1 Loss=1.1134, Q2 Loss=1.1134, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8002
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1114, Q2 Loss=1.1114, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1493
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9619, Q2 Loss=0.9619, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2747
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0643, Q2 Loss=1.0643, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3550
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8968, Q2 Loss=0.8968, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2141

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.1%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (39.2%)
Target update: 0.00s (2.2%)
Priority update: 0.00s (0.2%)
Actor loss: -0.057266
Q1 loss: 1.029560
Q2 loss: 1.029560
Current threshold: -37.0996
Global Scale Offset: 0.0235
Reward stats: mean=-0.0066, std=0.1251, count=518
----------------------------------------------
SAC Update - Actor Loss: -0.0573, Q1 Loss: 1.0296, Q2 Loss: 1.0296, Entropy: 0.0000, Mean TD Error: 0.9587, Threshold: -37.0996
tensor([ 0.2293,  0.6417,  0.5995,  0.6194, -0.1192,  0.5317,  0.8169,  1.1512,
         1.2226,  0.3197,  0.3242,  1.0098, -0.0014,  0.0049, -0.3630, -0.1870],
       device='cuda:0')
Original likelihood: -22.780296325683594
Adjusted likelihood: -22.780296325683594
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.911677594995126
Current ori: tensor([-0.0014,  0.0049, -0.3630], device='cuda:0')
Middle force: tensor([0.5416, 0.9641, 0.5985, 0.5195, 0.5072, 0.5059, 0.8425, 0.5513, 0.6123],
       device='cuda:0')
Thumb force: tensor([0.8848, 0.8873, 0.6213, 0.5139, 0.8945, 0.5279, 0.5908, 0.5451, 0.5025],
       device='cuda:0')
Index force: tensor([0.5443, 0.6711, 0.5235, 0.8996, 0.8063, 0.5359, 0.5233, 0.5492, 0.5469],
       device='cuda:0')
Storing NORMAL transition: reward=0.0695 (scaled=0.0695), steps=1
Reward stats updated: mean -0.0066 -> -0.0065, std: 0.1251
Collected 519 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2980, Q2 Loss=1.2980, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0091
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=3.4159, Q2 Loss=3.4159, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.2148
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.2177, Q2 Loss=1.2177, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0531
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.2203, Q2 Loss=1.2203, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4678
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.8744, Q2 Loss=1.8744, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8543

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (18.2%)
Actor update: 0.09s (40.6%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 1.805260
Q2 loss: 1.805260
Current threshold: -37.1190
Global Scale Offset: 0.0233
Reward stats: mean=-0.0065, std=0.1251, count=519
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.8053, Q2 Loss: 1.8053, Entropy: 0.0000, Mean TD Error: 2.3198, Threshold: -37.1190
tensor([ 0.2166,  0.6457,  0.5761,  0.6244, -0.1232,  0.5130,  0.8533,  1.1276,
         1.2513,  0.3124,  0.3173,  0.9564, -0.0096,  0.0091, -0.4331,  0.0799],
       device='cuda:0')
Original likelihood: -24.29456329345703
Adjusted likelihood: -24.29456329345703
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.66300532099558
Current ori: tensor([-0.0096,  0.0091, -0.4331], device='cuda:0')
Middle force: tensor([0.8830, 0.5829, 0.5145, 0.5073, 0.5044, 0.7138, 0.6494, 0.5145],
       device='cuda:0')
Thumb force: tensor([0.8369, 0.6053, 0.5116, 0.8570, 0.5246, 0.5183, 0.6361, 0.6096],
       device='cuda:0')
Index force: tensor([0.6621, 0.5126, 0.8672, 0.7413, 0.5400, 0.5529, 0.6706, 0.5626],
       device='cuda:0')
Storing NORMAL transition: reward=0.0783 (scaled=0.0783), steps=1
Reward stats updated: mean -0.0065 -> -0.0063, std: 0.1250
Collected 520 transitions for RL
SAC Update 1/5: Actor Loss=-0.1195, Q1 Loss=2.1268, Q2 Loss=2.1268, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5764
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0487, Q2 Loss=1.0487, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7114
SAC Update 3/5: Actor Loss=-0.0398, Q1 Loss=1.2255, Q2 Loss=1.2255, Entropy=0.0303, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4184
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6682, Q2 Loss=0.6682, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5981
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.6334, Q2 Loss=0.6334, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2216

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.0%)
Q1 update: 0.05s (18.3%)
Q2 update: 0.05s (18.7%)
Actor update: 0.11s (43.2%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.031862
Q1 loss: 1.140529
Q2 loss: 1.140529
Current threshold: -37.1448
Global Scale Offset: 0.0231
Reward stats: mean=-0.0063, std=0.1250, count=520
----------------------------------------------
SAC Update - Actor Loss: -0.0319, Q1 Loss: 1.1405, Q2 Loss: 1.1405, Entropy: 0.0061, Mean TD Error: 0.9052, Threshold: -37.1448
tensor([ 0.1755,  0.4995,  0.5889,  0.5604, -0.1548,  0.4912,  0.9084,  1.1272,
         1.3615,  0.3088,  0.2235,  0.9244, -0.0405,  0.0334, -0.5446,  1.3592],
       device='cuda:0')
Original likelihood: -26.32706069946289
Adjusted likelihood: -26.32706069946289
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.613407196011394
Current ori: tensor([-0.0405,  0.0334, -0.5446], device='cuda:0')
Middle force: tensor([0.5904, 0.5160, 0.5063, 0.5044, 0.8150, 0.5434, 0.6099],
       device='cuda:0')
Thumb force: tensor([0.5983, 0.5095, 0.8225, 0.5180, 0.5752, 0.5337, 0.5008],
       device='cuda:0')
Index force: tensor([0.5174, 0.8504, 0.7710, 0.5289, 0.5175, 0.5388, 0.5380],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0771 (scaled=-0.0771), steps=1
Reward stats updated: mean -0.0063 -> -0.0064, std: 0.1249
Collected 521 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.0167, Q2 Loss=1.0167, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0068
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9635, Q2 Loss=0.9635, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6613
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6719, Q2 Loss=0.6719, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2773
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5823, Q2 Loss=1.5823, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2986
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=11.5832, Q2 Loss=11.5832, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=8.7501

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.5%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (20.0%)
Actor update: 0.12s (42.5%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 3.163528
Q2 loss: 3.163528
Current threshold: -37.1674
Global Scale Offset: 0.0229
Reward stats: mean=-0.0064, std=0.1249, count=521
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 3.1635, Q2 Loss: 3.1635, Entropy: 0.0000, Mean TD Error: 2.5988, Threshold: -37.1674
tensor([ 0.2337,  0.5597,  0.7239,  0.4889, -0.0769,  0.5167,  0.9448,  1.1143,
         1.3486,  0.3213,  0.1770,  0.8808, -0.0590, -0.0166, -0.6885,  2.3482],
       device='cuda:0')
Original likelihood: -28.626792907714844
Adjusted likelihood: -28.626792907714844
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.4310477880062535
Current ori: tensor([-0.0590, -0.0166, -0.6885], device='cuda:0')
Middle force: tensor([0.5933, 0.5414, 0.5623, 0.5095, 0.5031, 0.5355], device='cuda:0')
Thumb force: tensor([0.5559, 0.5510, 0.5667, 0.5006, 0.5910, 0.6157], device='cuda:0')
Index force: tensor([0.5445, 0.5264, 0.5796, 0.6820, 0.5650, 0.5798], device='cuda:0')
Storing NORMAL transition: reward=-0.0499 (scaled=-0.0499), steps=1
Reward stats updated: mean -0.0064 -> -0.0065, std: 0.1248
Collected 522 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9715, Q2 Loss=0.9715, Entropy=0.0000, Time=0.10sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9484
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2178, Q2 Loss=1.2178, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1129
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3097, Q2 Loss=1.3097, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3911
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.0996, Q2 Loss=1.0996, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8870
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8382, Q2 Loss=0.8382, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1099

------ SAC Update Summary (5 iterations) ------
Total time: 0.31s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (12.9%)
Q1 update: 0.05s (16.2%)
Q2 update: 0.10s (31.3%)
Actor update: 0.11s (36.5%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.087345
Q2 loss: 1.087345
Current threshold: -37.1808
Global Scale Offset: 0.0228
Reward stats: mean=-0.0065, std=0.1248, count=522
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0873, Q2 Loss: 1.0873, Entropy: 0.0000, Mean TD Error: 1.0899, Threshold: -37.1808
tensor([ 0.1685,  0.5043,  0.7049,  0.6386,  0.0098,  0.5564,  0.9838,  1.1167,
         1.3983,  0.1527,  0.1366,  0.7809, -0.0409, -0.0874, -0.7259,  3.0875],
       device='cuda:0')
Original likelihood: -24.798831939697266
Adjusted likelihood: -24.798831939697266
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.077211540017743
Current ori: tensor([-0.0409, -0.0874, -0.7259], device='cuda:0')
Middle force: tensor([0.5009, 0.9346, 0.6423, 0.5495, 0.5620], device='cuda:0')
Thumb force: tensor([0.5144, 0.5566, 0.5327, 0.5973, 0.5309], device='cuda:0')
Index force: tensor([0.9480, 0.8536, 0.5108, 0.5258, 0.5448], device='cuda:0')
Storing NORMAL transition: reward=-0.0375 (scaled=-0.0375), steps=1
Reward stats updated: mean -0.0065 -> -0.0066, std: 0.1247
Collected 523 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5855, Q2 Loss=1.5855, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1240
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9161, Q2 Loss=0.9161, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2799
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8734, Q2 Loss=0.8734, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7143
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3542, Q2 Loss=1.3542, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9107
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9141, Q2 Loss=0.9141, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8920

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (17.5%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (40.6%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 1.128655
Q2 loss: 1.128655
Current threshold: -37.1887
Global Scale Offset: 0.0228
Reward stats: mean=-0.0066, std=0.1247, count=523
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.1287, Q2 Loss: 1.1287, Entropy: 0.0000, Mean TD Error: 0.7842, Threshold: -37.1887
tensor([ 0.1342,  0.5154,  0.6792,  0.6842,  0.0566,  0.5553,  1.0000,  1.0380,
         1.4062,  0.2027,  0.0947,  0.6478, -0.0709, -0.1429, -0.7230,  2.1865],
       device='cuda:0')
Original likelihood: -35.88918685913086
Adjusted likelihood: -35.88918685913086
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 4.05706525297137
Current ori: tensor([-0.0709, -0.1429, -0.7230], device='cuda:0')
Middle force: tensor([0.9328, 0.6391, 0.5563, 0.5723], device='cuda:0')
Thumb force: tensor([0.5499, 0.5320, 0.5917, 0.5280], device='cuda:0')
Index force: tensor([0.8332, 0.5106, 0.5212, 0.5357], device='cuda:0')
Storing NORMAL transition: reward=-0.0124 (scaled=-0.0124), steps=1
Reward stats updated: mean -0.0066 -> -0.0066, std: 0.1246
Collected 524 transitions for RL
SAC Update 1/5: Actor Loss=-0.1784, Q1 Loss=3.1535, Q2 Loss=3.1535, Entropy=0.1936, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1172
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4257, Q2 Loss=1.4257, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5141
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8599, Q2 Loss=0.8599, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5216
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9159, Q2 Loss=0.9159, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2708
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7634, Q2 Loss=0.7634, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4383

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.6%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (41.3%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.035685
Q1 loss: 1.423683
Q2 loss: 1.423683
Current threshold: -37.1957
Global Scale Offset: 0.0227
Reward stats: mean=-0.0066, std=0.1246, count=524
----------------------------------------------
SAC Update - Actor Loss: -0.0357, Q1 Loss: 1.4237, Q2 Loss: 1.4237, Entropy: 0.0387, Mean TD Error: 1.3724, Threshold: -37.1957
tensor([ 0.1064,  0.5102,  0.6829,  0.7023,  0.1003,  0.6393,  0.9955,  1.0223,
         1.3889,  0.2403,  0.0805,  0.6618, -0.0731, -0.1553, -0.7283,  2.6226],
       device='cuda:0')
Original likelihood: -39.46529006958008
Adjusted likelihood: -39.46529006958008
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 38.145721435546875
Projection step: 1, Loss: 38.58087921142578
Projection step: 2, Loss: 37.88217544555664
Projection step: 3, Loss: 37.44377136230469
Projection step: 4, Loss: 38.94380569458008
Projection step: 5, Loss: 39.971405029296875
Projection step: 6, Loss: 37.52916717529297
Projection step: 7, Loss: 38.85603332519531
Projection step: 8, Loss: 39.12427520751953
Projection step: 9, Loss: 35.71880340576172
Projection step: 10, Loss: 37.78827667236328
Projection step: 11, Loss: 37.966705322265625
Projection step: 12, Loss: 35.32026290893555
Projection step: 13, Loss: 40.079917907714844
Projection step: 14, Loss: 37.240806579589844
Projection step: 15, Loss: 38.42079162597656
Projection step: 16, Loss: 38.64426040649414
Projection step: 17, Loss: 37.75242614746094
Projection step: 18, Loss: 36.82423400878906
Projection step: 19, Loss: 36.652366638183594
Projection step: 20, Loss: 38.628746032714844
Projection step: 21, Loss: 38.928096771240234
Projection step: 22, Loss: 38.45293426513672
Projection step: 23, Loss: 36.70623779296875
Projection step: 24, Loss: 38.65746307373047
Final likelihood: tensor([-31.9589, -39.8329, -33.7465, -36.3621, -42.0062, -31.9568, -33.9927,
        -32.9045, -33.9996, -42.7977, -40.8879, -32.7293, -31.9221, -44.4035,
        -42.5205, -33.3520])
Final projection likelihood: -36.5858
1 mode projection succeeded
New goal: tensor([ 0.1216,  0.4762,  0.6918,  0.6646,  0.0788,  0.6262,  0.9573,  0.9942,
         1.3989,  0.2156,  0.0806,  0.6618, -0.0689, -0.1525, -0.3608],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -40.66083526611328
Adjusted likelihood: -40.66083526611328
Likelihood residual: 0.0
Original likelihood: -40.75523376464844
Adjusted likelihood: -40.75523376464844
Likelihood residual: 0.0
{'index': 40.75523376464844, 'thumb_middle': 40.66083526611328}
Current yaw: tensor([-0.0731, -0.1553, -0.7283], device='cuda:0')
2 thumb_middle
tensor([ 0.1064,  0.5102,  0.6829,  0.7023,  0.1003,  0.6393,  0.9955,  1.0223,
         1.3889,  0.2403,  0.0805,  0.6618, -0.0731, -0.1553, -0.7283,  2.6226],
       device='cuda:0')
Solve time for step 1 8.910699811007362
Current ori: tensor([-0.0731, -0.1553, -0.7283], device='cuda:0')
Index force: tensor([0.5015, 0.5744, 0.5670, 0.5856], device='cuda:0')
tensor([ 0.1159,  0.5515,  0.7183,  0.6860,  0.0142,  0.6113,  0.9354,  1.0027,
         1.3457,  0.2102, -0.0240,  0.6042, -0.1214, -0.2597, -0.7268,  2.5962],
       device='cuda:0')
Solve time for step 2 3.678442454955075
Current ori: tensor([-0.1214, -0.2597, -0.7268], device='cuda:0')
Index force: tensor([0.5591, 0.5586, 0.5758], device='cuda:0')
tensor([ 0.0835,  0.6036,  0.7761,  0.7160,  0.0323,  0.6583,  0.9424,  0.9965,
         1.3352,  0.1996, -0.0976,  0.6034, -0.1415, -0.3099, -0.7169,  3.7482],
       device='cuda:0')
Solve time for step 3 3.4602513770223595
Current ori: tensor([-0.1415, -0.3099, -0.7169], device='cuda:0')
Index force: tensor([0.5005, 0.5460], device='cuda:0')
tensor([ 0.0656,  0.6478,  0.7917,  0.7048,  0.0607,  0.7004,  0.9357,  0.9536,
         1.3288,  0.2059, -0.1477,  0.5871, -0.1613, -0.3463, -0.7017,  4.3749],
       device='cuda:0')
Solve time for step 4 3.3046220800024457
Current ori: tensor([-0.1613, -0.3463, -0.7017], device='cuda:0')
Index force: tensor([0.5360], device='cuda:0')
Storing RECOVERY transition: reward=-0.1264 (scaled=-0.0140), steps=9
Reward stats updated: mean -0.0066 -> -0.0066, std: 0.1245
Collected 525 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1459, Q2 Loss=1.1459, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8071
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1744, Q2 Loss=1.1744, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1694
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2941, Q2 Loss=1.2941, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5137
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7798, Q2 Loss=0.7798, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5898
SAC Update 5/5: Actor Loss=-0.0842, Q1 Loss=0.9635, Q2 Loss=0.9635, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7851

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (17.7%)
Q1 update: 0.06s (21.8%)
Q2 update: 0.05s (18.7%)
Actor update: 0.11s (38.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.062882
Q1 loss: 1.071545
Q2 loss: 1.071545
Current threshold: -37.1998
Global Scale Offset: 0.0227
Reward stats: mean=-0.0066, std=0.1245, count=525
----------------------------------------------
SAC Update - Actor Loss: -0.0629, Q1 Loss: 1.0715, Q2 Loss: 1.0715, Entropy: 0.0000, Mean TD Error: 0.7730, Threshold: -37.1998
Original likelihood: -252.69290161132812
Adjusted likelihood: -252.69290161132812
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 266.04791259765625
Projection step: 1, Loss: 247.5252685546875
Projection step: 2, Loss: 251.55667114257812
Projection step: 3, Loss: 254.0196533203125
Projection step: 4, Loss: 260.3544616699219
Projection step: 5, Loss: 251.06588745117188
Projection step: 6, Loss: 243.6381072998047
Projection step: 7, Loss: 248.86126708984375
Projection step: 8, Loss: 253.8822784423828
Projection step: 9, Loss: 253.00660705566406
Projection step: 10, Loss: 234.70455932617188
Projection step: 11, Loss: 254.18521118164062
Projection step: 12, Loss: 259.2019958496094
Projection step: 13, Loss: 250.7003631591797
Projection step: 14, Loss: 265.9700927734375
Projection step: 15, Loss: 251.5435791015625
Projection step: 16, Loss: 249.131591796875
Projection step: 17, Loss: 243.72755432128906
Projection step: 18, Loss: 239.97720336914062
Projection step: 19, Loss: 250.3795166015625
Projection step: 20, Loss: 258.2776184082031
Projection step: 21, Loss: 250.58700561523438
Projection step: 22, Loss: 242.55406188964844
Projection step: 23, Loss: 244.33493041992188
Projection step: 24, Loss: 240.09555053710938
Final likelihood: tensor([-281.3180, -246.1924, -262.7630, -262.4321, -162.5109, -250.7317,
        -250.2305, -265.4610, -240.9982, -251.9734, -279.0192, -252.4129,
        -245.3693, -220.3519, -281.5223, -266.3894])
Final projection likelihood: -251.2298
1 mode projection failed, trying anyway
New goal: tensor([ 0.0405,  0.7315,  0.8329,  0.7166,  0.0757,  1.0013,  0.8013,  0.7418,
         1.3253,  0.2185, -0.0955,  0.6098, -0.1473, -0.3084, -0.6482],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0070]], device='cuda:0') tensor([[0.0080]], device='cuda:0')
Original likelihood: -297.5799560546875
Adjusted likelihood: -297.5799560546875
Likelihood residual: 0.0
Original likelihood: -246.6457977294922
Adjusted likelihood: -246.6457977294922
Likelihood residual: 0.0
{'index': 246.6457977294922, 'thumb_middle': 297.5799560546875}
Current yaw: tensor([-0.1478, -0.3083, -0.6840], device='cuda:0')
3 index
tensor([ 0.0367,  0.7322,  0.8354,  0.7135,  0.0779,  1.0114,  0.7970,  0.7333,
         1.3318,  0.2123, -0.0958,  0.6025, -0.1478, -0.3083, -0.6840,  3.7614],
       device='cuda:0')
Solve time for step 1 10.57430390897207
Current ori: tensor([-0.1478, -0.3083, -0.6840], device='cuda:0')
Middle force: tensor([0.5618, 0.5772, 0.5307, 0.5740], device='cuda:0')
Thumb force: tensor([0.5854, 0.6152, 0.5889, 0.5742], device='cuda:0')
tensor([-0.0331,  0.7258,  0.8105,  0.7009,  0.0703,  1.0847,  0.8183,  0.7321,
         1.3213,  0.2342, -0.1528,  0.6309, -0.1600, -0.3393, -0.6543,  3.2646],
       device='cuda:0')
Solve time for step 2 4.259470422985032
Current ori: tensor([-0.1600, -0.3393, -0.6543], device='cuda:0')
Middle force: tensor([0.5724, 0.5280, 0.5750], device='cuda:0')
Thumb force: tensor([0.6074, 0.5833, 0.5656], device='cuda:0')
tensor([-0.0241,  0.7505,  0.8165,  0.7048,  0.0740,  1.1163,  0.8352,  0.7346,
         1.3219,  0.2242, -0.1693,  0.6316, -0.1672, -0.3478, -0.6313,  3.5669],
       device='cuda:0')
Solve time for step 3 3.986444456037134
Current ori: tensor([-0.1672, -0.3478, -0.6313], device='cuda:0')
Middle force: tensor([0.5100, 0.5398], device='cuda:0')
Thumb force: tensor([0.5763, 0.5816], device='cuda:0')
tensor([-0.0088,  0.7684,  0.8243,  0.7088,  0.0676,  1.1504,  0.8359,  0.7443,
         1.3238,  0.2236, -0.1788,  0.5885, -0.1876, -0.3614, -0.6031,  3.8132],
       device='cuda:0')
Solve time for step 4 3.9530297379824333
Current ori: tensor([-0.1876, -0.3614, -0.6031], device='cuda:0')
Middle force: tensor([0.5265], device='cuda:0')
Thumb force: tensor([0.6043], device='cuda:0')
Storing RECOVERY transition: reward=-0.1933 (scaled=-0.0215), steps=9
Reward stats updated: mean -0.0066 -> -0.0066, std: 0.1243
Collected 526 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.8126, Q2 Loss=0.8126, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0499
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0662, Q2 Loss=1.0662, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2709
SAC Update 3/5: Actor Loss=-0.0933, Q1 Loss=1.0261, Q2 Loss=1.0261, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9130
SAC Update 4/5: Actor Loss=-0.2248, Q1 Loss=1.0932, Q2 Loss=1.0932, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5422
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3373, Q2 Loss=1.3373, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2451

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.5%)
Q1 update: 0.04s (18.4%)
Q2 update: 0.04s (19.5%)
Actor update: 0.09s (41.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.109661
Q1 loss: 1.067092
Q2 loss: 1.067092
Current threshold: -37.2144
Global Scale Offset: 0.0225
Reward stats: mean=-0.0066, std=0.1243, count=526
----------------------------------------------
SAC Update - Actor Loss: -0.1097, Q1 Loss: 1.0671, Q2 Loss: 1.0671, Entropy: 0.0000, Mean TD Error: 0.8042, Threshold: -37.2144
Original likelihood: -334.5311584472656
Adjusted likelihood: -334.5311584472656
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 33
Loaded trajectory sampler
Current yaw: tensor([ 0.0005,  0.0145, -0.0448], device='cuda:0')
Current yaw: tensor([ 0.0005,  0.0145, -0.0448], device='cuda:0')
1 turn
Sampling time 3.620282499003224
tensor([ 1.2582e-01,  6.1884e-01,  5.2630e-01,  6.1604e-01, -1.3654e-01,
         5.1951e-01,  9.6369e-01,  8.7576e-01,  1.2506e+00,  2.6008e-01,
         2.5712e-01,  1.1530e+00,  5.4562e-04,  1.4472e-02, -4.4799e-02,
         5.5612e-02], device='cuda:0')
Original likelihood: -18.303607940673828
Adjusted likelihood: -18.303607940673828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.114313950994983
Current ori: tensor([ 0.0005,  0.0145, -0.0448], device='cuda:0')
Middle force: tensor([0.5445, 0.5543, 1.6450, 0.6112, 0.5911, 0.5566, 0.5531, 0.5484, 0.5020,
        1.8876, 0.9087, 0.5578], device='cuda:0')
Thumb force: tensor([0.7051, 0.8395, 1.8511, 0.7197, 1.0573, 0.5865, 0.5921, 0.7856, 2.0764,
        0.5724, 0.5223, 0.5682], device='cuda:0')
Index force: tensor([0.5397, 0.8970, 0.5821, 0.5482, 0.6024, 0.5844, 0.6221, 0.9185, 0.6929,
        0.7291, 0.5667, 0.5662], device='cuda:0')
Storing NORMAL transition: reward=0.1115 (scaled=0.1115), steps=1
Reward stats updated: mean -0.0066 -> -0.0064, std: 0.1243
Collected 527 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9256, Q2 Loss=0.9256, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7434
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7725, Q2 Loss=0.7725, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7391
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9106, Q2 Loss=0.9106, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1149
SAC Update 4/5: Actor Loss=-0.0037, Q1 Loss=1.0868, Q2 Loss=1.0868, Entropy=0.3085, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4203
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8769, Q2 Loss=0.8769, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9875

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.6%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (19.7%)
Actor update: 0.11s (41.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.000735
Q1 loss: 0.914482
Q2 loss: 0.914482
Current threshold: -37.2393
Global Scale Offset: 0.0222
Reward stats: mean=-0.0064, std=0.1243, count=527
----------------------------------------------
SAC Update - Actor Loss: -0.0007, Q1 Loss: 0.9145, Q2 Loss: 0.9145, Entropy: 0.0617, Mean TD Error: 0.8010, Threshold: -37.2393
tensor([ 0.1645,  0.6708,  0.5255,  0.5299, -0.1842,  0.3944,  1.0957,  0.9294,
         1.1968,  0.3003,  0.4424,  0.9598,  0.0327,  0.0303, -0.1585, -0.9313],
       device='cuda:0')
Original likelihood: -30.70099449157715
Adjusted likelihood: -30.70099449157715
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.582594912964851
Current ori: tensor([ 0.0327,  0.0303, -0.1585], device='cuda:0')
Middle force: tensor([0.5294, 1.6748, 0.6081, 0.5921, 0.5513, 0.5288, 0.5257, 0.5013, 1.8435,
        0.8961, 0.5573], device='cuda:0')
Thumb force: tensor([0.8851, 1.7885, 0.7162, 1.0388, 0.5840, 0.6044, 0.8174, 2.0458, 0.5756,
        0.5218, 0.5648], device='cuda:0')
Index force: tensor([0.9041, 0.5970, 0.5425, 0.5955, 0.5866, 0.6435, 0.9431, 0.7198, 0.7147,
        0.5638, 0.5634], device='cuda:0')
Storing NORMAL transition: reward=-0.0144 (scaled=-0.0144), steps=1
Reward stats updated: mean -0.0064 -> -0.0064, std: 0.1242
Collected 528 transitions for RL
SAC Update 1/5: Actor Loss=-0.0360, Q1 Loss=2.1187, Q2 Loss=2.1187, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.4009
SAC Update 2/5: Actor Loss=-0.0606, Q1 Loss=1.1594, Q2 Loss=1.1594, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8398
SAC Update 3/5: Actor Loss=-0.1088, Q1 Loss=1.0489, Q2 Loss=1.0489, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8137
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3038, Q2 Loss=1.3038, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6019
SAC Update 5/5: Actor Loss=-0.0055, Q1 Loss=0.8459, Q2 Loss=0.8459, Entropy=0.3406, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7872

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.2%)
Q1 update: 0.04s (18.5%)
Q2 update: 0.04s (18.8%)
Actor update: 0.09s (40.4%)
Target update: 0.00s (1.8%)
Priority update: 0.00s (0.2%)
Actor loss: -0.042175
Q1 loss: 1.295334
Q2 loss: 1.295334
Current threshold: -37.2710
Global Scale Offset: 0.0219
Reward stats: mean=-0.0064, std=0.1242, count=528
----------------------------------------------
SAC Update - Actor Loss: -0.0422, Q1 Loss: 1.2953, Q2 Loss: 1.2953, Entropy: 0.0681, Mean TD Error: 1.4887, Threshold: -37.2710
tensor([ 0.1380,  0.7698,  0.3747,  0.4815, -0.1653,  0.4456,  0.9980,  0.9305,
         1.2812,  0.1837,  0.3941,  0.9193,  0.0133,  0.0324, -0.1432, -1.0213],
       device='cuda:0')
Original likelihood: -31.100994110107422
Adjusted likelihood: -31.100994110107422
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.24481622496387
Current ori: tensor([ 0.0133,  0.0324, -0.1432], device='cuda:0')
Middle force: tensor([1.2694, 1.3816, 0.5327, 0.5033, 0.5046, 0.5556, 0.5264, 0.5294, 0.5016,
        0.5040], device='cuda:0')
Thumb force: tensor([1.0792, 1.0072, 0.7116, 0.8261, 0.5123, 1.0931, 0.6086, 0.6203, 0.5146,
        0.5573], device='cuda:0')
Index force: tensor([0.5761, 0.5093, 0.9023, 0.5307, 0.5461, 0.5132, 0.5073, 0.5557, 0.6624,
        0.7858], device='cuda:0')
Storing NORMAL transition: reward=0.2495 (scaled=0.2495), steps=1
Reward stats updated: mean -0.0064 -> -0.0060, std: 0.1246
Collected 529 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0476, Q2 Loss=1.0476, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8271
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.8858, Q2 Loss=0.8858, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5844
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3151, Q2 Loss=1.3151, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4844
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9567, Q2 Loss=0.9567, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7116
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=2.5167, Q2 Loss=2.5167, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1691

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (16.5%)
Q1 update: 0.05s (18.8%)
Q2 update: 0.06s (21.3%)
Actor update: 0.11s (39.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.344397
Q2 loss: 1.344397
Current threshold: -37.2947
Global Scale Offset: 0.0218
Reward stats: mean=-0.0060, std=0.1246, count=529
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.3444, Q2 Loss: 1.3444, Entropy: 0.0000, Mean TD Error: 1.1553, Threshold: -37.2947
tensor([ 9.7428e-02,  6.8136e-01,  3.5078e-01,  3.1748e-01, -1.1433e-01,
         3.9510e-01,  1.0675e+00,  1.0386e+00,  1.3277e+00,  1.2829e-01,
         3.4174e-01,  8.8727e-01,  1.8466e-02, -3.0174e-04, -3.9221e-01,
        -4.6564e+00], device='cuda:0')
Original likelihood: -30.16384506225586
Adjusted likelihood: -30.16384506225586
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.973823558015283
Current ori: tensor([ 1.8466e-02, -3.0174e-04, -3.9221e-01], device='cuda:0')
Middle force: tensor([0.6127, 0.5885, 0.5482, 0.5236, 0.5765, 0.5034, 1.7932, 0.8944, 0.5546],
       device='cuda:0')
Thumb force: tensor([0.6831, 1.0061, 0.5751, 0.5766, 0.6863, 1.9132, 0.5615, 0.5165, 0.5593],
       device='cuda:0')
Index force: tensor([0.5339, 0.5874, 0.5862, 0.6616, 0.8772, 0.6549, 0.6968, 0.5592, 0.5586],
       device='cuda:0')
Storing NORMAL transition: reward=0.0307 (scaled=0.0307), steps=1
Reward stats updated: mean -0.0060 -> -0.0059, std: 0.1245
Collected 530 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4950, Q2 Loss=1.4950, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1494
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9411, Q2 Loss=0.9411, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8044
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0036, Q2 Loss=1.0036, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8900
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6632, Q2 Loss=0.6632, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0658
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0419, Q2 Loss=1.0419, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5614

------ SAC Update Summary (5 iterations) ------
Total time: 0.31s, Avg iteration: 0.06s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (15.1%)
Q1 update: 0.06s (20.1%)
Q2 update: 0.06s (20.1%)
Actor update: 0.13s (40.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.3%)
Actor loss: 0.000000
Q1 loss: 1.028965
Q2 loss: 1.028965
Current threshold: -37.3087
Global Scale Offset: 0.0217
Reward stats: mean=-0.0059, std=0.1245, count=530
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.0290, Q2 Loss: 1.0290, Entropy: 0.0000, Mean TD Error: 0.6942, Threshold: -37.3087
tensor([ 0.1145,  0.7231,  0.4584,  0.3803, -0.1180,  0.3554,  1.0253,  1.1188,
         1.5000, -0.1050,  0.3074,  0.9031,  0.0234,  0.0188, -0.4251, -5.5667],
       device='cuda:0')
Original likelihood: -25.581056594848633
Adjusted likelihood: -25.581056594848633
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.736973521008622
Current ori: tensor([ 0.0234,  0.0188, -0.4251], device='cuda:0')
Middle force: tensor([0.5846, 0.5460, 0.5155, 0.5448, 0.5014, 1.7480, 0.8735, 0.5519],
       device='cuda:0')
Thumb force: tensor([0.9894, 0.5745, 0.5860, 0.7119, 1.8905, 0.5622, 0.5167, 0.5579],
       device='cuda:0')
Index force: tensor([0.5820, 0.5831, 0.6748, 0.9009, 0.7011, 0.6947, 0.5574, 0.5566],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0010 (scaled=-0.0010), steps=1
Reward stats updated: mean -0.0059 -> -0.0059, std: 0.1244
Collected 531 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.6821, Q2 Loss=1.6821, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3987
SAC Update 2/5: Actor Loss=-0.0385, Q1 Loss=0.7525, Q2 Loss=0.7525, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2221
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6615, Q2 Loss=0.6615, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1902
SAC Update 4/5: Actor Loss=-0.0092, Q1 Loss=1.1281, Q2 Loss=1.1281, Entropy=0.3365, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3455
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2756, Q2 Loss=1.2756, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2531

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.0%)
Q1 update: 0.05s (20.7%)
Q2 update: 0.05s (19.1%)
Actor update: 0.11s (42.0%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.055595
Q1 loss: 1.099973
Q2 loss: 1.099973
Current threshold: -37.3215
Global Scale Offset: 0.0216
Reward stats: mean=-0.0059, std=0.1244, count=531
----------------------------------------------
SAC Update - Actor Loss: -0.0556, Q1 Loss: 1.1000, Q2 Loss: 1.1000, Entropy: 0.0673, Mean TD Error: 0.4819, Threshold: -37.3215
tensor([ 0.1820,  0.7917,  0.4117,  0.3960, -0.0808,  0.4169,  1.0609,  1.0507,
         1.4041,  0.2559,  0.2541,  0.7484,  0.0120, -0.0201, -0.4228, -5.5147],
       device='cuda:0')
Original likelihood: -36.960235595703125
Adjusted likelihood: -36.960235595703125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9995)
Solve time for step 6 4.697304462024476
Current ori: tensor([ 0.0120, -0.0201, -0.4228], device='cuda:0')
Middle force: tensor([0.5437, 0.5127, 0.5271, 0.5010, 1.6980, 0.8505, 0.5487],
       device='cuda:0')
Thumb force: tensor([0.5699, 0.5924, 0.7661, 1.8753, 0.5639, 0.5175, 0.5571],
       device='cuda:0')
Index force: tensor([0.5778, 0.6688, 0.8870, 0.7058, 0.6952, 0.5555, 0.5553],
       device='cuda:0')
Storing NORMAL transition: reward=-0.1550 (scaled=-0.1550), steps=1
Reward stats updated: mean -0.0059 -> -0.0062, std: 0.1244
Collected 532 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.7208, Q2 Loss=0.7208, Entropy=0.0022, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8091
SAC Update 2/5: Actor Loss=-0.0074, Q1 Loss=0.7380, Q2 Loss=0.7380, Entropy=0.3460, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3991
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.8455, Q2 Loss=0.8455, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4165
SAC Update 4/5: Actor Loss=-0.0084, Q1 Loss=1.2473, Q2 Loss=1.2473, Entropy=0.3183, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7478
SAC Update 5/5: Actor Loss=-0.0116, Q1 Loss=1.4604, Q2 Loss=1.4604, Entropy=0.3112, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9066

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (16.0%)
Q1 update: 0.04s (18.1%)
Q2 update: 0.05s (19.0%)
Actor update: 0.11s (43.1%)
Target update: 0.00s (1.9%)
Priority update: 0.00s (0.1%)
Actor loss: -0.051540
Q1 loss: 1.002403
Q2 loss: 1.002403
Current threshold: -37.3475
Global Scale Offset: 0.0216
Reward stats: mean=-0.0062, std=0.1244, count=532
----------------------------------------------
SAC Update - Actor Loss: -0.0515, Q1 Loss: 1.0024, Q2 Loss: 1.0024, Entropy: 0.1955, Mean TD Error: 0.8558, Threshold: -37.3475
tensor([ 0.0616,  0.7093,  0.3958,  0.4709, -0.1711,  0.4128,  0.9813,  1.0542,
         1.4999,  0.2721,  0.1640,  0.7819,  0.0342,  0.0350, -0.2699, -5.9368],
       device='cuda:0')
Original likelihood: -28.06103515625
Adjusted likelihood: -28.06103515625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.381497949012555
Current ori: tensor([ 0.0342,  0.0350, -0.2699], device='cuda:0')
Middle force: tensor([0.5153, 0.5307, 0.5010, 1.6661, 0.8400, 0.5457], device='cuda:0')
Thumb force: tensor([0.5814, 0.7661, 1.8428, 0.5659, 0.5177, 0.5563], device='cuda:0')
Index force: tensor([0.6562, 0.8481, 0.6808, 0.6891, 0.5524, 0.5540], device='cuda:0')
Storing NORMAL transition: reward=0.0434 (scaled=0.0434), steps=1
Reward stats updated: mean -0.0062 -> -0.0061, std: 0.1243
Collected 533 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.9889, Q2 Loss=0.9889, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3643
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6857, Q2 Loss=1.6857, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7212
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2373, Q2 Loss=1.2373, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0558
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1531, Q2 Loss=1.1531, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2054
SAC Update 5/5: Actor Loss=-0.2945, Q1 Loss=0.9432, Q2 Loss=0.9432, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0868

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (16.9%)
Q1 update: 0.04s (19.9%)
Q2 update: 0.04s (19.2%)
Actor update: 0.09s (40.1%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.104957
Q1 loss: 1.201642
Q2 loss: 1.201642
Current threshold: -37.3769
Global Scale Offset: 0.0215
Reward stats: mean=-0.0061, std=0.1243, count=533
----------------------------------------------
SAC Update - Actor Loss: -0.1050, Q1 Loss: 1.2016, Q2 Loss: 1.2016, Entropy: 0.0000, Mean TD Error: 0.8867, Threshold: -37.3769
tensor([ 0.0840,  0.7380,  0.3770,  0.4520, -0.1534,  0.4051,  1.0086,  1.0501,
         1.5000,  0.3060,  0.2009,  0.7189,  0.0268,  0.0237, -0.3117, -5.8611],
       device='cuda:0')
Original likelihood: -28.22713851928711
Adjusted likelihood: -28.22713851928711
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.070074240036774
Current ori: tensor([ 0.0268,  0.0237, -0.3117], device='cuda:0')
Middle force: tensor([0.5330, 0.5009, 1.6349, 0.8305, 0.5440], device='cuda:0')
Thumb force: tensor([0.7498, 1.8116, 0.5653, 0.5173, 0.5547], device='cuda:0')
Index force: tensor([0.8269, 0.6746, 0.6832, 0.5503, 0.5522], device='cuda:0')
Storing NORMAL transition: reward=0.0265 (scaled=0.0265), steps=1
Reward stats updated: mean -0.0061 -> -0.0060, std: 0.1242
Collected 534 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=2.5499, Q2 Loss=2.5499, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0350
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1505, Q2 Loss=1.1505, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5261
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0503, Q2 Loss=1.0503, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9627
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8232, Q2 Loss=0.8232, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4798
SAC Update 5/5: Actor Loss=-0.0650, Q1 Loss=2.4056, Q2 Loss=2.4056, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6031

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.1%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (40.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.3%)
Actor loss: -0.059051
Q1 loss: 1.595894
Q2 loss: 1.595894
Current threshold: -37.3984
Global Scale Offset: 0.0214
Reward stats: mean=-0.0060, std=0.1242, count=534
----------------------------------------------
SAC Update - Actor Loss: -0.0591, Q1 Loss: 1.5959, Q2 Loss: 1.5959, Entropy: 0.0000, Mean TD Error: 1.5213, Threshold: -37.3984
tensor([ 0.0933,  0.7568,  0.3616,  0.4258, -0.1419,  0.4062,  1.0188,  1.0456,
         1.5000,  0.2737,  0.2253,  0.6709,  0.0217,  0.0171, -0.3374, -5.8763],
       device='cuda:0')
Original likelihood: -27.619850158691406
Adjusted likelihood: -27.619850158691406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 3.9876593010267243
Current ori: tensor([ 0.0217,  0.0171, -0.3374], device='cuda:0')
Middle force: tensor([0.5009, 1.6105, 0.8246, 0.5428], device='cuda:0')
Thumb force: tensor([1.7711, 0.5621, 0.5162, 0.5528], device='cuda:0')
Index force: tensor([0.6647, 0.6764, 0.5484, 0.5502], device='cuda:0')
Storing NORMAL transition: reward=-0.0019 (scaled=-0.0019), steps=1
Reward stats updated: mean -0.0060 -> -0.0060, std: 0.1241
Collected 535 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7891, Q2 Loss=0.7891, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2162
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3930, Q2 Loss=1.3930, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9034
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6598, Q2 Loss=0.6598, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1061
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2166, Q2 Loss=1.2166, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7893
SAC Update 5/5: Actor Loss=-0.2201, Q1 Loss=1.3182, Q2 Loss=1.3182, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5602

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.3%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (38.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.044020
Q1 loss: 1.075347
Q2 loss: 1.075347
Current threshold: -37.4240
Global Scale Offset: 0.0212
Reward stats: mean=-0.0060, std=0.1241, count=535
----------------------------------------------
SAC Update - Actor Loss: -0.0440, Q1 Loss: 1.0753, Q2 Loss: 1.0753, Entropy: 0.0000, Mean TD Error: 0.7150, Threshold: -37.4240
tensor([ 0.0956,  0.7768,  0.3202,  0.3941, -0.1716,  0.4306,  1.0484,  1.0582,
         1.5000,  0.3528,  0.1141,  0.6868,  0.0168,  0.0140, -0.3349, -5.8771],
       device='cuda:0')
Original likelihood: -30.408117294311523
Adjusted likelihood: -30.408117294311523
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 10 3.806378601992037
Current ori: tensor([ 0.0168,  0.0140, -0.3349], device='cuda:0')
Middle force: tensor([0.6915, 0.5213, 0.5968], device='cuda:0')
Thumb force: tensor([0.6816, 0.5584, 0.5442], device='cuda:0')
Index force: tensor([0.5205, 0.5308, 0.5429], device='cuda:0')
Storing NORMAL transition: reward=-0.0824 (scaled=-0.0824), steps=1
Reward stats updated: mean -0.0060 -> -0.0061, std: 0.1240
Collected 536 transitions for RL
SAC Update 1/5: Actor Loss=-0.0775, Q1 Loss=2.6510, Q2 Loss=2.6510, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.7169
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1055, Q2 Loss=1.1055, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1393
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.8598, Q2 Loss=0.8598, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5614
SAC Update 4/5: Actor Loss=-0.2182, Q1 Loss=1.2962, Q2 Loss=1.2962, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4970
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9993, Q2 Loss=0.9993, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8179

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (15.8%)
Q1 update: 0.05s (18.3%)
Q2 update: 0.04s (18.0%)
Actor update: 0.09s (36.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.151251
Q1 loss: 1.382377
Q2 loss: 1.382377
Current threshold: -37.4496
Global Scale Offset: 0.0210
Reward stats: mean=-0.0061, std=0.1240, count=536
----------------------------------------------
SAC Update - Actor Loss: -0.1513, Q1 Loss: 1.3824, Q2 Loss: 1.3824, Entropy: 0.0000, Mean TD Error: 1.3465, Threshold: -37.4496
tensor([ 0.0566,  0.7022,  0.4340,  0.3750, -0.2270,  0.4807,  1.0175,  1.0392,
         1.4999,  0.3575,  0.0516,  0.7127,  0.0325,  0.0288, -0.2545, -6.0130],
       device='cuda:0')
Original likelihood: -37.583621978759766
Adjusted likelihood: -37.583621978759766
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.1067)
Solve time for step 11 3.5946938329725526
Current ori: tensor([ 0.0325,  0.0288, -0.2545], device='cuda:0')
Middle force: tensor([0.5007, 0.5007], device='cuda:0')
Thumb force: tensor([1.0815, 0.5032], device='cuda:0')
Index force: tensor([0.5065, 0.9110], device='cuda:0')
Storing NORMAL transition: reward=0.0179 (scaled=0.0179), steps=1
Reward stats updated: mean -0.0061 -> -0.0061, std: 0.1239
Collected 537 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4784, Q2 Loss=1.4784, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0235
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6624, Q2 Loss=0.6624, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1045
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7127, Q2 Loss=0.7127, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5345
SAC Update 4/5: Actor Loss=-0.1968, Q1 Loss=1.2127, Q2 Loss=1.2127, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3964
SAC Update 5/5: Actor Loss=-0.0131, Q1 Loss=0.6850, Q2 Loss=0.6850, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7377

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.0%)
Q1 update: 0.06s (19.8%)
Q2 update: 0.05s (17.8%)
Actor update: 0.11s (39.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.041983
Q1 loss: 0.950227
Q2 loss: 0.950227
Current threshold: -37.4648
Global Scale Offset: 0.0208
Reward stats: mean=-0.0061, std=0.1239, count=537
----------------------------------------------
SAC Update - Actor Loss: -0.0420, Q1 Loss: 0.9502, Q2 Loss: 0.9502, Entropy: 0.0000, Mean TD Error: 0.7593, Threshold: -37.4648
tensor([ 0.0914,  0.7196,  0.4351,  0.3986, -0.1997,  0.4946,  1.0152,  1.0507,
         1.5000,  0.3780,  0.1476,  0.7514,  0.0294,  0.0100, -0.2710, -5.9538],
       device='cuda:0')
Original likelihood: -31.17331314086914
Adjusted likelihood: -31.17331314086914
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 12 3.459910044039134
Current ori: tensor([ 0.0294,  0.0100, -0.2710], device='cuda:0')
Middle force: tensor([0.5009], device='cuda:0')
Thumb force: tensor([0.5026], device='cuda:0')
Index force: tensor([0.8725], device='cuda:0')
Storing NORMAL transition: reward=-0.0830 (scaled=-0.0830), steps=1
Reward stats updated: mean -0.0061 -> -0.0062, std: 0.1238
Collected 538 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.2128, Q2 Loss=2.2128, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4481
SAC Update 2/5: Actor Loss=-0.3524, Q1 Loss=1.8722, Q2 Loss=1.8722, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2774
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0165, Q2 Loss=1.0165, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0308
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5679, Q2 Loss=1.5679, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5557
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7817, Q2 Loss=0.7817, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5162

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.9%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (38.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.070478
Q1 loss: 1.490249
Q2 loss: 1.490249
Current threshold: -37.4738
Global Scale Offset: 0.0207
Reward stats: mean=-0.0062, std=0.1238, count=538
----------------------------------------------
SAC Update - Actor Loss: -0.0705, Q1 Loss: 1.4902, Q2 Loss: 1.4902, Entropy: 0.0000, Mean TD Error: 1.5656, Threshold: -37.4738
Original likelihood: -31.272384643554688
Adjusted likelihood: -31.272384643554688
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0380,  0.0349, -0.1899], device='cuda:0')
2 turn
Sampling time 3.770946478995029
tensor([ 0.0444,  0.6808,  0.4492,  0.3978, -0.1932,  0.4613,  1.0156,  0.9201,
         1.4846,  0.4504,  0.1988,  0.7607,  0.0380,  0.0349, -0.1899, -6.1212],
       device='cuda:0')
Original likelihood: -30.681671142578125
Adjusted likelihood: -30.681671142578125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.49156827601837
Current ori: tensor([ 0.0380,  0.0349, -0.1899], device='cuda:0')
Middle force: tensor([0.6215, 1.1762, 1.4136, 0.7078, 1.0835, 0.7402, 0.6278, 0.5811, 0.9252,
        0.5797, 0.5327, 0.5108], device='cuda:0')
Thumb force: tensor([0.8287, 2.9297, 0.6662, 0.6014, 0.6468, 0.5977, 0.7916, 0.6126, 1.2213,
        0.6279, 0.5983, 1.2283], device='cuda:0')
Index force: tensor([0.7930, 1.3609, 0.5462, 0.5762, 0.5283, 0.6392, 0.9287, 0.5879, 0.5394,
        0.6171, 0.5579, 0.5579], device='cuda:0')
Storing NORMAL transition: reward=0.0023 (scaled=0.0023), steps=1
Reward stats updated: mean -0.0062 -> -0.0062, std: 0.1237
Collected 539 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3981, Q2 Loss=1.3981, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6169
SAC Update 2/5: Actor Loss=-0.0031, Q1 Loss=0.9392, Q2 Loss=0.9392, Entropy=0.2153, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7346
SAC Update 3/5: Actor Loss=-0.0353, Q1 Loss=0.9304, Q2 Loss=0.9304, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7089
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8737, Q2 Loss=0.8737, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5555
SAC Update 5/5: Actor Loss=-0.1648, Q1 Loss=1.3876, Q2 Loss=1.3876, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2003

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.9%)
Q1 update: 0.06s (21.8%)
Q2 update: 0.06s (20.1%)
Actor update: 0.10s (38.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.040638
Q1 loss: 1.105795
Q2 loss: 1.105795
Current threshold: -37.4770
Global Scale Offset: 0.0207
Reward stats: mean=-0.0062, std=0.1237, count=539
----------------------------------------------
SAC Update - Actor Loss: -0.0406, Q1 Loss: 1.1058, Q2 Loss: 1.1058, Entropy: 0.0431, Mean TD Error: 1.1632, Threshold: -37.4770
tensor([ 1.3580e-01,  6.9013e-01,  5.2526e-01,  4.1666e-01, -1.9262e-01,
         5.3285e-01,  8.7158e-01,  9.7109e-01,  1.5000e+00,  3.1232e-01,
        -2.1732e-02,  1.0764e+00,  3.8157e-02, -2.6817e-03, -1.9108e-01,
        -5.7506e+00], device='cuda:0')
Original likelihood: -30.300369262695312
Adjusted likelihood: -30.300369262695312
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.602628820983227
Current ori: tensor([ 0.0382, -0.0027, -0.1911], device='cuda:0')
Middle force: tensor([0.5028, 0.5007, 0.5238, 0.5902, 0.6115, 1.0403, 0.7673, 0.7554, 0.6294,
        0.5770, 0.6136], device='cuda:0')
Thumb force: tensor([1.8881, 1.3601, 0.6329, 1.0414, 0.7502, 1.4700, 0.6045, 0.7155, 0.6732,
        0.5691, 0.6117], device='cuda:0')
Index force: tensor([0.8542, 0.7850, 0.6374, 0.5552, 0.5521, 0.5767, 0.5281, 0.5360, 0.5693,
        0.6232, 0.6035], device='cuda:0')
Storing NORMAL transition: reward=-0.0495 (scaled=-0.0495), steps=1
Reward stats updated: mean -0.0062 -> -0.0063, std: 0.1236
Collected 540 transitions for RL
SAC Update 1/5: Actor Loss=-0.0331, Q1 Loss=1.3578, Q2 Loss=1.3578, Entropy=0.0786, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4979
SAC Update 2/5: Actor Loss=-0.1566, Q1 Loss=1.5098, Q2 Loss=1.5098, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4834
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0597, Q2 Loss=1.0597, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3765
SAC Update 4/5: Actor Loss=-0.0952, Q1 Loss=1.5316, Q2 Loss=1.5316, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0721
SAC Update 5/5: Actor Loss=-0.1427, Q1 Loss=1.1783, Q2 Loss=1.1783, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8917

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.3%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (19.7%)
Actor update: 0.10s (42.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.085499
Q1 loss: 1.327433
Q2 loss: 1.327433
Current threshold: -37.5004
Global Scale Offset: 0.0206
Reward stats: mean=-0.0063, std=0.1236, count=540
----------------------------------------------
SAC Update - Actor Loss: -0.0855, Q1 Loss: 1.3274, Q2 Loss: 1.3274, Entropy: 0.0157, Mean TD Error: 1.2643, Threshold: -37.5004
tensor([ 1.1642e-01,  7.6844e-01,  3.6135e-01,  4.6298e-01, -1.7900e-01,
         7.1075e-01,  7.4522e-01,  9.7819e-01,  1.5000e+00,  3.9322e-01,
        -1.0943e-02,  9.0080e-01,  2.6535e-02,  4.2322e-03, -1.4080e-01,
        -5.8945e+00], device='cuda:0')
Original likelihood: -31.855873107910156
Adjusted likelihood: -31.855873107910156
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.18582193099428
Current ori: tensor([ 0.0265,  0.0042, -0.1408], device='cuda:0')
Middle force: tensor([1.3764, 0.6898, 1.0679, 0.7286, 0.6043, 0.5721, 0.9047, 0.5693, 0.5300,
        0.5096], device='cuda:0')
Thumb force: tensor([0.6345, 0.6024, 0.6272, 0.5934, 0.8056, 0.6120, 1.1891, 0.6277, 0.5916,
        1.1909], device='cuda:0')
Index force: tensor([0.5391, 0.5616, 0.5221, 0.6193, 0.8949, 0.5772, 0.5332, 0.6071, 0.5522,
        0.5527], device='cuda:0')
Storing NORMAL transition: reward=-0.0215 (scaled=-0.0215), steps=1
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1235
Collected 541 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3562, Q2 Loss=1.3562, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7990
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2183, Q2 Loss=1.2183, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4150
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4729, Q2 Loss=1.4729, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2709
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.5208, Q2 Loss=1.5208, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5753
SAC Update 5/5: Actor Loss=-0.0322, Q1 Loss=0.9673, Q2 Loss=0.9673, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9764

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (15.7%)
Q1 update: 0.05s (20.5%)
Q2 update: 0.05s (19.4%)
Actor update: 0.10s (41.1%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.052482
Q1 loss: 1.307092
Q2 loss: 1.307092
Current threshold: -37.5142
Global Scale Offset: 0.0205
Reward stats: mean=-0.0063, std=0.1235, count=541
----------------------------------------------
SAC Update - Actor Loss: -0.0525, Q1 Loss: 1.3071, Q2 Loss: 1.3071, Entropy: 0.0000, Mean TD Error: 1.4073, Threshold: -37.5142
tensor([ 1.3165e-01,  6.5016e-01,  4.9859e-01,  5.7121e-01, -2.9657e-01,
         8.1417e-01,  8.3457e-01,  8.9314e-01,  1.4878e+00,  4.7928e-01,
         4.3552e-02,  9.9980e-01,  5.6234e-02,  4.3151e-03, -1.2138e-01,
        -5.8336e+00], device='cuda:0')
Original likelihood: -46.97323989868164
Adjusted likelihood: -46.97323989868164
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 45.872344970703125
Projection step: 1, Loss: 44.42851638793945
Projection step: 2, Loss: 44.511016845703125
Projection step: 3, Loss: 43.7796630859375
Projection step: 4, Loss: 43.237571716308594
Projection step: 5, Loss: 43.204185485839844
Projection step: 6, Loss: 42.8721809387207
Projection step: 7, Loss: 41.649322509765625
Projection step: 8, Loss: 40.45518493652344
Projection step: 9, Loss: 40.8519287109375
Projection step: 10, Loss: 40.36355972290039
Projection step: 11, Loss: 39.45751190185547
Projection step: 12, Loss: 38.22055435180664
Projection step: 13, Loss: 39.666725158691406
Projection step: 14, Loss: 37.98490524291992
Projection step: 15, Loss: 37.02826690673828
Projection step: 16, Loss: 37.147560119628906
Projection step: 17, Loss: 36.38610076904297
Projection step: 18, Loss: 35.951148986816406
Projection step: 19, Loss: 35.6375846862793
Projection step: 20, Loss: 35.923095703125
Projection step: 21, Loss: 34.786781311035156
Projection step: 22, Loss: 33.95441436767578
Projection step: 23, Loss: 33.880836486816406
Projection step: 24, Loss: 34.22882843017578
Final likelihood: tensor([-32.5128, -33.2023, -34.3404, -32.8034, -36.0456, -31.7737, -33.8311,
        -36.5278, -31.9857, -31.8381, -36.4084, -31.9461, -30.8893, -32.2920,
        -34.3366, -35.5122])
Final projection likelihood: -33.5153
1 mode projection succeeded
New goal: tensor([ 0.1454,  0.6527,  0.5533,  0.6038, -0.2172,  0.7848,  0.8172,  0.8659,
         1.5256,  0.4128,  0.0884,  1.1722,  0.0507,  0.0079,  1.9690],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0031]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -39.99886703491211
Adjusted likelihood: -39.99886703491211
Likelihood residual: 0.0
Original likelihood: -42.94293975830078
Adjusted likelihood: -42.94293975830078
Likelihood residual: 0.0
{'index': 42.94293975830078, 'thumb_middle': 39.99886703491211}
Current yaw: tensor([ 0.0562,  0.0043, -0.1214], device='cuda:0')
3 thumb_middle
tensor([ 1.3165e-01,  6.5016e-01,  4.9859e-01,  5.7121e-01, -2.9657e-01,
         8.1417e-01,  8.3457e-01,  8.9314e-01,  1.4878e+00,  4.7928e-01,
         4.3552e-02,  9.9980e-01,  5.6234e-02,  4.3151e-03, -1.2138e-01,
        -5.8336e+00], device='cuda:0')
Solve time for step 1 9.008438538992777
Current ori: tensor([ 0.0562,  0.0043, -0.1214], device='cuda:0')
Index force: tensor([0.5484, 0.5881, 0.5878, 0.5974], device='cuda:0')
tensor([ 0.1292,  0.6391,  0.5111,  0.5718, -0.3172,  0.7839,  0.7765,  0.8480,
         1.4247,  0.4164, -0.0469,  1.0695,  0.0585,  0.0062, -0.1213, -5.8304],
       device='cuda:0')
Solve time for step 2 3.5087845789967105
Current ori: tensor([ 0.0585,  0.0062, -0.1213], device='cuda:0')
Index force: tensor([0.5807, 0.5820, 0.5922], device='cuda:0')
tensor([ 0.1219,  0.6090,  0.5342,  0.5933, -0.3186,  0.7904,  0.7737,  0.8360,
         1.4317,  0.3799, -0.0517,  1.0804,  0.0665,  0.0123, -0.1213, -5.8193],
       device='cuda:0')
Solve time for step 3 3.448904924967792
Current ori: tensor([ 0.0665,  0.0123, -0.1213], device='cuda:0')
Index force: tensor([0.5475, 0.5544], device='cuda:0')
tensor([ 1.4182e-01,  5.9818e-01,  5.5598e-01,  6.2002e-01, -3.1069e-01,
         7.8097e-01,  7.8926e-01,  8.3867e-01,  1.4195e+00,  3.9303e-01,
        -5.1939e-02,  1.0872e+00,  7.0930e-02,  4.8383e-03, -1.2133e-01,
        -5.7655e+00], device='cuda:0')
Solve time for step 4 3.4008281609858386
Current ori: tensor([ 0.0709,  0.0048, -0.1213], device='cuda:0')
Index force: tensor([0.5419], device='cuda:0')
Storing RECOVERY transition: reward=-0.0032 (scaled=-0.0011), steps=3
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1234
Collected 542 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.8777, Q2 Loss=1.8777, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8015
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8998, Q2 Loss=0.8998, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2641
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.2564, Q2 Loss=1.2564, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2970
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.1583, Q2 Loss=1.1583, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6394
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4021, Q2 Loss=1.4021, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0905

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.6%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (18.8%)
Actor update: 0.11s (39.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.318840
Q2 loss: 1.318840
Current threshold: -37.5225
Global Scale Offset: 0.0205
Reward stats: mean=-0.0063, std=0.1234, count=542
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.3188, Q2 Loss: 1.3188, Entropy: 0.0000, Mean TD Error: 0.8185, Threshold: -37.5225
Original likelihood: -44.50685119628906
Adjusted likelihood: -44.50685119628906
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 44.54019546508789
Projection step: 1, Loss: 44.30824661254883
Projection step: 2, Loss: 43.24867248535156
Projection step: 3, Loss: 43.03266906738281
Projection step: 4, Loss: 41.86167907714844
Projection step: 5, Loss: 42.00029373168945
Projection step: 6, Loss: 40.69152069091797
Projection step: 7, Loss: 40.40376281738281
Projection step: 8, Loss: 40.09705352783203
Projection step: 9, Loss: 38.604713439941406
Projection step: 10, Loss: 38.366722106933594
Projection step: 11, Loss: 37.715545654296875
Projection step: 12, Loss: 38.017005920410156
Projection step: 13, Loss: 37.436283111572266
Projection step: 14, Loss: 37.62184143066406
Projection step: 15, Loss: 37.22629928588867
Projection step: 16, Loss: 36.20917892456055
Projection step: 17, Loss: 37.050907135009766
Projection step: 18, Loss: 36.008888244628906
Projection step: 19, Loss: 34.4975471496582
Projection step: 20, Loss: 35.075347900390625
Projection step: 21, Loss: 35.406517028808594
Projection step: 22, Loss: 33.71209716796875
Projection step: 23, Loss: 34.07599639892578
Projection step: 24, Loss: 34.2685546875
Final likelihood: tensor([-31.0188, -28.2165, -32.9625, -35.3495, -32.2669, -32.8855, -34.0427,
        -33.7586, -35.4861, -33.4647, -36.4508, -33.7951, -35.0223, -33.1313,
        -36.5663, -35.8337])
Final projection likelihood: -33.7657
1 mode projection succeeded
New goal: tensor([ 0.1392,  0.6180,  0.5697,  0.6544, -0.1938,  0.7676,  0.8054,  0.8377,
         1.5008,  0.3305,  0.0625,  1.2623,  0.0630,  0.0112,  0.2335],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -35.977264404296875
Adjusted likelihood: -35.977264404296875
Likelihood residual: 0.0
Original likelihood: -43.901817321777344
Adjusted likelihood: -43.901817321777344
Likelihood residual: 0.0
{'index': 43.901817321777344, 'thumb_middle': 35.977264404296875}
Current yaw: tensor([ 0.0688,  0.0081, -0.1195], device='cuda:0')
4 thumb_middle
tensor([ 1.2686e-01,  6.0680e-01,  5.3314e-01,  6.1130e-01, -2.6148e-01,
         8.1028e-01,  8.2600e-01,  8.4519e-01,  1.4851e+00,  3.9537e-01,
         2.9202e-04,  1.1278e+00,  6.8843e-02,  8.0571e-03, -1.1949e-01,
        -5.9143e+00], device='cuda:0')
Solve time for step 1 9.011803884990513
Current ori: tensor([ 0.0688,  0.0081, -0.1195], device='cuda:0')
Index force: tensor([0.5620, 0.5770, 0.5836, 0.5952], device='cuda:0')
tensor([ 0.1241,  0.6005,  0.5340,  0.6200, -0.2937,  0.7692,  0.7704,  0.8149,
         1.4122,  0.3063, -0.0589,  1.1744,  0.0706,  0.0109, -0.1194, -5.8739],
       device='cuda:0')
Solve time for step 2 3.6175316009903327
Current ori: tensor([ 0.0706,  0.0109, -0.1194], device='cuda:0')
Index force: tensor([0.5701, 0.5791, 0.5898], device='cuda:0')
tensor([ 0.1240,  0.5697,  0.5540,  0.6642, -0.2902,  0.7691,  0.7690,  0.8045,
         1.4173,  0.3046, -0.0724,  1.1911,  0.0808,  0.0145, -0.1194, -5.8327],
       device='cuda:0')
Solve time for step 3 3.583986431010999
Current ori: tensor([ 0.0808,  0.0145, -0.1194], device='cuda:0')
Index force: tensor([0.5734, 0.5847], device='cuda:0')
tensor([ 0.1293,  0.5801,  0.5481,  0.6583, -0.2923,  0.7681,  0.7794,  0.8144,
         1.4190,  0.2923, -0.0758,  1.1864,  0.0780,  0.0113, -0.1194, -5.8345],
       device='cuda:0')
Solve time for step 4 3.3656440500053577
Current ori: tensor([ 0.0780,  0.0113, -0.1194], device='cuda:0')
Index force: tensor([0.5760], device='cuda:0')
Storing RECOVERY transition: reward=-0.0034 (scaled=-0.0011), steps=3
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1233
Collected 543 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.8958, Q2 Loss=0.8958, Entropy=0.0005, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1970
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9755, Q2 Loss=0.9755, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1443
SAC Update 3/5: Actor Loss=-0.0845, Q1 Loss=0.9787, Q2 Loss=0.9787, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9227
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6558, Q2 Loss=0.6558, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0562
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2488, Q2 Loss=1.2488, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.0612

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.9%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (18.6%)
Actor update: 0.10s (39.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.016899
Q1 loss: 0.950914
Q2 loss: 0.950914
Current threshold: -37.5273
Global Scale Offset: 0.0205
Reward stats: mean=-0.0063, std=0.1233, count=543
----------------------------------------------
SAC Update - Actor Loss: -0.0169, Q1 Loss: 0.9509, Q2 Loss: 0.9509, Entropy: 0.0001, Mean TD Error: 1.8763, Threshold: -37.5273
Original likelihood: -43.95285415649414
Adjusted likelihood: -43.95285415649414
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 42.831180572509766
Projection step: 1, Loss: 43.34245300292969
Projection step: 2, Loss: 43.42856216430664
Projection step: 3, Loss: 42.418243408203125
Projection step: 4, Loss: 42.25707244873047
Projection step: 5, Loss: 40.57078552246094
Projection step: 6, Loss: 40.38092041015625
Projection step: 7, Loss: 40.316349029541016
Projection step: 8, Loss: 39.62837600708008
Projection step: 9, Loss: 39.17512512207031
Projection step: 10, Loss: 39.92339324951172
Projection step: 11, Loss: 39.01218795776367
Projection step: 12, Loss: 39.01885223388672
Projection step: 13, Loss: 37.345947265625
Projection step: 14, Loss: 38.3406867980957
Projection step: 15, Loss: 37.26591873168945
Projection step: 16, Loss: 36.82807159423828
Projection step: 17, Loss: 36.580169677734375
Projection step: 18, Loss: 36.93190383911133
Projection step: 19, Loss: 36.613433837890625
Projection step: 20, Loss: 37.118099212646484
Projection step: 21, Loss: 35.263771057128906
Projection step: 22, Loss: 35.64232635498047
Projection step: 23, Loss: 36.15850830078125
Projection step: 24, Loss: 34.82196807861328
Final likelihood: tensor([-34.8672, -34.5727, -34.8655, -35.7520, -35.4435, -34.6564, -36.0465,
        -36.0347, -36.0914, -36.6386, -36.0781, -32.4423, -30.6873, -33.7380,
        -37.2793, -34.6719])
Final projection likelihood: -34.9916
1 mode projection succeeded
New goal: tensor([ 0.0947,  0.5860,  0.5615,  0.6637, -0.1894,  0.7485,  0.7401,  0.8161,
         1.4616,  0.2605,  0.0521,  1.3319,  0.0760,  0.0236, -0.3471],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0029]], device='cuda:0')
Original likelihood: -38.534637451171875
Adjusted likelihood: -38.534637451171875
Likelihood residual: 0.0
Original likelihood: -45.23885726928711
Adjusted likelihood: -45.23885726928711
Likelihood residual: 0.0
{'index': 45.23885726928711, 'thumb_middle': 38.534637451171875}
Current yaw: tensor([ 0.0823,  0.0208, -0.1214], device='cuda:0')
5 thumb_middle
tensor([ 8.4174e-02,  5.6389e-01,  5.3280e-01,  6.4360e-01, -2.3973e-01,
         8.0402e-01,  7.9897e-01,  8.2775e-01,  1.4749e+00,  3.2000e-01,
        -1.1741e-03,  1.2294e+00,  8.2285e-02,  2.0778e-02, -1.2139e-01,
        -6.0345e+00], device='cuda:0')
Solve time for step 1 9.242692909028847
Current ori: tensor([ 0.0823,  0.0208, -0.1214], device='cuda:0')
Index force: tensor([0.5679, 0.5861, 0.5662, 0.5926], device='cuda:0')
tensor([ 0.0747,  0.5362,  0.5452,  0.6765, -0.2734,  0.7631,  0.7294,  0.7943,
         1.4014,  0.2324, -0.0424,  1.2609,  0.0896,  0.0409, -0.1214, -5.8941],
       device='cuda:0')
Solve time for step 2 3.622200875019189
Current ori: tensor([ 0.0896,  0.0409, -0.1214], device='cuda:0')
Index force: tensor([0.5786, 0.5600, 0.5853], device='cuda:0')
tensor([ 0.0883,  0.5466,  0.5430,  0.6783, -0.2776,  0.7688,  0.7176,  0.8021,
         1.4046,  0.2225, -0.0580,  1.2796,  0.0810,  0.0514, -0.1214, -5.7035],
       device='cuda:0')
Solve time for step 3 3.566753803053871
Current ori: tensor([ 0.0810,  0.0514, -0.1214], device='cuda:0')
Index force: tensor([0.5533, 0.5785], device='cuda:0')
tensor([ 0.1035,  0.5456,  0.5594,  0.6779, -0.2776,  0.7601,  0.7147,  0.7952,
         1.3990,  0.2505, -0.0551,  1.2828,  0.0764,  0.0527, -0.1214, -5.6054],
       device='cuda:0')
Solve time for step 4 3.3771346699795686
Current ori: tensor([ 0.0764,  0.0527, -0.1214], device='cuda:0')
Index force: tensor([0.5727], device='cuda:0')
Storing RECOVERY transition: reward=-0.0099 (scaled=-0.0033), steps=3
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1232
Collected 544 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9917, Q2 Loss=0.9917, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6967
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3084, Q2 Loss=1.3084, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5730
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0883, Q2 Loss=1.0883, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6608
SAC Update 4/5: Actor Loss=-0.0238, Q1 Loss=0.9768, Q2 Loss=0.9768, Entropy=0.0295, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3453
SAC Update 5/5: Actor Loss=-0.3912, Q1 Loss=6.0448, Q2 Loss=6.0448, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.8478

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (13.9%)
Q1 update: 0.05s (16.4%)
Q2 update: 0.09s (32.5%)
Actor update: 0.10s (34.3%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.083005
Q1 loss: 2.081988
Q2 loss: 2.081988
Current threshold: -37.5367
Global Scale Offset: 0.0204
Reward stats: mean=-0.0063, std=0.1232, count=544
----------------------------------------------
SAC Update - Actor Loss: -0.0830, Q1 Loss: 2.0820, Q2 Loss: 2.0820, Entropy: 0.0059, Mean TD Error: 1.4247, Threshold: -37.5367
Original likelihood: -36.31505584716797
Adjusted likelihood: -36.31505584716797
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0811,  0.0355, -0.1153], device='cuda:0')
6 turn
Sampling time 3.593203980999533
tensor([ 1.0138e-01,  5.5533e-01,  5.5208e-01,  6.6201e-01, -2.1482e-01,
         7.8629e-01,  7.4765e-01,  8.1243e-01,  1.4593e+00,  2.5517e-01,
         1.1771e-03,  1.3238e+00,  8.1065e-02,  3.5452e-02, -1.1529e-01,
        -5.7943e+00], device='cuda:0')
Original likelihood: -38.41754913330078
Adjusted likelihood: -38.41754913330078
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 38.05021667480469
Projection step: 1, Loss: 37.68609619140625
Projection step: 2, Loss: 36.50287628173828
Projection step: 3, Loss: 37.49966049194336
Projection step: 4, Loss: 36.49913787841797
Projection step: 5, Loss: 35.74488067626953
Projection step: 6, Loss: 35.74945831298828
Projection step: 7, Loss: 36.73943328857422
Projection step: 8, Loss: 37.34530258178711
Projection step: 9, Loss: 36.48835372924805
Projection step: 10, Loss: 35.19904327392578
Projection step: 11, Loss: 36.30649185180664
Projection step: 12, Loss: 34.839561462402344
Projection step: 13, Loss: 35.27448654174805
Projection step: 14, Loss: 34.72957992553711
Projection step: 15, Loss: 34.023658752441406
Projection step: 16, Loss: 35.696407318115234
Projection step: 17, Loss: 33.87627410888672
Projection step: 18, Loss: 33.503150939941406
Projection step: 19, Loss: 35.2946891784668
Projection step: 20, Loss: 34.35337829589844
Projection step: 21, Loss: 33.558799743652344
Projection step: 22, Loss: 35.33391571044922
Projection step: 23, Loss: 33.12068176269531
Projection step: 24, Loss: 34.86482620239258
Final likelihood: tensor([-33.2175, -33.4061, -33.4919, -33.2178, -29.9181, -34.4794, -30.3172,
        -30.3170, -35.1491, -29.0389, -34.5273, -33.4436, -35.5948, -35.5130,
        -34.4191, -28.8295])
Final projection likelihood: -32.8050
1 mode projection succeeded
New goal: tensor([ 0.0919,  0.5793,  0.5654,  0.6518, -0.1848,  0.7381,  0.7152,  0.8055,
         1.4422,  0.2275,  0.0547,  1.3687,  0.0758,  0.0360, -0.0693],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -30.68152618408203
Adjusted likelihood: -30.68152618408203
Likelihood residual: 0.0
Original likelihood: -38.615516662597656
Adjusted likelihood: -38.615516662597656
Likelihood residual: 0.0
{'index': 38.615516662597656, 'thumb_middle': 30.68152618408203}
Current yaw: tensor([ 0.0811,  0.0355, -0.1153], device='cuda:0')
7 thumb_middle
tensor([ 1.0138e-01,  5.5533e-01,  5.5208e-01,  6.6201e-01, -2.1482e-01,
         7.8629e-01,  7.4765e-01,  8.1243e-01,  1.4593e+00,  2.5517e-01,
         1.1771e-03,  1.3238e+00,  8.1065e-02,  3.5452e-02, -1.1529e-01,
        -5.7943e+00], device='cuda:0')
Solve time for step 1 9.140934374008793
Current ori: tensor([ 0.0811,  0.0355, -0.1153], device='cuda:0')
Index force: tensor([0.5980, 0.5003, 0.5997, 0.6074], device='cuda:0')
tensor([ 0.1237,  0.5236,  0.5855,  0.7251, -0.2495,  0.7514,  0.6991,  0.7883,
         1.3830,  0.2062, -0.0427,  1.3187,  0.0907,  0.0355, -0.1153, -5.6811],
       device='cuda:0')
Solve time for step 2 3.903927206003573
Current ori: tensor([ 0.0907,  0.0355, -0.1153], device='cuda:0')
Index force: tensor([0.5002, 0.5916, 0.6018], device='cuda:0')
tensor([ 0.1157,  0.5293,  0.5705,  0.7227, -0.2560,  0.7512,  0.6883,  0.7838,
         1.3930,  0.2012, -0.0413,  1.3229,  0.0907,  0.0352, -0.1153, -5.7351],
       device='cuda:0')
Solve time for step 3 3.4198459920007735
Current ori: tensor([ 0.0907,  0.0352, -0.1153], device='cuda:0')
Index force: tensor([0.5834, 0.5959], device='cuda:0')
tensor([ 0.0899,  0.5422,  0.5624,  0.6555, -0.2597,  0.7507,  0.6840,  0.7841,
         1.3930,  0.1985, -0.0402,  1.3273,  0.0825,  0.0443, -0.1153, -5.7751],
       device='cuda:0')
Solve time for step 4 3.407574531971477
Current ori: tensor([ 0.0825,  0.0443, -0.1153], device='cuda:0')
Index force: tensor([0.5771], device='cuda:0')
Storing RECOVERY transition: reward=-0.0031 (scaled=-0.0031), steps=0
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1231
Collected 545 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9823, Q2 Loss=0.9823, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3826
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3812, Q2 Loss=1.3812, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0668
SAC Update 3/5: Actor Loss=-0.1177, Q1 Loss=1.0578, Q2 Loss=1.0578, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7697
SAC Update 4/5: Actor Loss=-0.0097, Q1 Loss=0.9916, Q2 Loss=0.9916, Entropy=0.0393, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4864
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8755, Q2 Loss=0.8755, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3677

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.9%)
Target Q: 0.04s (17.7%)
Q1 update: 0.04s (19.7%)
Q2 update: 0.04s (18.9%)
Actor update: 0.09s (39.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.025475
Q1 loss: 1.057669
Q2 loss: 1.057669
Current threshold: -37.5527
Global Scale Offset: 0.0203
Reward stats: mean=-0.0063, std=0.1231, count=545
----------------------------------------------
SAC Update - Actor Loss: -0.0255, Q1 Loss: 1.0577, Q2 Loss: 1.0577, Entropy: 0.0079, Mean TD Error: 0.8146, Threshold: -37.5527
Original likelihood: -37.76529312133789
Adjusted likelihood: -37.76529312133789
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0215)
State is out of distribution
Projection step: 0, Loss: 38.524085998535156
Projection step: 1, Loss: 38.435829162597656
Projection step: 2, Loss: 38.004722595214844
Projection step: 3, Loss: 38.80138397216797
Projection step: 4, Loss: 37.83234786987305
Projection step: 5, Loss: 35.614418029785156
Projection step: 6, Loss: 37.574581146240234
Projection step: 7, Loss: 36.41252136230469
Projection step: 8, Loss: 35.304508209228516
Projection step: 9, Loss: 36.943485260009766
Projection step: 10, Loss: 36.06561279296875
Projection step: 11, Loss: 35.602664947509766
Projection step: 12, Loss: 34.6713981628418
Projection step: 13, Loss: 35.78839111328125
Projection step: 14, Loss: 35.059234619140625
Projection step: 15, Loss: 34.471710205078125
Projection step: 16, Loss: 34.15374755859375
Projection step: 17, Loss: 35.15152359008789
Projection step: 18, Loss: 34.90860366821289
Projection step: 19, Loss: 35.88174819946289
Projection step: 20, Loss: 33.87660217285156
Projection step: 21, Loss: 34.747779846191406
Projection step: 22, Loss: 33.81841278076172
Projection step: 23, Loss: 33.95444107055664
Projection step: 24, Loss: 34.828208923339844
Final likelihood: tensor([-34.2974, -36.9742, -35.1222, -33.7717, -34.2937, -35.0840, -32.5190,
        -40.4372, -37.3114, -34.7306, -29.4176, -34.4272, -34.9085, -34.5810,
        -33.9808, -36.0360])
Final projection likelihood: -34.8683
1 mode projection succeeded
New goal: tensor([ 0.0714,  0.5774,  0.5465,  0.6590, -0.1763,  0.7300,  0.6884,  0.8093,
         1.4348,  0.2001,  0.0537,  1.3980,  0.0802,  0.0376, -0.0250],
       device='cuda:0')
tensor([[0.0028]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0027]], device='cuda:0')
Original likelihood: -34.437862396240234
Adjusted likelihood: -34.437862396240234
Likelihood residual: 0.0
Original likelihood: -36.7138671875
Adjusted likelihood: -36.7138671875
Likelihood residual: 0.0
{'index': 36.7138671875, 'thumb_middle': 34.437862396240234}
Current yaw: tensor([ 0.0851,  0.0372, -0.1129], device='cuda:0')
8 thumb_middle
tensor([ 0.0776,  0.5461,  0.5480,  0.6498, -0.1983,  0.7821,  0.7299,  0.8089,
         1.4519,  0.2241,  0.0093,  1.3598,  0.0851,  0.0372, -0.1129, -5.9166],
       device='cuda:0')
Solve time for step 1 8.92758929298725
Current ori: tensor([ 0.0851,  0.0372, -0.1129], device='cuda:0')
Index force: tensor([0.5845, 0.5825, 0.5963, 0.5726], device='cuda:0')
tensor([ 0.0738,  0.5440,  0.5425,  0.6583, -0.2482,  0.7422,  0.6703,  0.7894,
         1.3847,  0.1753, -0.0332,  1.3551,  0.0817,  0.0551, -0.1129, -5.7420],
       device='cuda:0')
Solve time for step 2 3.680325382971205
Current ori: tensor([ 0.0817,  0.0551, -0.1129], device='cuda:0')
Index force: tensor([0.5724, 0.5874, 0.5661], device='cuda:0')
tensor([ 0.0851,  0.5575,  0.5382,  0.6524, -0.2511,  0.7456,  0.6638,  0.7886,
         1.3829,  0.1703, -0.0361,  1.3566,  0.0703,  0.0641, -0.1129, -5.5808],
       device='cuda:0')
Solve time for step 3 3.579093553998973
Current ori: tensor([ 0.0703,  0.0641, -0.1129], device='cuda:0')
Index force: tensor([0.5702, 0.5686], device='cuda:0')
tensor([ 0.1000,  0.5534,  0.5499,  0.6685, -0.2538,  0.7372,  0.6746,  0.7807,
         1.3875,  0.1568, -0.0344,  1.3687,  0.0677,  0.0641, -0.1129, -5.4527],
       device='cuda:0')
Solve time for step 4 3.497636733984109
Current ori: tensor([ 0.0677,  0.0641, -0.1129], device='cuda:0')
Index force: tensor([0.5454], device='cuda:0')
Storing RECOVERY transition: reward=-0.0148 (scaled=-0.0148), steps=0
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1230
Collected 546 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2692, Q2 Loss=1.2692, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2212
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0005, Q2 Loss=1.0005, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9307
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1010, Q2 Loss=1.1010, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0501
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9072, Q2 Loss=0.9072, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6076
SAC Update 5/5: Actor Loss=-0.4353, Q1 Loss=4.0198, Q2 Loss=4.0198, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.4474

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.0%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (18.6%)
Actor update: 0.11s (39.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.087063
Q1 loss: 1.659549
Q2 loss: 1.659549
Current threshold: -37.5653
Global Scale Offset: 0.0203
Reward stats: mean=-0.0063, std=0.1230, count=546
----------------------------------------------
SAC Update - Actor Loss: -0.0871, Q1 Loss: 1.6595, Q2 Loss: 1.6595, Entropy: 0.0000, Mean TD Error: 1.2514, Threshold: -37.5653
Original likelihood: -34.203155517578125
Adjusted likelihood: -34.203155517578125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0750,  0.0558, -0.1012], device='cuda:0')
9 turn
Sampling time 3.7309788590064272
tensor([ 0.0878,  0.5537,  0.5444,  0.6557, -0.2086,  0.7598,  0.6987,  0.8056,
         1.4583,  0.1937,  0.0245,  1.3970,  0.0750,  0.0558, -0.1012, -5.6603],
       device='cuda:0')
Original likelihood: -37.80284118652344
Adjusted likelihood: -37.80284118652344
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0117)
State is out of distribution
Projection step: 0, Loss: 33.48761749267578
Projection step: 1, Loss: 33.832000732421875
Projection step: 2, Loss: 35.41809844970703
Projection step: 3, Loss: 35.082984924316406
Projection step: 4, Loss: 34.822784423828125
Projection step: 5, Loss: 35.5970344543457
Projection step: 6, Loss: 33.77229309082031
Projection step: 7, Loss: 33.27692413330078
Projection step: 8, Loss: 34.083106994628906
Projection step: 9, Loss: 35.45123291015625
Projection step: 10, Loss: 34.63508224487305
Projection step: 11, Loss: 34.835227966308594
Projection step: 12, Loss: 34.78775405883789
Projection step: 13, Loss: 34.033390045166016
Projection step: 14, Loss: 34.44496154785156
Projection step: 15, Loss: 33.78466033935547
Projection step: 16, Loss: 33.201438903808594
Projection step: 17, Loss: 34.724918365478516
Projection step: 18, Loss: 34.56095886230469
Projection step: 19, Loss: 35.468482971191406
Projection step: 20, Loss: 32.492374420166016
Projection step: 21, Loss: 31.318321228027344
Projection step: 22, Loss: 33.68169403076172
Projection step: 23, Loss: 32.476619720458984
Projection step: 24, Loss: 34.11631393432617
Final likelihood: tensor([-34.1484, -39.9625, -30.7586, -28.6388, -29.7346, -30.1858, -32.5763,
        -34.3405, -32.8567, -30.3557, -37.0494, -34.0730, -28.7952, -43.2250,
        -36.9102, -30.1018])
Final projection likelihood: -33.3570
1 mode projection succeeded
New goal: tensor([ 0.0699,  0.5734,  0.5038,  0.6913, -0.1869,  0.7255,  0.6818,  0.7923,
         1.4530,  0.1993,  0.0611,  1.4256,  0.0723,  0.0551,  0.2152],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[0.0030]], device='cuda:0')
Original likelihood: -34.415260314941406
Adjusted likelihood: -34.415260314941406
Likelihood residual: 0.0
Original likelihood: -35.961849212646484
Adjusted likelihood: -35.961849212646484
Likelihood residual: 0.0
{'index': 35.961849212646484, 'thumb_middle': 34.415260314941406}
Current yaw: tensor([ 0.0750,  0.0558, -0.1012], device='cuda:0')
10 thumb_middle
tensor([ 0.0878,  0.5537,  0.5444,  0.6557, -0.2086,  0.7598,  0.6987,  0.8056,
         1.4583,  0.1937,  0.0245,  1.3970,  0.0750,  0.0558, -0.1012, -5.6603],
       device='cuda:0')
Solve time for step 1 8.936253692954779
Current ori: tensor([ 0.0750,  0.0558, -0.1012], device='cuda:0')
Index force: tensor([0.5854, 0.6016, 0.6041, 0.5915], device='cuda:0')
tensor([ 0.0826,  0.5593,  0.5136,  0.6897, -0.2597,  0.7352,  0.6584,  0.7756,
         1.3925,  0.1645, -0.0277,  1.3850,  0.0710,  0.0696, -0.1012, -5.5599],
       device='cuda:0')
Solve time for step 2 3.8192938880529255
Current ori: tensor([ 0.0710,  0.0696, -0.1012], device='cuda:0')
Index force: tensor([0.5902, 0.5930, 0.5822], device='cuda:0')
tensor([ 0.0936,  0.5684,  0.5075,  0.6986, -0.2604,  0.7388,  0.6575,  0.7742,
         1.3890,  0.1609, -0.0335,  1.3860,  0.0644,  0.0710, -0.1012, -5.4423],
       device='cuda:0')
Solve time for step 3 3.5263600400066935
Current ori: tensor([ 0.0644,  0.0710, -0.1012], device='cuda:0')
Index force: tensor([0.5480, 0.5669], device='cuda:0')
tensor([ 0.0945,  0.5617,  0.5202,  0.6934, -0.2658,  0.7386,  0.6622,  0.7768,
         1.3915,  0.1775, -0.0366,  1.3866,  0.0624,  0.0744, -0.1012, -5.3873],
       device='cuda:0')
Solve time for step 4 3.240382061980199
Current ori: tensor([ 0.0624,  0.0744, -0.1012], device='cuda:0')
Index force: tensor([0.5510], device='cuda:0')
Storing RECOVERY transition: reward=-0.0209 (scaled=-0.0209), steps=0
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1228
Collected 547 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6739, Q2 Loss=0.6739, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1706
SAC Update 2/5: Actor Loss=-0.1733, Q1 Loss=1.4079, Q2 Loss=1.4079, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1800
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8165, Q2 Loss=0.8165, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6242
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.0701, Q2 Loss=2.0701, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8659
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1568, Q2 Loss=1.1568, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8183

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (19.3%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.04s (18.0%)
Actor update: 0.10s (39.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.034657
Q1 loss: 1.225056
Q2 loss: 1.225056
Current threshold: -37.5727
Global Scale Offset: 0.0203
Reward stats: mean=-0.0063, std=0.1228, count=547
----------------------------------------------
SAC Update - Actor Loss: -0.0347, Q1 Loss: 1.2251, Q2 Loss: 1.2251, Entropy: 0.0000, Mean TD Error: 0.9318, Threshold: -37.5727
Original likelihood: -35.669437408447266
Adjusted likelihood: -35.669437408447266
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 0.0732,  0.0721, -0.0816], device='cuda:0')
11 turn
Sampling time 3.699909065035172
tensor([ 0.0729,  0.5616,  0.4969,  0.6990, -0.2281,  0.7582,  0.6796,  0.7924,
         1.4743,  0.1846,  0.0207,  1.4206,  0.0732,  0.0721, -0.0816, -5.6070],
       device='cuda:0')
Original likelihood: -36.64451599121094
Adjusted likelihood: -36.64451599121094
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.589349818008486
Current ori: tensor([ 0.0732,  0.0721, -0.0816], device='cuda:0')
Middle force: tensor([0.6933, 0.5489, 0.5244, 0.5364, 0.7872, 0.6586, 0.5668, 0.5283, 1.0869,
        0.5769, 1.0646, 0.5564], device='cuda:0')
Thumb force: tensor([1.4082, 0.5859, 0.5067, 0.5748, 1.0928, 0.6569, 0.5842, 1.1740, 0.6633,
        0.5872, 0.8383, 0.5595], device='cuda:0')
Index force: tensor([0.5118, 0.6079, 0.7653, 0.6799, 0.5049, 0.6548, 0.5827, 0.5303, 0.5657,
        0.5881, 0.5167, 0.5077], device='cuda:0')
Storing NORMAL transition: reward=0.1270 (scaled=0.1270), steps=1
Reward stats updated: mean -0.0063 -> -0.0061, std: 0.1229
Collected 548 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.7627, Q2 Loss=0.7627, Entropy=0.0368, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3946
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.6379, Q2 Loss=0.6379, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0801
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6574, Q2 Loss=0.6574, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1077
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7577, Q2 Loss=0.7577, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7176
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1762, Q2 Loss=1.1762, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4457

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.2%)
Q1 update: 0.06s (20.2%)
Q2 update: 0.05s (19.9%)
Actor update: 0.11s (41.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092107
Q1 loss: 0.798358
Q2 loss: 0.798358
Current threshold: -37.5772
Global Scale Offset: 0.0202
Reward stats: mean=-0.0061, std=0.1229, count=548
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 0.7984, Q2 Loss: 0.7984, Entropy: 0.0074, Mean TD Error: 0.3491, Threshold: -37.5772
tensor([ 0.0152,  0.6639,  0.3302,  0.6311, -0.2542,  0.6972,  0.6954,  0.9141,
         1.4375,  0.2875,  0.0818,  1.3700,  0.0888,  0.0852, -0.2178,  6.1108],
       device='cuda:0')
Original likelihood: -33.80438995361328
Adjusted likelihood: -33.80438995361328
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.557184498989955
Current ori: tensor([ 0.0888,  0.0852, -0.2178], device='cuda:0')
Middle force: tensor([0.5483, 0.5224, 0.5348, 0.7891, 0.6532, 0.5628, 0.5265, 1.0658, 0.5733,
        1.0470, 0.5535], device='cuda:0')
Thumb force: tensor([0.5876, 0.5064, 0.5729, 1.0727, 0.6498, 0.5766, 1.1478, 0.6528, 0.5827,
        0.8205, 0.5535], device='cuda:0')
Index force: tensor([0.6227, 0.7545, 0.6739, 0.5047, 0.6541, 0.5798, 0.5275, 0.5619, 0.5819,
        0.5146, 0.5066], device='cuda:0')
Storing NORMAL transition: reward=0.0587 (scaled=0.0587), steps=1
Reward stats updated: mean -0.0061 -> -0.0060, std: 0.1228
Collected 549 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5125, Q2 Loss=1.5125, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.1215
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3514, Q2 Loss=1.3514, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0678
SAC Update 3/5: Actor Loss=-0.1043, Q1 Loss=1.9064, Q2 Loss=1.9064, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4961
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.9146, Q2 Loss=1.9146, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.1751
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9425, Q2 Loss=0.9425, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8530

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.5%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.06s (20.0%)
Actor update: 0.12s (43.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.020852
Q1 loss: 1.525484
Q2 loss: 1.525484
Current threshold: -37.5798
Global Scale Offset: 0.0202
Reward stats: mean=-0.0060, std=0.1228, count=549
----------------------------------------------
SAC Update - Actor Loss: -0.0209, Q1 Loss: 1.5255, Q2 Loss: 1.5255, Entropy: 0.0000, Mean TD Error: 3.7427, Threshold: -37.5798
tensor([ 0.0700,  0.6526,  0.3071,  0.8325, -0.2277,  0.6484,  0.7961,  0.9873,
         1.3237,  0.4502,  0.0572,  1.4476,  0.1065,  0.0510, -0.2781,  6.0641],
       device='cuda:0')
Original likelihood: -37.6282958984375
Adjusted likelihood: -37.6282958984375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.3216)
State is out of distribution
Projection step: 0, Loss: 37.720733642578125
Projection step: 1, Loss: 37.724578857421875
Projection step: 2, Loss: 38.25965118408203
Projection step: 3, Loss: 35.983116149902344
Projection step: 4, Loss: 36.1657829284668
Projection step: 5, Loss: 35.519775390625
Projection step: 6, Loss: 34.583221435546875
Projection step: 7, Loss: 35.497467041015625
Projection step: 8, Loss: 35.75629425048828
Projection step: 9, Loss: 34.954994201660156
Projection step: 10, Loss: 34.375579833984375
Projection step: 11, Loss: 34.70454788208008
Projection step: 12, Loss: 33.77510070800781
Projection step: 13, Loss: 33.73228454589844
Projection step: 14, Loss: 33.767189025878906
Projection step: 15, Loss: 33.507041931152344
Projection step: 16, Loss: 32.84185791015625
Projection step: 17, Loss: 33.09532165527344
Projection step: 18, Loss: 33.243141174316406
Projection step: 19, Loss: 32.51044845581055
Projection step: 20, Loss: 32.68634033203125
Projection step: 21, Loss: 32.24980545043945
Projection step: 22, Loss: 32.224788665771484
Projection step: 23, Loss: 32.149169921875
Projection step: 24, Loss: 31.648162841796875
Final likelihood: tensor([-32.0305, -22.4591, -32.3491, -31.4621, -30.8475, -31.2647, -30.8813,
        -32.6261, -31.9308, -31.7350, -32.1549, -31.2488, -32.3783, -29.1984,
        -31.4859, -31.5263])
Final projection likelihood: -30.9737
1 mode projection succeeded
New goal: tensor([ 0.0357,  0.5970,  0.2272,  0.8493, -0.1900,  0.6455,  0.7541,  1.0032,
         1.3886,  0.4111,  0.0814,  1.4095,  0.1038,  0.0529, -0.5123],
       device='cuda:0')
tensor([[0.0042]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0024]], device='cuda:0')
Original likelihood: -34.60474395751953
Adjusted likelihood: -34.60474395751953
Likelihood residual: 0.0
Original likelihood: -29.804853439331055
Adjusted likelihood: -29.804853439331055
Likelihood residual: 0.0
{'index': 29.804853439331055, 'thumb_middle': 34.60474395751953}
Current yaw: tensor([ 0.1065,  0.0510, -0.2781], device='cuda:0')
12 index
tensor([ 0.0700,  0.6526,  0.3071,  0.8325, -0.2277,  0.6484,  0.7961,  0.9873,
         1.3237,  0.4502,  0.0572,  1.4476,  0.1065,  0.0510, -0.2781,  6.0641],
       device='cuda:0')
Solve time for step 1 10.488574370043352
Current ori: tensor([ 0.1065,  0.0510, -0.2781], device='cuda:0')
Middle force: tensor([0.5683, 0.5091, 0.5338, 0.5109], device='cuda:0')
Thumb force: tensor([0.5949, 0.5125, 0.6864, 0.6005], device='cuda:0')
tensor([ 0.0131,  0.5149,  0.2245,  0.8550, -0.2422,  0.6987,  0.7924,  1.0089,
         1.3847,  0.4254,  0.0554,  1.4239,  0.2023,  0.0758, -0.2541, -5.3603],
       device='cuda:0')
Solve time for step 2 4.08334355003899
Current ori: tensor([ 0.2023,  0.0758, -0.2541], device='cuda:0')
Middle force: tensor([0.5068, 0.5264, 0.5065], device='cuda:0')
Thumb force: tensor([0.5118, 0.6983, 0.6162], device='cuda:0')
tensor([-4.9678e-03,  4.1030e-01,  2.1860e-01,  8.6871e-01, -1.6064e-01,
         7.7835e-01,  7.6482e-01,  9.6989e-01,  1.4476e+00,  4.3092e-01,
         7.1082e-02,  1.3995e+00,  2.6142e-01,  4.5143e-02, -2.0601e-01,
        -5.8891e+00], device='cuda:0')
Solve time for step 3 4.06519377999939
Current ori: tensor([ 0.2614,  0.0451, -0.2060], device='cuda:0')
Middle force: tensor([0.5146, 0.5495], device='cuda:0')
Thumb force: tensor([0.5333, 0.6209], device='cuda:0')
tensor([ 0.0175,  0.3599,  0.2342,  0.8546, -0.0970,  0.8066,  0.8365,  1.0731,
         1.4757,  0.4448,  0.0736,  1.3830,  0.2629,  0.0187, -0.2246, -5.9573],
       device='cuda:0')
Solve time for step 4 3.9296535839675926
Current ori: tensor([ 0.2629,  0.0187, -0.2246], device='cuda:0')
Middle force: tensor([0.5458], device='cuda:0')
Thumb force: tensor([0.6102], device='cuda:0')
Storing RECOVERY transition: reward=-0.0459 (scaled=-0.0230), steps=2
Reward stats updated: mean -0.0060 -> -0.0060, std: 0.1227
Collected 550 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1613, Q2 Loss=1.1613, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6255
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7623, Q2 Loss=0.7623, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6097
SAC Update 3/5: Actor Loss=-0.1347, Q1 Loss=1.0604, Q2 Loss=1.0604, Entropy=0.0000, Time=0.10sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5431
SAC Update 4/5: Actor Loss=-0.2698, Q1 Loss=2.2207, Q2 Loss=2.2207, Entropy=0.0181, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2462
SAC Update 5/5: Actor Loss=-0.3380, Q1 Loss=2.1264, Q2 Loss=2.1264, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7207

------ SAC Update Summary (5 iterations) ------
Total time: 0.31s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (15.7%)
Q1 update: 0.05s (16.7%)
Q2 update: 0.05s (15.5%)
Actor update: 0.16s (49.3%)
Target update: 0.00s (1.1%)
Priority update: 0.00s (0.1%)
Actor loss: -0.148519
Q1 loss: 1.466232
Q2 loss: 1.466232
Current threshold: -37.5918
Global Scale Offset: 0.0202
Reward stats: mean=-0.0060, std=0.1227, count=550
----------------------------------------------
SAC Update - Actor Loss: -0.1485, Q1 Loss: 1.4662, Q2 Loss: 1.4662, Entropy: 0.0036, Mean TD Error: 1.3490, Threshold: -37.5918
Original likelihood: -60.186859130859375
Adjusted likelihood: -60.186859130859375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 60.071258544921875
Projection step: 1, Loss: 58.131046295166016
Projection step: 2, Loss: 55.98490524291992
Projection step: 3, Loss: 58.2253532409668
Projection step: 4, Loss: 55.945587158203125
Projection step: 5, Loss: 57.09196472167969
Projection step: 6, Loss: 58.0384407043457
Projection step: 7, Loss: 57.46272277832031
Projection step: 8, Loss: 57.15252685546875
Projection step: 9, Loss: 54.32950973510742
Projection step: 10, Loss: 54.11359786987305
Projection step: 11, Loss: 54.80255126953125
Projection step: 12, Loss: 56.53414535522461
Projection step: 13, Loss: 55.17881774902344
Projection step: 14, Loss: 52.76692199707031
Projection step: 15, Loss: 54.9755744934082
Projection step: 16, Loss: 55.49458312988281
Projection step: 17, Loss: 54.02009582519531
Projection step: 18, Loss: 54.55401611328125
Projection step: 19, Loss: 52.20622253417969
Projection step: 20, Loss: 52.26542282104492
Projection step: 21, Loss: 52.48131561279297
Projection step: 22, Loss: 52.442317962646484
Projection step: 23, Loss: 51.813819885253906
Projection step: 24, Loss: 52.386688232421875
Final likelihood: tensor([-43.1588, -44.0245, -45.1398, -53.8086, -55.0256, -49.6207, -44.4801,
        -47.5805, -55.0204, -50.6618, -46.2531, -49.4785, -51.7053, -50.6627,
        -47.8569, -55.5231])
Final projection likelihood: -49.3750
1 mode projection failed, trying anyway
New goal: tensor([ 0.2024,  0.4994,  0.3106,  0.8697, -0.1041,  0.7410,  0.7766,  1.0937,
         1.4834,  0.3982,  0.1058,  1.2686,  0.2293,  0.0094, -0.1473],
       device='cuda:0')
tensor([[0.0026]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[0.0015]], device='cuda:0')
Original likelihood: -55.787384033203125
Adjusted likelihood: -55.787384033203125
Likelihood residual: 0.0
Original likelihood: -50.654563903808594
Adjusted likelihood: -50.654563903808594
Likelihood residual: 0.0
{'index': 50.654563903808594, 'thumb_middle': 55.787384033203125}
Current yaw: tensor([ 0.2338,  0.0071, -0.2885], device='cuda:0')
13 index
tensor([ 0.2367,  0.5071,  0.2067,  0.8266, -0.1123,  0.7850,  0.8208,  1.0650,
         1.4894,  0.4239,  0.0498,  1.3257,  0.2338,  0.0071, -0.2885, -6.1666],
       device='cuda:0')
Solve time for step 1 10.607259429001715
Current ori: tensor([ 0.2338,  0.0071, -0.2885], device='cuda:0')
Middle force: tensor([0.5215, 0.5218, 0.5084, 0.5361], device='cuda:0')
Thumb force: tensor([0.6456, 0.6814, 0.5098, 0.5416], device='cuda:0')
tensor([ 0.1498,  0.2464,  0.3309,  0.8750, -0.1242,  0.8188,  0.8178,  1.0871,
         1.5000,  0.4239,  0.0861,  1.2802,  0.2757,  0.0287, -0.2807,  6.2447],
       device='cuda:0')
Solve time for step 2 4.288372342009097
Current ori: tensor([ 0.2757,  0.0287, -0.2807], device='cuda:0')
Middle force: tensor([0.5218, 0.5057, 0.5308], device='cuda:0')
Thumb force: tensor([0.6743, 0.5095, 0.5376], device='cuda:0')
tensor([ 0.1492,  0.2567,  0.3498,  0.8814, -0.1178,  0.8280,  0.8020,  1.1389,
         1.5000,  0.4391,  0.0755,  1.2816,  0.2722,  0.0232, -0.2970, -6.2158],
       device='cuda:0')
Solve time for step 3 4.122152563999407
Current ori: tensor([ 0.2722,  0.0232, -0.2970], device='cuda:0')
Middle force: tensor([0.5091, 0.5168], device='cuda:0')
Thumb force: tensor([0.5866, 0.5428], device='cuda:0')
tensor([ 0.1522,  0.2536,  0.3487,  0.8809, -0.0377,  0.9066,  0.7690,  1.0554,
         1.5000,  0.4498,  0.0737,  1.2770,  0.2780, -0.0141, -0.2352, -5.9316],
       device='cuda:0')
Solve time for step 4 3.9975676970207132
Current ori: tensor([ 0.2780, -0.0141, -0.2352], device='cuda:0')
Middle force: tensor([0.5112], device='cuda:0')
Thumb force: tensor([0.5338], device='cuda:0')
Storing RECOVERY transition: reward=-0.1134 (scaled=-0.0567), steps=2
Reward stats updated: mean -0.0060 -> -0.0061, std: 0.1226
Collected 551 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2313, Q2 Loss=1.2313, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7185
SAC Update 2/5: Actor Loss=-0.1416, Q1 Loss=1.0511, Q2 Loss=1.0511, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3657
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.6557, Q2 Loss=0.6557, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0740
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6909, Q2 Loss=0.6909, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0875
SAC Update 5/5: Actor Loss=-0.2305, Q1 Loss=0.6697, Q2 Loss=0.6697, Entropy=0.3405, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1729

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.0%)
Q1 update: 0.04s (19.6%)
Q2 update: 0.04s (18.1%)
Actor update: 0.09s (40.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.120478
Q1 loss: 0.859751
Q2 loss: 0.859751
Current threshold: -37.6111
Global Scale Offset: 0.0201
Reward stats: mean=-0.0061, std=0.1226, count=551
----------------------------------------------
SAC Update - Actor Loss: -0.1205, Q1 Loss: 0.8598, Q2 Loss: 0.8598, Entropy: 0.0681, Mean TD Error: 0.4837, Threshold: -37.6111
Original likelihood: -57.892852783203125
Adjusted likelihood: -57.892852783203125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 58.474884033203125
Projection step: 1, Loss: 60.60700225830078
Projection step: 2, Loss: 59.12636947631836
Projection step: 3, Loss: 59.2666130065918
Projection step: 4, Loss: 57.34423828125
Projection step: 5, Loss: 58.07666015625
Projection step: 6, Loss: 56.97523498535156
Projection step: 7, Loss: 57.00387954711914
Projection step: 8, Loss: 56.399688720703125
Projection step: 9, Loss: 56.777793884277344
Projection step: 10, Loss: 57.56982421875
Projection step: 11, Loss: 54.89402770996094
Projection step: 12, Loss: 56.05137634277344
Projection step: 13, Loss: 55.2812385559082
Projection step: 14, Loss: 57.1014404296875
Projection step: 15, Loss: 56.188812255859375
Projection step: 16, Loss: 54.750755310058594
Projection step: 17, Loss: 54.236488342285156
Projection step: 18, Loss: 54.78429412841797
Projection step: 19, Loss: 54.29753112792969
Projection step: 20, Loss: 54.80926513671875
Projection step: 21, Loss: 55.02473449707031
Projection step: 22, Loss: 53.71645736694336
Projection step: 23, Loss: 54.00550079345703
Projection step: 24, Loss: 54.64826583862305
Final likelihood: tensor([-50.9850, -52.1148, -50.0070, -55.5328, -61.7077, -52.4521, -57.5130,
        -55.7763, -54.4275, -53.5555, -53.2853, -37.0353, -44.2411, -52.1965,
        -55.2574, -68.0875])
Final projection likelihood: -53.3859
1 mode projection failed, trying anyway
New goal: tensor([ 0.1680,  0.4043,  0.4218,  0.9620, -0.0418,  0.8347,  0.7746,  1.1348,
         1.4843,  0.4209,  0.0916,  1.2646,  0.2534, -0.0369, -0.2925],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0025]], device='cuda:0') tensor([[0.0022]], device='cuda:0')
Original likelihood: -55.27344512939453
Adjusted likelihood: -55.27344512939453
Likelihood residual: 0.0
Original likelihood: -65.35072326660156
Adjusted likelihood: -65.35072326660156
Likelihood residual: 0.0
{'index': 65.35072326660156, 'thumb_middle': 55.27344512939453}
Current yaw: tensor([ 0.2569, -0.0376, -0.2241], device='cuda:0')
14 thumb_middle
tensor([ 0.1774,  0.3876,  0.3394,  0.8785, -0.0511,  0.9034,  0.7830,  1.1040,
         1.4999,  0.4423,  0.0606,  1.2836,  0.2569, -0.0376, -0.2241, -5.5884],
       device='cuda:0')
Solve time for step 1 8.682085514010396
Current ori: tensor([ 0.2569, -0.0376, -0.2241], device='cuda:0')
Index force: tensor([0.5670, 0.5888, 0.5862, 0.5615], device='cuda:0')
tensor([ 2.2283e-01,  4.8025e-01,  4.2044e-01,  9.3765e-01, -1.4720e-01,
         7.6668e-01,  7.5802e-01,  1.1109e+00,  1.4386e+00,  4.1601e-01,
        -4.7576e-03,  1.2292e+00,  2.4697e-01, -6.6739e-02, -1.7145e-01,
        -5.4970e+00], device='cuda:0')
Solve time for step 2 3.5908600230468437
Current ori: tensor([ 0.2470, -0.0667, -0.1714], device='cuda:0')
Index force: tensor([0.5797, 0.5775, 0.5539], device='cuda:0')
tensor([ 1.8257e-01,  5.2714e-01,  4.2674e-01,  9.7521e-01, -1.1259e-01,
         7.9338e-01,  7.3905e-01,  1.0919e+00,  1.4337e+00,  3.9530e-01,
         4.2376e-03,  1.2158e+00,  2.7547e-01, -1.3682e-01, -1.1800e-01,
        -4.5258e+00], device='cuda:0')
Solve time for step 3 3.437459983979352
Current ori: tensor([ 0.2755, -0.1368, -0.1180], device='cuda:0')
Index force: tensor([0.5644, 0.5400], device='cuda:0')
tensor([ 0.1355,  0.5757,  0.4287,  0.9593, -0.0631,  0.8551,  0.7655,  1.1283,
         1.4299,  0.4149, -0.0179,  1.2066,  0.3632, -0.3045,  0.0555, -3.0174],
       device='cuda:0')
Solve time for step 4 3.4680540640256368
Current ori: tensor([ 0.3632, -0.3045,  0.0555], device='cuda:0')
Index force: tensor([0.5348], device='cuda:0')
Storing RECOVERY transition: reward=-0.4940 (scaled=-0.2470), steps=2
Reward stats updated: mean -0.0061 -> -0.0065, std: 0.1229
Collected 552 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.7936, Q2 Loss=0.7936, Entropy=0.0063, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4099
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3141, Q2 Loss=1.3141, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4646
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.2794, Q2 Loss=2.2794, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0735
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2306, Q2 Loss=1.2306, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3321
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.0677, Q2 Loss=1.0677, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8792

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.3%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (17.9%)
Actor update: 0.11s (40.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046055
Q1 loss: 1.337062
Q2 loss: 1.337062
Current threshold: -37.6228
Global Scale Offset: 0.0200
Reward stats: mean=-0.0065, std=0.1229, count=552
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.3371, Q2 Loss: 1.3371, Entropy: 0.0013, Mean TD Error: 1.0318, Threshold: -37.6228
Original likelihood: -277.1507568359375
Adjusted likelihood: -277.1507568359375
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 284.2264404296875
Projection step: 1, Loss: 285.82843017578125
Projection step: 2, Loss: 284.4571228027344
Projection step: 3, Loss: 278.91778564453125
Projection step: 4, Loss: 293.8713073730469
Projection step: 5, Loss: 294.34375
Projection step: 6, Loss: 283.5420837402344
Projection step: 7, Loss: 286.22113037109375
Projection step: 8, Loss: 296.2640686035156
Projection step: 9, Loss: 285.680419921875
Projection step: 10, Loss: 263.06072998046875
Projection step: 11, Loss: 277.74298095703125
Projection step: 12, Loss: 287.27935791015625
Projection step: 13, Loss: 286.59014892578125
Projection step: 14, Loss: 293.49395751953125
Projection step: 15, Loss: 300.021240234375
Projection step: 16, Loss: 298.8446960449219
Projection step: 17, Loss: 290.287841796875
Projection step: 18, Loss: 288.8424072265625
Projection step: 19, Loss: 281.96527099609375
Projection step: 20, Loss: 287.10693359375
Projection step: 21, Loss: 296.0138854980469
Projection step: 22, Loss: 294.4488525390625
Projection step: 23, Loss: 260.29217529296875
Projection step: 24, Loss: 286.36505126953125
Final likelihood: tensor([-210.7742, -278.9760, -263.4960, -338.3988, -327.3665, -306.4570,
        -289.8422, -268.2058, -222.9831, -317.3485, -274.2075, -290.2835,
        -286.0833, -242.8177, -298.8839, -250.3049])
Final projection likelihood: -279.1518
1 mode projection failed, trying anyway
New goal: tensor([ 0.1063,  0.7692,  0.4517,  0.9621,  0.0944,  0.9521,  0.8603,  1.1278,
         1.4194,  0.4087,  0.0053,  1.3195,  0.3682, -0.3173,  0.1004],
       device='cuda:0')
tensor([[0.0018]], device='cuda:0') tensor([[0.0219]], device='cuda:0') tensor([[0.0061]], device='cuda:0')
Original likelihood: -280.13800048828125
Adjusted likelihood: -280.13800048828125
Likelihood residual: 0.0
Original likelihood: -201.87879943847656
Adjusted likelihood: -201.87879943847656
Likelihood residual: 0.0
{'index': 201.87879943847656, 'thumb_middle': 280.13800048828125}
Current yaw: tensor([ 0.3681, -0.3168,  0.0823], device='cuda:0')
15 index
tensor([ 0.1012,  0.7688,  0.4460,  0.9603,  0.0951,  0.9518,  0.8649,  1.1223,
         1.4275,  0.4044,  0.0059,  1.3157,  0.3681, -0.3168,  0.0823, -1.5505],
       device='cuda:0')
Solve time for step 1 10.49980426998809
Current ori: tensor([ 0.3681, -0.3168,  0.0823], device='cuda:0')
Middle force: tensor([0.6047, 0.5571, 0.5628, 0.6218], device='cuda:0')
Thumb force: tensor([0.6076, 0.5367, 0.5260, 0.6680], device='cuda:0')
tensor([ 0.0772,  0.7630,  0.4632,  0.9638,  0.1935,  1.0003,  0.7980,  1.0333,
         1.4389,  0.2216,  0.0061,  1.2111,  0.3599, -0.2942,  0.0096, -1.3796],
       device='cuda:0')
Solve time for step 2 4.201033169985749
Current ori: tensor([ 0.3599, -0.2942,  0.0096], device='cuda:0')
Middle force: tensor([0.5216, 0.5422, 0.5246], device='cuda:0')
Thumb force: tensor([0.6224, 0.6583, 0.5064], device='cuda:0')
tensor([ 0.1372,  0.7574,  0.4668,  0.9634,  0.2337,  1.0388,  0.7938,  0.9910,
         1.4301,  0.1766, -0.0477,  1.2526,  0.3613, -0.2982,  0.0033, -2.2635],
       device='cuda:0')
Solve time for step 3 4.020000545016956
Current ori: tensor([ 0.3613, -0.2982,  0.0033], device='cuda:0')
Middle force: tensor([0.5285, 0.5230], device='cuda:0')
Thumb force: tensor([0.6146, 0.5062], device='cuda:0')
tensor([ 0.0702,  0.6627,  0.4712,  0.9661,  0.2117,  1.0552,  0.7985,  0.9597,
         1.3957,  0.2820, -0.0507,  1.2486,  0.3674, -0.3168,  0.0306, -2.2155],
       device='cuda:0')
Solve time for step 4 3.8147503209766
Current ori: tensor([ 0.3674, -0.3168,  0.0306], device='cuda:0')
Middle force: tensor([0.6353], device='cuda:0')
Thumb force: tensor([0.6811], device='cuda:0')
Storing RECOVERY transition: reward=-0.4782 (scaled=-0.2391), steps=2
Reward stats updated: mean -0.0065 -> -0.0070, std: 0.1232
Collected 553 transitions for RL
SAC Update 1/5: Actor Loss=-0.2048, Q1 Loss=1.6311, Q2 Loss=1.6311, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3603
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6690, Q2 Loss=0.6690, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6381
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8404, Q2 Loss=0.8404, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5156
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3511, Q2 Loss=1.3511, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0569
SAC Update 5/5: Actor Loss=-0.0005, Q1 Loss=0.7256, Q2 Loss=0.7256, Entropy=0.3466, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1630

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (20.1%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.04s (17.9%)
Actor update: 0.09s (38.4%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.1%)
Actor loss: -0.041048
Q1 loss: 1.043464
Q2 loss: 1.043464
Current threshold: -37.6298
Global Scale Offset: 0.0200
Reward stats: mean=-0.0070, std=0.1232, count=553
----------------------------------------------
SAC Update - Actor Loss: -0.0410, Q1 Loss: 1.0435, Q2 Loss: 1.0435, Entropy: 0.0693, Mean TD Error: 0.9468, Threshold: -37.6298
Original likelihood: -309.8250427246094
Adjusted likelihood: -309.8250427246094
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 34
Loaded trajectory sampler
Current yaw: tensor([ 0.0004,  0.0145, -0.0449], device='cuda:0')
Current yaw: tensor([ 0.0004,  0.0145, -0.0449], device='cuda:0')
1 turn
Sampling time 3.7532344970386475
tensor([ 1.2793e-01,  5.9331e-01,  5.7853e-01,  5.8796e-01, -1.2135e-01,
         5.6829e-01,  8.6507e-01,  8.9878e-01,  1.2554e+00,  2.4993e-01,
         2.6552e-01,  1.1259e+00,  3.7056e-04,  1.4473e-02, -4.4853e-02,
         3.0192e-02], device='cuda:0')
Original likelihood: -19.92194366455078
Adjusted likelihood: -19.92194366455078
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.076963101979345
Current ori: tensor([ 0.0004,  0.0145, -0.0449], device='cuda:0')
Middle force: tensor([0.7199, 0.8815, 0.5287, 0.5000, 1.0137, 0.8440, 0.5663, 0.5950, 0.5107,
        0.5055, 0.5384, 0.5438], device='cuda:0')
Thumb force: tensor([0.9748, 1.3410, 1.0480, 0.9141, 0.7783, 0.5532, 0.5161, 1.2093, 0.6443,
        0.8215, 0.5132, 0.5978], device='cuda:0')
Index force: tensor([1.3009, 1.5207, 0.5335, 0.5290, 0.5241, 0.5678, 0.5831, 0.6538, 0.6903,
        0.7040, 0.5574, 0.7105], device='cuda:0')
Storing NORMAL transition: reward=-0.0339 (scaled=-0.0339), steps=1
Reward stats updated: mean -0.0070 -> -0.0070, std: 0.1231
Collected 554 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9861, Q2 Loss=0.9861, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6873
SAC Update 2/5: Actor Loss=-0.0004, Q1 Loss=0.6921, Q2 Loss=0.6921, Entropy=0.3465, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0259
SAC Update 3/5: Actor Loss=-0.0693, Q1 Loss=0.9040, Q2 Loss=0.9040, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8372
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0580, Q2 Loss=1.0580, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4789
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.9281, Q2 Loss=1.9281, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1695

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (18.6%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.7%)
Actor update: 0.08s (39.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.059996
Q1 loss: 1.113656
Q2 loss: 1.113656
Current threshold: -37.6348
Global Scale Offset: 0.0200
Reward stats: mean=-0.0070, std=0.1231, count=554
----------------------------------------------
SAC Update - Actor Loss: -0.0600, Q1 Loss: 1.1137, Q2 Loss: 1.1137, Entropy: 0.0693, Mean TD Error: 1.0398, Threshold: -37.6348
tensor([ 0.1160,  0.5983,  0.5750,  0.5577, -0.1312,  0.5943,  0.8093,  0.9113,
         1.2249,  0.2963,  0.3069,  1.1002, -0.0037,  0.0163, -0.0110,  0.0789],
       device='cuda:0')
Original likelihood: -18.89269256591797
Adjusted likelihood: -18.89269256591797
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.632555196993053
Current ori: tensor([-0.0037,  0.0163, -0.0110], device='cuda:0')
Middle force: tensor([0.8868, 0.5280, 0.5000, 1.0039, 0.8359, 0.5644, 0.5903, 0.5097, 0.5028,
        0.5372, 0.5414], device='cuda:0')
Thumb force: tensor([1.3082, 1.0295, 0.9028, 0.7672, 0.5497, 0.5148, 1.1939, 0.6350, 0.8140,
        0.5122, 0.5917], device='cuda:0')
Index force: tensor([1.4864, 0.5318, 0.5281, 0.5227, 0.5646, 0.5799, 0.6470, 0.6923, 0.7381,
        0.5558, 0.7085], device='cuda:0')
Storing NORMAL transition: reward=-0.0013 (scaled=-0.0013), steps=1
Reward stats updated: mean -0.0070 -> -0.0070, std: 0.1230
Collected 555 transitions for RL
SAC Update 1/5: Actor Loss=-0.0018, Q1 Loss=1.3668, Q2 Loss=1.3668, Entropy=0.3459, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2635
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.7063, Q2 Loss=1.7063, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9341
SAC Update 3/5: Actor Loss=-0.3839, Q1 Loss=1.1297, Q2 Loss=1.1297, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6295
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=3.2485, Q2 Loss=3.2485, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.3203
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.9362, Q2 Loss=1.9362, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.1644

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (39.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.077139
Q1 loss: 1.877511
Q2 loss: 1.877511
Current threshold: -37.6400
Global Scale Offset: 0.0200
Reward stats: mean=-0.0070, std=0.1230, count=555
----------------------------------------------
SAC Update - Actor Loss: -0.0771, Q1 Loss: 1.8775, Q2 Loss: 1.8775, Entropy: 0.0692, Mean TD Error: 3.6623, Threshold: -37.6400
tensor([ 0.1772,  0.6384,  0.5712,  0.5289, -0.2691,  0.5539,  0.8144,  1.0061,
         1.1808,  0.2562,  0.2384,  1.1605, -0.0085,  0.0406, -0.0110, -1.2254],
       device='cuda:0')
Original likelihood: -40.28261184692383
Adjusted likelihood: -40.28261184692383
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.30342483520508
Projection step: 1, Loss: 38.458473205566406
Projection step: 2, Loss: 37.36924743652344
Projection step: 3, Loss: 36.59252166748047
Projection step: 4, Loss: 35.42241287231445
Projection step: 5, Loss: 35.867740631103516
Projection step: 6, Loss: 34.46916961669922
Projection step: 7, Loss: 33.89067459106445
Projection step: 8, Loss: 32.64033508300781
Projection step: 9, Loss: 33.460479736328125
Projection step: 10, Loss: 33.41865539550781
Projection step: 11, Loss: 32.59334945678711
Projection step: 12, Loss: 32.112449645996094
Projection step: 13, Loss: 32.0323371887207
Projection step: 14, Loss: 29.942424774169922
Projection step: 15, Loss: 30.86617088317871
Projection step: 16, Loss: 30.261009216308594
Projection step: 17, Loss: 29.468416213989258
Projection step: 18, Loss: 28.913372039794922
Projection step: 19, Loss: 27.053808212280273
Projection step: 20, Loss: 27.59008026123047
Projection step: 21, Loss: 28.350597381591797
Projection step: 22, Loss: 26.938404083251953
Projection step: 23, Loss: 27.96883773803711
Projection step: 24, Loss: 26.250110626220703
Final likelihood: tensor([-22.6747, -23.8115, -30.0391, -26.7694, -21.3133, -30.3155, -24.4944,
        -26.1321, -23.4719, -21.3891, -21.9243, -27.0078, -25.4772, -23.0054,
        -31.5525, -23.2654])
Final projection likelihood: -25.1652
1 mode projection succeeded
New goal: tensor([ 0.1427,  0.6330,  0.5275,  0.5455, -0.1868,  0.5589,  0.7871,  0.9694,
         1.2482,  0.2164,  0.2211,  1.1955, -0.0157,  0.0366, -0.4036],
       device='cuda:0')
tensor([[0.0148]], device='cuda:0') tensor([[0.0096]], device='cuda:0') tensor([[0.0038]], device='cuda:0')
Original likelihood: -26.220199584960938
Adjusted likelihood: -26.220199584960938
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 26.220199584960938}
Current yaw: tensor([-0.0085,  0.0406, -0.0110], device='cuda:0')
2 thumb_middle
tensor([ 0.1772,  0.6384,  0.5712,  0.5289, -0.2691,  0.5539,  0.8144,  1.0061,
         1.1808,  0.2562,  0.2384,  1.1605, -0.0085,  0.0406, -0.0110, -1.2254],
       device='cuda:0')
Solve time for step 1 8.966456012974959
Current ori: tensor([-0.0085,  0.0406, -0.0110], device='cuda:0')
Index force: tensor([0.5739, 0.5860, 0.5878, 0.5791], device='cuda:0')
tensor([ 0.1655,  0.6640,  0.5392,  0.5289, -0.2669,  0.5566,  0.7923,  0.9687,
         1.2255,  0.2225,  0.2023,  1.1874, -0.0113,  0.0470, -0.0110, -1.2992],
       device='cuda:0')
Solve time for step 2 3.531624377006665
Current ori: tensor([-0.0113,  0.0470, -0.0110], device='cuda:0')
Index force: tensor([0.5788, 0.5828, 0.5735], device='cuda:0')
tensor([ 0.1705,  0.6686,  0.5356,  0.5354, -0.2677,  0.5651,  0.7954,  0.9638,
         1.2480,  0.2188,  0.1628,  1.1956, -0.0216,  0.0469, -0.0110, -1.0713],
       device='cuda:0')
Solve time for step 3 3.4567085999879055
Current ori: tensor([-0.0216,  0.0469, -0.0110], device='cuda:0')
Index force: tensor([0.5730, 0.5650], device='cuda:0')
tensor([ 0.1653,  0.6618,  0.5320,  0.5496, -0.2689,  0.5685,  0.7789,  0.9700,
         1.2481,  0.2008,  0.1923,  1.1854, -0.0200,  0.0494, -0.0110, -1.0639],
       device='cuda:0')
Solve time for step 4 3.448646374978125
Current ori: tensor([-0.0200,  0.0494, -0.0110], device='cuda:0')
Index force: tensor([0.5000], device='cuda:0')
Storing RECOVERY transition: reward=-0.0033 (scaled=-0.0017), steps=2
Reward stats updated: mean -0.0070 -> -0.0070, std: 0.1229
Collected 556 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3080, Q2 Loss=1.3080, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4998
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3565, Q2 Loss=1.3565, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3416
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7535, Q2 Loss=0.7535, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3426
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.7054, Q2 Loss=0.7054, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2462
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7910, Q2 Loss=0.7910, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1840

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (21.4%)
Q1 update: 0.04s (18.5%)
Q2 update: 0.04s (18.1%)
Actor update: 0.09s (38.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 0.982878
Q2 loss: 0.982878
Current threshold: -37.6431
Global Scale Offset: 0.0200
Reward stats: mean=-0.0070, std=0.1229, count=556
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 0.9829, Q2 Loss: 0.9829, Entropy: 0.0000, Mean TD Error: 0.5228, Threshold: -37.6431
Original likelihood: -28.382171630859375
Adjusted likelihood: -28.382171630859375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0203,  0.0472, -0.0085], device='cuda:0')
3 turn
Sampling time 3.6172300440375693
tensor([ 0.1633,  0.6670,  0.5229,  0.5497, -0.2038,  0.6061,  0.7910,  0.9755,
         1.3120,  0.2390,  0.2319,  1.2143, -0.0203,  0.0472, -0.0085, -0.9944],
       device='cuda:0')
Original likelihood: -28.184955596923828
Adjusted likelihood: -28.184955596923828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.11633291101316
Current ori: tensor([-0.0203,  0.0472, -0.0085], device='cuda:0')
Middle force: tensor([0.9655, 2.2264, 0.5194, 1.3353, 0.6370, 0.8218, 0.5010, 0.8357, 0.5010,
        0.5896, 0.4381, 0.3822], device='cuda:0')
Thumb force: tensor([0.8768, 1.2510, 1.6658, 0.5941, 0.5475, 0.8643, 0.5992, 0.6488, 1.4115,
        0.4839, 0.5921, 0.5142], device='cuda:0')
Index force: tensor([0.5923, 1.7339, 0.6511, 0.7158, 0.7332, 0.7326, 0.6900, 0.7907, 0.6912,
        0.5350, 0.5845, 0.8028], device='cuda:0')
Storing NORMAL transition: reward=-0.0027 (scaled=-0.0027), steps=1
Reward stats updated: mean -0.0070 -> -0.0070, std: 0.1227
Collected 557 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7176, Q2 Loss=0.7176, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8024
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7534, Q2 Loss=0.7534, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4582
SAC Update 3/5: Actor Loss=-0.2523, Q1 Loss=0.8173, Q2 Loss=0.8173, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6650
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.5486, Q2 Loss=1.5486, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7229
SAC Update 5/5: Actor Loss=-0.0822, Q1 Loss=0.6800, Q2 Loss=0.6800, Entropy=0.0012, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1696

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.6%)
Q1 update: 0.04s (19.2%)
Q2 update: 0.04s (19.5%)
Actor update: 0.09s (39.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.112955
Q1 loss: 0.903386
Q2 loss: 0.903386
Current threshold: -37.6540
Global Scale Offset: 0.0199
Reward stats: mean=-0.0070, std=0.1227, count=557
----------------------------------------------
SAC Update - Actor Loss: -0.1130, Q1 Loss: 0.9034, Q2 Loss: 0.9034, Entropy: 0.0002, Mean TD Error: 0.9636, Threshold: -37.6540
tensor([ 0.2291,  0.6862,  0.5717,  0.5677, -0.2627,  0.7617,  0.7702,  0.7696,
         1.1990,  0.4804,  0.1700,  1.2536, -0.0512,  0.0505, -0.0081, -0.1182],
       device='cuda:0')
Original likelihood: -42.06664276123047
Adjusted likelihood: -42.06664276123047
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 41.61381530761719
Projection step: 1, Loss: 42.16864013671875
Projection step: 2, Loss: 39.12285232543945
Projection step: 3, Loss: 38.81806182861328
Projection step: 4, Loss: 39.7924919128418
Projection step: 5, Loss: 39.1490592956543
Projection step: 6, Loss: 38.24878692626953
Projection step: 7, Loss: 38.45198440551758
Projection step: 8, Loss: 38.764678955078125
Projection step: 9, Loss: 36.09347152709961
Projection step: 10, Loss: 37.1975212097168
Projection step: 11, Loss: 37.00661087036133
Projection step: 12, Loss: 37.91798782348633
Projection step: 13, Loss: 36.292823791503906
Projection step: 14, Loss: 36.145381927490234
Projection step: 15, Loss: 35.362823486328125
Projection step: 16, Loss: 34.27936553955078
Projection step: 17, Loss: 33.98875045776367
Projection step: 18, Loss: 35.317955017089844
Projection step: 19, Loss: 33.49530029296875
Projection step: 20, Loss: 33.416717529296875
Projection step: 21, Loss: 32.632568359375
Projection step: 22, Loss: 33.484130859375
Projection step: 23, Loss: 32.4843635559082
Projection step: 24, Loss: 32.43330764770508
Final likelihood: tensor([-38.2041, -31.8980, -32.2844, -27.2248, -32.3625, -33.0478, -28.9898,
        -28.8296, -32.1851, -37.5078, -30.7968, -29.8925, -31.7311, -32.1966,
        -34.0619, -31.9176])
Final projection likelihood: -32.0707
1 mode projection succeeded
New goal: tensor([ 0.1928,  0.6366,  0.5178,  0.5605, -0.2159,  0.7515,  0.7674,  0.8028,
         1.2845,  0.3896,  0.1736,  1.1943, -0.0535,  0.0465,  1.6487],
       device='cuda:0')
tensor([[0.0085]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0030]], device='cuda:0')
Original likelihood: -35.614192962646484
Adjusted likelihood: -35.614192962646484
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 35.614192962646484}
Current yaw: tensor([-0.0512,  0.0505, -0.0081], device='cuda:0')
4 thumb_middle
tensor([ 0.2291,  0.6862,  0.5717,  0.5677, -0.2627,  0.7617,  0.7702,  0.7696,
         1.1990,  0.4804,  0.1700,  1.2536, -0.0512,  0.0505, -0.0081, -0.1182],
       device='cuda:0')
Solve time for step 1 9.017723219003528
Current ori: tensor([-0.0512,  0.0505, -0.0081], device='cuda:0')
Index force: tensor([0.5754, 0.6024, 0.6096, 0.6031], device='cuda:0')
tensor([ 0.1906,  0.7516,  0.5881,  0.5854, -0.2858,  0.7421,  0.7304,  0.7724,
         1.2376,  0.3951,  0.1126,  1.1800, -0.0931,  0.0930, -0.0430, -0.6461],
       device='cuda:0')
Solve time for step 2 3.6617480989662
Current ori: tensor([-0.0931,  0.0930, -0.0430], device='cuda:0')
Index force: tensor([0.5954, 0.5995, 0.5971], device='cuda:0')
tensor([ 0.1422,  0.8135,  0.6110,  0.5764, -0.2943,  0.7369,  0.7163,  0.7700,
         1.2731,  0.3821,  0.1217,  1.1784, -0.1415,  0.1176, -0.1064, -0.0708],
       device='cuda:0')
Solve time for step 3 3.6656240290030837
Current ori: tensor([-0.1415,  0.1176, -0.1064], device='cuda:0')
Index force: tensor([0.5939, 0.5386], device='cuda:0')
tensor([ 0.0841,  0.8492,  0.6293,  0.6019, -0.2951,  0.7424,  0.7215,  0.7747,
         1.2910,  0.3834,  0.1317,  1.1799, -0.3036,  0.2190, -0.1342,  1.1890],
       device='cuda:0')
Solve time for step 4 3.411894664983265
Current ori: tensor([-0.3036,  0.2190, -0.1342], device='cuda:0')
Index force: tensor([0.5253], device='cuda:0')
Storing RECOVERY transition: reward=-0.4212 (scaled=-0.4212), steps=1
Reward stats updated: mean -0.0070 -> -0.0077, std: 0.1239
Collected 558 transitions for RL
SAC Update 1/5: Actor Loss=-0.2847, Q1 Loss=1.9262, Q2 Loss=1.9262, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.4145
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7950, Q2 Loss=0.7950, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3913
SAC Update 3/5: Actor Loss=-0.0079, Q1 Loss=0.7235, Q2 Loss=0.7235, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0389
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0511, Q2 Loss=1.0511, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3667
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8716, Q2 Loss=0.8716, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4808

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.3%)
Q1 update: 0.05s (18.5%)
Q2 update: 0.05s (18.6%)
Actor update: 0.10s (40.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.058519
Q1 loss: 1.073497
Q2 loss: 1.073497
Current threshold: -37.6888
Global Scale Offset: 0.0197
Reward stats: mean=-0.0077, std=0.1239, count=558
----------------------------------------------
SAC Update - Actor Loss: -0.0585, Q1 Loss: 1.0735, Q2 Loss: 1.0735, Entropy: 0.0000, Mean TD Error: 1.3384, Threshold: -37.6888
Original likelihood: -592.262939453125
Adjusted likelihood: -592.262939453125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 35
Loaded trajectory sampler
Current yaw: tensor([-0.0028,  0.0149, -0.0446], device='cuda:0')
Current yaw: tensor([-0.0028,  0.0149, -0.0446], device='cuda:0')
1 turn
Sampling time 3.619475423998665
tensor([ 0.1586,  0.6489,  0.5270,  0.5985, -0.1148,  0.5481,  0.8686,  0.9525,
         1.2615,  0.2990,  0.2128,  1.1622, -0.0028,  0.0149, -0.0446,  0.1294],
       device='cuda:0')
Original likelihood: -19.272672653198242
Adjusted likelihood: -19.272672653198242
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.229314457043074
Current ori: tensor([-0.0028,  0.0149, -0.0446], device='cuda:0')
Middle force: tensor([0.5883, 0.5707, 1.1858, 0.5684, 1.1331, 0.6401, 0.5374, 0.5224, 0.5124,
        0.5914, 0.6175, 1.0034], device='cuda:0')
Thumb force: tensor([0.8915, 0.8915, 0.7777, 1.0762, 1.0048, 0.6612, 0.5268, 0.9458, 0.5447,
        0.6188, 0.5974, 0.5059], device='cuda:0')
Index force: tensor([0.6033, 0.6067, 0.5658, 0.5747, 0.8105, 0.5318, 1.0447, 0.9559, 0.5704,
        0.5898, 0.6081, 0.5567], device='cuda:0')
Storing NORMAL transition: reward=0.0688 (scaled=0.0688), steps=1
Reward stats updated: mean -0.0077 -> -0.0076, std: 0.1238
Collected 559 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.3981, Q2 Loss=1.3981, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9691
SAC Update 2/5: Actor Loss=-0.0651, Q1 Loss=1.0959, Q2 Loss=1.0959, Entropy=0.0002, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5786
SAC Update 3/5: Actor Loss=-0.0159, Q1 Loss=0.7926, Q2 Loss=0.7926, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8566
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7830, Q2 Loss=0.7830, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5961
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9854, Q2 Loss=0.9854, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4629

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.6%)
Q1 update: 0.05s (19.0%)
Q2 update: 0.05s (19.3%)
Actor update: 0.11s (42.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.062252
Q1 loss: 1.010978
Q2 loss: 1.010978
Current threshold: -37.7314
Global Scale Offset: 0.0194
Reward stats: mean=-0.0076, std=0.1238, count=559
----------------------------------------------
SAC Update - Actor Loss: -0.0623, Q1 Loss: 1.0110, Q2 Loss: 1.0110, Entropy: 0.0000, Mean TD Error: 1.0927, Threshold: -37.7314
tensor([ 0.1618,  0.6628,  0.5151,  0.5906, -0.1269,  0.5725,  0.8350,  0.9910,
         1.2839,  0.2910,  0.2105,  1.1194, -0.0085,  0.0062, -0.1133,  0.3556],
       device='cuda:0')
Original likelihood: -21.046810150146484
Adjusted likelihood: -21.046810150146484
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.547019616002217
Current ori: tensor([-0.0085,  0.0062, -0.1133], device='cuda:0')
Middle force: tensor([0.5663, 1.1658, 0.5651, 1.1231, 0.6334, 0.5358, 0.5193, 0.5108, 0.5873,
        0.6130, 0.9919], device='cuda:0')
Thumb force: tensor([0.8716, 0.7638, 1.0580, 0.9796, 0.6525, 0.5237, 0.9363, 0.5394, 0.6123,
        0.5918, 0.5052], device='cuda:0')
Index force: tensor([0.5973, 0.5620, 0.5703, 0.8020, 0.5287, 1.0310, 0.9421, 0.5682, 0.5864,
        0.6027, 0.5530], device='cuda:0')
Storing NORMAL transition: reward=0.0426 (scaled=0.0426), steps=1
Reward stats updated: mean -0.0076 -> -0.0075, std: 0.1237
Collected 560 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7687, Q2 Loss=0.7687, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6657
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.8195, Q2 Loss=0.8195, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9787
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0837, Q2 Loss=1.0837, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5053
SAC Update 4/5: Actor Loss=-0.1165, Q1 Loss=2.1002, Q2 Loss=2.1002, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5859
SAC Update 5/5: Actor Loss=-0.2786, Q1 Loss=1.7582, Q2 Loss=1.7582, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3146

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.1%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.05s (18.9%)
Actor update: 0.11s (41.3%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.079031
Q1 loss: 1.306061
Q2 loss: 1.306061
Current threshold: -37.7732
Global Scale Offset: 0.0190
Reward stats: mean=-0.0075, std=0.1237, count=560
----------------------------------------------
SAC Update - Actor Loss: -0.0790, Q1 Loss: 1.3061, Q2 Loss: 1.3061, Entropy: 0.0000, Mean TD Error: 1.4100, Threshold: -37.7732
tensor([ 0.2365,  0.6898,  0.5729,  0.5635, -0.1742,  0.5887,  0.7785,  0.9664,
         1.3597,  0.3094,  0.1723,  1.0860, -0.0322,  0.0442, -0.1594,  0.9042],
       device='cuda:0')
Original likelihood: -28.512348175048828
Adjusted likelihood: -28.512348175048828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.198015284026042
Current ori: tensor([-0.0322,  0.0442, -0.1594], device='cuda:0')
Middle force: tensor([1.1429, 0.5591, 1.0654, 0.6270, 0.5324, 0.5116, 0.5094, 0.5778, 0.6081,
        0.9767], device='cuda:0')
Thumb force: tensor([0.7585, 1.0616, 1.0722, 0.6507, 0.5274, 1.0631, 0.5419, 0.6263, 0.5883,
        0.5049], device='cuda:0')
Index force: tensor([0.5546, 0.5620, 0.7335, 0.5244, 0.9955, 0.8632, 0.5592, 0.5755, 0.5980,
        0.5488], device='cuda:0')
Storing NORMAL transition: reward=0.0914 (scaled=0.0914), steps=1
Reward stats updated: mean -0.0075 -> -0.0073, std: 0.1237
Collected 561 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2801, Q2 Loss=1.2801, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3473
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7580, Q2 Loss=0.7580, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6171
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5125, Q2 Loss=1.5125, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8669
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5500, Q2 Loss=1.5500, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8944
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3738, Q2 Loss=1.3738, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4597

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (1.0%)
Target Q: 0.04s (18.7%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (19.7%)
Actor update: 0.09s (38.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.294869
Q2 loss: 1.294869
Current threshold: -37.8363
Global Scale Offset: 0.0184
Reward stats: mean=-0.0073, std=0.1237, count=561
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.2949, Q2 Loss: 1.2949, Entropy: 0.0000, Mean TD Error: 1.2371, Threshold: -37.8363
tensor([ 0.2398,  0.6845,  0.5811,  0.5834, -0.1691,  0.5636,  0.8061,  1.0053,
         1.3678,  0.3172,  0.1628,  1.0543, -0.0303,  0.0395, -0.2510,  1.0440],
       device='cuda:0')
Original likelihood: -27.885669708251953
Adjusted likelihood: -27.885669708251953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.08153697400121
Current ori: tensor([-0.0303,  0.0395, -0.2510], device='cuda:0')
Middle force: tensor([0.5086, 0.6401, 1.5045, 0.5041, 0.5130, 0.5099, 0.5072, 0.5017, 0.5013],
       device='cuda:0')
Thumb force: tensor([0.9672, 0.8513, 1.0517, 0.7255, 0.6241, 1.1067, 0.5434, 0.9147, 0.5167],
       device='cuda:0')
Index force: tensor([0.6221, 0.5041, 0.5180, 0.5042, 0.5108, 0.5055, 0.5466, 0.5256, 0.5461],
       device='cuda:0')
Storing NORMAL transition: reward=0.0077 (scaled=0.0077), steps=1
Reward stats updated: mean -0.0073 -> -0.0073, std: 0.1236
Collected 562 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2983, Q2 Loss=1.2983, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7224
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.6520, Q2 Loss=0.6520, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7450
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7895, Q2 Loss=0.7895, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7306
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.8136, Q2 Loss=1.8136, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7695
SAC Update 5/5: Actor Loss=-0.2095, Q1 Loss=1.1008, Q2 Loss=1.1008, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8315

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.5%)
Q1 update: 0.04s (18.1%)
Q2 update: 0.04s (19.2%)
Actor update: 0.08s (40.2%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.3%)
Actor loss: -0.087944
Q1 loss: 1.130847
Q2 loss: 1.130847
Current threshold: -37.8850
Global Scale Offset: 0.0179
Reward stats: mean=-0.0073, std=0.1236, count=562
----------------------------------------------
SAC Update - Actor Loss: -0.0879, Q1 Loss: 1.1308, Q2 Loss: 1.1308, Entropy: 0.0000, Mean TD Error: 0.9598, Threshold: -37.8850
tensor([ 0.1717,  0.7182,  0.5198,  0.4104, -0.2144,  0.5694,  0.7646,  0.9933,
         1.5000,  0.0827,  0.2207,  0.8954, -0.0411,  0.0750, -0.2667,  0.9441],
       device='cuda:0')
Original likelihood: -31.418312072753906
Adjusted likelihood: -31.418312072753906
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.74174484598916
Current ori: tensor([-0.0411,  0.0750, -0.2667], device='cuda:0')
Middle force: tensor([1.0681, 0.6185, 0.5325, 0.5100, 0.5081, 0.5738, 0.6022, 0.9575],
       device='cuda:0')
Thumb force: tensor([0.9692, 0.6386, 0.5221, 1.0454, 0.5355, 0.6172, 0.5766, 0.5035],
       device='cuda:0')
Index force: tensor([0.7553, 0.5221, 0.9198, 0.8324, 0.5521, 0.5681, 0.5902, 0.5427],
       device='cuda:0')
Storing NORMAL transition: reward=0.0280 (scaled=0.0280), steps=1
Reward stats updated: mean -0.0073 -> -0.0072, std: 0.1235
Collected 563 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6824, Q2 Loss=0.6824, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3706
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.8773, Q2 Loss=0.8773, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6120
SAC Update 3/5: Actor Loss=-0.0767, Q1 Loss=1.5108, Q2 Loss=1.5108, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2780
SAC Update 4/5: Actor Loss=-0.0963, Q1 Loss=1.9370, Q2 Loss=1.9370, Entropy=0.0001, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.6135
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7853, Q2 Loss=0.7853, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6067

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.9%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.6%)
Actor update: 0.08s (39.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.080641
Q1 loss: 1.158564
Q2 loss: 1.158564
Current threshold: -37.9493
Global Scale Offset: 0.0173
Reward stats: mean=-0.0072, std=0.1235, count=563
----------------------------------------------
SAC Update - Actor Loss: -0.0806, Q1 Loss: 1.1586, Q2 Loss: 1.1586, Entropy: 0.0000, Mean TD Error: 1.2962, Threshold: -37.9493
tensor([ 0.1327,  0.7096,  0.4601,  0.4868, -0.2363,  0.5531,  0.8077,  0.9306,
         1.3985,  0.1619,  0.2946,  0.9490, -0.0596,  0.0923, -0.3047,  1.2458],
       device='cuda:0')
Original likelihood: -30.93515396118164
Adjusted likelihood: -30.93515396118164
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.626496071985457
Current ori: tensor([-0.0596,  0.0923, -0.3047], device='cuda:0')
Middle force: tensor([0.5129, 0.5159, 0.5292, 0.5357, 0.5054, 0.5011, 0.5560],
       device='cuda:0')
Thumb force: tensor([0.7928, 0.5385, 1.1449, 0.6236, 0.6237, 0.6424, 0.5562],
       device='cuda:0')
Index force: tensor([0.5595, 0.5555, 0.5094, 0.5065, 0.5800, 0.7102, 0.5524],
       device='cuda:0')
Storing NORMAL transition: reward=0.0803 (scaled=0.0803), steps=1
Reward stats updated: mean -0.0072 -> -0.0071, std: 0.1234
Collected 564 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9073, Q2 Loss=0.9073, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6187
SAC Update 2/5: Actor Loss=-0.0534, Q1 Loss=0.7792, Q2 Loss=0.7792, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2375
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7833, Q2 Loss=0.7833, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1046
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.6261, Q2 Loss=1.6261, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2068
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0207, Q2 Loss=1.0207, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7037

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.3%)
Q1 update: 0.06s (19.5%)
Q2 update: 0.06s (19.8%)
Actor update: 0.11s (40.0%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.010688
Q1 loss: 1.023328
Q2 loss: 1.023328
Current threshold: -37.9873
Global Scale Offset: 0.0169
Reward stats: mean=-0.0071, std=0.1234, count=564
----------------------------------------------
SAC Update - Actor Loss: -0.0107, Q1 Loss: 1.0233, Q2 Loss: 1.0233, Entropy: 0.0000, Mean TD Error: 0.7743, Threshold: -37.9873
tensor([ 0.1469,  0.7189,  0.4802,  0.4525, -0.2157,  0.5697,  0.8422,  0.9246,
         1.4351,  0.3391,  0.1399,  0.9459, -0.0874,  0.0737, -0.3978,  1.3698],
       device='cuda:0')
Original likelihood: -31.8714599609375
Adjusted likelihood: -31.8714599609375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.3325525429681875
Current ori: tensor([-0.0874,  0.0737, -0.3978], device='cuda:0')
Middle force: tensor([0.5484, 1.1275, 0.8665, 0.5089, 0.5343, 0.5211], device='cuda:0')
Thumb force: tensor([0.8554, 0.6885, 1.2763, 0.5439, 1.1530, 0.5983], device='cuda:0')
Index force: tensor([0.5989, 0.7865, 0.9775, 0.6095, 0.5667, 0.5013], device='cuda:0')
Storing NORMAL transition: reward=-0.0346 (scaled=-0.0346), steps=1
Reward stats updated: mean -0.0071 -> -0.0071, std: 0.1233
Collected 565 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6454, Q2 Loss=0.6454, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2058
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1327, Q2 Loss=1.1327, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5132
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6979, Q2 Loss=0.6979, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4065
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.0660, Q2 Loss=1.0660, Entropy=0.0009, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5659
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3003, Q2 Loss=1.3003, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5790

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.9%)
Q1 update: 0.05s (19.4%)
Q2 update: 0.05s (19.7%)
Actor update: 0.12s (42.5%)
Target update: 0.00s (1.8%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 0.968463
Q2 loss: 0.968463
Current threshold: -38.0099
Global Scale Offset: 0.0166
Reward stats: mean=-0.0071, std=0.1233, count=565
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 0.9685, Q2 Loss: 0.9685, Entropy: 0.0002, Mean TD Error: 0.4541, Threshold: -38.0099
tensor([ 0.1990,  0.8695,  0.6835,  0.6646, -0.2667,  0.5250,  0.9470,  1.0076,
         1.3462,  0.4809,  0.2048,  1.0415, -0.0782,  0.0946, -0.3622, -0.2713],
       device='cuda:0')
Original likelihood: -42.94225311279297
Adjusted likelihood: -42.94225311279297
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 46.28972625732422
Projection step: 1, Loss: 42.961727142333984
Projection step: 2, Loss: 41.952293395996094
Projection step: 3, Loss: 41.40310287475586
Projection step: 4, Loss: 41.72868728637695
Projection step: 5, Loss: 40.90723419189453
Projection step: 6, Loss: 40.598419189453125
Projection step: 7, Loss: 39.551422119140625
Projection step: 8, Loss: 38.39150619506836
Projection step: 9, Loss: 39.14459228515625
Projection step: 10, Loss: 37.668975830078125
Projection step: 11, Loss: 35.48660659790039
Projection step: 12, Loss: 37.419830322265625
Projection step: 13, Loss: 37.88094711303711
Projection step: 14, Loss: 37.09022521972656
Projection step: 15, Loss: 36.145179748535156
Projection step: 16, Loss: 36.17774200439453
Projection step: 17, Loss: 35.07117462158203
Projection step: 18, Loss: 33.653236389160156
Projection step: 19, Loss: 33.70223617553711
Projection step: 20, Loss: 35.77113342285156
Projection step: 21, Loss: 34.50007247924805
Projection step: 22, Loss: 35.82776641845703
Projection step: 23, Loss: 35.28472900390625
Projection step: 24, Loss: 35.394874572753906
Final likelihood: tensor([-31.3765, -30.1613, -32.4567, -32.6169, -31.5322, -32.3807, -36.0678,
        -35.8620, -33.9115, -36.1897, -35.9042, -30.6080, -32.7647, -32.7400,
        -23.3779, -50.9139])
Final projection likelihood: -33.6790
1 mode projection succeeded
New goal: tensor([ 0.1977,  0.7568,  0.5018,  0.6107, -0.2327,  0.4920,  0.8696,  0.9720,
         1.3545,  0.4230,  0.1953,  1.0130, -0.0775,  0.0912,  0.8568],
       device='cuda:0')
tensor([[0.0019]], device='cuda:0') tensor([[0.0021]], device='cuda:0') tensor([[0.0092]], device='cuda:0')
Original likelihood: -38.40403747558594
Adjusted likelihood: -38.40403747558594
Likelihood residual: 0.0
{'index': 38.40403747558594, 'thumb_middle': inf}
Current yaw: tensor([-0.0782,  0.0946, -0.3622], device='cuda:0')
2 index
tensor([ 0.1990,  0.8695,  0.6835,  0.6646, -0.2667,  0.5250,  0.9470,  1.0076,
         1.3462,  0.4809,  0.2048,  1.0415, -0.0782,  0.0946, -0.3622, -0.2713],
       device='cuda:0')
Solve time for step 1 10.532904403982684
Current ori: tensor([-0.0782,  0.0946, -0.3622], device='cuda:0')
Middle force: tensor([0.5709, 0.5807, 0.5816, 0.5254], device='cuda:0')
Thumb force: tensor([0.5573, 0.5334, 0.5797, 0.6017], device='cuda:0')
tensor([ 0.2750,  0.7754,  0.5275,  0.6167, -0.2642,  0.5473,  0.9181,  0.9971,
         1.3809,  0.4388,  0.1842,  1.0143, -0.0921,  0.0976, -0.3705, -0.7327],
       device='cuda:0')
Solve time for step 2 4.105605975026265
Current ori: tensor([-0.0921,  0.0976, -0.3705], device='cuda:0')
Middle force: tensor([0.5412, 0.5054, 0.5241], device='cuda:0')
Thumb force: tensor([0.5708, 0.5461, 0.5392], device='cuda:0')
tensor([ 0.2665,  0.7658,  0.5033,  0.6077, -0.2681,  0.5579,  0.9019,  0.9844,
         1.4003,  0.4309,  0.1720,  1.0052, -0.1043,  0.1067, -0.3971, -0.3719],
       device='cuda:0')
Solve time for step 3 4.072244412032887
Current ori: tensor([-0.1043,  0.1067, -0.3971], device='cuda:0')
Middle force: tensor([0.5294, 0.5021], device='cuda:0')
Thumb force: tensor([0.5962, 0.5985], device='cuda:0')
tensor([ 0.2544,  0.7634,  0.5042,  0.6094, -0.2707,  0.5634,  0.8966,  0.9703,
         1.4066,  0.4370,  0.1605,  1.0155, -0.1110,  0.1115, -0.4074,  0.5854],
       device='cuda:0')
Solve time for step 4 3.810235870012548
Current ori: tensor([-0.1110,  0.1115, -0.4074], device='cuda:0')
Middle force: tensor([0.5369], device='cuda:0')
Thumb force: tensor([0.5637], device='cuda:0')
Storing RECOVERY transition: reward=0.0160 (scaled=0.0023), steps=7
Reward stats updated: mean -0.0071 -> -0.0071, std: 0.1232
Collected 566 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9842, Q2 Loss=0.9842, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5552
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=3.5223, Q2 Loss=3.5223, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6854
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9308, Q2 Loss=0.9308, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3399
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0622, Q2 Loss=1.0622, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1674
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7428, Q2 Loss=0.7428, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6541

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.2%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.04s (18.2%)
Actor update: 0.09s (38.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.448455
Q2 loss: 1.448455
Current threshold: -38.0232
Global Scale Offset: 0.0165
Reward stats: mean=-0.0071, std=0.1232, count=566
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.4485, Q2 Loss: 1.4485, Entropy: 0.0000, Mean TD Error: 1.0804, Threshold: -38.0232
Original likelihood: -32.379127502441406
Adjusted likelihood: -32.379127502441406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.1117,  0.1126, -0.4103], device='cuda:0')
3 turn
Sampling time 3.70598454301944
tensor([ 0.1275,  0.7799,  0.5104,  0.6115, -0.2707,  0.5622,  0.8947,  0.9762,
         1.3982,  0.4555,  0.1491,  1.0546, -0.1117,  0.1126, -0.4103,  1.4478],
       device='cuda:0')
Original likelihood: -25.38471794128418
Adjusted likelihood: -25.38471794128418
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.064806138980202
Current ori: tensor([-0.1117,  0.1126, -0.4103], device='cuda:0')
Middle force: tensor([0.5208, 0.5938, 0.8636, 1.0223, 0.3482, 0.4888, 0.5193, 0.5427, 0.6198,
        1.0886, 0.5012, 0.9520], device='cuda:0')
Thumb force: tensor([0.9073, 1.4270, 0.6652, 1.1079, 0.5061, 0.9555, 0.8113, 1.5621, 1.6996,
        0.8858, 0.4163, 0.8663], device='cuda:0')
Index force: tensor([1.1347, 0.8080, 0.5747, 0.4960, 0.4561, 0.4473, 0.4990, 0.5030, 0.5012,
        0.4314, 0.7478, 0.4985], device='cuda:0')
Storing NORMAL transition: reward=0.2593 (scaled=0.2593), steps=1
Reward stats updated: mean -0.0071 -> -0.0066, std: 0.1236
Collected 567 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.7729, Q2 Loss=0.7729, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5011
SAC Update 2/5: Actor Loss=-0.0309, Q1 Loss=1.3389, Q2 Loss=1.3389, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1390
SAC Update 3/5: Actor Loss=-0.0133, Q1 Loss=0.6646, Q2 Loss=0.6646, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4100
SAC Update 4/5: Actor Loss=-0.1408, Q1 Loss=1.1339, Q2 Loss=1.1339, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8003
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9438, Q2 Loss=0.9438, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0854

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.9%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.04s (19.1%)
Actor update: 0.09s (40.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.083041
Q1 loss: 0.970831
Q2 loss: 0.970831
Current threshold: -38.0311
Global Scale Offset: 0.0164
Reward stats: mean=-0.0066, std=0.1236, count=567
----------------------------------------------
SAC Update - Actor Loss: -0.0830, Q1 Loss: 0.9708, Q2 Loss: 0.9708, Entropy: 0.0000, Mean TD Error: 0.9872, Threshold: -38.0311
tensor([ 0.1182,  0.8346,  0.4250,  0.7539, -0.1844,  0.4717,  0.9114,  1.2836,
         1.5000,  0.3265,  0.2062,  0.7210, -0.1143,  0.1154, -0.6963, -0.0738],
       device='cuda:0')
Original likelihood: -33.72452163696289
Adjusted likelihood: -33.72452163696289
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.488350352039561
Current ori: tensor([-0.1143,  0.1154, -0.6963], device='cuda:0')
Middle force: tensor([0.5501, 0.4971, 1.5905, 0.4842, 0.5804, 1.2785, 0.4994, 0.3830, 0.4753,
        0.4906, 1.1224], device='cuda:0')
Thumb force: tensor([0.9674, 1.7028, 1.0268, 1.0363, 0.5554, 1.1463, 0.5942, 1.4023, 1.9967,
        0.4697, 1.1689], device='cuda:0')
Index force: tensor([0.5664, 0.5026, 0.5124, 0.4978, 0.5888, 0.5147, 0.6761, 0.4794, 0.4837,
        0.5115, 0.4885], device='cuda:0')
Storing NORMAL transition: reward=-0.1600 (scaled=-0.1600), steps=1
Reward stats updated: mean -0.0066 -> -0.0069, std: 0.1237
Collected 568 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.0998, Q2 Loss=1.0998, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4289
SAC Update 2/5: Actor Loss=-0.2259, Q1 Loss=1.7693, Q2 Loss=1.7693, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4416
SAC Update 3/5: Actor Loss=-0.0000, Q1 Loss=0.7323, Q2 Loss=0.7323, Entropy=0.0001, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5497
SAC Update 4/5: Actor Loss=-0.1584, Q1 Loss=1.1563, Q2 Loss=1.1563, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7064
SAC Update 5/5: Actor Loss=-0.0252, Q1 Loss=0.8380, Q2 Loss=0.8380, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0379

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.2%)
Q1 update: 0.04s (18.5%)
Q2 update: 0.04s (18.3%)
Actor update: 0.08s (39.7%)
Target update: 0.00s (1.8%)
Priority update: 0.00s (0.2%)
Actor loss: -0.127943
Q1 loss: 1.119146
Q2 loss: 1.119146
Current threshold: -38.0380
Global Scale Offset: 0.0164
Reward stats: mean=-0.0069, std=0.1237, count=568
----------------------------------------------
SAC Update - Actor Loss: -0.1279, Q1 Loss: 1.1191, Q2 Loss: 1.1191, Entropy: 0.0000, Mean TD Error: 0.8329, Threshold: -38.0380
tensor([ 0.0163,  0.8514,  0.4260,  0.7700, -0.1475,  0.5121,  1.0007,  1.2437,
         1.5000,  0.2294,  0.2534,  0.4201, -0.2726,  0.2715, -0.7048,  1.2650],
       device='cuda:0')
Original likelihood: -207.64059448242188
Adjusted likelihood: -207.64059448242188
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 204.48721313476562
Projection step: 1, Loss: 213.8610382080078
Projection step: 2, Loss: 211.039794921875
Projection step: 3, Loss: 208.7008056640625
Projection step: 4, Loss: 211.9761962890625
Projection step: 5, Loss: 209.7399444580078
Projection step: 6, Loss: 214.14309692382812
Projection step: 7, Loss: 211.565185546875
Projection step: 8, Loss: 214.60121154785156
Projection step: 9, Loss: 212.2135009765625
Projection step: 10, Loss: 214.56683349609375
Projection step: 11, Loss: 214.8212432861328
Projection step: 12, Loss: 207.88632202148438
Projection step: 13, Loss: 210.10179138183594
Projection step: 14, Loss: 210.9060821533203
Projection step: 15, Loss: 217.52682495117188
Projection step: 16, Loss: 208.060302734375
Projection step: 17, Loss: 209.3313446044922
Projection step: 18, Loss: 214.57659912109375
Projection step: 19, Loss: 215.14486694335938
Projection step: 20, Loss: 206.43548583984375
Projection step: 21, Loss: 211.37037658691406
Projection step: 22, Loss: 213.68557739257812
Projection step: 23, Loss: 215.180419921875
Projection step: 24, Loss: 210.98648071289062
Final likelihood: tensor([-225.7203, -216.5515, -227.8499, -221.9220, -214.4862, -190.2404,
        -212.6001, -208.9629, -188.5605, -213.5175, -228.5826, -215.4407,
        -205.7820, -221.7082, -189.3760, -217.9846])
Final projection likelihood: -212.4553
1 mode projection failed, trying anyway
New goal: tensor([ 0.0181,  0.8517,  0.4255,  0.7712, -0.1483,  0.5119,  1.0020,  1.2508,
         1.4967,  0.2304,  0.2520,  0.4184, -0.2726,  0.2715, -0.7114],
       device='cuda:0')
tensor([[0.0267]], device='cuda:0') tensor([[0.0196]], device='cuda:0') tensor([[0.0333]], device='cuda:0')
Original likelihood: -128.30331420898438
Adjusted likelihood: -128.30331420898438
Likelihood residual: 0.0
Original likelihood: -160.71156311035156
Adjusted likelihood: -160.71156311035156
Likelihood residual: 0.0
{'index': 160.71156311035156, 'thumb_middle': 128.30331420898438}
Current yaw: tensor([-0.2726,  0.2715, -0.7048], device='cuda:0')
4 thumb_middle
tensor([ 0.0163,  0.8514,  0.4260,  0.7700, -0.1475,  0.5121,  1.0007,  1.2437,
         1.5000,  0.2294,  0.2534,  0.4201, -0.2726,  0.2715, -0.7048,  1.2650],
       device='cuda:0')
Solve time for step 1 9.12149191298522
Current ori: tensor([-0.2726,  0.2715, -0.7048], device='cuda:0')
Index force: tensor([0.6442, 0.5073, 0.5063, 0.5918], device='cuda:0')
tensor([-0.0413,  0.9074,  0.4483,  0.7641, -0.2149,  0.5122,  1.0216,  1.2742,
         1.4941,  0.2172,  0.2500,  0.4335, -0.6229,  0.6185, -0.7048,  2.7502],
       device='cuda:0')
Solve time for step 2 3.5551540610031225
Current ori: tensor([-0.6229,  0.6185, -0.7048], device='cuda:0')
Index force: tensor([0.5048, 0.5229, 0.5808], device='cuda:0')
tensor([-0.3689,  1.2531,  0.6202,  0.9179, -0.2712,  0.4672,  1.0226,  1.3020,
         1.5000,  0.2162,  0.2517,  0.4619, -1.0838,  1.0696, -0.7048,  1.8057],
       device='cuda:0')
Solve time for step 3 3.6722917179577053
Current ori: tensor([-1.0838,  1.0696, -0.7048], device='cuda:0')
Index force: tensor([0.5184, 0.5226], device='cuda:0')
tensor([-0.4700,  1.5778,  0.6736,  1.1049, -0.2722,  0.5004,  1.0365,  1.3005,
         1.4284,  0.1798,  0.2143,  0.4453, -1.3849,  1.3508, -0.7048,  2.3739],
       device='cuda:0')
Solve time for step 4 3.5464976119692437
Current ori: tensor([-1.3849,  1.3508, -0.7048], device='cuda:0')
Index force: tensor([0.5224], device='cuda:0')
Storing RECOVERY transition: reward=-1.6763 (scaled=-0.8381), steps=2
Reward stats updated: mean -0.0069 -> -0.0084, std: 0.1284
Collected 569 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8063, Q2 Loss=0.8063, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9796
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2331, Q2 Loss=1.2331, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6715
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2918, Q2 Loss=1.2918, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1123
SAC Update 4/5: Actor Loss=-0.2688, Q1 Loss=0.7692, Q2 Loss=0.7692, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7712
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.3370, Q2 Loss=1.3370, Entropy=0.0037, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0224

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (16.7%)
Q1 update: 0.04s (20.4%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (40.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.053764
Q1 loss: 1.087480
Q2 loss: 1.087480
Current threshold: -38.0490
Global Scale Offset: 0.0163
Reward stats: mean=-0.0084, std=0.1284, count=569
----------------------------------------------
SAC Update - Actor Loss: -0.0538, Q1 Loss: 1.0875, Q2 Loss: 1.0875, Entropy: 0.0007, Mean TD Error: 1.1114, Threshold: -38.0490
Original likelihood: -1322.3424072265625
Adjusted likelihood: -1322.3424072265625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 36
Loaded trajectory sampler
Current yaw: tensor([-0.0005,  0.0141, -0.0334], device='cuda:0')
Current yaw: tensor([-0.0005,  0.0141, -0.0334], device='cuda:0')
1 turn
Sampling time 3.6742877979995683
tensor([ 1.0923e-01,  6.2088e-01,  5.1480e-01,  6.0106e-01, -1.1928e-01,
         5.3383e-01,  9.1567e-01,  8.7765e-01,  1.2342e+00,  2.8128e-01,
         2.3575e-01,  1.2196e+00, -5.4829e-04,  1.4106e-02, -3.3391e-02,
         2.1071e-02], device='cuda:0')
Original likelihood: -14.750091552734375
Adjusted likelihood: -14.750091552734375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.991168446023948
Current ori: tensor([-0.0005,  0.0141, -0.0334], device='cuda:0')
Middle force: tensor([1.0879, 2.8849, 0.6219, 0.6847, 1.6202, 0.5080, 0.7469, 0.5879, 0.5805,
        0.5201, 0.6724, 0.5100], device='cuda:0')
Thumb force: tensor([1.7509, 1.5263, 1.5595, 1.1575, 0.8504, 0.5477, 0.5741, 0.6095, 0.6207,
        0.5632, 0.5461, 0.6439], device='cuda:0')
Index force: tensor([1.2667, 1.2310, 0.5963, 0.8651, 0.8136, 0.6629, 0.6110, 0.5969, 0.6306,
        0.5901, 0.5297, 0.6746], device='cuda:0')
Storing NORMAL transition: reward=0.1410 (scaled=0.1410), steps=1
Reward stats updated: mean -0.0084 -> -0.0081, std: 0.1284
Collected 570 transitions for RL
SAC Update 1/5: Actor Loss=-0.1788, Q1 Loss=1.6336, Q2 Loss=1.6336, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5194
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=4.2272, Q2 Loss=4.2272, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.1184
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.0353, Q2 Loss=2.0353, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8286
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8798, Q2 Loss=0.8798, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1535
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8777, Q2 Loss=0.8777, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1094

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.9%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (18.2%)
Actor update: 0.10s (38.4%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.081817
Q1 loss: 1.930727
Q2 loss: 1.930727
Current threshold: -38.0555
Global Scale Offset: 0.0162
Reward stats: mean=-0.0081, std=0.1284, count=570
----------------------------------------------
SAC Update - Actor Loss: -0.0818, Q1 Loss: 1.9307, Q2 Loss: 1.9307, Entropy: 0.0000, Mean TD Error: 2.3458, Threshold: -38.0555
tensor([ 0.1525,  0.6446,  0.5121,  0.6304, -0.1262,  0.5037,  1.0154,  0.8325,
         1.3219,  0.2577,  0.2071,  1.0188, -0.0233,  0.0025, -0.1749,  0.9421],
       device='cuda:0')
Original likelihood: -22.146995544433594
Adjusted likelihood: -22.146995544433594
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.6192761400016025
Current ori: tensor([-0.0233,  0.0025, -0.1749], device='cuda:0')
Middle force: tensor([2.8090, 0.6175, 0.6723, 1.5864, 0.5062, 0.7309, 0.5830, 0.5664, 0.5168,
        0.6573, 0.5094], device='cuda:0')
Thumb force: tensor([1.4999, 1.5298, 1.1427, 0.8430, 0.5500, 0.5804, 0.6052, 0.6350, 0.5663,
        0.5491, 0.6393], device='cuda:0')
Index force: tensor([1.1988, 0.5926, 0.8578, 0.8067, 0.6633, 0.6000, 0.5946, 0.6217, 0.5848,
        0.5277, 0.6680], device='cuda:0')
Storing NORMAL transition: reward=0.0452 (scaled=0.0452), steps=1
Reward stats updated: mean -0.0081 -> -0.0080, std: 0.1283
Collected 571 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5985, Q2 Loss=1.5985, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4026
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.6328, Q2 Loss=0.6328, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1373
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0950, Q2 Loss=1.0950, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2538
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5877, Q2 Loss=1.5877, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2009
SAC Update 5/5: Actor Loss=-0.3304, Q1 Loss=1.1112, Q2 Loss=1.1112, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1623

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.0%)
Q1 update: 0.06s (19.0%)
Q2 update: 0.06s (20.8%)
Actor update: 0.12s (42.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.158193
Q1 loss: 1.205034
Q2 loss: 1.205034
Current threshold: -38.0594
Global Scale Offset: 0.0162
Reward stats: mean=-0.0080, std=0.1283, count=571
----------------------------------------------
SAC Update - Actor Loss: -0.1582, Q1 Loss: 1.2050, Q2 Loss: 1.2050, Entropy: 0.0000, Mean TD Error: 1.0314, Threshold: -38.0594
tensor([ 0.2664,  0.6552,  0.6057,  0.5914, -0.1293,  0.3931,  1.0123,  0.9810,
         1.3954,  0.2044,  0.1726,  1.0629, -0.0235,  0.0368, -0.2222,  0.4658],
       device='cuda:0')
Original likelihood: -24.28103256225586
Adjusted likelihood: -24.28103256225586
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.17705619498156
Current ori: tensor([-0.0235,  0.0368, -0.2222], device='cuda:0')
Middle force: tensor([1.4744, 0.5017, 0.5246, 0.6346, 0.5449, 0.5025, 0.5248, 0.5079, 0.5630,
        0.5325], device='cuda:0')
Thumb force: tensor([1.6638, 0.5180, 1.9085, 0.9478, 0.5271, 0.7988, 1.2431, 0.5787, 0.6455,
        1.0216], device='cuda:0')
Index force: tensor([0.5113, 0.6974, 0.7869, 0.5247, 0.5534, 0.6454, 0.5365, 0.6361, 0.5519,
        0.5807], device='cuda:0')
Storing NORMAL transition: reward=0.0883 (scaled=0.0883), steps=1
Reward stats updated: mean -0.0080 -> -0.0078, std: 0.1283
Collected 572 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=2.9139, Q2 Loss=2.9139, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.0191
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4187, Q2 Loss=1.4187, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5016
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=5.3052, Q2 Loss=5.3052, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.4526
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.8946, Q2 Loss=0.8946, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3850
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7016, Q2 Loss=0.7016, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2562

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.3%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.06s (20.3%)
Actor update: 0.12s (42.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.184207
Q1 loss: 2.246801
Q2 loss: 2.246801
Current threshold: -38.0616
Global Scale Offset: 0.0162
Reward stats: mean=-0.0078, std=0.1283, count=572
----------------------------------------------
SAC Update - Actor Loss: -0.1842, Q1 Loss: 2.2468, Q2 Loss: 2.2468, Entropy: 0.0000, Mean TD Error: 2.5229, Threshold: -38.0616
tensor([ 0.1916,  0.6193,  0.6373,  0.5056, -0.0817,  0.2857,  1.0193,  1.1593,
         1.4124,  0.2006,  0.2089,  0.9855, -0.0241,  0.0507, -0.3140,  0.3969],
       device='cuda:0')
Original likelihood: -29.93155288696289
Adjusted likelihood: -29.93155288696289
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.072390311979689
Current ori: tensor([-0.0241,  0.0507, -0.3140], device='cuda:0')
Middle force: tensor([0.6436, 1.5265, 0.5026, 0.7014, 0.5762, 0.5498, 0.5123, 0.6335, 0.5071],
       device='cuda:0')
Thumb force: tensor([1.1201, 0.8345, 0.5646, 0.5966, 0.5932, 0.6508, 0.5712, 0.5532, 0.6443],
       device='cuda:0')
Index force: tensor([0.8369, 0.7959, 0.6828, 0.5757, 0.5898, 0.6028, 0.5714, 0.5242, 0.6523],
       device='cuda:0')
Storing NORMAL transition: reward=0.0364 (scaled=0.0364), steps=1
Reward stats updated: mean -0.0078 -> -0.0078, std: 0.1282
Collected 573 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=3.2023, Q2 Loss=3.2023, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.8881
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1623, Q2 Loss=1.1623, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8176
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2084, Q2 Loss=1.2084, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6208
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.9781, Q2 Loss=0.9781, Entropy=0.0058, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1308
SAC Update 5/5: Actor Loss=-0.0395, Q1 Loss=0.7464, Q2 Loss=0.7464, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1697

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.4%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (40.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.053954
Q1 loss: 1.459502
Q2 loss: 1.459502
Current threshold: -38.0630
Global Scale Offset: 0.0162
Reward stats: mean=-0.0078, std=0.1282, count=573
----------------------------------------------
SAC Update - Actor Loss: -0.0540, Q1 Loss: 1.4595, Q2 Loss: 1.4595, Entropy: 0.0012, Mean TD Error: 1.3254, Threshold: -38.0630
tensor([ 0.1431,  0.6787,  0.4881,  0.5280, -0.0483,  0.3433,  0.9568,  1.1352,
         1.3712,  0.3182,  0.1949,  0.9788, -0.0644,  0.0473, -0.3595,  0.8346],
       device='cuda:0')
Original likelihood: -23.81024742126465
Adjusted likelihood: -23.81024742126465
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.771321730979253
Current ori: tensor([-0.0644,  0.0473, -0.3595], device='cuda:0')
Middle force: tensor([0.5121, 0.7704, 0.5011, 0.5004, 0.5079, 0.5048, 0.5357, 0.5257],
       device='cuda:0')
Thumb force: tensor([0.5980, 1.2694, 0.6586, 0.5518, 0.5308, 0.5589, 0.5933, 0.5187],
       device='cuda:0')
Index force: tensor([0.5428, 0.5349, 0.6040, 0.8080, 0.6764, 0.6253, 0.5522, 0.5506],
       device='cuda:0')
Storing NORMAL transition: reward=-0.1070 (scaled=-0.1070), steps=1
Reward stats updated: mean -0.0078 -> -0.0079, std: 0.1281
Collected 574 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4322, Q2 Loss=1.4322, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=6.7593
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7452, Q2 Loss=0.7452, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6666
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4634, Q2 Loss=1.4634, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9702
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=2.1611, Q2 Loss=2.1611, Entropy=0.0059, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9664
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=1.0536, Q2 Loss=1.0536, Entropy=0.0060, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1824

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.5%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (18.8%)
Actor update: 0.10s (38.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000004
Q1 loss: 1.371100
Q2 loss: 1.371100
Current threshold: -38.0638
Global Scale Offset: 0.0162
Reward stats: mean=-0.0079, std=0.1281, count=574
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 1.3711, Q2 Loss: 1.3711, Entropy: 0.0024, Mean TD Error: 2.3090, Threshold: -38.0638
tensor([ 0.1433,  0.6567,  0.4678,  0.5753,  0.0557,  0.3576,  0.9722,  1.1050,
         1.3021,  0.3432,  0.0560,  1.2489, -0.0377, -0.0162, -0.2397,  2.3910],
       device='cuda:0')
Original likelihood: -26.196304321289062
Adjusted likelihood: -26.196304321289062
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.5574752379907295
Current ori: tensor([-0.0377, -0.0162, -0.2397], device='cuda:0')
Middle force: tensor([0.5024, 0.7146, 0.5679, 0.5491, 0.5110, 0.6448, 0.5084],
       device='cuda:0')
Thumb force: tensor([0.5403, 0.5579, 0.5809, 0.6137, 0.5480, 0.5366, 0.6071],
       device='cuda:0')
Index force: tensor([0.6787, 0.5882, 0.5880, 0.5996, 0.5725, 0.5216, 0.6479],
       device='cuda:0')
Storing NORMAL transition: reward=0.0238 (scaled=0.0238), steps=1
Reward stats updated: mean -0.0079 -> -0.0079, std: 0.1280
Collected 575 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.7229, Q2 Loss=0.7229, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2105
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9975, Q2 Loss=0.9975, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0762
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4759, Q2 Loss=1.4759, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6811
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.8502, Q2 Loss=2.8502, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.9842
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4324, Q2 Loss=1.4324, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9913

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.4%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.1%)
Actor update: 0.08s (39.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 1.495782
Q2 loss: 1.495782
Current threshold: -38.0644
Global Scale Offset: 0.0162
Reward stats: mean=-0.0079, std=0.1280, count=575
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.4958, Q2 Loss: 1.4958, Entropy: 0.0000, Mean TD Error: 2.1887, Threshold: -38.0644
tensor([ 0.1301,  0.6702,  0.4684,  0.6020,  0.0534,  0.3515,  0.9770,  1.1138,
         1.3125,  0.3382,  0.0720,  1.1980, -0.0405, -0.0130, -0.2639,  2.5325],
       device='cuda:0')
Original likelihood: -23.740861892700195
Adjusted likelihood: -23.740861892700195
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.3217827820335515
Current ori: tensor([-0.0405, -0.0130, -0.2639], device='cuda:0')
Middle force: tensor([0.7069, 0.5658, 0.5469, 0.5100, 0.6432, 0.5078], device='cuda:0')
Thumb force: tensor([0.5525, 0.5755, 0.6059, 0.5433, 0.5337, 0.6014], device='cuda:0')
Index force: tensor([0.5853, 0.5846, 0.5953, 0.5700, 0.5200, 0.6422], device='cuda:0')
Storing NORMAL transition: reward=0.0109 (scaled=0.0109), steps=1
Reward stats updated: mean -0.0079 -> -0.0078, std: 0.1279
Collected 576 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0424, Q2 Loss=1.0424, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9597
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.0189, Q2 Loss=2.0189, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6747
SAC Update 3/5: Actor Loss=-0.2194, Q1 Loss=1.3001, Q2 Loss=1.3001, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4450
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.7143, Q2 Loss=0.7143, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3853
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7059, Q2 Loss=0.7059, Entropy=0.0061, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9012

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.8%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (18.6%)
Actor update: 0.10s (38.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.135990
Q1 loss: 1.156338
Q2 loss: 1.156338
Current threshold: -38.0647
Global Scale Offset: 0.0162
Reward stats: mean=-0.0078, std=0.1279, count=576
----------------------------------------------
SAC Update - Actor Loss: -0.1360, Q1 Loss: 1.1563, Q2 Loss: 1.1563, Entropy: 0.0012, Mean TD Error: 1.2732, Threshold: -38.0647
tensor([ 0.1189,  0.6398,  0.4808,  0.6369,  0.0480,  0.3439,  0.9788,  1.1201,
         1.3177,  0.3344,  0.0826,  1.1802, -0.0394, -0.0087, -0.2746,  2.3147],
       device='cuda:0')
Original likelihood: -22.356521606445312
Adjusted likelihood: -22.356521606445312
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.0358299859799445
Current ori: tensor([-0.0394, -0.0087, -0.2746], device='cuda:0')
Middle force: tensor([0.5620, 0.5444, 0.5090, 0.6410, 0.5072], device='cuda:0')
Thumb force: tensor([0.5689, 0.5989, 0.5399, 0.5312, 0.5960], device='cuda:0')
Index force: tensor([0.5809, 0.5913, 0.5673, 0.5183, 0.6362], device='cuda:0')
Storing NORMAL transition: reward=-0.0052 (scaled=-0.0052), steps=1
Reward stats updated: mean -0.0078 -> -0.0078, std: 0.1278
Collected 577 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9265, Q2 Loss=0.9265, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1093
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1793, Q2 Loss=1.1793, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7525
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.5784, Q2 Loss=1.5784, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1228
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2954, Q2 Loss=1.2954, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=6.8338
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1911, Q2 Loss=1.1911, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3374

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (18.3%)
Q1 update: 0.04s (18.5%)
Q2 update: 0.04s (18.6%)
Actor update: 0.08s (40.5%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.234151
Q2 loss: 1.234151
Current threshold: -38.0649
Global Scale Offset: 0.0162
Reward stats: mean=-0.0078, std=0.1278, count=577
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.2342, Q2 Loss: 1.2342, Entropy: 0.0000, Mean TD Error: 2.0311, Threshold: -38.0649
tensor([ 0.0187,  0.5971,  0.3910,  0.7233, -0.1154,  0.4281,  0.9690,  1.1307,
         1.3297,  0.3738,  0.1543,  1.1037, -0.0656,  0.0386, -0.2767,  1.8315],
       device='cuda:0')
Original likelihood: -23.834693908691406
Adjusted likelihood: -23.834693908691406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 3.9877371990005486
Current ori: tensor([-0.0656,  0.0386, -0.2767], device='cuda:0')
Middle force: tensor([0.5447, 0.5097, 0.6594, 0.5076], device='cuda:0')
Thumb force: tensor([0.5888, 0.5355, 0.5267, 0.5876], device='cuda:0')
Index force: tensor([0.5856, 0.5625, 0.5152, 0.6288], device='cuda:0')
Storing NORMAL transition: reward=0.0155 (scaled=0.0155), steps=1
Reward stats updated: mean -0.0078 -> -0.0078, std: 0.1277
Collected 578 transitions for RL
SAC Update 1/5: Actor Loss=-0.0124, Q1 Loss=0.7599, Q2 Loss=0.7599, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8038
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8487, Q2 Loss=0.8487, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4664
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.6732, Q2 Loss=0.6732, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4725
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9936, Q2 Loss=0.9936, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3347
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9764, Q2 Loss=0.9764, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3062

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.5%)
Q1 update: 0.04s (18.6%)
Q2 update: 0.04s (18.6%)
Actor update: 0.08s (40.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.002481
Q1 loss: 0.850361
Q2 loss: 0.850361
Current threshold: -38.0650
Global Scale Offset: 0.0162
Reward stats: mean=-0.0078, std=0.1277, count=578
----------------------------------------------
SAC Update - Actor Loss: -0.0025, Q1 Loss: 0.8504, Q2 Loss: 0.8504, Entropy: 0.0000, Mean TD Error: 0.8767, Threshold: -38.0650
tensor([ 0.0344,  0.5535,  0.4630,  0.7345, -0.1058,  0.4098,  0.9992,  1.1358,
         1.3359,  0.3552,  0.1288,  1.1385, -0.0565,  0.0311, -0.2896,  1.8036],
       device='cuda:0')
Original likelihood: -20.646183013916016
Adjusted likelihood: -20.646183013916016
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 10 3.881640063947998
Current ori: tensor([-0.0565,  0.0311, -0.2896], device='cuda:0')
Middle force: tensor([0.5087, 0.6554, 0.5070], device='cuda:0')
Thumb force: tensor([0.5336, 0.5258, 0.5842], device='cuda:0')
Index force: tensor([0.5602, 0.5141, 0.6252], device='cuda:0')
Storing NORMAL transition: reward=0.0386 (scaled=0.0386), steps=1
Reward stats updated: mean -0.0078 -> -0.0077, std: 0.1276
Collected 579 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0090, Q2 Loss=1.0090, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7975
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0142, Q2 Loss=1.0142, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6840
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=3.5810, Q2 Loss=3.5810, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.2809
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=3.6925, Q2 Loss=3.6925, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.2032
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.5442, Q2 Loss=1.5442, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5835

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.4%)
Q1 update: 0.04s (17.8%)
Q2 update: 0.04s (18.6%)
Actor update: 0.10s (42.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 2.168169
Q2 loss: 2.168169
Current threshold: -38.0651
Global Scale Offset: 0.0162
Reward stats: mean=-0.0077, std=0.1276, count=579
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 2.1682, Q2 Loss: 2.1682, Entropy: 0.0000, Mean TD Error: 2.9098, Threshold: -38.0651
tensor([-0.0337,  0.5155,  0.4392,  0.7626, -0.1052,  0.3638,  1.0736,  1.1103,
         1.3410,  0.4308,  0.0733,  1.1975, -0.0462,  0.0311, -0.3272,  2.0669],
       device='cuda:0')
Original likelihood: -28.287973403930664
Adjusted likelihood: -28.287973403930664
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 11 3.7586853679968044
Current ori: tensor([-0.0462,  0.0311, -0.3272], device='cuda:0')
Middle force: tensor([0.5531, 0.5255], device='cuda:0')
Thumb force: tensor([0.5481, 0.5115], device='cuda:0')
Index force: tensor([0.5384, 0.5414], device='cuda:0')
Storing NORMAL transition: reward=0.0063 (scaled=0.0063), steps=1
Reward stats updated: mean -0.0077 -> -0.0077, std: 0.1275
Collected 580 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2052, Q2 Loss=1.2052, Entropy=0.0000, Time=0.09sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8229
SAC Update 2/5: Actor Loss=-0.0967, Q1 Loss=1.8409, Q2 Loss=1.8409, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5431
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.5014, Q2 Loss=1.5014, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3917
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.3803, Q2 Loss=2.3803, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8557
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9594, Q2 Loss=0.9594, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1932

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (17.9%)
Q1 update: 0.05s (18.4%)
Q2 update: 0.06s (19.1%)
Actor update: 0.12s (41.1%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.111449
Q1 loss: 1.577449
Q2 loss: 1.577449
Current threshold: -38.0651
Global Scale Offset: 0.0162
Reward stats: mean=-0.0077, std=0.1275, count=580
----------------------------------------------
SAC Update - Actor Loss: -0.1114, Q1 Loss: 1.5774, Q2 Loss: 1.5774, Entropy: 0.0000, Mean TD Error: 1.5613, Threshold: -38.0651
tensor([ 5.2149e-02,  4.5161e-01,  5.8387e-01,  7.9933e-01, -2.6427e-02,
         3.8332e-01,  9.8702e-01,  1.2209e+00,  1.2881e+00,  3.5854e-01,
         1.1719e-01,  1.1963e+00, -3.6346e-02, -1.7646e-03, -3.2975e-01,
         1.9682e+00], device='cuda:0')
Original likelihood: -35.14249801635742
Adjusted likelihood: -35.14249801635742
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 12 3.3955315179773606
Current ori: tensor([-0.0363, -0.0018, -0.3298], device='cuda:0')
Middle force: tensor([0.5052], device='cuda:0')
Thumb force: tensor([0.5908], device='cuda:0')
Index force: tensor([0.6261], device='cuda:0')
Storing NORMAL transition: reward=0.0120 (scaled=0.0120), steps=1
Reward stats updated: mean -0.0077 -> -0.0077, std: 0.1274
Collected 581 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1225, Q2 Loss=1.1225, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2115
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8107, Q2 Loss=0.8107, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4147
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2412, Q2 Loss=1.2412, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0546
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1001, Q2 Loss=1.1001, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6943
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.6738, Q2 Loss=0.6738, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1943

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.3%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (19.3%)
Actor update: 0.09s (39.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 0.989676
Q2 loss: 0.989676
Current threshold: -38.0651
Global Scale Offset: 0.0162
Reward stats: mean=-0.0077, std=0.1274, count=581
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 0.9897, Q2 Loss: 0.9897, Entropy: 0.0000, Mean TD Error: 0.5139, Threshold: -38.0651
Original likelihood: -26.805402755737305
Adjusted likelihood: -26.805402755737305
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0262, -0.0153, -0.3410], device='cuda:0')
2 turn
Sampling time 3.586220064025838
tensor([ 0.0693,  0.4362,  0.5975,  0.8451,  0.0731,  0.3140,  0.9915,  1.2492,
         1.2790,  0.3596,  0.0596,  1.2565, -0.0262, -0.0153, -0.3410,  2.0486],
       device='cuda:0')
Original likelihood: -26.965843200683594
Adjusted likelihood: -26.965843200683594
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.51499368000077
Current ori: tensor([-0.0262, -0.0153, -0.3410], device='cuda:0')
Middle force: tensor([0.7403, 0.5075, 0.5602, 0.5255, 0.5101, 0.6979, 0.9200, 0.5539, 0.6361,
        0.6586, 0.5764, 0.5384], device='cuda:0')
Thumb force: tensor([0.5083, 0.9085, 0.5515, 0.5859, 0.6174, 2.0148, 0.5484, 0.5604, 0.5710,
        0.5853, 0.5844, 0.6455], device='cuda:0')
Index force: tensor([0.7714, 0.5890, 0.5930, 0.5445, 0.8324, 0.6174, 1.0343, 0.6208, 0.5989,
        0.5724, 0.6268, 0.5359], device='cuda:0')
Storing NORMAL transition: reward=0.0162 (scaled=0.0162), steps=1
Reward stats updated: mean -0.0077 -> -0.0076, std: 0.1273
Collected 582 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=0.7223, Q2 Loss=0.7223, Entropy=0.0062, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7728
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9008, Q2 Loss=0.9008, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6253
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0952, Q2 Loss=1.0952, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4536
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.6936, Q2 Loss=0.6936, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1179
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3449, Q2 Loss=1.3449, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5736

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.4%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (19.4%)
Actor update: 0.10s (41.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 0.951364
Q2 loss: 0.951364
Current threshold: -38.0652
Global Scale Offset: 0.0162
Reward stats: mean=-0.0076, std=0.1273, count=582
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 0.9514, Q2 Loss: 0.9514, Entropy: 0.0012, Mean TD Error: 0.5086, Threshold: -38.0652
tensor([ 0.0033,  0.4434,  0.6135,  0.7078,  0.0226,  0.3379,  1.0124,  1.0599,
         1.3887,  0.1593,  0.1454,  1.0908, -0.0790,  0.0200, -0.3628,  1.8459],
       device='cuda:0')
Original likelihood: -21.11151885986328
Adjusted likelihood: -21.11151885986328
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.650715861003846
Current ori: tensor([-0.0790,  0.0200, -0.3628], device='cuda:0')
Middle force: tensor([0.5448, 0.6456, 1.4628, 0.6159, 0.7534, 0.7102, 0.5923, 0.5778, 0.5751,
        0.6627, 0.5915], device='cuda:0')
Thumb force: tensor([1.6137, 0.5483, 1.1266, 0.5942, 0.5478, 0.5343, 0.5783, 0.5649, 0.5258,
        0.5734, 0.5053], device='cuda:0')
Index force: tensor([0.6000, 0.5163, 1.0621, 0.5286, 0.6742, 0.5119, 0.5765, 0.5751, 0.5845,
        0.5577, 0.6123], device='cuda:0')
Storing NORMAL transition: reward=0.0137 (scaled=0.0137), steps=1
Reward stats updated: mean -0.0076 -> -0.0076, std: 0.1272
Collected 583 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7116, Q2 Loss=0.7116, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2887
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0907, Q2 Loss=1.0907, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5731
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9442, Q2 Loss=0.9442, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7324
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=4.2626, Q2 Loss=4.2626, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.5316
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7103, Q2 Loss=0.7103, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2762

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (18.9%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (19.3%)
Actor update: 0.10s (39.1%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.543876
Q2 loss: 1.543876
Current threshold: -38.0652
Global Scale Offset: 0.0162
Reward stats: mean=-0.0076, std=0.1272, count=583
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.5439, Q2 Loss: 1.5439, Entropy: 0.0000, Mean TD Error: 1.2804, Threshold: -38.0652
tensor([-0.0338,  0.4293,  0.5873,  0.7156, -0.0612,  0.2835,  1.1044,  1.0198,
         1.3752,  0.3090,  0.1821,  1.0968, -0.0904,  0.0790, -0.3848,  1.3944],
       device='cuda:0')
Original likelihood: -32.036521911621094
Adjusted likelihood: -32.036521911621094
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.13595800002804
Current ori: tensor([-0.0904,  0.0790, -0.3848], device='cuda:0')
Middle force: tensor([0.6048, 0.5042, 0.5354, 0.6021, 0.5663, 0.6965, 0.5558, 0.5436, 0.5601,
        0.5415], device='cuda:0')
Thumb force: tensor([1.1233, 1.2236, 0.5734, 0.5031, 0.5180, 1.5710, 0.5254, 0.5283, 0.5691,
        0.8693], device='cuda:0')
Index force: tensor([0.5854, 0.8091, 0.6061, 0.6139, 0.6623, 0.5507, 0.6428, 0.7506, 0.5161,
        0.5061], device='cuda:0')
Storing NORMAL transition: reward=-0.0679 (scaled=-0.0679), steps=1
Reward stats updated: mean -0.0076 -> -0.0077, std: 0.1271
Collected 584 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4792, Q2 Loss=1.4792, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0003
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8523, Q2 Loss=0.8523, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7586
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.8163, Q2 Loss=0.8163, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5104
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=1.1845, Q2 Loss=1.1845, Entropy=0.0062, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1265
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0847, Q2 Loss=1.0847, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5545

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (15.6%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.6%)
Actor update: 0.10s (43.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046053
Q1 loss: 1.083400
Q2 loss: 1.083400
Current threshold: -38.0652
Global Scale Offset: 0.0162
Reward stats: mean=-0.0077, std=0.1271, count=584
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0834, Q2 Loss: 1.0834, Entropy: 0.0012, Mean TD Error: 0.7901, Threshold: -38.0652
tensor([-0.0245,  0.5019,  0.4936,  0.7307, -0.1235,  0.3623,  1.0546,  1.1303,
         1.3990,  0.2834,  0.1789,  1.2416, -0.2096,  0.1937, -0.3799,  1.5204],
       device='cuda:0')
Original likelihood: -119.04647827148438
Adjusted likelihood: -119.04647827148438
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 112.32850646972656
Projection step: 1, Loss: 112.29515838623047
Projection step: 2, Loss: 115.47334289550781
Projection step: 3, Loss: 118.35845947265625
Projection step: 4, Loss: 110.84046936035156
Projection step: 5, Loss: 118.9803466796875
Projection step: 6, Loss: 109.96025085449219
Projection step: 7, Loss: 117.79360961914062
Projection step: 8, Loss: 124.16908264160156
Projection step: 9, Loss: 113.9942626953125
Projection step: 10, Loss: 109.37153625488281
Projection step: 11, Loss: 122.46542358398438
Projection step: 12, Loss: 117.78465270996094
Projection step: 13, Loss: 117.397705078125
Projection step: 14, Loss: 111.82854461669922
Projection step: 15, Loss: 111.6298828125
Projection step: 16, Loss: 120.54834747314453
Projection step: 17, Loss: 114.44892883300781
Projection step: 18, Loss: 106.92767333984375
Projection step: 19, Loss: 118.57014465332031
Projection step: 20, Loss: 119.90501403808594
Projection step: 21, Loss: 111.50492095947266
Projection step: 22, Loss: 113.22734069824219
Projection step: 23, Loss: 112.64849853515625
Projection step: 24, Loss: 107.21189880371094
Final likelihood: tensor([ -99.8513, -125.1069, -110.2691, -151.3361, -127.1914, -107.5918,
        -128.8088, -119.0503, -130.3908, -111.2987, -109.5671,  -98.1679,
        -118.9499, -135.9057, -123.4965,  -90.1054])
Final projection likelihood: -117.9430
1 mode projection failed, trying anyway
New goal: tensor([-0.0211,  0.5043,  0.4988,  0.7299, -0.1247,  0.3545,  1.0543,  1.1434,
         1.3948,  0.2856,  0.1814,  1.2360, -0.2083,  0.1939, -0.3195],
       device='cuda:0')
tensor([[0.0168]], device='cuda:0') tensor([[0.0123]], device='cuda:0') tensor([[0.0183]], device='cuda:0')
Original likelihood: -77.77877807617188
Adjusted likelihood: -77.77877807617188
Likelihood residual: 0.0
Original likelihood: -68.47415161132812
Adjusted likelihood: -68.47415161132812
Likelihood residual: 0.0
{'index': 68.47415161132812, 'thumb_middle': 77.77877807617188}
Current yaw: tensor([-0.2096,  0.1937, -0.3799], device='cuda:0')
3 index
tensor([-0.0245,  0.5019,  0.4936,  0.7307, -0.1235,  0.3623,  1.0546,  1.1303,
         1.3990,  0.2834,  0.1789,  1.2416, -0.2096,  0.1937, -0.3799,  1.5204],
       device='cuda:0')
Solve time for step 1 10.551715153036639
Current ori: tensor([-0.2096,  0.1937, -0.3799], device='cuda:0')
Middle force: tensor([0.5766, 0.5006, 0.5367, 0.5312], device='cuda:0')
Thumb force: tensor([0.5373, 0.5154, 0.5861, 0.5619], device='cuda:0')
tensor([-0.0294,  0.4652,  0.4705,  0.7149, -0.1313,  0.3974,  1.0572,  1.1350,
         1.4608,  0.3344,  0.2918,  1.1813, -0.5019,  0.4610, -0.3799,  1.3770],
       device='cuda:0')
Solve time for step 2 4.604774256993551
Current ori: tensor([-0.5019,  0.4610, -0.3799], device='cuda:0')
Middle force: tensor([0.5007, 0.5249, 0.5241], device='cuda:0')
Thumb force: tensor([0.5137, 0.5725, 0.5446], device='cuda:0')
tensor([-0.1275,  0.5514,  0.4970,  0.7142, -0.2435,  0.6930,  1.0723,  1.0317,
         1.5000,  0.4633,  0.1518,  1.3795, -0.9871,  0.8796, -0.3799,  1.9385],
       device='cuda:0')
Solve time for step 3 4.231659089971799
Current ori: tensor([-0.9871,  0.8796, -0.3799], device='cuda:0')
Middle force: tensor([0.5105, 0.5071], device='cuda:0')
Thumb force: tensor([0.5290, 0.5101], device='cuda:0')
tensor([-0.2490,  0.7574,  0.4752,  0.6337, -0.2077,  1.0195,  0.8600,  0.7394,
         1.5000,  0.4607, -0.0635,  1.2183, -1.4313,  1.1584, -0.3799,  2.0548],
       device='cuda:0')
Solve time for step 4 4.092421236040536
Current ori: tensor([-1.4313,  1.1584, -0.3799], device='cuda:0')
Middle force: tensor([0.5072], device='cuda:0')
Thumb force: tensor([0.5328], device='cuda:0')
Storing RECOVERY transition: reward=-1.5516 (scaled=-0.5172), steps=3
Reward stats updated: mean -0.0077 -> -0.0086, std: 0.1287
Collected 585 transitions for RL
SAC Update 1/5: Actor Loss=-0.0000, Q1 Loss=2.1240, Q2 Loss=2.1240, Entropy=0.0062, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9641
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9758, Q2 Loss=0.9758, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5786
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3210, Q2 Loss=1.3210, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3607
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.8385, Q2 Loss=0.8385, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5465
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0493, Q2 Loss=1.0493, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2482

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (20.0%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.04s (18.1%)
Actor update: 0.09s (37.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.3%)
Actor loss: -0.046055
Q1 loss: 1.261693
Q2 loss: 1.261693
Current threshold: -38.0653
Global Scale Offset: 0.0162
Reward stats: mean=-0.0086, std=0.1287, count=585
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.2617, Q2 Loss: 1.2617, Entropy: 0.0012, Mean TD Error: 0.9396, Threshold: -38.0653
Original likelihood: -1289.683837890625
Adjusted likelihood: -1289.683837890625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 37
Loaded trajectory sampler
Current yaw: tensor([-0.0029,  0.0145, -0.0272], device='cuda:0')
Current yaw: tensor([-0.0029,  0.0145, -0.0272], device='cuda:0')
1 turn
Sampling time 3.614299099019263
tensor([ 0.1470,  0.5394,  0.6449,  0.6385, -0.1367,  0.5493,  0.9043,  0.9135,
         1.2490,  0.2426,  0.2596,  1.1732, -0.0029,  0.0145, -0.0272,  0.1902],
       device='cuda:0')
Original likelihood: -23.975215911865234
Adjusted likelihood: -23.975215911865234
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.237170703010634
Current ori: tensor([-0.0029,  0.0145, -0.0272], device='cuda:0')
Middle force: tensor([0.5209, 0.5762, 1.2936, 0.5702, 1.2349, 0.6461, 0.5408, 0.5144, 0.5137,
        0.9283, 0.6934, 0.5451], device='cuda:0')
Thumb force: tensor([0.8943, 0.8863, 0.8215, 1.0476, 0.9925, 0.6567, 0.5185, 0.9126, 0.5342,
        0.5298, 0.6791, 0.6367], device='cuda:0')
Index force: tensor([0.6307, 0.6183, 0.5377, 0.5685, 0.8072, 0.5318, 1.0273, 0.9917, 0.5812,
        0.6054, 0.7587, 0.6401], device='cuda:0')
Storing NORMAL transition: reward=0.0507 (scaled=0.0507), steps=1
Reward stats updated: mean -0.0086 -> -0.0085, std: 0.1286
Collected 586 transitions for RL
SAC Update 1/5: Actor Loss=-0.0574, Q1 Loss=0.8264, Q2 Loss=0.8264, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4155
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8729, Q2 Loss=0.8729, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7496
SAC Update 3/5: Actor Loss=-0.1516, Q1 Loss=1.2474, Q2 Loss=1.2474, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9611
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.8611, Q2 Loss=1.8611, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.5946
SAC Update 5/5: Actor Loss=-0.4548, Q1 Loss=4.1822, Q2 Loss=4.1822, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.5292

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.5%)
Q1 update: 0.04s (19.6%)
Q2 update: 0.04s (18.6%)
Actor update: 0.09s (40.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.132742
Q1 loss: 1.798002
Q2 loss: 1.798002
Current threshold: -38.0653
Global Scale Offset: 0.0162
Reward stats: mean=-0.0085, std=0.1286, count=586
----------------------------------------------
SAC Update - Actor Loss: -0.1327, Q1 Loss: 1.7980, Q2 Loss: 1.7980, Entropy: 0.0000, Mean TD Error: 1.8500, Threshold: -38.0653
tensor([ 0.1474,  0.5783,  0.6003,  0.6239, -0.1437,  0.5579,  0.8866,  0.9545,
         1.2982,  0.2093,  0.2503,  1.0909, -0.0135,  0.0099, -0.0780,  0.2869],
       device='cuda:0')
Original likelihood: -24.118349075317383
Adjusted likelihood: -24.118349075317383
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.5383438499993645
Current ori: tensor([-0.0135,  0.0099, -0.0780], device='cuda:0')
Middle force: tensor([0.5022, 1.2891, 0.5085, 1.0072, 0.5731, 0.5016, 0.5738, 0.5451, 0.5551,
        0.5322, 0.5906], device='cuda:0')
Thumb force: tensor([0.5951, 0.5024, 0.9293, 0.6339, 0.6131, 0.7030, 0.9265, 0.6045, 0.6579,
        0.5470, 0.5192], device='cuda:0')
Index force: tensor([0.6853, 0.5153, 0.8804, 0.5327, 0.5686, 0.7987, 0.5832, 0.5766, 0.5900,
        0.5537, 0.5909], device='cuda:0')
Storing NORMAL transition: reward=-0.0114 (scaled=-0.0114), steps=1
Reward stats updated: mean -0.0085 -> -0.0085, std: 0.1285
Collected 587 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2905, Q2 Loss=1.2905, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0598
SAC Update 2/5: Actor Loss=-0.0460, Q1 Loss=1.0889, Q2 Loss=1.0889, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9640
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3435, Q2 Loss=1.3435, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2192
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1726, Q2 Loss=1.1726, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3805
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7502, Q2 Loss=0.7502, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2615

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.1%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.9%)
Actor update: 0.09s (37.9%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.009202
Q1 loss: 1.129152
Q2 loss: 1.129152
Current threshold: -38.0653
Global Scale Offset: 0.0162
Reward stats: mean=-0.0085, std=0.1285, count=587
----------------------------------------------
SAC Update - Actor Loss: -0.0092, Q1 Loss: 1.1292, Q2 Loss: 1.1292, Entropy: 0.0000, Mean TD Error: 0.9770, Threshold: -38.0653
tensor([ 0.1573,  0.5409,  0.6511,  0.6429, -0.1485,  0.5653,  0.9175,  0.9047,
         1.2563,  0.2695,  0.2521,  1.1255, -0.0038,  0.0061, -0.0664,  0.3122],
       device='cuda:0')
Original likelihood: -24.157238006591797
Adjusted likelihood: -24.157238006591797
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.548363011039328
Current ori: tensor([-0.0038,  0.0061, -0.0664], device='cuda:0')
Middle force: tensor([1.2658, 0.5074, 0.9937, 0.5693, 0.5012, 0.5698, 0.5433, 0.5528, 0.5303,
        0.5874], device='cuda:0')
Thumb force: tensor([0.5017, 0.9177, 0.6256, 0.6080, 0.7214, 0.9139, 0.6010, 0.6528, 0.5451,
        0.5181], device='cuda:0')
Index force: tensor([0.5135, 0.8703, 0.5303, 0.5649, 0.8075, 0.5784, 0.5720, 0.5846, 0.5503,
        0.5863], device='cuda:0')
Storing NORMAL transition: reward=-0.0030 (scaled=-0.0030), steps=1
Reward stats updated: mean -0.0085 -> -0.0085, std: 0.1284
Collected 588 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6797, Q2 Loss=0.6797, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3834
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8689, Q2 Loss=0.8689, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5416
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7991, Q2 Loss=0.7991, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0797
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0523, Q2 Loss=1.0523, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2075
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2087, Q2 Loss=1.2087, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5862

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.1%)
Q1 update: 0.06s (19.9%)
Q2 update: 0.06s (20.0%)
Actor update: 0.12s (42.8%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 0.921743
Q2 loss: 0.921743
Current threshold: -38.0654
Global Scale Offset: 0.0162
Reward stats: mean=-0.0085, std=0.1284, count=588
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 0.9217, Q2 Loss: 0.9217, Entropy: 0.0000, Mean TD Error: 0.9597, Threshold: -38.0654
tensor([ 0.1843,  0.4947,  0.7255,  0.6715, -0.2872,  0.6343,  1.0766,  0.9471,
         1.2834,  0.1660,  0.2607,  1.1157,  0.0115, -0.0065, -0.0635,  0.3291],
       device='cuda:0')
Original likelihood: -41.80864715576172
Adjusted likelihood: -41.80864715576172
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 42.36908721923828
Projection step: 1, Loss: 41.297645568847656
Projection step: 2, Loss: 40.70497512817383
Projection step: 3, Loss: 40.20699691772461
Projection step: 4, Loss: 39.094398498535156
Projection step: 5, Loss: 37.99831771850586
Projection step: 6, Loss: 38.13796615600586
Projection step: 7, Loss: 36.28063201904297
Projection step: 8, Loss: 35.863075256347656
Projection step: 9, Loss: 35.30703353881836
Projection step: 10, Loss: 34.067222595214844
Projection step: 11, Loss: 33.72361755371094
Projection step: 12, Loss: 35.05363082885742
Projection step: 13, Loss: 33.25900650024414
Projection step: 14, Loss: 32.94783020019531
Projection step: 15, Loss: 32.35352325439453
Projection step: 16, Loss: 30.30980110168457
Projection step: 17, Loss: 30.384370803833008
Projection step: 18, Loss: 28.603782653808594
Projection step: 19, Loss: 29.72408676147461
Projection step: 20, Loss: 30.10777473449707
Projection step: 21, Loss: 30.15532875061035
Projection step: 22, Loss: 28.0422420501709
Projection step: 23, Loss: 26.85808563232422
Projection step: 24, Loss: 27.089752197265625
Final likelihood: tensor([-26.6173, -24.7510, -27.7375, -23.7454, -32.0282, -22.5568, -31.4750,
        -27.2500, -27.7773, -27.5975, -25.3778, -27.6470, -22.8295, -25.3580,
        -30.1231, -29.4192])
Final projection likelihood: -27.0182
1 mode projection succeeded
New goal: tensor([ 1.6455e-01,  4.8589e-01,  6.6851e-01,  6.8456e-01, -1.8548e-01,
         6.7954e-01,  1.0640e+00,  8.5706e-01,  1.3379e+00,  2.1111e-01,
         2.1367e-01,  1.0617e+00,  1.0988e-02, -1.2079e-03,  9.1279e-02],
       device='cuda:0')
tensor([[0.0021]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -31.31775665283203
Adjusted likelihood: -31.31775665283203
Likelihood residual: 0.0
Original likelihood: -34.63909149169922
Adjusted likelihood: -34.63909149169922
Likelihood residual: 0.0
{'index': 34.63909149169922, 'thumb_middle': 31.31775665283203}
Current yaw: tensor([ 0.0115, -0.0065, -0.0635], device='cuda:0')
2 thumb_middle
tensor([ 0.1843,  0.4947,  0.7255,  0.6715, -0.2872,  0.6343,  1.0766,  0.9471,
         1.2834,  0.1660,  0.2607,  1.1157,  0.0115, -0.0065, -0.0635,  0.3291],
       device='cuda:0')
Solve time for step 1 8.935475266014691
Current ori: tensor([ 0.0115, -0.0065, -0.0635], device='cuda:0')
Index force: tensor([0.5706, 0.5783, 0.5645, 0.6201], device='cuda:0')
tensor([ 1.7010e-01,  5.1011e-01,  6.8285e-01,  6.8760e-01, -3.3212e-01,
         6.2264e-01,  9.9523e-01,  8.4268e-01,  1.2849e+00,  1.9903e-01,
         1.3830e-01,  1.0315e+00,  7.9376e-03,  1.0977e-03, -6.3564e-02,
         3.1914e-01], device='cuda:0')
Solve time for step 2 3.7356909060035832
Current ori: tensor([ 0.0079,  0.0011, -0.0636], device='cuda:0')
Index force: tensor([0.5688, 0.5567, 0.6097], device='cuda:0')
tensor([ 1.7322e-01,  4.9558e-01,  6.9446e-01,  7.0920e-01, -3.2867e-01,
         6.1793e-01,  1.0008e+00,  8.1986e-01,  1.2963e+00,  1.8368e-01,
         1.2835e-01,  1.0294e+00,  1.3337e-02,  4.8249e-04, -6.3564e-02,
         3.2984e-01], device='cuda:0')
Solve time for step 3 3.5549560260260478
Current ori: tensor([ 0.0133,  0.0005, -0.0636], device='cuda:0')
Index force: tensor([0.5468, 0.5975], device='cuda:0')
tensor([ 0.1507,  0.4759,  0.6848,  0.7350, -0.3337,  0.6175,  0.9912,  0.8108,
         1.3124,  0.1877,  0.1335,  1.0299,  0.0194,  0.0142, -0.0636,  0.3099],
       device='cuda:0')
Solve time for step 4 3.394957096024882
Current ori: tensor([ 0.0194,  0.0142, -0.0636], device='cuda:0')
Index force: tensor([0.5946], device='cuda:0')
Storing RECOVERY transition: reward=0.0150 (scaled=0.0050), steps=3
Reward stats updated: mean -0.0085 -> -0.0084, std: 0.1283
Collected 589 transitions for RL
SAC Update 1/5: Actor Loss=-0.1042, Q1 Loss=0.9802, Q2 Loss=0.9802, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5021
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.7567, Q2 Loss=1.7567, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0027
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0827, Q2 Loss=1.0827, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1051
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.2086, Q2 Loss=1.2086, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4557
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3865, Q2 Loss=1.3865, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8019

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.0%)
Q1 update: 0.06s (20.5%)
Q2 update: 0.06s (19.5%)
Actor update: 0.12s (41.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.158990
Q1 loss: 1.282948
Q2 loss: 1.282948
Current threshold: -38.0654
Global Scale Offset: 0.0162
Reward stats: mean=-0.0084, std=0.1283, count=589
----------------------------------------------
SAC Update - Actor Loss: -0.1590, Q1 Loss: 1.2829, Q2 Loss: 1.2829, Entropy: 0.0000, Mean TD Error: 0.9735, Threshold: -38.0654
Original likelihood: -36.45348358154297
Adjusted likelihood: -36.45348358154297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([ 1.4725e-02,  6.0359e-06, -7.8563e-02], device='cuda:0')
3 turn
Sampling time 3.6094016970018856
tensor([ 1.7379e-01,  4.9968e-01,  6.8151e-01,  7.2617e-01, -2.7064e-01,
         6.6996e-01,  1.0288e+00,  8.2265e-01,  1.3663e+00,  2.1800e-01,
         1.7077e-01,  1.0549e+00,  1.4725e-02,  6.0359e-06, -7.8563e-02,
         3.3304e-01], device='cuda:0')
Original likelihood: -35.74059295654297
Adjusted likelihood: -35.74059295654297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.123723403026816
Current ori: tensor([ 1.4725e-02,  6.0359e-06, -7.8563e-02], device='cuda:0')
Middle force: tensor([2.5800, 1.9612, 1.0959, 0.5102, 0.5096, 0.5174, 0.8866, 0.9564, 0.6240,
        0.5002, 0.6027, 0.5865], device='cuda:0')
Thumb force: tensor([0.9264, 2.5321, 0.8846, 0.5555, 2.3926, 0.5914, 0.5813, 0.5643, 1.0300,
        0.5653, 0.5787, 0.6123], device='cuda:0')
Index force: tensor([1.3858, 0.5273, 1.0515, 0.6048, 0.7225, 0.5306, 0.5287, 0.5937, 0.5565,
        0.5455, 0.6059, 0.6393], device='cuda:0')
Storing NORMAL transition: reward=0.0949 (scaled=0.0949), steps=1
Reward stats updated: mean -0.0084 -> -0.0083, std: 0.1282
Collected 590 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.5081, Q2 Loss=1.5081, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3919
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.8618, Q2 Loss=0.8618, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7040
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0917, Q2 Loss=1.0917, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2337
SAC Update 4/5: Actor Loss=-0.1052, Q1 Loss=3.2002, Q2 Loss=3.2002, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.8759
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=3.7422, Q2 Loss=3.7422, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.1971

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (13.9%)
Q1 update: 0.06s (20.1%)
Q2 update: 0.06s (20.0%)
Actor update: 0.12s (43.0%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.113150
Q1 loss: 2.080799
Q2 loss: 2.080799
Current threshold: -38.0654
Global Scale Offset: 0.0162
Reward stats: mean=-0.0083, std=0.1282, count=590
----------------------------------------------
SAC Update - Actor Loss: -0.1131, Q1 Loss: 2.0808, Q2 Loss: 2.0808, Entropy: 0.0000, Mean TD Error: 3.2805, Threshold: -38.0654
tensor([ 0.1460,  0.4495,  0.6825,  0.8028, -0.2061,  0.6905,  0.9884,  0.9734,
         1.2330,  0.3957,  0.2174,  1.0139,  0.0244, -0.0524, -0.1771, -0.8603],
       device='cuda:0')
Original likelihood: -37.26708221435547
Adjusted likelihood: -37.26708221435547
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.61708922800608
Current ori: tensor([ 0.0244, -0.0524, -0.1771], device='cuda:0')
Middle force: tensor([1.9297, 1.1782, 0.5327, 0.5175, 0.5342, 0.8898, 1.0068, 0.6522, 0.5014,
        0.6386, 0.5847], device='cuda:0')
Thumb force: tensor([2.4590, 0.7627, 0.5220, 2.3138, 0.5554, 0.5728, 0.5354, 0.9849, 0.5322,
        0.5522, 0.6084], device='cuda:0')
Index force: tensor([0.5268, 1.0482, 0.5838, 0.6816, 0.5244, 0.5265, 0.6031, 0.5474, 0.5415,
        0.5986, 0.6334], device='cuda:0')
Storing NORMAL transition: reward=0.0227 (scaled=0.0227), steps=1
Reward stats updated: mean -0.0083 -> -0.0082, std: 0.1281
Collected 591 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0121, Q2 Loss=1.0121, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7541
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0905, Q2 Loss=1.0905, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=6.7467
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0468, Q2 Loss=1.0468, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7899
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8731, Q2 Loss=0.8731, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6597
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3225, Q2 Loss=1.3225, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8857

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.9%)
Q1 update: 0.04s (19.4%)
Q2 update: 0.04s (19.6%)
Actor update: 0.09s (40.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.069012
Q2 loss: 1.069012
Current threshold: -38.0654
Global Scale Offset: 0.0162
Reward stats: mean=-0.0082, std=0.1281, count=591
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0690, Q2 Loss: 1.0690, Entropy: 0.0000, Mean TD Error: 2.3672, Threshold: -38.0654
tensor([ 0.1492,  0.3894,  0.6744,  0.6705, -0.2145,  0.6756,  0.9770,  0.9685,
         1.2474,  0.4249,  0.2358,  0.9549,  0.0480, -0.0724, -0.2053, -1.3604],
       device='cuda:0')
Original likelihood: -45.726322174072266
Adjusted likelihood: -45.726322174072266
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 44.99513626098633
Projection step: 1, Loss: 42.66704559326172
Projection step: 2, Loss: 42.391883850097656
Projection step: 3, Loss: 43.51587677001953
Projection step: 4, Loss: 40.671417236328125
Projection step: 5, Loss: 39.852725982666016
Projection step: 6, Loss: 40.22488021850586
Projection step: 7, Loss: 39.05492401123047
Projection step: 8, Loss: 39.189720153808594
Projection step: 9, Loss: 39.50597381591797
Projection step: 10, Loss: 37.43560791015625
Projection step: 11, Loss: 38.26747512817383
Projection step: 12, Loss: 37.633182525634766
Projection step: 13, Loss: 36.73563766479492
Projection step: 14, Loss: 36.63075637817383
Projection step: 15, Loss: 35.950469970703125
Projection step: 16, Loss: 35.40503692626953
Projection step: 17, Loss: 36.537166595458984
Projection step: 18, Loss: 36.23206329345703
Projection step: 19, Loss: 35.08325958251953
Projection step: 20, Loss: 33.9240608215332
Projection step: 21, Loss: 34.146263122558594
Projection step: 22, Loss: 34.76374435424805
Projection step: 23, Loss: 34.62989044189453
Projection step: 24, Loss: 32.804527282714844
Final likelihood: tensor([-38.8597, -31.1596, -35.5521, -39.0023, -49.0750, -31.3621, -33.6779,
        -32.9018, -32.5025, -32.2480, -32.1777, -31.3813, -39.4981, -31.1676,
        -31.6867, -34.3110])
Final projection likelihood: -34.7852
1 mode projection succeeded
New goal: tensor([ 0.1285,  0.4227,  0.7373,  0.7885, -0.1503,  0.6636,  0.9953,  0.9080,
         1.2931,  0.3088,  0.2082,  0.9653,  0.0447, -0.0662, -0.0980],
       device='cuda:0')
tensor([[0.0019]], device='cuda:0') tensor([[0.0070]], device='cuda:0') tensor([[0.0106]], device='cuda:0')
Original likelihood: -36.034881591796875
Adjusted likelihood: -36.034881591796875
Likelihood residual: 0.0
Original likelihood: -36.20872116088867
Adjusted likelihood: -36.20872116088867
Likelihood residual: 0.0
{'index': 36.20872116088867, 'thumb_middle': 36.034881591796875}
Current yaw: tensor([ 0.0480, -0.0724, -0.2053], device='cuda:0')
4 thumb_middle
tensor([ 0.1492,  0.3894,  0.6744,  0.6705, -0.2145,  0.6756,  0.9770,  0.9685,
         1.2474,  0.4249,  0.2358,  0.9549,  0.0480, -0.0724, -0.2053, -1.3604],
       device='cuda:0')
Solve time for step 1 9.158122067980003
Current ori: tensor([ 0.0480, -0.0724, -0.2053], device='cuda:0')
Index force: tensor([0.5922, 0.6103, 0.5819, 0.5930], device='cuda:0')
tensor([ 0.1038,  0.4289,  0.7285,  0.7674, -0.2268,  0.6508,  0.9577,  0.8964,
         1.2497,  0.3213,  0.1258,  0.9219,  0.1208, -0.1822, -0.2054, -1.9659],
       device='cuda:0')
Solve time for step 2 3.6026068469509482
Current ori: tensor([ 0.1208, -0.1822, -0.2054], device='cuda:0')
Index force: tensor([0.5985, 0.5746, 0.5842], device='cuda:0')
tensor([ 0.0429,  0.4869,  0.7905,  0.7986, -0.1718,  0.6915,  0.9960,  0.9048,
         1.2465,  0.3099,  0.0884,  0.9112,  0.2530, -0.3797, -0.2367, -2.2626],
       device='cuda:0')
Solve time for step 3 3.523854438040871
Current ori: tensor([ 0.2530, -0.3797, -0.2367], device='cuda:0')
Index force: tensor([0.5878, 0.5631], device='cuda:0')
tensor([ 0.0335,  0.7580,  1.0661,  0.9453, -0.0520,  0.7677,  1.0397,  0.9009,
         1.2250,  0.3117,  0.0379,  0.8926,  0.2758, -0.3943, -0.3539, -1.8619],
       device='cuda:0')
Solve time for step 4 3.4020080189802684
Current ori: tensor([ 0.2758, -0.3943, -0.3539], device='cuda:0')
Index force: tensor([0.5151], device='cuda:0')
Storing RECOVERY transition: reward=-0.0892 (scaled=-0.0446), steps=2
Reward stats updated: mean -0.0082 -> -0.0083, std: 0.1280
Collected 592 transitions for RL
SAC Update 1/5: Actor Loss=-0.1495, Q1 Loss=1.1353, Q2 Loss=1.1353, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5659
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=1.3227, Q2 Loss=1.3227, Entropy=0.0063, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9770
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7613, Q2 Loss=0.7613, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3202
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5652, Q2 Loss=1.5652, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3116
SAC Update 5/5: Actor Loss=-0.2246, Q1 Loss=1.3595, Q2 Loss=1.3595, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5487

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.7%)
Q1 update: 0.05s (18.9%)
Q2 update: 0.05s (17.6%)
Actor update: 0.10s (40.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.074819
Q1 loss: 1.228802
Q2 loss: 1.228802
Current threshold: -38.0654
Global Scale Offset: 0.0162
Reward stats: mean=-0.0083, std=0.1280, count=592
----------------------------------------------
SAC Update - Actor Loss: -0.0748, Q1 Loss: 1.2288, Q2 Loss: 1.2288, Entropy: 0.0013, Mean TD Error: 0.7447, Threshold: -38.0654
Original likelihood: -356.4039611816406
Adjusted likelihood: -356.4039611816406
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 38
Loaded trajectory sampler
Current yaw: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
Current yaw: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
1 turn
Sampling time 3.8278369750478305
tensor([ 1.3765e-01,  5.6035e-01,  6.1055e-01,  6.3259e-01, -1.2318e-01,
         5.2382e-01,  9.4887e-01,  8.3885e-01,  1.2292e+00,  3.4374e-01,
         2.2342e-01,  1.2037e+00, -9.3357e-04,  1.4822e-02, -2.8876e-02,
         2.6114e-01], device='cuda:0')
Original likelihood: -20.5035457611084
Adjusted likelihood: -20.5035457611084
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.867422738985624
Current ori: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
Middle force: tensor([0.5454, 0.5176, 1.5655, 0.6081, 0.5917, 0.5538, 0.5033, 0.5612, 0.5404,
        0.5909, 0.5340, 0.5040], device='cuda:0')
Thumb force: tensor([0.7028, 0.8751, 1.8135, 0.7522, 1.0029, 0.5729, 0.5807, 1.0482, 1.0792,
        0.6041, 0.6640, 0.6527], device='cuda:0')
Index force: tensor([0.5569, 0.8788, 0.5809, 0.5477, 0.6095, 0.5855, 0.6428, 0.5655, 0.5317,
        0.5951, 0.5501, 0.5998], device='cuda:0')
Storing NORMAL transition: reward=0.1148 (scaled=0.1148), steps=1
Reward stats updated: mean -0.0083 -> -0.0081, std: 0.1280
Collected 593 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1300, Q2 Loss=1.1300, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2797
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0279, Q2 Loss=1.0279, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8391
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8764, Q2 Loss=0.8764, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0928
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1782, Q2 Loss=1.1782, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1651
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7596, Q2 Loss=0.7596, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3792

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.0%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (20.0%)
Actor update: 0.11s (42.3%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 0.994416
Q2 loss: 0.994416
Current threshold: -38.0654
Global Scale Offset: 0.0162
Reward stats: mean=-0.0081, std=0.1280, count=593
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 0.9944, Q2 Loss: 0.9944, Entropy: 0.0000, Mean TD Error: 0.9512, Threshold: -38.0654
tensor([ 0.1560,  0.6364,  0.6012,  0.4844, -0.0963,  0.4843,  1.0372,  0.7930,
         1.3265,  0.2953,  0.0867,  1.1190, -0.0248, -0.0054, -0.1443,  0.3309],
       device='cuda:0')
Original likelihood: -23.066497802734375
Adjusted likelihood: -23.066497802734375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.625954849005211
Current ori: tensor([-0.0248, -0.0054, -0.1443], device='cuda:0')
Middle force: tensor([0.5733, 1.2131, 0.5737, 1.2045, 0.6371, 0.5445, 0.5201, 0.5619, 0.5974,
        0.5895, 0.5566], device='cuda:0')
Thumb force: tensor([0.8455, 0.7445, 1.0046, 0.9610, 0.6325, 0.5220, 0.8952, 0.6238, 0.5965,
        0.5898, 0.9868], device='cuda:0')
Index force: tensor([0.5979, 0.5531, 0.5749, 0.8139, 0.5432, 0.9725, 0.9042, 0.5846, 0.5959,
        0.5785, 0.5532], device='cuda:0')
Storing NORMAL transition: reward=-0.0002 (scaled=-0.0002), steps=1
Reward stats updated: mean -0.0081 -> -0.0080, std: 0.1279
Collected 594 transitions for RL
SAC Update 1/5: Actor Loss=-0.0038, Q1 Loss=0.7265, Q2 Loss=0.7265, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9020
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.5974, Q2 Loss=2.5974, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.0040
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2685, Q2 Loss=1.2685, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9121
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2367, Q2 Loss=1.2367, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2225
SAC Update 5/5: Actor Loss=-0.0406, Q1 Loss=1.0846, Q2 Loss=1.0846, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1036

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (14.9%)
Q1 update: 0.05s (20.5%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (41.9%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.054932
Q1 loss: 1.382761
Q2 loss: 1.382761
Current threshold: -38.0655
Global Scale Offset: 0.0162
Reward stats: mean=-0.0080, std=0.1279, count=594
----------------------------------------------
SAC Update - Actor Loss: -0.0549, Q1 Loss: 1.3828, Q2 Loss: 1.3828, Entropy: 0.0000, Mean TD Error: 2.8289, Threshold: -38.0655
tensor([ 0.1388,  0.6453,  0.5383,  0.5508, -0.1506,  0.5374,  0.9613,  0.8731,
         1.4208,  0.3252,  0.0735,  0.8923, -0.0230,  0.0064, -0.1441,  0.3448],
       device='cuda:0')
Original likelihood: -25.497665405273438
Adjusted likelihood: -25.497665405273438
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.079095263965428
Current ori: tensor([-0.0230,  0.0064, -0.1441], device='cuda:0')
Middle force: tensor([0.5589, 0.5183, 0.5217, 0.6001, 0.5029, 0.6740, 0.5552, 0.6820, 0.5220,
        0.5009], device='cuda:0')
Thumb force: tensor([0.5861, 0.7697, 0.5582, 0.8910, 0.7548, 0.8749, 0.5877, 0.5914, 0.6041,
        0.5791], device='cuda:0')
Index force: tensor([0.5994, 0.7636, 0.5663, 0.5733, 0.5037, 0.5088, 0.5626, 0.6841, 0.5693,
        0.5495], device='cuda:0')
Storing NORMAL transition: reward=-0.0366 (scaled=-0.0366), steps=1
Reward stats updated: mean -0.0080 -> -0.0081, std: 0.1278
Collected 595 transitions for RL
SAC Update 1/5: Actor Loss=-0.0393, Q1 Loss=0.7679, Q2 Loss=0.7679, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2167
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2714, Q2 Loss=1.2714, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4631
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9304, Q2 Loss=0.9304, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7905
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0129, Q2 Loss=1.0129, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7479
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9718, Q2 Loss=0.9718, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7988

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.0%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (41.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.007855
Q1 loss: 0.990876
Q2 loss: 0.990876
Current threshold: -38.0655
Global Scale Offset: 0.0162
Reward stats: mean=-0.0081, std=0.1278, count=595
----------------------------------------------
SAC Update - Actor Loss: -0.0079, Q1 Loss: 0.9909, Q2 Loss: 0.9909, Entropy: 0.0000, Mean TD Error: 0.8034, Threshold: -38.0655
tensor([ 0.1085,  0.5348,  0.6738,  0.5145, -0.2192,  0.5215,  0.9839,  0.8881,
         1.4791,  0.2892,  0.1365,  0.9216,  0.0044,  0.0407, -0.1087, -0.0075],
       device='cuda:0')
Original likelihood: -38.459197998046875
Adjusted likelihood: -38.459197998046875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 38.30187225341797
Projection step: 1, Loss: 37.35014343261719
Projection step: 2, Loss: 36.48313903808594
Projection step: 3, Loss: 34.9300537109375
Projection step: 4, Loss: 34.73114776611328
Projection step: 5, Loss: 33.538970947265625
Projection step: 6, Loss: 33.56446075439453
Projection step: 7, Loss: 32.285438537597656
Projection step: 8, Loss: 31.733264923095703
Projection step: 9, Loss: 31.42011833190918
Projection step: 10, Loss: 29.802249908447266
Projection step: 11, Loss: 29.901887893676758
Projection step: 12, Loss: 27.687305450439453
Projection step: 13, Loss: 27.279443740844727
Projection step: 14, Loss: 26.651567459106445
Projection step: 15, Loss: 27.332786560058594
Projection step: 16, Loss: 26.76477813720703
Projection step: 17, Loss: 26.206300735473633
Projection step: 18, Loss: 24.84475326538086
Projection step: 19, Loss: 23.78033447265625
Projection step: 20, Loss: 23.987449645996094
Projection step: 21, Loss: 22.944072723388672
Projection step: 22, Loss: 22.90825843811035
Projection step: 23, Loss: 21.362548828125
Projection step: 24, Loss: 21.96453857421875
Final likelihood: tensor([-21.5011, -20.0454, -18.2214, -24.0664, -21.1479, -20.8279, -25.1307,
        -20.5233, -21.0246, -23.6263, -17.9090, -20.1063, -20.4090, -20.3538,
        -25.0424, -22.7290])
Final projection likelihood: -21.4165
1 mode projection succeeded
New goal: tensor([ 0.1016,  0.5239,  0.6196,  0.6334, -0.1362,  0.5681,  0.9227,  0.8352,
         1.4708,  0.3137,  0.1678,  1.0985,  0.0020,  0.0275,  0.1606],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0026]], device='cuda:0') tensor([[0.0028]], device='cuda:0')
Original likelihood: -24.72783088684082
Adjusted likelihood: -24.72783088684082
Likelihood residual: 0.0
Original likelihood: -21.617549896240234
Adjusted likelihood: -21.617549896240234
Likelihood residual: 0.0
{'index': 21.617549896240234, 'thumb_middle': 24.72783088684082}
Current yaw: tensor([ 0.0044,  0.0407, -0.1087], device='cuda:0')
2 index
tensor([ 0.1085,  0.5348,  0.6738,  0.5145, -0.2192,  0.5215,  0.9839,  0.8881,
         1.4791,  0.2892,  0.1365,  0.9216,  0.0044,  0.0407, -0.1087, -0.0075],
       device='cuda:0')
Solve time for step 1 10.317419158993289
Current ori: tensor([ 0.0044,  0.0407, -0.1087], device='cuda:0')
Middle force: tensor([0.5732, 0.6094, 0.5267, 0.5589], device='cuda:0')
Thumb force: tensor([0.5212, 0.5477, 0.5373, 0.6320], device='cuda:0')
tensor([ 0.1476,  0.4742,  0.5795,  0.5897, -0.2082,  0.5319,  0.9740,  0.8961,
         1.4778,  0.2786,  0.1114,  0.9561,  0.0054,  0.0329, -0.0905, -1.4408],
       device='cuda:0')
Solve time for step 2 4.320124064979609
Current ori: tensor([ 0.0054,  0.0329, -0.0905], device='cuda:0')
Middle force: tensor([0.6032, 0.5248, 0.5555], device='cuda:0')
Thumb force: tensor([0.5432, 0.5353, 0.6286], device='cuda:0')
tensor([ 0.1500,  0.4739,  0.5751,  0.6041, -0.1767,  0.5524,  0.9841,  0.8971,
         1.4556,  0.3029,  0.0731,  1.0006,  0.0028,  0.0061, -0.0972, -2.1936],
       device='cuda:0')
Solve time for step 3 4.017948029038962
Current ori: tensor([ 0.0028,  0.0061, -0.0972], device='cuda:0')
Middle force: tensor([0.5996, 0.5269], device='cuda:0')
Thumb force: tensor([0.5468, 0.5416], device='cuda:0')
tensor([ 1.5487e-01,  4.7592e-01,  5.7454e-01,  6.0766e-01, -1.7709e-01,
         5.6012e-01,  9.7491e-01,  8.8769e-01,  1.4594e+00,  2.9737e-01,
         7.3753e-02,  9.9071e-01, -1.3792e-03,  7.5444e-03, -9.3251e-02,
        -2.6247e+00], device='cuda:0')
Solve time for step 4 4.015507134026848
Current ori: tensor([-0.0014,  0.0075, -0.0933], device='cuda:0')
Middle force: tensor([0.5001], device='cuda:0')
Thumb force: tensor([0.5157], device='cuda:0')
Storing RECOVERY transition: reward=-0.0179 (scaled=-0.0060), steps=3
Reward stats updated: mean -0.0081 -> -0.0081, std: 0.1277
Collected 596 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9517, Q2 Loss=0.9517, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8894
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.7637, Q2 Loss=0.7637, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5138
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2293, Q2 Loss=1.2293, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9962
SAC Update 4/5: Actor Loss=-0.1426, Q1 Loss=1.0599, Q2 Loss=1.0599, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1392
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8590, Q2 Loss=0.8590, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0538

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.1%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (19.9%)
Actor update: 0.09s (39.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.074566
Q1 loss: 0.972722
Q2 loss: 0.972722
Current threshold: -38.0655
Global Scale Offset: 0.0162
Reward stats: mean=-0.0081, std=0.1277, count=596
----------------------------------------------
SAC Update - Actor Loss: -0.0746, Q1 Loss: 0.9727, Q2 Loss: 0.9727, Entropy: 0.0000, Mean TD Error: 0.7185, Threshold: -38.0655
Original likelihood: -24.666244506835938
Adjusted likelihood: -24.666244506835938
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0030,  0.0085, -0.0890], device='cuda:0')
3 turn
Sampling time 3.5783711170079187
tensor([ 0.1028,  0.5328,  0.6153,  0.6304, -0.1850,  0.5686,  0.9745,  0.8848,
         1.4546,  0.3196,  0.0667,  1.0032, -0.0030,  0.0085, -0.0890, -2.7236],
       device='cuda:0')
Original likelihood: -24.93760871887207
Adjusted likelihood: -24.93760871887207
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.868991445982829
Current ori: tensor([-0.0030,  0.0085, -0.0890], device='cuda:0')
Middle force: tensor([1.2451, 0.5104, 0.5068, 0.5130, 0.6070, 0.6144, 1.0655, 0.8030, 0.8098,
        0.6279, 0.5308, 0.5098], device='cuda:0')
Thumb force: tensor([1.8854, 1.9614, 1.4219, 0.5712, 1.1002, 0.7891, 1.4778, 0.5942, 0.7124,
        0.6787, 0.5766, 0.5482], device='cuda:0')
Index force: tensor([0.5678, 0.8314, 0.7317, 0.6517, 0.5671, 0.5485, 0.5831, 0.5222, 0.5863,
        0.5665, 0.6931, 0.7185], device='cuda:0')
Storing NORMAL transition: reward=0.0210 (scaled=0.0210), steps=1
Reward stats updated: mean -0.0081 -> -0.0080, std: 0.1276
Collected 597 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4114, Q2 Loss=1.4114, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9556
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1102, Q2 Loss=1.1102, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7382
SAC Update 3/5: Actor Loss=-0.3361, Q1 Loss=1.5820, Q2 Loss=1.5820, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5530
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2537, Q2 Loss=1.2537, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6620
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.9307, Q2 Loss=1.9307, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6367

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (19.2%)
Actor update: 0.09s (38.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.067220
Q1 loss: 1.457583
Q2 loss: 1.457583
Current threshold: -38.0902
Global Scale Offset: 0.0160
Reward stats: mean=-0.0080, std=0.1276, count=597
----------------------------------------------
SAC Update - Actor Loss: -0.0672, Q1 Loss: 1.4576, Q2 Loss: 1.4576, Entropy: 0.0000, Mean TD Error: 1.1091, Threshold: -38.0902
tensor([ 0.0994,  0.5608,  0.5813,  0.6167, -0.2474,  0.6215,  0.9780,  0.9499,
         1.4711,  0.2939,  0.0671,  0.9709, -0.0106,  0.0094, -0.1101, -2.7201],
       device='cuda:0')
Original likelihood: -31.20282554626465
Adjusted likelihood: -31.20282554626465
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.57504023303045
Current ori: tensor([-0.0106,  0.0094, -0.1101], device='cuda:0')
Middle force: tensor([0.5036, 0.7733, 0.5264, 1.2153, 0.6316, 1.0170, 0.5060, 0.6926, 0.5005,
        0.5984, 0.6056], device='cuda:0')
Thumb force: tensor([0.5361, 2.0954, 0.5501, 0.5648, 0.6332, 0.5149, 0.5658, 0.9893, 0.5652,
        0.6226, 0.7111], device='cuda:0')
Index force: tensor([0.6953, 0.6569, 0.5626, 0.7519, 0.5462, 0.5153, 0.6342, 0.5585, 0.5811,
        0.5938, 0.5028], device='cuda:0')
Storing NORMAL transition: reward=-0.0787 (scaled=-0.0787), steps=1
Reward stats updated: mean -0.0080 -> -0.0082, std: 0.1275
Collected 598 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5214, Q2 Loss=1.5214, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0121
SAC Update 2/5: Actor Loss=-0.1452, Q1 Loss=2.0858, Q2 Loss=2.0858, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3782
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.1358, Q2 Loss=2.1358, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9304
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.6397, Q2 Loss=2.6397, Entropy=0.0002, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9411
SAC Update 5/5: Actor Loss=-0.0517, Q1 Loss=1.7386, Q2 Loss=1.7386, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.2774

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.4%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.5%)
Actor update: 0.08s (39.8%)
Target update: 0.00s (1.9%)
Priority update: 0.00s (0.3%)
Actor loss: -0.085432
Q1 loss: 2.024244
Q2 loss: 2.024244
Current threshold: -38.1176
Global Scale Offset: 0.0158
Reward stats: mean=-0.0082, std=0.1275, count=598
----------------------------------------------
SAC Update - Actor Loss: -0.0854, Q1 Loss: 2.0242, Q2 Loss: 2.0242, Entropy: 0.0000, Mean TD Error: 2.1078, Threshold: -38.1176
tensor([ 8.2666e-02,  5.0578e-01,  6.2585e-01,  6.2393e-01, -1.6502e-01,
         6.5841e-01,  1.0160e+00,  9.0444e-01,  1.3782e+00,  2.5740e-01,
        -1.6824e-02,  1.0263e+00, -1.7284e-03, -5.8430e-02, -3.4196e-02,
        -2.4375e+00], device='cuda:0')
Original likelihood: -28.372974395751953
Adjusted likelihood: -28.372974395751953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.202800525003113
Current ori: tensor([-0.0017, -0.0584, -0.0342], device='cuda:0')
Middle force: tensor([0.7902, 0.5292, 1.2096, 0.6337, 1.0146, 0.5204, 0.7263, 0.5030, 0.6337,
        0.6745], device='cuda:0')
Thumb force: tensor([2.0212, 0.5436, 0.5577, 0.6215, 0.5127, 0.5339, 0.9423, 0.5349, 0.5922,
        0.6653], device='cuda:0')
Index force: tensor([0.6401, 0.5561, 0.7432, 0.5436, 0.5137, 0.5877, 0.5514, 0.5329, 0.5813,
        0.5015], device='cuda:0')
Storing NORMAL transition: reward=-0.0650 (scaled=-0.0650), steps=1
Reward stats updated: mean -0.0082 -> -0.0083, std: 0.1275
Collected 599 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9104, Q2 Loss=0.9104, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1972
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9721, Q2 Loss=0.9721, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2756
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9805, Q2 Loss=0.9805, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4995
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4908, Q2 Loss=1.4908, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2587
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.0657, Q2 Loss=1.0657, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3220

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.5%)
Q1 update: 0.05s (20.7%)
Q2 update: 0.05s (18.9%)
Actor update: 0.10s (41.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.083893
Q2 loss: 1.083893
Current threshold: -38.1338
Global Scale Offset: 0.0157
Reward stats: mean=-0.0083, std=0.1275, count=599
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.0839, Q2 Loss: 1.0839, Entropy: 0.0000, Mean TD Error: 0.7106, Threshold: -38.1338
tensor([-7.6150e-03,  5.2950e-01,  5.3900e-01,  6.1817e-01, -1.5752e-01,
         7.0790e-01,  9.6203e-01,  9.2588e-01,  1.3789e+00,  3.3596e-01,
        -1.4906e-03,  1.0964e+00, -7.4391e-04, -7.1311e-02,  2.9810e-02,
        -1.0915e+00], device='cuda:0')
Original likelihood: -37.99799728393555
Adjusted likelihood: -37.99799728393555
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=0.9427)
Solve time for step 4 4.907334471005015
Current ori: tensor([-0.0007, -0.0713,  0.0298], device='cuda:0')
Middle force: tensor([0.7062, 0.5013, 1.0048, 0.8245, 0.5207, 0.5130, 0.5034, 0.6193, 1.2446],
       device='cuda:0')
Thumb force: tensor([0.5448, 0.5626, 0.6775, 0.7079, 0.5178, 0.5326, 0.5053, 0.5337, 0.9687],
       device='cuda:0')
Index force: tensor([0.9067, 0.9556, 0.5186, 0.5195, 0.5794, 0.5813, 0.6762, 0.5348, 0.5481],
       device='cuda:0')
Storing NORMAL transition: reward=0.0686 (scaled=0.0686), steps=1
Reward stats updated: mean -0.0083 -> -0.0081, std: 0.1274
Collected 600 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8438, Q2 Loss=0.8438, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9500
SAC Update 2/5: Actor Loss=-0.1295, Q1 Loss=1.0646, Q2 Loss=1.0646, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5181
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1424, Q2 Loss=1.1424, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5913
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1010, Q2 Loss=1.1010, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2481
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7576, Q2 Loss=0.7576, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4469

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.1%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (39.8%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.025899
Q1 loss: 0.981878
Q2 loss: 0.981878
Current threshold: -38.1433
Global Scale Offset: 0.0156
Reward stats: mean=-0.0081, std=0.1274, count=600
----------------------------------------------
SAC Update - Actor Loss: -0.0259, Q1 Loss: 0.9819, Q2 Loss: 0.9819, Entropy: 0.0000, Mean TD Error: 0.7509, Threshold: -38.1433
tensor([ 0.0321,  0.5297,  0.5360,  0.6321, -0.1977,  0.7459,  0.9241,  0.8503,
         1.4683,  0.4030,  0.0273,  0.8680, -0.0318, -0.0430, -0.0373, -1.5794],
       device='cuda:0')
Original likelihood: -30.93585205078125
Adjusted likelihood: -30.93585205078125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.646496609027963
Current ori: tensor([-0.0318, -0.0430, -0.0373], device='cuda:0')
Middle force: tensor([0.5957, 0.5864, 0.7845, 0.5024, 0.5221, 0.5536, 0.5009, 0.5548],
       device='cuda:0')
Thumb force: tensor([1.7978, 0.5468, 0.5329, 0.5475, 0.6476, 0.5561, 0.5807, 0.5427],
       device='cuda:0')
Index force: tensor([0.5713, 0.5017, 0.6341, 0.6641, 0.5479, 0.5328, 0.7355, 0.5623],
       device='cuda:0')
Storing NORMAL transition: reward=0.0030 (scaled=0.0030), steps=1
Reward stats updated: mean -0.0081 -> -0.0081, std: 0.1273
Collected 601 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7948, Q2 Loss=0.7948, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6146
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1707, Q2 Loss=1.1707, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3678
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8297, Q2 Loss=0.8297, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7041
SAC Update 4/5: Actor Loss=-0.1181, Q1 Loss=1.0509, Q2 Loss=1.0509, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6457
SAC Update 5/5: Actor Loss=-0.0805, Q1 Loss=1.4767, Q2 Loss=1.4767, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1974

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (17.7%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (18.7%)
Actor update: 0.11s (40.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.039723
Q1 loss: 1.064553
Q2 loss: 1.064553
Current threshold: -38.1490
Global Scale Offset: 0.0156
Reward stats: mean=-0.0081, std=0.1273, count=601
----------------------------------------------
SAC Update - Actor Loss: -0.0397, Q1 Loss: 1.0646, Q2 Loss: 1.0646, Entropy: 0.0000, Mean TD Error: 0.9059, Threshold: -38.1490
tensor([-0.1020,  0.4753,  0.5096,  0.7363, -0.2841,  0.8020,  0.9031,  0.9845,
         1.4524,  0.4239,  0.0291,  0.9556,  0.0059, -0.0392, -0.0392, -2.1347],
       device='cuda:0')
Original likelihood: -42.55535125732422
Adjusted likelihood: -42.55535125732422
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 43.8224983215332
Projection step: 1, Loss: 43.11220932006836
Projection step: 2, Loss: 43.631507873535156
Projection step: 3, Loss: 42.95578384399414
Projection step: 4, Loss: 42.50632095336914
Projection step: 5, Loss: 40.36655044555664
Projection step: 6, Loss: 39.83672332763672
Projection step: 7, Loss: 43.28352737426758
Projection step: 8, Loss: 41.02873611450195
Projection step: 9, Loss: 38.563819885253906
Projection step: 10, Loss: 41.28480911254883
Projection step: 11, Loss: 39.02931594848633
Projection step: 12, Loss: 39.88599395751953
Projection step: 13, Loss: 38.70998001098633
Projection step: 14, Loss: 37.17063903808594
Projection step: 15, Loss: 36.40301513671875
Projection step: 16, Loss: 36.18760681152344
Projection step: 17, Loss: 37.278175354003906
Projection step: 18, Loss: 34.98961639404297
Projection step: 19, Loss: 34.68004608154297
Projection step: 20, Loss: 33.574134826660156
Projection step: 21, Loss: 34.465091705322266
Projection step: 22, Loss: 34.63412857055664
Projection step: 23, Loss: 34.4101676940918
Projection step: 24, Loss: 34.448822021484375
Final likelihood: tensor([-32.2030, -34.6945, -39.7601, -32.5757, -34.6865, -37.6224, -31.7606,
        -32.8463, -39.7047, -37.4400, -30.8268, -31.3803, -30.8519, -31.7561,
        -42.5772, -32.3807])
Final projection likelihood: -34.5667
1 mode projection succeeded
New goal: tensor([-0.0718,  0.4363,  0.6727,  0.7586, -0.2214,  0.7358,  0.8373,  1.0234,
         1.4066,  0.3225,  0.0680,  0.9377,  0.0055, -0.0365,  0.0281],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0027]], device='cuda:0') tensor([[0.0020]], device='cuda:0')
Original likelihood: -31.182762145996094
Adjusted likelihood: -31.182762145996094
Likelihood residual: 0.0
Original likelihood: -38.900672912597656
Adjusted likelihood: -38.900672912597656
Likelihood residual: 0.0
{'index': 38.900672912597656, 'thumb_middle': 31.182762145996094}
Current yaw: tensor([ 0.0059, -0.0392, -0.0392], device='cuda:0')
4 thumb_middle
tensor([-0.1020,  0.4753,  0.5096,  0.7363, -0.2841,  0.8020,  0.9031,  0.9845,
         1.4524,  0.4239,  0.0291,  0.9556,  0.0059, -0.0392, -0.0392, -2.1347],
       device='cuda:0')
Solve time for step 1 9.392196175002027
Current ori: tensor([ 0.0059, -0.0392, -0.0392], device='cuda:0')
Index force: tensor([0.5784, 0.5963, 0.5476, 0.6171], device='cuda:0')
tensor([-0.0669,  0.4479,  0.6673,  0.7736, -0.2855,  0.7445,  0.8307,  1.0032,
         1.3857,  0.3296, -0.0138,  0.9060,  0.0035, -0.1235, -0.0413, -3.2952],
       device='cuda:0')
Solve time for step 2 3.567539375042543
Current ori: tensor([ 0.0035, -0.1235, -0.0413], device='cuda:0')
Index force: tensor([0.5874, 0.5423, 0.6063], device='cuda:0')
tensor([-0.0889,  0.5006,  0.7040,  0.7779, -0.2524,  0.7603,  0.8416,  1.0198,
         1.3573,  0.3173, -0.0513,  0.8826,  0.0109, -0.2412, -0.0785, -5.3147],
       device='cuda:0')
Solve time for step 3 3.583985264005605
Current ori: tensor([ 0.0109, -0.2412, -0.0785], device='cuda:0')
Index force: tensor([0.5052, 0.5739], device='cuda:0')
tensor([-0.1037,  0.5424,  0.7407,  0.7812, -0.2153,  0.7798,  0.8433,  0.9969,
         1.3324,  0.3107, -0.0799,  0.8618,  0.0357, -0.3057, -0.1952, -5.8717],
       device='cuda:0')
Solve time for step 4 3.4800407519796863
Current ori: tensor([ 0.0357, -0.3057, -0.1952], device='cuda:0')
Index force: tensor([0.5022], device='cuda:0')
Storing RECOVERY transition: reward=0.1379 (scaled=0.0276), steps=5
Reward stats updated: mean -0.0081 -> -0.0081, std: 0.1272
Collected 602 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=4.2843, Q2 Loss=4.2843, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.3637
SAC Update 2/5: Actor Loss=-0.1418, Q1 Loss=2.6069, Q2 Loss=2.6069, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.9507
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3651, Q2 Loss=1.3651, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8045
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.7740, Q2 Loss=0.7740, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0315
SAC Update 5/5: Actor Loss=-0.0000, Q1 Loss=0.8685, Q2 Loss=0.8685, Entropy=0.0533, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0544

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.5%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (39.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.074419
Q1 loss: 1.979763
Q2 loss: 1.979763
Current threshold: -38.1524
Global Scale Offset: 0.0155
Reward stats: mean=-0.0081, std=0.1272, count=602
----------------------------------------------
SAC Update - Actor Loss: -0.0744, Q1 Loss: 1.9798, Q2 Loss: 1.9798, Entropy: 0.0107, Mean TD Error: 2.8409, Threshold: -38.1524
Original likelihood: -306.617431640625
Adjusted likelihood: -306.617431640625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 39
Loaded trajectory sampler
Current yaw: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
Current yaw: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
1 turn
Sampling time 3.847100260958541
tensor([ 1.3731e-01,  5.7584e-01,  6.1818e-01,  5.7511e-01, -1.2023e-01,
         5.4035e-01,  8.8422e-01,  9.3840e-01,  1.2149e+00,  2.7464e-01,
         2.5331e-01,  1.2669e+00, -9.3357e-04,  1.4822e-02, -2.8876e-02,
         2.6114e-01], device='cuda:0')
Original likelihood: -21.699880599975586
Adjusted likelihood: -21.699880599975586
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.245598976965994
Current ori: tensor([-0.0009,  0.0148, -0.0289], device='cuda:0')
Middle force: tensor([0.5762, 0.5071, 0.4963, 1.2248, 0.6058, 0.5173, 0.6118, 0.5418, 0.6014,
        0.5456, 0.5345, 0.5372], device='cuda:0')
Thumb force: tensor([1.2390, 0.5291, 0.5898, 0.5883, 0.7292, 0.5931, 0.5690, 0.5717, 0.5988,
        0.5881, 0.6667, 0.6937], device='cuda:0')
Index force: tensor([0.9337, 0.7284, 0.8365, 0.5578, 0.8557, 0.8589, 0.6047, 0.5381, 0.5971,
        0.5672, 0.6307, 0.5691], device='cuda:0')
Storing NORMAL transition: reward=-0.0006 (scaled=-0.0006), steps=1
Reward stats updated: mean -0.0081 -> -0.0080, std: 0.1271
Collected 603 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.6634, Q2 Loss=0.6634, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2043
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.6927, Q2 Loss=1.6927, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0218
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8211, Q2 Loss=0.8211, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4617
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1786, Q2 Loss=1.1786, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3777
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=3.6636, Q2 Loss=3.6636, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.2630

------ SAC Update Summary (5 iterations) ------
Total time: 0.30s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.5%)
Q1 update: 0.06s (19.8%)
Q2 update: 0.06s (19.5%)
Actor update: 0.13s (42.6%)
Target update: 0.01s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.603888
Q2 loss: 1.603888
Current threshold: -38.1545
Global Scale Offset: 0.0155
Reward stats: mean=-0.0080, std=0.1271, count=603
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.6039, Q2 Loss: 1.6039, Entropy: 0.0000, Mean TD Error: 2.0657, Threshold: -38.1545
tensor([ 0.2216,  0.6788,  0.5737,  0.4915, -0.1234,  0.5496,  0.9560,  0.9151,
         1.2749,  0.2334,  0.0528,  1.3144, -0.0059, -0.0134, -0.0283, -4.5926],
       device='cuda:0')
Original likelihood: -29.59215545654297
Adjusted likelihood: -29.59215545654297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.69470645202091
Current ori: tensor([-0.0059, -0.0134, -0.0283], device='cuda:0')
Middle force: tensor([0.5063, 0.5002, 1.1977, 0.5887, 0.5106, 0.6041, 0.5387, 0.5934, 0.5388,
        0.5311, 0.5337], device='cuda:0')
Thumb force: tensor([0.5263, 0.6059, 0.5890, 0.7627, 0.6141, 0.5675, 0.5700, 0.5966, 0.5939,
        0.6716, 0.6980], device='cuda:0')
Index force: tensor([0.7199, 0.8505, 0.5556, 0.8241, 0.8389, 0.6002, 0.5356, 0.5920, 0.5605,
        0.6216, 0.5625], device='cuda:0')
Storing NORMAL transition: reward=-0.0575 (scaled=-0.0575), steps=1
Reward stats updated: mean -0.0080 -> -0.0081, std: 0.1270
Collected 604 transitions for RL
SAC Update 1/5: Actor Loss=-0.1171, Q1 Loss=1.7137, Q2 Loss=1.7137, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1599
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3095, Q2 Loss=1.3095, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3927
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.7986, Q2 Loss=2.7986, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.1071
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7593, Q2 Loss=0.7593, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3802
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8165, Q2 Loss=0.8165, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3592

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.5%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.7%)
Actor update: 0.08s (40.3%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.023415
Q1 loss: 1.479496
Q2 loss: 1.479496
Current threshold: -38.1557
Global Scale Offset: 0.0155
Reward stats: mean=-0.0081, std=0.1270, count=604
----------------------------------------------
SAC Update - Actor Loss: -0.0234, Q1 Loss: 1.4795, Q2 Loss: 1.4795, Entropy: 0.0000, Mean TD Error: 2.0798, Threshold: -38.1557
tensor([ 0.1025,  0.6292,  0.5169,  0.4199, -0.1931,  0.4491,  1.0640,  0.8435,
         1.4010,  0.1060,  0.1424,  1.3014,  0.0186,  0.0372,  0.0279,  4.9444],
       device='cuda:0')
Original likelihood: -31.574527740478516
Adjusted likelihood: -31.574527740478516
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.241991622024216
Current ori: tensor([0.0186, 0.0372, 0.0279], device='cuda:0')
Middle force: tensor([0.5002, 1.1907, 0.6049, 0.5139, 0.6106, 0.5405, 0.5914, 0.5392, 0.5354,
        0.5369], device='cuda:0')
Thumb force: tensor([0.6068, 0.5935, 0.7141, 0.5726, 0.5558, 0.5617, 0.5889, 0.5800, 0.6430,
        0.6711], device='cuda:0')
Index force: tensor([0.8377, 0.5523, 0.8320, 0.8512, 0.5935, 0.5314, 0.5903, 0.5599, 0.6190,
        0.5610], device='cuda:0')
Storing NORMAL transition: reward=-0.0072 (scaled=-0.0072), steps=1
Reward stats updated: mean -0.0081 -> -0.0081, std: 0.1269
Collected 605 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7271, Q2 Loss=0.7271, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4401
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7369, Q2 Loss=0.7369, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4535
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=2.3256, Q2 Loss=2.3256, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8332
SAC Update 4/5: Actor Loss=-0.0000, Q1 Loss=0.8755, Q2 Loss=0.8755, Entropy=0.0706, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8132
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4675, Q2 Loss=1.4675, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3913

------ SAC Update Summary (5 iterations) ------
Total time: 0.29s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.6%)
Q1 update: 0.06s (19.8%)
Q2 update: 0.06s (20.0%)
Actor update: 0.12s (42.5%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046061
Q1 loss: 1.226521
Q2 loss: 1.226521
Current threshold: -38.1564
Global Scale Offset: 0.0155
Reward stats: mean=-0.0081, std=0.1269, count=605
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.2265, Q2 Loss: 1.2265, Entropy: 0.0141, Mean TD Error: 0.9862, Threshold: -38.1564
tensor([ 1.0502e-01,  6.3913e-01,  4.8257e-01,  5.9661e-01, -1.0061e-01,
         4.5092e-01,  9.9268e-01,  9.2538e-01,  1.2977e+00,  2.3996e-01,
         1.4418e-01,  1.0777e+00,  2.4294e-02,  2.0276e-03,  3.6140e-02,
         4.0679e+00], device='cuda:0')
Original likelihood: -20.871030807495117
Adjusted likelihood: -20.871030807495117
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.10312404402066
Current ori: tensor([0.0243, 0.0020, 0.0361], device='cuda:0')
Middle force: tensor([0.5098, 0.5513, 0.8316, 1.1001, 0.5051, 0.8128, 0.5115, 0.5026, 0.5159],
       device='cuda:0')
Thumb force: tensor([0.5113, 0.5235, 1.2854, 0.6568, 0.5199, 0.7483, 0.6009, 0.5946, 0.5764],
       device='cuda:0')
Index force: tensor([0.6185, 0.5651, 0.9076, 0.5103, 0.5839, 0.5058, 0.5377, 0.6886, 0.5003],
       device='cuda:0')
Storing NORMAL transition: reward=-0.1457 (scaled=-0.1457), steps=1
Reward stats updated: mean -0.0081 -> -0.0083, std: 0.1269
Collected 606 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.7970, Q2 Loss=0.7970, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4824
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.3445, Q2 Loss=2.3445, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1947
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=2.4802, Q2 Loss=2.4802, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.0581
SAC Update 4/5: Actor Loss=-0.0515, Q1 Loss=1.1544, Q2 Loss=1.1544, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0301
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3905, Q2 Loss=1.3905, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3497

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (19.6%)
Q1 update: 0.04s (17.9%)
Q2 update: 0.04s (19.1%)
Actor update: 0.08s (39.1%)
Target update: 0.00s (1.8%)
Priority update: 0.00s (0.2%)
Actor loss: -0.148464
Q1 loss: 1.633311
Q2 loss: 1.633311
Current threshold: -38.1567
Global Scale Offset: 0.0155
Reward stats: mean=-0.0083, std=0.1269, count=606
----------------------------------------------
SAC Update - Actor Loss: -0.1485, Q1 Loss: 1.6333, Q2 Loss: 1.6333, Entropy: 0.0000, Mean TD Error: 2.8230, Threshold: -38.1567
tensor([ 0.1429,  0.6230,  0.5627,  0.5633, -0.1179,  0.6546,  0.7979,  0.9331,
         1.2319,  0.3491,  0.1622,  1.2720,  0.0274, -0.0140,  0.1817,  3.9594],
       device='cuda:0')
Original likelihood: -21.087322235107422
Adjusted likelihood: -21.087322235107422
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.8929820350022055
Current ori: tensor([ 0.0274, -0.0140,  0.1817], device='cuda:0')
Middle force: tensor([0.5474, 0.8266, 1.0917, 0.5047, 0.8106, 0.5113, 0.5028, 0.5162],
       device='cuda:0')
Thumb force: tensor([0.5217, 1.2637, 0.6472, 0.5184, 0.7348, 0.5942, 0.5833, 0.5704],
       device='cuda:0')
Index force: tensor([0.5614, 0.8919, 0.5091, 0.5794, 0.5052, 0.5353, 0.6764, 0.5002],
       device='cuda:0')
Storing NORMAL transition: reward=0.0210 (scaled=0.0210), steps=1
Reward stats updated: mean -0.0083 -> -0.0083, std: 0.1268
Collected 607 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4441, Q2 Loss=1.4441, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9635
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1113, Q2 Loss=1.1113, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7130
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.9899, Q2 Loss=0.9899, Entropy=0.0591, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0469
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1770, Q2 Loss=1.1770, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7057
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.0221, Q2 Loss=1.0221, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2545

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.7%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (38.5%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.4%)
Actor loss: -0.092119
Q1 loss: 1.148874
Q2 loss: 1.148874
Current threshold: -38.1571
Global Scale Offset: 0.0155
Reward stats: mean=-0.0083, std=0.1268, count=607
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.1489, Q2 Loss: 1.1489, Entropy: 0.0118, Mean TD Error: 0.9367, Threshold: -38.1571
tensor([ 2.2402e-02,  6.0253e-01,  4.5732e-01,  5.8492e-01, -1.3308e-01,
         6.2732e-01,  8.3666e-01,  9.0072e-01,  1.3087e+00,  3.3388e-01,
         1.6064e-01,  1.1306e+00,  2.5632e-02, -1.7632e-03,  1.6089e-01,
         3.4503e+00], device='cuda:0')
Original likelihood: -21.852798461914062
Adjusted likelihood: -21.852798461914062
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.632558773038909
Current ori: tensor([ 0.0256, -0.0018,  0.1609], device='cuda:0')
Middle force: tensor([0.5905, 0.9984, 0.8194, 0.8359, 0.5078, 0.5133, 0.5747],
       device='cuda:0')
Thumb force: tensor([0.7193, 1.3704, 0.5465, 0.6906, 0.5059, 0.5756, 0.5831],
       device='cuda:0')
Index force: tensor([0.5282, 0.5687, 0.5138, 0.5729, 0.5484, 0.5847, 0.5739],
       device='cuda:0')
Storing NORMAL transition: reward=0.0796 (scaled=0.0796), steps=1
Reward stats updated: mean -0.0083 -> -0.0082, std: 0.1268
Collected 608 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1464, Q2 Loss=1.1464, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0037
SAC Update 2/5: Actor Loss=-0.0840, Q1 Loss=1.5124, Q2 Loss=1.5124, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2514
SAC Update 3/5: Actor Loss=-0.0068, Q1 Loss=0.7076, Q2 Loss=0.7076, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2832
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2304, Q2 Loss=1.2304, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1690
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0374, Q2 Loss=1.0374, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8755

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.9%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (19.6%)
Actor update: 0.12s (43.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.018144
Q1 loss: 1.126826
Q2 loss: 1.126826
Current threshold: -38.1574
Global Scale Offset: 0.0155
Reward stats: mean=-0.0082, std=0.1268, count=608
----------------------------------------------
SAC Update - Actor Loss: -0.0181, Q1 Loss: 1.1268, Q2 Loss: 1.1268, Entropy: 0.0000, Mean TD Error: 1.1166, Threshold: -38.1574
tensor([ 0.0131,  0.5891,  0.4596,  0.6024, -0.1457,  0.6150,  0.8255,  0.9615,
         1.3015,  0.3992,  0.2220,  1.0119,  0.0250,  0.0041,  0.0813,  3.4414],
       device='cuda:0')
Original likelihood: -21.88330841064453
Adjusted likelihood: -21.88330841064453
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.411540629982483
Current ori: tensor([0.0250, 0.0041, 0.0813], device='cuda:0')
Middle force: tensor([0.9756, 0.8084, 0.8301, 0.5077, 0.5127, 0.5712], device='cuda:0')
Thumb force: tensor([1.3506, 0.5461, 0.6831, 0.5051, 0.5716, 0.5806], device='cuda:0')
Index force: tensor([0.5635, 0.5128, 0.5687, 0.5438, 0.5796, 0.5705], device='cuda:0')
Storing NORMAL transition: reward=0.1015 (scaled=0.1015), steps=1
Reward stats updated: mean -0.0082 -> -0.0080, std: 0.1267
Collected 609 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6797, Q2 Loss=0.6797, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0284
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9772, Q2 Loss=0.9772, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1353
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0424, Q2 Loss=1.0424, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1831
SAC Update 4/5: Actor Loss=-0.1413, Q1 Loss=1.0897, Q2 Loss=1.0897, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3028
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8317, Q2 Loss=0.8317, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4049

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (19.5%)
Q1 update: 0.05s (19.6%)
Q2 update: 0.05s (18.4%)
Actor update: 0.10s (38.7%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.028259
Q1 loss: 0.924135
Q2 loss: 0.924135
Current threshold: -38.1575
Global Scale Offset: 0.0155
Reward stats: mean=-0.0080, std=0.1267, count=609
----------------------------------------------
SAC Update - Actor Loss: -0.0283, Q1 Loss: 0.9241, Q2 Loss: 0.9241, Entropy: 0.0000, Mean TD Error: 0.2109, Threshold: -38.1575
tensor([ 2.1993e-02,  6.0227e-01,  4.5249e-01,  5.9552e-01, -1.4240e-01,
         5.9847e-01,  8.3719e-01,  1.0134e+00,  1.3455e+00,  3.5490e-01,
         2.5924e-01,  8.6975e-01,  2.2541e-02, -1.1579e-03, -2.0192e-02,
         3.5706e+00], device='cuda:0')
Original likelihood: -23.668922424316406
Adjusted likelihood: -23.668922424316406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.2947798240347765
Current ori: tensor([ 0.0225, -0.0012, -0.0202], device='cuda:0')
Middle force: tensor([0.7959, 0.8255, 0.5069, 0.5113, 0.5678], device='cuda:0')
Thumb force: tensor([0.5451, 0.6766, 0.5047, 0.5672, 0.5782], device='cuda:0')
Index force: tensor([0.5115, 0.5643, 0.5407, 0.5776, 0.5670], device='cuda:0')
Storing NORMAL transition: reward=0.0439 (scaled=0.0439), steps=1
Reward stats updated: mean -0.0080 -> -0.0079, std: 0.1266
Collected 610 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6952, Q2 Loss=0.6952, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5904
SAC Update 2/5: Actor Loss=-0.0000, Q1 Loss=0.7365, Q2 Loss=0.7365, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3238
SAC Update 3/5: Actor Loss=-0.0001, Q1 Loss=0.9597, Q2 Loss=0.9597, Entropy=0.0600, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0694
SAC Update 4/5: Actor Loss=-0.0001, Q1 Loss=0.9896, Q2 Loss=0.9896, Entropy=0.0601, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8996
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1097, Q2 Loss=1.1097, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2495

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (16.2%)
Q1 update: 0.06s (20.9%)
Q2 update: 0.05s (19.3%)
Actor update: 0.11s (40.5%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000033
Q1 loss: 0.898143
Q2 loss: 0.898143
Current threshold: -38.1578
Global Scale Offset: 0.0155
Reward stats: mean=-0.0079, std=0.1266, count=610
----------------------------------------------
SAC Update - Actor Loss: -0.0000, Q1 Loss: 0.8981, Q2 Loss: 0.8981, Entropy: 0.0240, Mean TD Error: 0.8266, Threshold: -38.1578
tensor([-5.6236e-02,  5.8051e-01,  4.6816e-01,  4.7122e-01, -1.8633e-01,
         6.1230e-01,  9.6167e-01,  9.7359e-01,  1.3575e+00,  4.3027e-01,
         2.3978e-01,  7.8483e-01,  2.7857e-03, -1.8754e-02, -6.3983e-02,
         4.1138e+00], device='cuda:0')
Original likelihood: -31.507108688354492
Adjusted likelihood: -31.507108688354492
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 3.90894064999884
Current ori: tensor([ 0.0028, -0.0188, -0.0640], device='cuda:0')
Middle force: tensor([0.5743, 0.6155, 0.5918, 0.6128], device='cuda:0')
Thumb force: tensor([0.5716, 0.5371, 0.5648, 0.5819], device='cuda:0')
Index force: tensor([0.5664, 0.5135, 0.5527, 0.5183], device='cuda:0')
Storing NORMAL transition: reward=0.1032 (scaled=0.1032), steps=1
Reward stats updated: mean -0.0079 -> -0.0077, std: 0.1266
Collected 611 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3270, Q2 Loss=1.3270, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7790
SAC Update 2/5: Actor Loss=-0.1830, Q1 Loss=1.3887, Q2 Loss=1.3887, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0098
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2756, Q2 Loss=1.2756, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7526
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8237, Q2 Loss=0.8237, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5608
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.9275, Q2 Loss=0.9275, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5281

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (17.8%)
Q1 update: 0.04s (18.1%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (41.1%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.082647
Q1 loss: 1.148496
Q2 loss: 1.148496
Current threshold: -38.1582
Global Scale Offset: 0.0155
Reward stats: mean=-0.0077, std=0.1266, count=611
----------------------------------------------
SAC Update - Actor Loss: -0.0826, Q1 Loss: 1.1485, Q2 Loss: 1.1485, Entropy: 0.0000, Mean TD Error: 1.1261, Threshold: -38.1582
tensor([-0.1872,  0.5415,  0.5084,  0.6067, -0.1220,  0.6404,  0.9766,  0.9724,
         1.3779,  0.3870,  0.2251,  0.6642, -0.0161, -0.0671, -0.1730, -1.3550],
       device='cuda:0')
Original likelihood: -39.10755157470703
Adjusted likelihood: -39.10755157470703
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 39.06897735595703
Projection step: 1, Loss: 37.21782684326172
Projection step: 2, Loss: 36.85791015625
Projection step: 3, Loss: 36.09541702270508
Projection step: 4, Loss: 36.0416374206543
Projection step: 5, Loss: 35.8806266784668
Projection step: 6, Loss: 35.842864990234375
Projection step: 7, Loss: 34.05621337890625
Projection step: 8, Loss: 34.016937255859375
Projection step: 9, Loss: 32.99010467529297
Projection step: 10, Loss: 32.22665023803711
Projection step: 11, Loss: 32.120819091796875
Projection step: 12, Loss: 31.0963134765625
Projection step: 13, Loss: 34.82297897338867
Projection step: 14, Loss: 30.17410659790039
Projection step: 15, Loss: 29.77998161315918
Projection step: 16, Loss: 28.62693214416504
Projection step: 17, Loss: 28.564395904541016
Projection step: 18, Loss: 28.59603500366211
Projection step: 19, Loss: 27.979511260986328
Projection step: 20, Loss: 27.286270141601562
Projection step: 21, Loss: 25.818233489990234
Projection step: 22, Loss: 26.13006591796875
Projection step: 23, Loss: 25.218441009521484
Projection step: 24, Loss: 26.218555450439453
Final likelihood: tensor([-23.3898, -25.1516, -34.8637, -23.8540, -23.9584, -24.3636, -23.5782,
        -28.1236, -24.4796, -23.9073, -24.1400, -24.1482, -24.0432, -24.1032,
        -27.2622, -23.9709])
Final projection likelihood: -25.2086
1 mode projection succeeded
New goal: tensor([-0.1122,  0.4730,  0.6484,  0.5561, -0.0932,  0.6414,  0.8645,  1.0632,
         1.3400,  0.3383,  0.2586,  0.5762, -0.0166, -0.0654,  0.3951],
       device='cuda:0')
tensor([[0.0021]], device='cuda:0') tensor([[0.0028]], device='cuda:0') tensor([[0.0074]], device='cuda:0')
Original likelihood: -26.41712188720703
Adjusted likelihood: -26.41712188720703
Likelihood residual: 0.0
{'index': 26.41712188720703, 'thumb_middle': inf}
Current yaw: tensor([-0.0161, -0.0671, -0.1730], device='cuda:0')
2 index
tensor([-0.1872,  0.5415,  0.5084,  0.6067, -0.1220,  0.6404,  0.9766,  0.9724,
         1.3779,  0.3870,  0.2251,  0.6642, -0.0161, -0.0671, -0.1730, -1.3550],
       device='cuda:0')
Solve time for step 1 10.565084479982033
Current ori: tensor([-0.0161, -0.0671, -0.1730], device='cuda:0')
Middle force: tensor([0.5453, 0.5855, 0.5956, 0.5170], device='cuda:0')
Thumb force: tensor([0.5853, 0.6008, 0.5657, 0.5641], device='cuda:0')
tensor([-0.1555,  0.4407,  0.5845,  0.5370, -0.1107,  0.6864,  0.8939,  1.0421,
         1.3837,  0.3782,  0.2208,  0.6350, -0.0238, -0.0738, -0.1975,  4.4092],
       device='cuda:0')
Solve time for step 2 4.24123226600932
Current ori: tensor([-0.0238, -0.0738, -0.1975], device='cuda:0')
Middle force: tensor([0.5805, 0.5896, 0.5146], device='cuda:0')
Thumb force: tensor([0.5934, 0.5642, 0.5644], device='cuda:0')
tensor([-0.1546,  0.4270,  0.5928,  0.5301, -0.1228,  0.6969,  0.8713,  1.0428,
         1.3987,  0.3555,  0.2288,  0.6038, -0.0295, -0.0658, -0.1881, -4.7745],
       device='cuda:0')
Solve time for step 3 4.102301715989597
Current ori: tensor([-0.0295, -0.0658, -0.1881], device='cuda:0')
Middle force: tensor([0.5870, 0.5127], device='cuda:0')
Thumb force: tensor([0.5571, 0.5637], device='cuda:0')
tensor([-0.1496,  0.4267,  0.5977,  0.5254, -0.1203,  0.6989,  0.8700,  1.0438,
         1.3969,  0.3584,  0.2275,  0.6044, -0.0299, -0.0676, -0.1903, -2.5142],
       device='cuda:0')
Solve time for step 4 4.1210291550378315
Current ori: tensor([-0.0299, -0.0676, -0.1903], device='cuda:0')
Middle force: tensor([0.5554], device='cuda:0')
Thumb force: tensor([0.5512], device='cuda:0')
Storing RECOVERY transition: reward=0.0162 (scaled=0.0018), steps=9
Reward stats updated: mean -0.0077 -> -0.0077, std: 0.1265
Collected 612 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.7043, Q2 Loss=0.7043, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5528
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5909, Q2 Loss=1.5909, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=6.8971
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=5.8759, Q2 Loss=5.8759, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.7633
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.4225, Q2 Loss=2.4225, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.9028
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=5.6458, Q2 Loss=5.6458, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.7298

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.1%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.04s (17.4%)
Actor update: 0.10s (40.2%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 3.247874
Q2 loss: 3.247874
Current threshold: -38.1584
Global Scale Offset: 0.0155
Reward stats: mean=-0.0077, std=0.1265, count=612
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 3.2479, Q2 Loss: 3.2479, Entropy: 0.0000, Mean TD Error: 5.3692, Threshold: -38.1584
Original likelihood: -29.93025779724121
Adjusted likelihood: -29.93025779724121
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0312, -0.0711, -0.1913], device='cuda:0')
3 turn
Sampling time 3.686700391001068
tensor([-0.1297,  0.5002,  0.6507,  0.5555, -0.1303,  0.7127,  0.8789,  1.0480,
         1.3917,  0.3671,  0.2255,  0.6062, -0.0312, -0.0711, -0.1913, -1.3090],
       device='cuda:0')
Original likelihood: -29.926856994628906
Adjusted likelihood: -29.926856994628906
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.176558560982812
Current ori: tensor([-0.0312, -0.0711, -0.1913], device='cuda:0')
Middle force: tensor([1.3438, 1.7894, 0.5592, 0.7525, 1.3024, 0.5493, 0.6154, 0.5523, 0.5592,
        0.9659, 0.5813, 0.5624], device='cuda:0')
Thumb force: tensor([0.7790, 0.5575, 2.1202, 0.5169, 0.5623, 1.2589, 0.5119, 0.5646, 0.5379,
        1.4717, 0.9466, 0.5254], device='cuda:0')
Index force: tensor([1.1120, 0.9290, 0.5230, 0.4216, 0.6966, 0.5511, 0.5220, 0.5541, 0.5610,
        0.5278, 0.5330, 0.5322], device='cuda:0')
Storing NORMAL transition: reward=0.0871 (scaled=0.0871), steps=1
Reward stats updated: mean -0.0077 -> -0.0075, std: 0.1265
Collected 613 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3398, Q2 Loss=1.3398, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4217
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2853, Q2 Loss=1.2853, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4295
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0968, Q2 Loss=1.0968, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6922
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8764, Q2 Loss=0.8764, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6579
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.2794, Q2 Loss=1.2794, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8607

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.8%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (18.8%)
Actor update: 0.09s (37.7%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.175544
Q2 loss: 1.175544
Current threshold: -38.1586
Global Scale Offset: 0.0155
Reward stats: mean=-0.0075, std=0.1265, count=613
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.1755, Q2 Loss: 1.1755, Entropy: 0.0000, Mean TD Error: 1.0124, Threshold: -38.1586
tensor([-0.3305,  0.5085,  0.6099,  0.6411, -0.0380,  0.7695,  0.8996,  1.0518,
         1.4629,  0.1290,  0.0735,  0.6334, -0.0388, -0.1490, -0.3003, -0.9651],
       device='cuda:0')
Original likelihood: -45.50183868408203
Adjusted likelihood: -45.50183868408203
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 47.871360778808594
Projection step: 1, Loss: 48.37506103515625
Projection step: 2, Loss: 48.466670989990234
Projection step: 3, Loss: 47.773406982421875
Projection step: 4, Loss: 47.80341339111328
Projection step: 5, Loss: 46.23618698120117
Projection step: 6, Loss: 46.102928161621094
Projection step: 7, Loss: 46.268402099609375
Projection step: 8, Loss: 46.346981048583984
Projection step: 9, Loss: 45.06163787841797
Projection step: 10, Loss: 44.07146453857422
Projection step: 11, Loss: 45.02669906616211
Projection step: 12, Loss: 43.566925048828125
Projection step: 13, Loss: 44.47712707519531
Projection step: 14, Loss: 42.72507858276367
Projection step: 15, Loss: 42.93817901611328
Projection step: 16, Loss: 43.431941986083984
Projection step: 17, Loss: 43.436798095703125
Projection step: 18, Loss: 43.15599822998047
Projection step: 19, Loss: 41.26642990112305
Projection step: 20, Loss: 40.259464263916016
Projection step: 21, Loss: 42.16062545776367
Projection step: 22, Loss: 41.124725341796875
Projection step: 23, Loss: 39.24353790283203
Projection step: 24, Loss: 40.98374938964844
Final likelihood: tensor([-41.8125, -42.5394, -37.4126, -45.2097, -42.4571, -36.3258, -40.6757,
        -36.2760, -36.7542, -41.9559, -36.2330, -41.7165, -36.3521, -36.9865,
        -37.4874, -41.9160])
Final projection likelihood: -39.5069
1 mode projection failed, trying anyway
New goal: tensor([-0.2642,  0.4335,  0.6594,  0.6242, -0.0414,  0.7764,  0.8643,  1.1598,
         1.4433,  0.1010,  0.0954,  0.6505, -0.0355, -0.1451,  0.1756],
       device='cuda:0')
tensor([[0.0023]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0303]], device='cuda:0')
Original likelihood: -55.3356819152832
Adjusted likelihood: -55.3356819152832
Likelihood residual: 0.0
{'index': 55.3356819152832, 'thumb_middle': inf}
Current yaw: tensor([-0.0388, -0.1490, -0.3003], device='cuda:0')
4 index
tensor([-0.3305,  0.5085,  0.6099,  0.6411, -0.0380,  0.7695,  0.8996,  1.0518,
         1.4629,  0.1290,  0.0735,  0.6334, -0.0388, -0.1490, -0.3003, -0.9651],
       device='cuda:0')
Solve time for step 1 10.59282916400116
Current ori: tensor([-0.0388, -0.1490, -0.3003], device='cuda:0')
Middle force: tensor([0.5381, 0.5288, 0.5240, 0.5485], device='cuda:0')
Thumb force: tensor([0.6139, 0.6441, 0.6368, 0.6208], device='cuda:0')
tensor([-0.2750,  0.5219,  0.6657,  0.6206, -0.0546,  0.8235,  0.8510,  1.0995,
         1.4690,  0.0998,  0.0577,  0.6350, -0.0378, -0.1588, -0.3257, -1.5764],
       device='cuda:0')
Solve time for step 2 4.293565966014285
Current ori: tensor([-0.0378, -0.1588, -0.3257], device='cuda:0')
Middle force: tensor([0.5233, 0.5219, 0.5449], device='cuda:0')
Thumb force: tensor([0.6428, 0.6284, 0.6202], device='cuda:0')
tensor([-0.2626,  0.5211,  0.6799,  0.6220, -0.0686,  0.8472,  0.8394,  1.1019,
         1.4672,  0.1019,  0.0541,  0.6425, -0.0345, -0.1623, -0.3395, -2.6489],
       device='cuda:0')
Solve time for step 3 3.956812936987262
Current ori: tensor([-0.0345, -0.1623, -0.3395], device='cuda:0')
Middle force: tensor([0.5212, 0.5462], device='cuda:0')
Thumb force: tensor([0.6193, 0.6172], device='cuda:0')
tensor([-0.2493,  0.5433,  0.6852,  0.6198, -0.0825,  0.8520,  0.8448,  1.1038,
         1.4661,  0.1063,  0.0537,  0.6629, -0.0266, -0.1610, -0.3495, -3.3249],
       device='cuda:0')
Solve time for step 4 3.8542998260236345
Current ori: tensor([-0.0266, -0.1610, -0.3495], device='cuda:0')
Middle force: tensor([0.5452], device='cuda:0')
Thumb force: tensor([0.6157], device='cuda:0')
Storing RECOVERY transition: reward=0.0507 (scaled=0.0507), steps=1
Reward stats updated: mean -0.0075 -> -0.0074, std: 0.1264
Collected 614 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0574, Q2 Loss=1.0574, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9965
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1289, Q2 Loss=1.1289, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6519
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0092, Q2 Loss=1.0092, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4619
SAC Update 4/5: Actor Loss=-0.0003, Q1 Loss=1.3302, Q2 Loss=1.3302, Entropy=0.0672, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6935
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.8965, Q2 Loss=1.8965, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9072

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.5%)
Actor update: 0.08s (40.0%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046112
Q1 loss: 1.284453
Q2 loss: 1.284453
Current threshold: -38.1583
Global Scale Offset: 0.0155
Reward stats: mean=-0.0074, std=0.1264, count=614
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.2845, Q2 Loss: 1.2845, Entropy: 0.0134, Mean TD Error: 1.1422, Threshold: -38.1583
Original likelihood: -39.24281692504883
Adjusted likelihood: -39.24281692504883
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 42.110267639160156
Projection step: 1, Loss: 41.66365051269531
Projection step: 2, Loss: 39.79402160644531
Projection step: 3, Loss: 37.027069091796875
Projection step: 4, Loss: 38.3018798828125
Projection step: 5, Loss: 39.4751091003418
Projection step: 6, Loss: 40.20391845703125
Projection step: 7, Loss: 36.95215606689453
Projection step: 8, Loss: 42.39738845825195
Projection step: 9, Loss: 42.004886627197266
Projection step: 10, Loss: 41.083351135253906
Projection step: 11, Loss: 39.7684440612793
Projection step: 12, Loss: 37.72528839111328
Projection step: 13, Loss: 38.263336181640625
Projection step: 14, Loss: 38.771881103515625
Projection step: 15, Loss: 36.541221618652344
Projection step: 16, Loss: 39.86355209350586
Projection step: 17, Loss: 38.357078552246094
Projection step: 18, Loss: 40.09477996826172
Projection step: 19, Loss: 38.358863830566406
Projection step: 20, Loss: 38.776283264160156
Projection step: 21, Loss: 39.209014892578125
Projection step: 22, Loss: 39.28524398803711
Projection step: 23, Loss: 40.275875091552734
Projection step: 24, Loss: 42.372398376464844
Final likelihood: tensor([-47.4444, -32.9865, -47.5465, -42.5023, -31.4705, -29.0912, -50.2149,
        -48.6133, -31.0393, -48.0109, -46.6908, -37.8673, -47.5715, -31.6255,
        -30.3575, -32.1795])
Final projection likelihood: -39.7007
1 mode projection failed, trying anyway
New goal: tensor([-0.0995,  0.6250,  0.7229,  0.5135, -0.0646,  0.8849,  0.8141,  1.1294,
         1.4607,  0.0901,  0.0416,  0.6277, -0.0362, -0.1691, -0.0646],
       device='cuda:0')
tensor([[0.0024]], device='cuda:0') tensor([[0.0023]], device='cuda:0') tensor([[0.0041]], device='cuda:0')
Original likelihood: -34.947471618652344
Adjusted likelihood: -34.947471618652344
Likelihood residual: 0.0
Original likelihood: -35.90419006347656
Adjusted likelihood: -35.90419006347656
Likelihood residual: 0.0
{'index': 35.90419006347656, 'thumb_middle': 34.947471618652344}
Current yaw: tensor([-0.0369, -0.1722, -0.3652], device='cuda:0')
5 thumb_middle
tensor([-0.1319,  0.7041,  0.7124,  0.5866, -0.0870,  0.8907,  0.8262,  1.0634,
         1.4676,  0.0956,  0.0467,  0.6286, -0.0369, -0.1722, -0.3652, -3.2295],
       device='cuda:0')
Solve time for step 1 8.66263993497705
Current ori: tensor([-0.0369, -0.1722, -0.3652], device='cuda:0')
Index force: tensor([0.5276, 0.5212, 0.5216, 0.5412], device='cuda:0')
tensor([-0.1174,  0.7176,  0.7698,  0.5537, -0.1718,  0.8367,  0.7670,  1.1056,
         1.4169,  0.0983, -0.0678,  0.5930, -0.0581, -0.2843, -0.3907, -2.6787],
       device='cuda:0')
Solve time for step 2 3.710240355983842
Current ori: tensor([-0.0581, -0.2843, -0.3907], device='cuda:0')
Index force: tensor([0.5214, 0.5147, 0.5345], device='cuda:0')
tensor([-0.1349,  0.7513,  0.8001,  0.5605, -0.1543,  0.8425,  0.7761,  1.0971,
         1.4038,  0.0798, -0.1025,  0.5700, -0.0503, -0.3286, -0.4605, -2.2583],
       device='cuda:0')
Solve time for step 3 3.5741922090528533
Current ori: tensor([-0.0503, -0.3286, -0.4605], device='cuda:0')
Index force: tensor([0.5128, 0.5261], device='cuda:0')
tensor([-0.1564,  0.7915,  0.8469,  0.5661, -0.1523,  0.8712,  0.7739,  1.1197,
         1.3881,  0.0819, -0.1286,  0.5881, -0.0241, -0.3686, -0.5577, -2.3098],
       device='cuda:0')
Solve time for step 4 3.430313834978733
Current ori: tensor([-0.0241, -0.3686, -0.5577], device='cuda:0')
Index force: tensor([0.5213], device='cuda:0')
Storing RECOVERY transition: reward=0.1286 (scaled=0.1286), steps=1
Reward stats updated: mean -0.0074 -> -0.0072, std: 0.1264
Collected 615 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9257, Q2 Loss=0.9257, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1335
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8299, Q2 Loss=0.8299, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3239
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4878, Q2 Loss=1.4878, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9481
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.8222, Q2 Loss=0.8222, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5951
SAC Update 5/5: Actor Loss=-0.0595, Q1 Loss=0.8547, Q2 Loss=0.8547, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1704

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (17.1%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.06s (19.6%)
Actor update: 0.12s (40.9%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.057954
Q1 loss: 0.984060
Q2 loss: 0.984060
Current threshold: -38.1577
Global Scale Offset: 0.0155
Reward stats: mean=-0.0072, std=0.1264, count=615
----------------------------------------------
SAC Update - Actor Loss: -0.0580, Q1 Loss: 0.9841, Q2 Loss: 0.9841, Entropy: 0.0000, Mean TD Error: 0.4342, Threshold: -38.1577
Original likelihood: -162.64317321777344
Adjusted likelihood: -162.64317321777344
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 170.0543975830078
Projection step: 1, Loss: 189.15988159179688
Projection step: 2, Loss: 160.5169219970703
Projection step: 3, Loss: 167.90049743652344
Projection step: 4, Loss: 165.933837890625
Projection step: 5, Loss: 187.69161987304688
Projection step: 6, Loss: 168.1212158203125
Projection step: 7, Loss: 175.09619140625
Projection step: 8, Loss: 163.70643615722656
Projection step: 9, Loss: 153.40972900390625
Projection step: 10, Loss: 168.8681182861328
Projection step: 11, Loss: 177.12197875976562
Projection step: 12, Loss: 132.11819458007812
Projection step: 13, Loss: 174.57351684570312
Projection step: 14, Loss: 156.28668212890625
Projection step: 15, Loss: 153.49691772460938
Projection step: 16, Loss: 161.31295776367188
Projection step: 17, Loss: 179.62757873535156
Projection step: 18, Loss: 167.18020629882812
Projection step: 19, Loss: 137.3096923828125
Projection step: 20, Loss: 175.02761840820312
Projection step: 21, Loss: 189.50291442871094
Projection step: 22, Loss: 180.41326904296875
Projection step: 23, Loss: 171.66241455078125
Projection step: 24, Loss: 178.933837890625
Final likelihood: tensor([-219.5299, -164.5336, -197.3354, -117.2936, -126.6947, -269.4976,
        -151.9281, -211.3098, -155.5095,  -92.5526, -141.3291,  -81.0630,
        -157.3100,  -63.2645,  -82.8774, -185.2933])
Final projection likelihood: -151.0826
1 mode projection failed, trying anyway
New goal: tensor([-0.1750,  0.8785,  0.8911,  0.5964, -0.0472,  0.9687,  0.8600,  1.1261,
         1.3959,  0.0915, -0.0159,  0.5975,  0.0202, -0.3018, -0.5064],
       device='cuda:0')
tensor([[0.0022]], device='cuda:0') tensor([[0.0071]], device='cuda:0') tensor([[0.0069]], device='cuda:0')
Original likelihood: -257.87823486328125
Adjusted likelihood: -257.87823486328125
Likelihood residual: 0.0
Original likelihood: -152.17257690429688
Adjusted likelihood: -152.17257690429688
Likelihood residual: 0.0
{'index': 152.17257690429688, 'thumb_middle': 257.87823486328125}
Current yaw: tensor([ 0.0202, -0.3022, -0.6334], device='cuda:0')
6 index
tensor([-0.1818,  0.8865,  0.8966,  0.5802, -0.0474,  0.9768,  0.8543,  1.1140,
         1.4121,  0.0948, -0.0152,  0.5987,  0.0202, -0.3022, -0.6334, -2.3388],
       device='cuda:0')
Solve time for step 1 10.297840019979049
Current ori: tensor([ 0.0202, -0.3022, -0.6334], device='cuda:0')
Middle force: tensor([0.6134, 0.5902, 0.5738, 0.5035], device='cuda:0')
Thumb force: tensor([0.5350, 0.5448, 0.5855, 0.5004], device='cuda:0')
tensor([-0.2105,  0.8929,  0.8644,  0.5703, -0.0677,  1.0388,  0.8604,  1.0987,
         1.4046,  0.0933, -0.0309,  0.6378,  0.0415, -0.3183, -0.7055, -2.0482],
       device='cuda:0')
Solve time for step 2 4.190315840009134
Current ori: tensor([ 0.0415, -0.3183, -0.7055], device='cuda:0')
Middle force: tensor([0.5811, 0.5692, 0.5019], device='cuda:0')
Thumb force: tensor([0.5393, 0.5741, 0.5002], device='cuda:0')
tensor([-0.2224,  0.8931,  0.8538,  0.5716, -0.0803,  1.1033,  0.8470,  1.0547,
         1.4044,  0.0796, -0.0277,  0.6521,  0.0674, -0.3257, -0.8021, -1.7210],
       device='cuda:0')
Solve time for step 3 4.082803063036408
Current ori: tensor([ 0.0674, -0.3257, -0.8021], device='cuda:0')
Middle force: tensor([0.5603, 0.5007], device='cuda:0')
Thumb force: tensor([0.5780, 0.5004], device='cuda:0')
tensor([-0.2006,  0.9020,  0.8734,  0.5834, -0.0593,  1.1410,  0.8282,  1.0051,
         1.3942,  0.0836, -0.0759,  0.6619,  0.0577, -0.3536, -0.7903, -1.7708],
       device='cuda:0')
Solve time for step 4 3.9609676779946312
Current ori: tensor([ 0.0577, -0.3536, -0.7903], device='cuda:0')
Middle force: tensor([0.5505], device='cuda:0')
Thumb force: tensor([0.5003], device='cuda:0')
Storing RECOVERY transition: reward=0.0574 (scaled=0.0574), steps=1
Reward stats updated: mean -0.0072 -> -0.0071, std: 0.1263
Collected 616 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2808, Q2 Loss=1.2808, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7094
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0216, Q2 Loss=1.0216, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1398
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2430, Q2 Loss=1.2430, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9466
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9165, Q2 Loss=0.9165, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5261
SAC Update 5/5: Actor Loss=-0.0206, Q1 Loss=1.0615, Q2 Loss=1.0615, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0293

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.3%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.7%)
Actor update: 0.09s (40.3%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.004112
Q1 loss: 1.104685
Q2 loss: 1.104685
Current threshold: -38.1574
Global Scale Offset: 0.0155
Reward stats: mean=-0.0071, std=0.1263, count=616
----------------------------------------------
SAC Update - Actor Loss: -0.0041, Q1 Loss: 1.1047, Q2 Loss: 1.1047, Entropy: 0.0000, Mean TD Error: 1.4702, Threshold: -38.1574
Original likelihood: -356.8448791503906
Adjusted likelihood: -356.8448791503906
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 40
Loaded trajectory sampler
Current yaw: tensor([-0.0020,  0.0145, -0.0306], device='cuda:0')
Current yaw: tensor([-0.0020,  0.0145, -0.0306], device='cuda:0')
1 turn
Sampling time 3.6159928439883515
tensor([ 0.1325,  0.6235,  0.5459,  0.5786, -0.1405,  0.5314,  0.9109,  0.9780,
         1.2055,  0.3028,  0.2566,  1.2264, -0.0020,  0.0145, -0.0306,  0.2450],
       device='cuda:0')
Original likelihood: -21.591419219970703
Adjusted likelihood: -21.591419219970703
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.266296379035339
Current ori: tensor([-0.0020,  0.0145, -0.0306], device='cuda:0')
Middle force: tensor([0.5713, 1.6773, 0.5244, 0.5149, 0.6347, 1.6146, 0.5717, 0.5047, 0.5410,
        0.5689, 0.5086, 0.5711], device='cuda:0')
Thumb force: tensor([0.7121, 0.9206, 0.4960, 0.9621, 0.8723, 1.0534, 0.7177, 0.6162, 1.0550,
        0.9548, 0.6401, 0.6164], device='cuda:0')
Index force: tensor([0.5094, 0.6960, 0.9274, 0.6165, 0.5060, 0.5200, 0.5009, 0.5913, 0.5337,
        0.5514, 0.7046, 0.5839], device='cuda:0')
Storing NORMAL transition: reward=0.1379 (scaled=0.1379), steps=1
Reward stats updated: mean -0.0071 -> -0.0069, std: 0.1264
Collected 617 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9246, Q2 Loss=0.9246, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7517
SAC Update 2/5: Actor Loss=-0.0389, Q1 Loss=1.0742, Q2 Loss=1.0742, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1256
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.9744, Q2 Loss=0.9744, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2937
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7248, Q2 Loss=0.7248, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3499
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8407, Q2 Loss=0.8407, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9277

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.4%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.7%)
Actor update: 0.08s (40.3%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.053835
Q1 loss: 0.907740
Q2 loss: 0.907740
Current threshold: -38.1572
Global Scale Offset: 0.0155
Reward stats: mean=-0.0069, std=0.1264, count=617
----------------------------------------------
SAC Update - Actor Loss: -0.0538, Q1 Loss: 0.9077, Q2 Loss: 0.9077, Entropy: 0.0000, Mean TD Error: 1.2897, Threshold: -38.1572
tensor([ 0.1602,  0.6155,  0.5332,  0.6817, -0.1400,  0.4945,  0.9287,  1.0585,
         1.2997,  0.1770,  0.2316,  1.1602,  0.0036,  0.0060, -0.1684,  0.5044],
       device='cuda:0')
Original likelihood: -25.424314498901367
Adjusted likelihood: -25.424314498901367
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.576668955967762
Current ori: tensor([ 0.0036,  0.0060, -0.1684], device='cuda:0')
Middle force: tensor([1.6328, 0.5242, 0.5136, 0.6285, 1.5838, 0.5668, 0.5041, 0.5378, 0.5635,
        0.5063, 0.5683], device='cuda:0')
Thumb force: tensor([0.8985, 0.5017, 0.9468, 0.8617, 1.0366, 0.7143, 0.6130, 1.0454, 0.9469,
        0.6436, 0.6120], device='cuda:0')
Index force: tensor([0.6836, 0.9194, 0.6120, 0.5052, 0.5193, 0.5006, 0.5866, 0.5310, 0.5482,
        0.7093, 0.5801], device='cuda:0')
Storing NORMAL transition: reward=0.1139 (scaled=0.1139), steps=1
Reward stats updated: mean -0.0069 -> -0.0067, std: 0.1264
Collected 618 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.8972, Q2 Loss=0.8972, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5729
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=3.3905, Q2 Loss=3.3905, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.2837
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8771, Q2 Loss=0.8771, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9553
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3005, Q2 Loss=1.3005, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8974
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1843, Q2 Loss=1.1843, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8648

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.0%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (19.6%)
Actor update: 0.11s (40.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.529914
Q2 loss: 1.529914
Current threshold: -38.1571
Global Scale Offset: 0.0155
Reward stats: mean=-0.0067, std=0.1264, count=618
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.5299, Q2 Loss: 1.5299, Entropy: 0.0000, Mean TD Error: 2.3148, Threshold: -38.1571
tensor([ 0.1561,  0.6242,  0.5381,  0.6412, -0.1257,  0.5005,  0.9160,  1.1275,
         1.3303,  0.1416,  0.2276,  1.0898, -0.0023, -0.0019, -0.2823,  0.7439],
       device='cuda:0')
Original likelihood: -25.36791229248047
Adjusted likelihood: -25.36791229248047
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.255949342972599
Current ori: tensor([-0.0023, -0.0019, -0.2823], device='cuda:0')
Middle force: tensor([0.5202, 0.5127, 0.6254, 1.5613, 0.5648, 0.5039, 0.5368, 0.5611, 0.5060,
        0.5670], device='cuda:0')
Thumb force: tensor([0.5013, 0.9254, 0.8489, 1.0171, 0.7042, 0.6026, 1.0265, 0.9317, 0.6316,
        0.6056], device='cuda:0')
Index force: tensor([0.9058, 0.6081, 0.5044, 0.5179, 0.5004, 0.5832, 0.5291, 0.5457, 0.7062,
        0.5768], device='cuda:0')
Storing NORMAL transition: reward=-0.0354 (scaled=-0.0354), steps=1
Reward stats updated: mean -0.0067 -> -0.0067, std: 0.1263
Collected 619 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3393, Q2 Loss=1.3393, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3197
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8209, Q2 Loss=0.8209, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3071
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=3.3248, Q2 Loss=3.3248, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.2870
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2116, Q2 Loss=1.2116, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6553
SAC Update 5/5: Actor Loss=-0.1430, Q1 Loss=1.1678, Q2 Loss=1.1678, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6577

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.6%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (19.2%)
Actor update: 0.10s (38.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.074649
Q1 loss: 1.572885
Q2 loss: 1.572885
Current threshold: -38.1570
Global Scale Offset: 0.0155
Reward stats: mean=-0.0067, std=0.1263, count=619
----------------------------------------------
SAC Update - Actor Loss: -0.0746, Q1 Loss: 1.5729, Q2 Loss: 1.5729, Entropy: 0.0000, Mean TD Error: 2.0454, Threshold: -38.1570
tensor([ 0.1497,  0.6108,  0.4691,  0.8036, -0.1380,  0.5097,  0.8823,  1.1639,
         1.3506,  0.1237,  0.2323,  1.0688,  0.0059,  0.0033, -0.2469,  0.8086],
       device='cuda:0')
Original likelihood: -27.209190368652344
Adjusted likelihood: -27.209190368652344
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.023933331947774
Current ori: tensor([ 0.0059,  0.0033, -0.2469], device='cuda:0')
Middle force: tensor([0.5111, 0.6200, 1.5354, 0.5605, 0.5037, 0.5338, 0.5570, 0.5051, 0.5649],
       device='cuda:0')
Thumb force: tensor([0.9072, 0.8373, 0.9997, 0.7026, 0.5958, 1.0198, 0.9253, 0.6278, 0.6006],
       device='cuda:0')
Index force: tensor([0.6024, 0.5037, 0.5165, 0.5003, 0.5790, 0.5266, 0.5424, 0.7033, 0.5733],
       device='cuda:0')
Storing NORMAL transition: reward=0.0884 (scaled=0.0884), steps=1
Reward stats updated: mean -0.0067 -> -0.0066, std: 0.1262
Collected 620 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9961, Q2 Loss=0.9961, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7921
SAC Update 2/5: Actor Loss=-0.0002, Q1 Loss=1.3587, Q2 Loss=1.3587, Entropy=0.0593, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3152
SAC Update 3/5: Actor Loss=-0.0557, Q1 Loss=0.9597, Q2 Loss=0.9597, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1065
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.9564, Q2 Loss=0.9564, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4496
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0693, Q2 Loss=1.0693, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1134

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.9%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (18.8%)
Actor update: 0.11s (43.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.057229
Q1 loss: 1.068052
Q2 loss: 1.068052
Current threshold: -38.1663
Global Scale Offset: 0.0154
Reward stats: mean=-0.0066, std=0.1262, count=620
----------------------------------------------
SAC Update - Actor Loss: -0.0572, Q1 Loss: 1.0681, Q2 Loss: 1.0681, Entropy: 0.0119, Mean TD Error: 0.7554, Threshold: -38.1663
tensor([ 0.1558,  0.5775,  0.6040,  0.6370, -0.1723,  0.4967,  0.9936,  1.2267,
         1.3039,  0.2102,  0.2848,  0.9716, -0.0030,  0.0017, -0.3353,  0.9169],
       device='cuda:0')
Original likelihood: -28.166425704956055
Adjusted likelihood: -28.166425704956055
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.64034352300223
Current ori: tensor([-0.0030,  0.0017, -0.3353], device='cuda:0')
Middle force: tensor([0.5002, 0.8535, 0.5034, 0.5523, 0.5503, 0.5790, 0.9201, 0.5474],
       device='cuda:0')
Thumb force: tensor([0.5633, 0.6170, 0.6308, 0.5181, 0.5923, 0.9444, 0.5439, 0.5152],
       device='cuda:0')
Index force: tensor([0.6124, 0.5552, 0.6643, 0.5623, 0.5577, 0.5724, 0.5353, 0.5462],
       device='cuda:0')
Storing NORMAL transition: reward=0.1073 (scaled=0.1073), steps=1
Reward stats updated: mean -0.0066 -> -0.0064, std: 0.1262
Collected 621 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2451, Q2 Loss=1.2451, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3367
SAC Update 2/5: Actor Loss=-0.0002, Q1 Loss=1.2300, Q2 Loss=1.2300, Entropy=0.0744, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9073
SAC Update 3/5: Actor Loss=-0.0004, Q1 Loss=1.5846, Q2 Loss=1.5846, Entropy=0.0778, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6453
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9735, Q2 Loss=0.9735, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2921
SAC Update 5/5: Actor Loss=-0.1532, Q1 Loss=1.1894, Q2 Loss=1.1894, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6091

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.7%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.3%)
Actor update: 0.10s (38.8%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.030750
Q1 loss: 1.244534
Q2 loss: 1.244534
Current threshold: -38.1775
Global Scale Offset: 0.0153
Reward stats: mean=-0.0064, std=0.1262, count=621
----------------------------------------------
SAC Update - Actor Loss: -0.0307, Q1 Loss: 1.2445, Q2 Loss: 1.2445, Entropy: 0.0304, Mean TD Error: 1.1581, Threshold: -38.1775
tensor([ 0.1836,  0.5910,  0.6447,  0.5808, -0.0839,  0.5122,  1.0874,  1.1263,
         1.3065,  0.1797,  0.1882,  0.9598, -0.0296, -0.0543, -0.4613,  1.2557],
       device='cuda:0')
Original likelihood: -27.452608108520508
Adjusted likelihood: -27.452608108520508
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.4102762680267915
Current ori: tensor([-0.0296, -0.0543, -0.4613], device='cuda:0')
Middle force: tensor([0.8425, 0.5075, 0.5518, 0.5486, 0.5739, 0.9086, 0.5519],
       device='cuda:0')
Thumb force: tensor([0.6068, 0.5864, 0.5154, 0.5864, 0.9282, 0.5406, 0.5121],
       device='cuda:0')
Index force: tensor([0.5524, 0.6248, 0.5601, 0.5547, 0.5712, 0.5329, 0.5431],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0775 (scaled=-0.0775), steps=1
Reward stats updated: mean -0.0064 -> -0.0065, std: 0.1261
Collected 622 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.0334, Q2 Loss=1.0334, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1701
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2951, Q2 Loss=1.2951, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.3155
SAC Update 3/5: Actor Loss=-0.2189, Q1 Loss=1.6154, Q2 Loss=1.6154, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2028
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.2670, Q2 Loss=1.2670, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5209
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2406, Q2 Loss=1.2406, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0910

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.1%)
Q1 update: 0.05s (20.4%)
Q2 update: 0.04s (18.2%)
Actor update: 0.09s (38.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.089836
Q1 loss: 1.290283
Q2 loss: 1.290283
Current threshold: -38.1845
Global Scale Offset: 0.0153
Reward stats: mean=-0.0065, std=0.1261, count=622
----------------------------------------------
SAC Update - Actor Loss: -0.0898, Q1 Loss: 1.2903, Q2 Loss: 1.2903, Entropy: 0.0000, Mean TD Error: 1.4601, Threshold: -38.1845
tensor([ 0.1320,  0.5514,  0.5574,  0.6896, -0.2291,  0.5186,  0.9032,  1.1786,
         1.4174,  0.1125,  0.3622,  0.7718, -0.0357,  0.0566, -0.3773, -0.4693],
       device='cuda:0')
Original likelihood: -30.295852661132812
Adjusted likelihood: -30.295852661132812
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.178416589042172
Current ori: tensor([-0.0357,  0.0566, -0.3773], device='cuda:0')
Middle force: tensor([0.5036, 0.5503, 0.5438, 0.5708, 0.8967, 0.5386], device='cuda:0')
Thumb force: tensor([0.6329, 0.5145, 0.5869, 0.9168, 0.5382, 0.5155], device='cuda:0')
Index force: tensor([0.6311, 0.5554, 0.5531, 0.5654, 0.5310, 0.5403], device='cuda:0')
Storing NORMAL transition: reward=-0.0109 (scaled=-0.0109), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1260
Collected 623 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6919, Q2 Loss=0.6919, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4253
SAC Update 2/5: Actor Loss=-0.0004, Q1 Loss=1.4609, Q2 Loss=1.4609, Entropy=0.1008, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3939
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.2704, Q2 Loss=2.2704, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0988
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1448, Q2 Loss=1.1448, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8210
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6963, Q2 Loss=1.6963, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5563

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.4%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.04s (18.5%)
Actor update: 0.08s (37.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.000071
Q1 loss: 1.452861
Q2 loss: 1.452861
Current threshold: -38.1894
Global Scale Offset: 0.0153
Reward stats: mean=-0.0065, std=0.1260, count=623
----------------------------------------------
SAC Update - Actor Loss: -0.0001, Q1 Loss: 1.4529, Q2 Loss: 1.4529, Entropy: 0.0202, Mean TD Error: 1.2590, Threshold: -38.1894
tensor([ 0.0966,  0.6170,  0.5355,  0.5213, -0.2745,  0.5605,  0.8226,  1.2802,
         1.2781,  0.2016,  0.3128,  0.7397, -0.0490,  0.0793, -0.3773, -0.6984],
       device='cuda:0')
Original likelihood: -34.52262878417969
Adjusted likelihood: -34.52262878417969
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.000364597013686
Current ori: tensor([-0.0490,  0.0793, -0.3773], device='cuda:0')
Middle force: tensor([0.5441, 0.5431, 0.5708, 0.8893, 0.5362], device='cuda:0')
Thumb force: tensor([0.5115, 0.5814, 0.8995, 0.5352, 0.5144], device='cuda:0')
Index force: tensor([0.5653, 0.5536, 0.5602, 0.5287, 0.5371], device='cuda:0')
Storing NORMAL transition: reward=0.0451 (scaled=0.0451), steps=1
Reward stats updated: mean -0.0065 -> -0.0064, std: 0.1259
Collected 624 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9541, Q2 Loss=0.9541, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9049
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9631, Q2 Loss=0.9631, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1593
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.4182, Q2 Loss=1.4182, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4094
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.4443, Q2 Loss=1.4443, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1236
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3749, Q2 Loss=1.3749, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9067

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.7%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.04s (17.6%)
Actor update: 0.09s (37.8%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.230904
Q2 loss: 1.230904
Current threshold: -38.1924
Global Scale Offset: 0.0153
Reward stats: mean=-0.0064, std=0.1259, count=624
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.2309, Q2 Loss: 1.2309, Entropy: 0.0000, Mean TD Error: 1.1008, Threshold: -38.1924
tensor([ 0.1115,  0.5900,  0.5631,  0.5791, -0.2402,  0.5418,  0.9080,  1.2254,
         1.3809,  0.1981,  0.3925,  0.6907, -0.0565,  0.0591, -0.4226, -0.4078],
       device='cuda:0')
Original likelihood: -34.976356506347656
Adjusted likelihood: -34.976356506347656
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 3.75320749002276
Current ori: tensor([-0.0565,  0.0591, -0.4226], device='cuda:0')
Middle force: tensor([0.5026, 0.5838, 0.5105, 0.5048], device='cuda:0')
Thumb force: tensor([0.7168, 1.2099, 0.8398, 0.5545], device='cuda:0')
Index force: tensor([0.6914, 0.5180, 0.5331, 0.5347], device='cuda:0')
Storing NORMAL transition: reward=0.0484 (scaled=0.0484), steps=1
Reward stats updated: mean -0.0064 -> -0.0063, std: 0.1259
Collected 625 transitions for RL
SAC Update 1/5: Actor Loss=-0.1760, Q1 Loss=1.1912, Q2 Loss=1.1912, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1115
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.0350, Q2 Loss=2.0350, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9908
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=2.2431, Q2 Loss=2.2431, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.8739
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.9243, Q2 Loss=1.9243, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7739
SAC Update 5/5: Actor Loss=-0.1430, Q1 Loss=1.2190, Q2 Loss=1.2190, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8791

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.4%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (18.4%)
Actor update: 0.10s (38.7%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.155907
Q1 loss: 1.722535
Q2 loss: 1.722535
Current threshold: -38.1942
Global Scale Offset: 0.0152
Reward stats: mean=-0.0063, std=0.1259, count=625
----------------------------------------------
SAC Update - Actor Loss: -0.1559, Q1 Loss: 1.7225, Q2 Loss: 1.7225, Entropy: 0.0000, Mean TD Error: 1.7258, Threshold: -38.1942
tensor([ 0.0980,  0.6412,  0.5038,  0.5114, -0.2518,  0.6302,  1.1351,  1.1993,
         1.3585,  0.3387,  0.3191,  0.7043, -0.0894,  0.0462, -0.5191, -0.0702],
       device='cuda:0')
Original likelihood: -40.693817138671875
Adjusted likelihood: -40.693817138671875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 37.53118133544922
Projection step: 1, Loss: 40.786163330078125
Projection step: 2, Loss: 39.65128707885742
Projection step: 3, Loss: 36.937408447265625
Projection step: 4, Loss: 37.25739669799805
Projection step: 5, Loss: 37.10467529296875
Projection step: 6, Loss: 37.213836669921875
Projection step: 7, Loss: 37.076560974121094
Projection step: 8, Loss: 36.94245910644531
Projection step: 9, Loss: 36.26704025268555
Projection step: 10, Loss: 36.281795501708984
Projection step: 11, Loss: 35.236419677734375
Projection step: 12, Loss: 34.4610481262207
Projection step: 13, Loss: 33.928741455078125
Projection step: 14, Loss: 32.830814361572266
Projection step: 15, Loss: 32.96858215332031
Projection step: 16, Loss: 32.574951171875
Projection step: 17, Loss: 32.67844009399414
Projection step: 18, Loss: 31.633752822875977
Projection step: 19, Loss: 32.49712371826172
Projection step: 20, Loss: 30.592914581298828
Projection step: 21, Loss: 31.500503540039062
Projection step: 22, Loss: 30.663137435913086
Projection step: 23, Loss: 30.3505916595459
Projection step: 24, Loss: 30.486984252929688
Final likelihood: tensor([-34.2761, -29.3606, -29.1212, -33.7217, -30.3540, -29.5571, -29.3787,
        -29.4153, -17.3771, -28.8413, -29.7715, -29.9047, -28.7337, -33.7009,
        -28.3210, -28.5803])
Final projection likelihood: -29.4010
1 mode projection succeeded
New goal: tensor([ 0.1125,  0.6175,  0.4860,  0.5598, -0.2050,  0.5664,  0.9741,  1.0517,
         1.3939,  0.2590,  0.2871,  0.7548, -0.0887,  0.0426, -0.0188],
       device='cuda:0')
tensor([[0.0027]], device='cuda:0') tensor([[0.0029]], device='cuda:0') tensor([[0.0024]], device='cuda:0')
Original likelihood: -30.639509201049805
Adjusted likelihood: -30.639509201049805
Likelihood residual: 0.0
Original likelihood: -34.862266540527344
Adjusted likelihood: -34.862266540527344
Likelihood residual: 0.0
{'index': 34.862266540527344, 'thumb_middle': 30.639509201049805}
Current yaw: tensor([-0.0894,  0.0462, -0.5191], device='cuda:0')
2 thumb_middle
tensor([ 0.0980,  0.6412,  0.5038,  0.5114, -0.2518,  0.6302,  1.1351,  1.1993,
         1.3585,  0.3387,  0.3191,  0.7043, -0.0894,  0.0462, -0.5191, -0.0702],
       device='cuda:0')
Solve time for step 1 8.849783726036549
Current ori: tensor([-0.0894,  0.0462, -0.5191], device='cuda:0')
Index force: tensor([0.5567, 0.5873, 0.6003, 0.6052], device='cuda:0')
tensor([ 0.0932,  0.6489,  0.4782,  0.5312, -0.3277,  0.5659,  0.9808,  1.0687,
         1.3433,  0.2550,  0.2355,  0.7229, -0.0952,  0.0512, -0.5173, -0.1032],
       device='cuda:0')
Solve time for step 2 3.471597213007044
Current ori: tensor([-0.0952,  0.0512, -0.5173], device='cuda:0')
Index force: tensor([0.5798, 0.5949, 0.5989], device='cuda:0')
tensor([ 0.1081,  0.6497,  0.4875,  0.5454, -0.3269,  0.5788,  0.9732,  1.0473,
         1.3441,  0.2370,  0.2158,  0.7251, -0.0916,  0.0448, -0.5173, -0.1497],
       device='cuda:0')
Solve time for step 3 3.888189436052926
Current ori: tensor([-0.0916,  0.0448, -0.5173], device='cuda:0')
Index force: tensor([0.5781, 0.5794], device='cuda:0')
tensor([ 0.1237,  0.6561,  0.4847,  0.5679, -0.3247,  0.5793,  0.9703,  1.0318,
         1.3445,  0.2393,  0.2082,  0.7158, -0.0927,  0.0320, -0.5173, -0.1083],
       device='cuda:0')
Solve time for step 4 3.3860470830113627
Current ori: tensor([-0.0927,  0.0320, -0.5173], device='cuda:0')
Index force: tensor([0.5613], device='cuda:0')
Storing RECOVERY transition: reward=0.0009 (scaled=0.0001), steps=9
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1258
Collected 626 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.8304, Q2 Loss=0.8304, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4415
SAC Update 2/5: Actor Loss=-0.1342, Q1 Loss=1.1138, Q2 Loss=1.1138, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5447
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1674, Q2 Loss=1.1674, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2872
SAC Update 4/5: Actor Loss=-0.0153, Q1 Loss=0.9629, Q2 Loss=0.9629, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0076
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7321, Q2 Loss=0.7321, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5463

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.6%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.1%)
Actor update: 0.09s (40.5%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.075938
Q1 loss: 0.961299
Q2 loss: 0.961299
Current threshold: -38.1952
Global Scale Offset: 0.0152
Reward stats: mean=-0.0063, std=0.1258, count=626
----------------------------------------------
SAC Update - Actor Loss: -0.0759, Q1 Loss: 0.9613, Q2 Loss: 0.9613, Entropy: 0.0000, Mean TD Error: 1.1654, Threshold: -38.1952
Original likelihood: -31.226783752441406
Adjusted likelihood: -31.226783752441406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0919,  0.0386, -0.5190], device='cuda:0')
3 turn
Sampling time 3.763057564967312
tensor([ 0.0970,  0.6357,  0.4873,  0.5649, -0.2535,  0.6243,  0.9770,  1.0582,
         1.4021,  0.2460,  0.2582,  0.7563, -0.0919,  0.0386, -0.5190, -0.0279],
       device='cuda:0')
Original likelihood: -33.637908935546875
Adjusted likelihood: -33.637908935546875
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 13.926540413987823
Current ori: tensor([-0.0919,  0.0386, -0.5190], device='cuda:0')
Middle force: tensor([0.8576, 0.5952, 0.6835, 0.5700, 0.8793, 0.5916, 0.5637, 0.7407, 1.9693,
        0.5185, 0.8268, 0.5745], device='cuda:0')
Thumb force: tensor([0.8480, 1.0835, 0.6522, 1.2349, 0.5569, 0.6341, 0.8035, 1.0233, 0.7505,
        0.8136, 0.7945, 0.6181], device='cuda:0')
Index force: tensor([1.0540, 0.5185, 0.8762, 0.7193, 0.6404, 0.5909, 0.5942, 0.5350, 1.0587,
        0.5157, 0.6947, 0.6153], device='cuda:0')
Storing NORMAL transition: reward=0.0067 (scaled=0.0067), steps=1
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1257
Collected 627 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.3016, Q2 Loss=1.3016, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5436
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.1760, Q2 Loss=1.1760, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1631
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=3.0661, Q2 Loss=3.0661, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.3088
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0639, Q2 Loss=1.0639, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7465
SAC Update 5/5: Actor Loss=-0.1538, Q1 Loss=1.2032, Q2 Loss=1.2032, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6778

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.06s (20.5%)
Q1 update: 0.05s (18.8%)
Q2 update: 0.05s (18.2%)
Actor update: 0.10s (38.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.122862
Q1 loss: 1.562161
Q2 loss: 1.562161
Current threshold: -38.1959
Global Scale Offset: 0.0152
Reward stats: mean=-0.0063, std=0.1257, count=627
----------------------------------------------
SAC Update - Actor Loss: -0.1229, Q1 Loss: 1.5622, Q2 Loss: 1.5622, Entropy: 0.0000, Mean TD Error: 1.4880, Threshold: -38.1959
tensor([ 0.1792,  0.5763,  0.5906,  0.6971, -0.1607,  0.6196,  0.8773,  1.1513,
         1.3512,  0.1983,  0.1229,  0.7785, -0.0747, -0.0119, -0.5218,  0.1647],
       device='cuda:0')
Original likelihood: -28.0937557220459
Adjusted likelihood: -28.0937557220459
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.5534584050183184
Current ori: tensor([-0.0747, -0.0119, -0.5218], device='cuda:0')
Middle force: tensor([0.5827, 0.6665, 0.5505, 0.8420, 0.5856, 0.5536, 0.7095, 1.9110, 0.5163,
        0.8085, 0.5685], device='cuda:0')
Thumb force: tensor([1.0346, 0.6464, 1.1689, 0.5559, 0.6221, 0.7945, 0.9998, 0.7479, 0.7931,
        0.7721, 0.6069], device='cuda:0')
Index force: tensor([0.5186, 0.8619, 0.7157, 0.6316, 0.5850, 0.5878, 0.5264, 1.0554, 0.5135,
        0.6866, 0.6095], device='cuda:0')
Storing NORMAL transition: reward=0.0076 (scaled=0.0076), steps=1
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1256
Collected 628 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.1834, Q2 Loss=1.1834, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1525
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.9796, Q2 Loss=0.9796, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6288
SAC Update 3/5: Actor Loss=-0.0773, Q1 Loss=0.9630, Q2 Loss=0.9630, Entropy=0.0019, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1858
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.9071, Q2 Loss=1.9071, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6034
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1830, Q2 Loss=1.1830, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8899

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.2%)
Q1 update: 0.05s (20.4%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (38.4%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.015470
Q1 loss: 1.243239
Q2 loss: 1.243239
Current threshold: -38.2223
Global Scale Offset: 0.0151
Reward stats: mean=-0.0063, std=0.1256, count=628
----------------------------------------------
SAC Update - Actor Loss: -0.0155, Q1 Loss: 1.2432, Q2 Loss: 1.2432, Entropy: 0.0004, Mean TD Error: 0.6921, Threshold: -38.2223
tensor([ 0.1736,  0.5229,  0.6316,  0.7449, -0.1722,  0.6023,  0.9513,  1.1291,
         1.4024,  0.2142,  0.2209,  0.7594, -0.0607, -0.0081, -0.5275,  0.2579],
       device='cuda:0')
Original likelihood: -21.924270629882812
Adjusted likelihood: -21.924270629882812
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.230951049015857
Current ori: tensor([-0.0607, -0.0081, -0.5275], device='cuda:0')
Middle force: tensor([0.6572, 0.5455, 0.8333, 0.5848, 0.5488, 0.6912, 1.8791, 0.5154, 0.7975,
        0.5690], device='cuda:0')
Thumb force: tensor([0.6366, 1.1095, 0.5487, 0.6091, 0.7827, 0.9730, 0.7351, 0.7743, 0.7572,
        0.5924], device='cuda:0')
Index force: tensor([0.8430, 0.6953, 0.6219, 0.5784, 0.5818, 0.5203, 1.0319, 0.5117, 0.6738,
        0.6032], device='cuda:0')
Storing NORMAL transition: reward=0.0286 (scaled=0.0286), steps=1
Reward stats updated: mean -0.0063 -> -0.0062, std: 0.1255
Collected 629 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3457, Q2 Loss=1.3457, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1900
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9332, Q2 Loss=0.9332, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3643
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.9894, Q2 Loss=1.9894, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7577
SAC Update 4/5: Actor Loss=-0.0902, Q1 Loss=0.9423, Q2 Loss=0.9423, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1931
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3059, Q2 Loss=1.3059, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1420

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.0%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.05s (19.8%)
Actor update: 0.10s (43.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.110138
Q1 loss: 1.303297
Q2 loss: 1.303297
Current threshold: -38.2512
Global Scale Offset: 0.0150
Reward stats: mean=-0.0062, std=0.1255, count=629
----------------------------------------------
SAC Update - Actor Loss: -0.1101, Q1 Loss: 1.3033, Q2 Loss: 1.3033, Entropy: 0.0000, Mean TD Error: 0.9294, Threshold: -38.2512
tensor([ 0.0638,  0.4511,  0.6056,  0.7568, -0.2266,  0.5667,  1.0063,  1.1758,
         1.4192,  0.3771,  0.1437,  0.8614, -0.0750,  0.0317, -0.5593,  0.5855],
       device='cuda:0')
Original likelihood: -29.04421043395996
Adjusted likelihood: -29.04421043395996
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.001865377998911
Current ori: tensor([-0.0750,  0.0317, -0.5593], device='cuda:0')
Middle force: tensor([0.5678, 0.8664, 0.5677, 0.8384, 1.4139, 0.6126, 0.8455, 0.7862, 0.5010],
       device='cuda:0')
Thumb force: tensor([1.0047, 0.5177, 0.6017, 0.5051, 0.7122, 0.5600, 0.5381, 0.7625, 0.5324],
       device='cuda:0')
Index force: tensor([0.6637, 0.6530, 0.5853, 0.5216, 0.8449, 0.5342, 0.5050, 0.9111, 0.5827],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0486 (scaled=-0.0486), steps=1
Reward stats updated: mean -0.0062 -> -0.0063, std: 0.1254
Collected 630 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.2296, Q2 Loss=1.2296, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5748
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.7707, Q2 Loss=0.7707, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3408
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.7532, Q2 Loss=0.7532, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4464
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.2517, Q2 Loss=2.2517, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.8863
SAC Update 5/5: Actor Loss=-0.1384, Q1 Loss=1.4124, Q2 Loss=1.4124, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4434

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (14.6%)
Q1 update: 0.05s (20.0%)
Q2 update: 0.05s (19.6%)
Actor update: 0.11s (42.5%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.165829
Q1 loss: 1.283516
Q2 loss: 1.283516
Current threshold: -38.2683
Global Scale Offset: 0.0149
Reward stats: mean=-0.0063, std=0.1254, count=630
----------------------------------------------
SAC Update - Actor Loss: -0.1658, Q1 Loss: 1.2835, Q2 Loss: 1.2835, Entropy: 0.0000, Mean TD Error: 1.3383, Threshold: -38.2683
tensor([-0.1063,  0.5192,  0.6449,  0.7567, -0.1184,  0.5638,  1.0330,  1.0560,
         1.3917,  0.4319, -0.0343,  0.9731, -0.2237,  0.0343, -0.5537,  0.7204],
       device='cuda:0')
Original likelihood: -80.5362548828125
Adjusted likelihood: -80.5362548828125
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 80.55661010742188
Projection step: 1, Loss: 84.20863342285156
Projection step: 2, Loss: 80.46797943115234
Projection step: 3, Loss: 80.29685974121094
Projection step: 4, Loss: 82.521728515625
Projection step: 5, Loss: 79.66651916503906
Projection step: 6, Loss: 80.75077819824219
Projection step: 7, Loss: 80.29914855957031
Projection step: 8, Loss: 78.00059509277344
Projection step: 9, Loss: 76.29676818847656
Projection step: 10, Loss: 77.12393951416016
Projection step: 11, Loss: 78.57925415039062
Projection step: 12, Loss: 81.8813705444336
Projection step: 13, Loss: 76.84164428710938
Projection step: 14, Loss: 77.3036880493164
Projection step: 15, Loss: 78.27227783203125
Projection step: 16, Loss: 84.72823333740234
Projection step: 17, Loss: 87.37754821777344
Projection step: 18, Loss: 79.38946533203125
Projection step: 19, Loss: 86.91114044189453
Projection step: 20, Loss: 70.46885681152344
Projection step: 21, Loss: 74.28279876708984
Projection step: 22, Loss: 80.855712890625
Projection step: 23, Loss: 88.09228515625
Projection step: 24, Loss: 83.86592102050781
Final likelihood: tensor([ -52.5572, -136.2032,  -70.4596,  -62.2020,  -54.6425,  -60.1745,
         -70.5879,  -59.6363,  -77.7177,  -99.3936, -113.5246,  -72.2058,
        -118.2690,  -78.6479,  -80.3825,  -66.3432])
Final projection likelihood: -79.5592
1 mode projection failed, trying anyway
New goal: tensor([-0.0937,  0.5093,  0.6374,  0.7309, -0.1118,  0.5596,  1.0122,  1.0665,
         1.3867,  0.4577,  0.0019,  0.9569, -0.2215,  0.0345, -0.4177],
       device='cuda:0')
tensor([[0.0103]], device='cuda:0') tensor([[0.0109]], device='cuda:0') tensor([[0.0065]], device='cuda:0')
Original likelihood: -102.31048583984375
Adjusted likelihood: -102.31048583984375
Likelihood residual: 0.0
Original likelihood: -52.881629943847656
Adjusted likelihood: -52.881629943847656
Likelihood residual: 0.0
{'index': 52.881629943847656, 'thumb_middle': 102.31048583984375}
Current yaw: tensor([-0.2237,  0.0343, -0.5537], device='cuda:0')
4 index
tensor([-0.1063,  0.5192,  0.6449,  0.7567, -0.1184,  0.5638,  1.0330,  1.0560,
         1.3917,  0.4319, -0.0343,  0.9731, -0.2237,  0.0343, -0.5537,  0.7204],
       device='cuda:0')
Solve time for step 1 10.762860357004683
Current ori: tensor([-0.2237,  0.0343, -0.5537], device='cuda:0')
Middle force: tensor([0.5098, 0.5258, 0.5455, 0.5226], device='cuda:0')
Thumb force: tensor([0.5464, 0.6434, 0.5824, 0.6544], device='cuda:0')
tensor([-0.1443,  0.4310,  0.5869,  0.7084, -0.0802,  0.6554,  1.0143,  1.0310,
         1.4442,  0.4677,  0.0209,  1.0146, -0.5560,  0.0819, -0.5537,  0.3533],
       device='cuda:0')
Solve time for step 2 4.200375432963483
Current ori: tensor([-0.5560,  0.0819, -0.5537], device='cuda:0')
Middle force: tensor([0.5192, 0.5531, 0.5296], device='cuda:0')
Thumb force: tensor([0.5810, 0.5587, 0.6342], device='cuda:0')
tensor([-0.1905,  0.6166,  0.6205,  0.6908, -0.1514,  0.9795,  0.9407,  0.8757,
         1.5000,  0.2703,  0.0395,  1.0435, -1.2439,  0.1486, -0.5537,  2.3931],
       device='cuda:0')
Solve time for step 3 4.172576421988197
Current ori: tensor([-1.2439,  0.1486, -0.5537], device='cuda:0')
Middle force: tensor([0.5453, 0.5548], device='cuda:0')
Thumb force: tensor([0.5629, 0.6052], device='cuda:0')
tensor([-0.1926,  0.6149,  0.5963,  0.6706, -0.3721,  1.2452,  0.8795,  1.0296,
         1.5000,  0.1661, -0.0284,  1.3050, -1.9432,  0.1399, -0.5536,  2.4988],
       device='cuda:0')
Solve time for step 4 4.0079759520012885
Current ori: tensor([-1.9432,  0.1399, -0.5536], device='cuda:0')
Middle force: tensor([0.5178], device='cuda:0')
Thumb force: tensor([0.6743], device='cuda:0')
Storing RECOVERY transition: reward=-1.6183 (scaled=-0.4046), steps=4
Reward stats updated: mean -0.0063 -> -0.0069, std: 0.1263
Collected 631 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.3313, Q2 Loss=1.3313, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3564
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.3944, Q2 Loss=1.3944, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5232
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1431, Q2 Loss=1.1431, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0452
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3392, Q2 Loss=1.3392, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4459
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2376, Q2 Loss=1.2376, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1458

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.4%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (19.2%)
Actor update: 0.09s (40.8%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.289116
Q2 loss: 1.289116
Current threshold: -38.2785
Global Scale Offset: 0.0149
Reward stats: mean=-0.0069, std=0.1263, count=631
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.2891, Q2 Loss: 1.2891, Entropy: 0.0000, Mean TD Error: 1.1033, Threshold: -38.2785
Original likelihood: -1293.5682373046875
Adjusted likelihood: -1293.5682373046875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 41
Loaded trajectory sampler
Current yaw: tensor([ 0.0002,  0.0141, -0.0452], device='cuda:0')
Current yaw: tensor([ 0.0002,  0.0141, -0.0452], device='cuda:0')
1 turn
Sampling time 3.5910485989879817
tensor([ 1.2538e-01,  6.0816e-01,  5.8626e-01,  5.2746e-01, -1.1340e-01,
         4.8872e-01,  9.6018e-01,  9.1602e-01,  1.2777e+00,  2.5176e-01,
         2.1937e-01,  1.1557e+00,  2.2178e-04,  1.4103e-02, -4.5174e-02,
         3.0707e-01], device='cuda:0')
Original likelihood: -17.127979278564453
Adjusted likelihood: -17.127979278564453
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.19320662500104
Current ori: tensor([ 0.0002,  0.0141, -0.0452], device='cuda:0')
Middle force: tensor([0.5199, 0.4954, 1.2524, 1.3342, 0.5685, 0.5309, 0.5094, 0.5334, 0.5539,
        0.5624, 0.5493, 0.5865], device='cuda:0')
Thumb force: tensor([0.7547, 0.5793, 1.1573, 1.0626, 0.7061, 0.8640, 0.5366, 1.1339, 0.6184,
        0.5882, 0.5889, 0.8559], device='cuda:0')
Index force: tensor([0.5002, 0.6572, 0.5890, 0.5304, 0.9051, 0.5400, 0.5817, 0.4961, 0.5137,
        0.6030, 0.6278, 0.6082], device='cuda:0')
Storing NORMAL transition: reward=0.0614 (scaled=0.0614), steps=1
Reward stats updated: mean -0.0069 -> -0.0068, std: 0.1262
Collected 632 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.6894, Q2 Loss=0.6894, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.0705
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.7615, Q2 Loss=1.7615, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4801
SAC Update 3/5: Actor Loss=-0.1189, Q1 Loss=1.0584, Q2 Loss=1.0584, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5245
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.2414, Q2 Loss=1.2414, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7539
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7816, Q2 Loss=0.7816, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4127

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (19.8%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (18.8%)
Actor update: 0.10s (38.1%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.069823
Q1 loss: 1.106466
Q2 loss: 1.106466
Current threshold: -38.2845
Global Scale Offset: 0.0148
Reward stats: mean=-0.0068, std=0.1262, count=632
----------------------------------------------
SAC Update - Actor Loss: -0.0698, Q1 Loss: 1.1065, Q2 Loss: 1.1065, Entropy: 0.0000, Mean TD Error: 0.6483, Threshold: -38.2845
tensor([ 0.1177,  0.6413,  0.5515,  0.4873, -0.1272,  0.5104,  0.9041,  0.9842,
         1.4342,  0.0456,  0.1504,  1.1091, -0.0102,  0.0110, -0.1066,  0.4689],
       device='cuda:0')
Original likelihood: -18.19968032836914
Adjusted likelihood: -18.19968032836914
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.5314066800056025
Current ori: tensor([-0.0102,  0.0110, -0.1066], device='cuda:0')
Middle force: tensor([0.5023, 1.2052, 1.2976, 0.5603, 0.5269, 0.5081, 0.5294, 0.5455, 0.5555,
        0.5459, 0.5811], device='cuda:0')
Thumb force: tensor([0.5673, 1.1118, 1.0164, 0.6877, 0.8367, 0.5330, 1.0964, 0.6092, 0.5817,
        0.5770, 0.8382], device='cuda:0')
Index force: tensor([0.6418, 0.5797, 0.5242, 0.8897, 0.5366, 0.5759, 0.5018, 0.5116, 0.5949,
        0.6217, 0.6021], device='cuda:0')
Storing NORMAL transition: reward=0.0387 (scaled=0.0387), steps=1
Reward stats updated: mean -0.0068 -> -0.0068, std: 0.1261
Collected 633 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2234, Q2 Loss=1.2234, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4920
SAC Update 2/5: Actor Loss=-0.2131, Q1 Loss=1.3167, Q2 Loss=1.3167, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3611
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2264, Q2 Loss=1.2264, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5100
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8032, Q2 Loss=0.8032, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7591
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.7839, Q2 Loss=1.7839, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6679

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.05s (18.0%)
Q1 update: 0.05s (18.9%)
Q2 update: 0.05s (18.3%)
Actor update: 0.09s (36.6%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.042613
Q1 loss: 1.270731
Q2 loss: 1.270731
Current threshold: -38.2880
Global Scale Offset: 0.0148
Reward stats: mean=-0.0068, std=0.1261, count=633
----------------------------------------------
SAC Update - Actor Loss: -0.0426, Q1 Loss: 1.2707, Q2 Loss: 1.2707, Entropy: 0.0000, Mean TD Error: 1.3580, Threshold: -38.2880
tensor([ 0.1745,  0.6265,  0.6337,  0.4771, -0.1595,  0.5109,  0.9265,  1.0292,
         1.3601,  0.1816,  0.2154,  1.0414, -0.0118,  0.0171, -0.1456, -0.0479],
       device='cuda:0')
Original likelihood: -25.581039428710938
Adjusted likelihood: -25.581039428710938
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.282159610011149
Current ori: tensor([-0.0118,  0.0171, -0.1456], device='cuda:0')
Middle force: tensor([0.5223, 1.8511, 0.5625, 1.3385, 0.5071, 1.1577, 0.5202, 0.5650, 0.6318,
        0.5505], device='cuda:0')
Thumb force: tensor([0.6057, 1.5468, 0.5033, 0.8411, 0.6646, 1.2226, 0.5489, 0.5240, 0.5405,
        0.5807], device='cuda:0')
Index force: tensor([0.5151, 0.6044, 0.5574, 0.6437, 0.7504, 0.5452, 0.5260, 0.5266, 0.5285,
        0.5472], device='cuda:0')
Storing NORMAL transition: reward=-0.0020 (scaled=-0.0020), steps=1
Reward stats updated: mean -0.0068 -> -0.0067, std: 0.1260
Collected 634 transitions for RL
SAC Update 1/5: Actor Loss=-0.1540, Q1 Loss=1.4929, Q2 Loss=1.4929, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4720
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3899, Q2 Loss=1.3899, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6593
SAC Update 3/5: Actor Loss=-0.4123, Q1 Loss=2.1240, Q2 Loss=2.1240, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1976
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=2.5176, Q2 Loss=2.5176, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0123
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8181, Q2 Loss=0.8181, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4234

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.4%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.05s (19.8%)
Actor update: 0.11s (42.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.113252
Q1 loss: 1.668513
Q2 loss: 1.668513
Current threshold: -38.2901
Global Scale Offset: 0.0148
Reward stats: mean=-0.0067, std=0.1260, count=634
----------------------------------------------
SAC Update - Actor Loss: -0.1133, Q1 Loss: 1.6685, Q2 Loss: 1.6685, Entropy: 0.0000, Mean TD Error: 1.5529, Threshold: -38.2901
tensor([ 0.1990,  0.6977,  0.4627,  0.5872, -0.2356,  0.5735,  0.9619,  0.9747,
         1.4568,  0.0429,  0.2490,  0.7946, -0.0360,  0.0329, -0.1459, -1.9217],
       device='cuda:0')
Original likelihood: -32.194068908691406
Adjusted likelihood: -32.194068908691406
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.052997873048298
Current ori: tensor([-0.0360,  0.0329, -0.1459], device='cuda:0')
Middle force: tensor([1.8292, 0.5617, 1.3315, 0.5057, 1.1546, 0.5174, 0.5632, 0.6266, 0.5485],
       device='cuda:0')
Thumb force: tensor([1.4947, 0.5032, 0.8257, 0.6877, 1.1954, 0.5549, 0.5223, 0.5410, 0.5790],
       device='cuda:0')
Index force: tensor([0.5931, 0.5516, 0.6441, 0.7259, 0.5442, 0.5221, 0.5255, 0.5255, 0.5436],
       device='cuda:0')
Storing NORMAL transition: reward=0.0626 (scaled=0.0626), steps=1
Reward stats updated: mean -0.0067 -> -0.0066, std: 0.1260
Collected 635 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=4.1327, Q2 Loss=4.1327, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.4443
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.7226, Q2 Loss=0.7226, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4336
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3545, Q2 Loss=1.3545, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8936
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.0473, Q2 Loss=2.0473, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8932
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9800, Q2 Loss=0.9800, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1566

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.8%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.4%)
Actor update: 0.08s (39.9%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.3%)
Actor loss: -0.092103
Q1 loss: 1.847398
Q2 loss: 1.847398
Current threshold: -38.2914
Global Scale Offset: 0.0148
Reward stats: mean=-0.0066, std=0.1260, count=635
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.8474, Q2 Loss: 1.8474, Entropy: 0.0000, Mean TD Error: 2.1643, Threshold: -38.2914
tensor([ 2.0692e-01,  7.0597e-01,  4.9372e-01,  6.0882e-01, -1.5287e-01,
         6.1717e-01,  8.3174e-01,  1.1915e+00,  1.5000e+00,  1.2602e-02,
         2.2579e-01,  5.6437e-01, -5.5733e-02, -1.6479e-03, -2.0995e-01,
        -1.7697e+00], device='cuda:0')
Original likelihood: -30.462581634521484
Adjusted likelihood: -30.462581634521484
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.826390418980736
Current ori: tensor([-0.0557, -0.0016, -0.2100], device='cuda:0')
Middle force: tensor([0.5584, 1.3100, 0.5064, 1.1345, 0.5164, 0.5603, 0.6233, 0.5484],
       device='cuda:0')
Thumb force: tensor([0.5026, 0.8253, 0.6607, 1.1825, 0.5466, 0.5210, 0.5383, 0.5722],
       device='cuda:0')
Index force: tensor([0.5491, 0.6475, 0.7249, 0.5438, 0.5230, 0.5246, 0.5237, 0.5417],
       device='cuda:0')
Storing NORMAL transition: reward=0.0144 (scaled=0.0144), steps=1
Reward stats updated: mean -0.0066 -> -0.0066, std: 0.1259
Collected 636 transitions for RL
SAC Update 1/5: Actor Loss=-0.1148, Q1 Loss=1.1512, Q2 Loss=1.1512, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9994
SAC Update 2/5: Actor Loss=-0.1467, Q1 Loss=1.2323, Q2 Loss=1.2323, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8845
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0188, Q2 Loss=1.0188, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8026
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.1975, Q2 Loss=1.1975, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0224
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8975, Q2 Loss=0.8975, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4325

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (13.9%)
Q1 update: 0.05s (19.0%)
Q2 update: 0.05s (20.4%)
Actor update: 0.11s (43.5%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.052293
Q1 loss: 1.099447
Q2 loss: 1.099447
Current threshold: -38.2921
Global Scale Offset: 0.0148
Reward stats: mean=-0.0066, std=0.1259, count=636
----------------------------------------------
SAC Update - Actor Loss: -0.0523, Q1 Loss: 1.0994, Q2 Loss: 1.0994, Entropy: 0.0000, Mean TD Error: 0.8283, Threshold: -38.2921
tensor([ 0.3308,  0.7563,  0.5622,  0.6450, -0.0920,  0.6479,  0.9733,  1.0251,
         1.5000,  0.0415,  0.2249,  0.4216, -0.0527, -0.0857, -0.2349, -1.5905],
       device='cuda:0')
Original likelihood: -32.472900390625
Adjusted likelihood: -32.472900390625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.4185976589797065
Current ori: tensor([-0.0527, -0.0857, -0.2349], device='cuda:0')
Middle force: tensor([1.2702, 0.5062, 1.1102, 0.5153, 0.5554, 0.6143, 0.5485],
       device='cuda:0')
Thumb force: tensor([0.8286, 0.6429, 1.1812, 0.5397, 0.5204, 0.5393, 0.5642],
       device='cuda:0')
Index force: tensor([0.6448, 0.7293, 0.5420, 0.5233, 0.5233, 0.5215, 0.5400],
       device='cuda:0')
Storing NORMAL transition: reward=0.0385 (scaled=0.0385), steps=1
Reward stats updated: mean -0.0066 -> -0.0065, std: 0.1258
Collected 637 transitions for RL
SAC Update 1/5: Actor Loss=-0.1397, Q1 Loss=1.1300, Q2 Loss=1.1300, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5488
SAC Update 2/5: Actor Loss=-0.0026, Q1 Loss=1.2029, Q2 Loss=1.2029, Entropy=0.3243, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8979
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.2466, Q2 Loss=1.2466, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1679
SAC Update 4/5: Actor Loss=-0.4601, Q1 Loss=4.1434, Q2 Loss=4.1434, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.6244
SAC Update 5/5: Actor Loss=-0.2109, Q1 Loss=1.3287, Q2 Loss=1.3287, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4380

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.7%)
Q1 update: 0.04s (18.7%)
Q2 update: 0.04s (18.7%)
Actor update: 0.08s (40.3%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.162654
Q1 loss: 1.810325
Q2 loss: 1.810325
Current threshold: -38.2959
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1258, count=637
----------------------------------------------
SAC Update - Actor Loss: -0.1627, Q1 Loss: 1.8103, Q2 Loss: 1.8103, Entropy: 0.0649, Mean TD Error: 1.3354, Threshold: -38.2959
tensor([ 0.3132,  0.7085,  0.5998,  0.6506, -0.0966,  0.6477,  0.9797,  1.0539,
         1.4992,  0.0699,  0.2755,  0.3266, -0.0515, -0.0760, -0.2725, -1.4912],
       device='cuda:0')
Original likelihood: -28.38813018798828
Adjusted likelihood: -28.38813018798828
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.422978354967199
Current ori: tensor([-0.0515, -0.0760, -0.2725], device='cuda:0')
Middle force: tensor([0.5057, 1.0830, 0.5139, 0.5528, 0.6124, 0.5476], device='cuda:0')
Thumb force: tensor([0.6205, 1.1759, 0.5347, 0.5182, 0.5343, 0.5589], device='cuda:0')
Index force: tensor([0.7363, 0.5428, 0.5233, 0.5217, 0.5199, 0.5366], device='cuda:0')
Storing NORMAL transition: reward=0.0205 (scaled=0.0205), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1257
Collected 638 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3886, Q2 Loss=1.3886, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3271
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.8320, Q2 Loss=0.8320, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4682
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9307, Q2 Loss=0.9307, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7339
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=4.9444, Q2 Loss=4.9444, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.6512
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5421, Q2 Loss=1.5421, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6585

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.3%)
Q1 update: 0.05s (19.9%)
Q2 update: 0.05s (18.8%)
Actor update: 0.10s (40.3%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.927553
Q2 loss: 1.927553
Current threshold: -38.2987
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1257, count=638
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.9276, Q2 Loss: 1.9276, Entropy: 0.0000, Mean TD Error: 2.3678, Threshold: -38.2987
tensor([ 0.2906,  0.6565,  0.6717,  0.5640, -0.0025,  0.5657,  0.9601,  1.1539,
         1.4980,  0.0768,  0.2905,  0.2620, -0.0617, -0.0765, -0.2962, -1.2396],
       device='cuda:0')
Original likelihood: -33.344146728515625
Adjusted likelihood: -33.344146728515625
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 8 4.163867958006449
Current ori: tensor([-0.0617, -0.0765, -0.2962], device='cuda:0')
Middle force: tensor([1.0394, 0.5122, 0.5498, 0.6117, 0.5455], device='cuda:0')
Thumb force: tensor([1.1713, 0.5294, 0.5162, 0.5278, 0.5525], device='cuda:0')
Index force: tensor([0.5474, 0.5257, 0.5210, 0.5199, 0.5353], device='cuda:0')
Storing NORMAL transition: reward=-0.0045 (scaled=-0.0045), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1256
Collected 639 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3060, Q2 Loss=1.3060, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0644
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.7371, Q2 Loss=0.7371, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2908
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.9809, Q2 Loss=1.9809, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.4480
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.4895, Q2 Loss=1.4895, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5647
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.0794, Q2 Loss=1.0794, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3307

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.09s (34.6%)
Q1 update: 0.04s (15.5%)
Q2 update: 0.04s (14.7%)
Actor update: 0.09s (32.3%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.138155
Q1 loss: 1.318592
Q2 loss: 1.318592
Current threshold: -38.3004
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1256, count=639
----------------------------------------------
SAC Update - Actor Loss: -0.1382, Q1 Loss: 1.3186, Q2 Loss: 1.3186, Entropy: 0.0000, Mean TD Error: 1.1397, Threshold: -38.3004
tensor([ 0.3106,  0.6522,  0.7374,  0.5174, -0.0963,  0.5679,  0.8448,  1.2513,
         1.4558,  0.1011,  0.4074,  0.1669, -0.0416, -0.0689, -0.2858, -1.4083],
       device='cuda:0')
Original likelihood: -32.37393569946289
Adjusted likelihood: -32.37393569946289
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 9 3.70186150202062
Current ori: tensor([-0.0416, -0.0689, -0.2858], device='cuda:0')
Middle force: tensor([0.5085, 0.5446, 0.5986, 0.5391], device='cuda:0')
Thumb force: tensor([0.5290, 0.5150, 0.5278, 0.5506], device='cuda:0')
Index force: tensor([0.5282, 0.5205, 0.5185, 0.5341], device='cuda:0')
Storing NORMAL transition: reward=0.0215 (scaled=0.0215), steps=1
Reward stats updated: mean -0.0065 -> -0.0064, std: 0.1255
Collected 640 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.9245, Q2 Loss=0.9245, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7565
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5691, Q2 Loss=1.5691, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0553
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9648, Q2 Loss=0.9648, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9062
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0058, Q2 Loss=1.0058, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5964
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=5.4429, Q2 Loss=5.4429, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.8034

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (15.8%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.05s (19.5%)
Actor update: 0.11s (41.6%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.981426
Q2 loss: 1.981426
Current threshold: -38.3013
Global Scale Offset: 0.0148
Reward stats: mean=-0.0064, std=0.1255, count=640
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.9814, Q2 Loss: 1.9814, Entropy: 0.0000, Mean TD Error: 2.2236, Threshold: -38.3013
tensor([ 0.2954,  0.6223,  0.7123,  0.6070, -0.0639,  0.5788,  0.9358,  1.1920,
         1.4683,  0.0803,  0.4251,  0.1283, -0.0394, -0.0597, -0.3056, -1.3324],
       device='cuda:0')
Original likelihood: -37.90613555908203
Adjusted likelihood: -37.90613555908203
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 10 3.8134956060093828
Current ori: tensor([-0.0394, -0.0597, -0.3056], device='cuda:0')
Middle force: tensor([0.5251, 0.5167, 0.5463], device='cuda:0')
Thumb force: tensor([0.5385, 0.5305, 0.7275], device='cuda:0')
Index force: tensor([0.5681, 0.5794, 0.5620], device='cuda:0')
Storing NORMAL transition: reward=-0.0277 (scaled=-0.0277), steps=1
Reward stats updated: mean -0.0064 -> -0.0065, std: 0.1254
Collected 641 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.8069, Q2 Loss=0.8069, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4387
SAC Update 2/5: Actor Loss=-0.2324, Q1 Loss=1.0544, Q2 Loss=1.0544, Entropy=0.3353, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7879
SAC Update 3/5: Actor Loss=-0.0574, Q1 Loss=1.2227, Q2 Loss=1.2227, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1010
SAC Update 4/5: Actor Loss=-0.0220, Q1 Loss=0.8463, Q2 Loss=0.8463, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.6270
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0322, Q2 Loss=1.0322, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7242

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (17.4%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (19.2%)
Actor update: 0.11s (40.2%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.108413
Q1 loss: 0.992496
Q2 loss: 0.992496
Current threshold: -38.3046
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1254, count=641
----------------------------------------------
SAC Update - Actor Loss: -0.1084, Q1 Loss: 0.9925, Q2 Loss: 0.9925, Entropy: 0.0671, Mean TD Error: 1.3358, Threshold: -38.3046
tensor([ 0.2697,  0.5696,  0.7222,  0.6620, -0.0808,  0.5691,  0.9370,  1.1800,
         1.4733,  0.0893,  0.4123,  0.1990, -0.0373, -0.0476, -0.2745, -1.2129],
       device='cuda:0')
Original likelihood: -31.765047073364258
Adjusted likelihood: -31.765047073364258
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 11 3.6022607519989833
Current ori: tensor([-0.0373, -0.0476, -0.2745], device='cuda:0')
Middle force: tensor([0.5154, 0.5434], device='cuda:0')
Thumb force: tensor([0.5241, 0.7164], device='cuda:0')
Index force: tensor([0.5733, 0.5579], device='cuda:0')
Storing NORMAL transition: reward=-0.0065 (scaled=-0.0065), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1253
Collected 642 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.1115, Q2 Loss=1.1115, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9608
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8836, Q2 Loss=0.8836, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7395
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8816, Q2 Loss=0.8816, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5000
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7190, Q2 Loss=0.7190, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6417
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.5751, Q2 Loss=1.5751, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0739

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (19.6%)
Q1 update: 0.04s (18.8%)
Q2 update: 0.04s (18.0%)
Actor update: 0.09s (39.5%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.046052
Q1 loss: 1.034174
Q2 loss: 1.034174
Current threshold: -38.3070
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1253, count=642
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0342, Q2 Loss: 1.0342, Entropy: 0.0000, Mean TD Error: 0.7832, Threshold: -38.3070
tensor([ 0.2426,  0.5313,  0.7293,  0.6866, -0.0858,  0.5740,  0.9300,  1.1843,
         1.4768,  0.0839,  0.4028,  0.2228, -0.0372, -0.0461, -0.2676, -1.0705],
       device='cuda:0')
Original likelihood: -26.82756805419922
Adjusted likelihood: -26.82756805419922
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 12 3.536579799023457
Current ori: tensor([-0.0372, -0.0461, -0.2676], device='cuda:0')
Middle force: tensor([0.5412], device='cuda:0')
Thumb force: tensor([0.7070], device='cuda:0')
Index force: tensor([0.5527], device='cuda:0')
Storing NORMAL transition: reward=-0.0093 (scaled=-0.0093), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1252
Collected 643 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=3.3574, Q2 Loss=3.3574, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.3566
SAC Update 2/5: Actor Loss=-0.2323, Q1 Loss=1.1120, Q2 Loss=1.1120, Entropy=0.3405, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2210
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.4978, Q2 Loss=1.4978, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8765
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.7942, Q2 Loss=0.7942, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3301
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=1.1020, Q2 Loss=1.1020, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4800

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.1%)
Q1 update: 0.04s (18.4%)
Q2 update: 0.04s (19.4%)
Actor update: 0.08s (40.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092515
Q1 loss: 1.572668
Q2 loss: 1.572668
Current threshold: -38.3110
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1252, count=643
----------------------------------------------
SAC Update - Actor Loss: -0.0925, Q1 Loss: 1.5727, Q2 Loss: 1.5727, Entropy: 0.0681, Mean TD Error: 2.0528, Threshold: -38.3110
Original likelihood: -26.60591697692871
Adjusted likelihood: -26.60591697692871
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Current yaw: tensor([-0.0250, -0.0380, -0.2558], device='cuda:0')
2 turn
Sampling time 3.588087447977159
tensor([ 0.2577,  0.4981,  0.7766,  0.7062, -0.1289,  0.5675,  0.9423,  1.1339,
         1.5000,  0.0285,  0.3982,  0.2534, -0.0250, -0.0380, -0.2558, -1.1607],
       device='cuda:0')
Original likelihood: -25.605796813964844
Adjusted likelihood: -25.605796813964844
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.189675525994971
Current ori: tensor([-0.0250, -0.0380, -0.2558], device='cuda:0')
Middle force: tensor([0.7675, 0.5342, 0.6571, 0.9510, 1.0115, 0.7408, 0.5011, 0.5404, 0.5766,
        0.5362, 0.5416, 0.9986], device='cuda:0')
Thumb force: tensor([1.1089, 0.9837, 1.0106, 0.7259, 1.7307, 0.7050, 0.6074, 1.0504, 0.6194,
        0.5107, 0.5881, 1.1528], device='cuda:0')
Index force: tensor([0.8446, 0.6021, 0.5159, 0.8272, 0.5405, 0.5727, 0.5858, 0.5921, 0.5986,
        0.5427, 0.5565, 0.5995], device='cuda:0')
Storing NORMAL transition: reward=-0.0468 (scaled=-0.0468), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1251
Collected 644 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.4123, Q2 Loss=1.4123, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3598
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.5853, Q2 Loss=1.5853, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2899
SAC Update 3/5: Actor Loss=-0.0030, Q1 Loss=1.1816, Q2 Loss=1.1816, Entropy=0.3436, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0240
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.7444, Q2 Loss=0.7444, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1848
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=3.1685, Q2 Loss=3.1685, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.3170

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.5%)
Target Q: 0.04s (14.3%)
Q1 update: 0.05s (19.1%)
Q2 update: 0.06s (21.4%)
Actor update: 0.12s (42.1%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092704
Q1 loss: 1.618410
Q2 loss: 1.618410
Current threshold: -38.3166
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1251, count=644
----------------------------------------------
SAC Update - Actor Loss: -0.0927, Q1 Loss: 1.6184, Q2 Loss: 1.6184, Entropy: 0.0687, Mean TD Error: 1.6351, Threshold: -38.3166
tensor([ 0.2464,  0.4555,  0.7613,  0.8233, -0.1276,  0.5879,  0.9759,  1.0388,
         1.5000, -0.0492,  0.3042,  0.5361, -0.0194, -0.0403, -0.2088, -0.8130],
       device='cuda:0')
Original likelihood: -19.269515991210938
Adjusted likelihood: -19.269515991210938
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.567365275986958
Current ori: tensor([-0.0194, -0.0403, -0.2088], device='cuda:0')
Middle force: tensor([0.5319, 0.6550, 0.9483, 1.0018, 0.7360, 0.5012, 0.5387, 0.5764, 0.5365,
        0.5412, 0.9929], device='cuda:0')
Thumb force: tensor([0.9684, 0.9934, 0.7081, 1.6981, 0.6992, 0.5927, 1.0354, 0.6119, 0.5095,
        0.5821, 1.1299], device='cuda:0')
Index force: tensor([0.5979, 0.5154, 0.8186, 0.5378, 0.5687, 0.5854, 0.5889, 0.5958, 0.5414,
        0.5548, 0.5949], device='cuda:0')
Storing NORMAL transition: reward=-0.0055 (scaled=-0.0055), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1250
Collected 645 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0231, Q2 Loss=1.0231, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5686
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=3.2732, Q2 Loss=3.2732, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.3543
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3109, Q2 Loss=1.3109, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7462
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3515, Q2 Loss=1.3515, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8836
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.3055, Q2 Loss=1.3055, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6810

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (16.4%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (19.3%)
Actor update: 0.10s (40.8%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.1%)
Actor loss: 0.000000
Q1 loss: 1.652845
Q2 loss: 1.652845
Current threshold: -38.3213
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1250, count=645
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.6528, Q2 Loss: 1.6528, Entropy: 0.0000, Mean TD Error: 2.0467, Threshold: -38.3213
tensor([ 0.1241,  0.3842,  0.6970,  0.8564, -0.2299,  0.5468,  1.1234,  1.0085,
         1.4443,  0.0295,  0.3205,  0.4028, -0.0467, -0.0798, -0.2090,  1.7721],
       device='cuda:0')
Original likelihood: -26.504579544067383
Adjusted likelihood: -26.504579544067383
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.0758703899919055
Current ori: tensor([-0.0467, -0.0798, -0.2090], device='cuda:0')
Middle force: tensor([0.5025, 0.5146, 0.5540, 0.5438, 0.5438, 0.5648, 0.5617, 0.5615, 0.5848,
        0.5687], device='cuda:0')
Thumb force: tensor([0.5640, 0.5622, 0.5598, 0.5564, 0.5502, 0.5563, 0.5474, 0.5599, 0.5392,
        0.5575], device='cuda:0')
Index force: tensor([0.5842, 0.5823, 0.5857, 0.5671, 0.5705, 0.5643, 0.5561, 0.5580, 0.5537,
        0.5614], device='cuda:0')
Storing NORMAL transition: reward=0.0049 (scaled=0.0049), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1249
Collected 646 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.0858, Q2 Loss=1.0858, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2852
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2944, Q2 Loss=1.2944, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4564
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0812, Q2 Loss=1.0812, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9130
SAC Update 4/5: Actor Loss=-0.0977, Q1 Loss=1.0294, Q2 Loss=1.0294, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6178
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=4.5261, Q2 Loss=4.5261, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.8838

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.9%)
Q1 update: 0.05s (18.8%)
Q2 update: 0.05s (19.7%)
Actor update: 0.10s (40.4%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.065586
Q1 loss: 1.803371
Q2 loss: 1.803371
Current threshold: -38.3241
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1249, count=646
----------------------------------------------
SAC Update - Actor Loss: -0.0656, Q1 Loss: 1.8034, Q2 Loss: 1.8034, Entropy: 0.0000, Mean TD Error: 1.6313, Threshold: -38.3241
tensor([ 0.0718,  0.3377,  0.7296,  0.8735, -0.1824,  0.6495,  1.0886,  1.0382,
         1.4333,  0.0992,  0.3747,  0.3256, -0.0536, -0.0570, -0.2118,  1.6056],
       device='cuda:0')
Original likelihood: -17.709484100341797
Adjusted likelihood: -17.709484100341797
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.066245045978576
Current ori: tensor([-0.0536, -0.0570, -0.2118], device='cuda:0')
Middle force: tensor([1.0584, 1.0945, 0.7498, 0.5055, 0.5382, 0.5897, 0.5618, 0.5497, 1.1522],
       device='cuda:0')
Thumb force: tensor([0.6299, 1.5749, 0.6762, 0.5412, 1.0018, 0.5913, 0.5074, 0.5655, 0.9597],
       device='cuda:0')
Index force: tensor([0.7339, 0.5302, 0.5619, 0.5515, 0.5850, 0.5841, 0.5258, 0.5475, 0.5697],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0248 (scaled=-0.0248), steps=1
Reward stats updated: mean -0.0065 -> -0.0066, std: 0.1248
Collected 647 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.7344, Q2 Loss=1.7344, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5034
SAC Update 2/5: Actor Loss=-0.1839, Q1 Loss=1.4232, Q2 Loss=1.4232, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0087
SAC Update 3/5: Actor Loss=-0.2345, Q1 Loss=1.3943, Q2 Loss=1.3943, Entropy=0.3465, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3563
SAC Update 4/5: Actor Loss=-0.1511, Q1 Loss=2.3127, Q2 Loss=2.3127, Entropy=0.3464, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.7938
SAC Update 5/5: Actor Loss=-0.2091, Q1 Loss=1.4225, Q2 Loss=1.4225, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7278

------ SAC Update Summary (5 iterations) ------
Total time: 0.28s, Avg iteration: 0.06s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.2%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (18.8%)
Actor update: 0.11s (41.4%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.155734
Q1 loss: 1.657402
Q2 loss: 1.657402
Current threshold: -38.3341
Global Scale Offset: 0.0148
Reward stats: mean=-0.0066, std=0.1248, count=647
----------------------------------------------
SAC Update - Actor Loss: -0.1557, Q1 Loss: 1.6574, Q2 Loss: 1.6574, Entropy: 0.1386, Mean TD Error: 1.4780, Threshold: -38.3341
tensor([ 0.0768,  0.4017,  0.6880,  0.9225, -0.2829,  0.5634,  1.1146,  1.0100,
         1.4469, -0.0182,  0.3085,  0.3124, -0.1020, -0.0956, -0.1982,  0.4641],
       device='cuda:0')
Original likelihood: -29.948593139648438
Adjusted likelihood: -29.948593139648438
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.789018245995976
Current ori: tensor([-0.1020, -0.0956, -0.1982], device='cuda:0')
Middle force: tensor([0.5169, 1.4815, 0.5029, 0.5573, 0.5526, 0.5338, 0.6232, 1.0368],
       device='cuda:0')
Thumb force: tensor([0.5282, 0.7427, 0.5259, 0.5909, 0.5472, 0.5394, 0.5836, 1.4484],
       device='cuda:0')
Index force: tensor([0.6173, 0.7905, 0.5251, 0.5808, 0.5655, 0.5896, 0.5889, 0.5658],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0349 (scaled=-0.0349), steps=1
Reward stats updated: mean -0.0066 -> -0.0066, std: 0.1247
Collected 648 transitions for RL
SAC Update 1/5: Actor Loss=-0.1415, Q1 Loss=2.4779, Q2 Loss=2.4779, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0090
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.9963, Q2 Loss=0.9963, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5754
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7288, Q2 Loss=0.7288, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1735
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.6059, Q2 Loss=2.6059, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.4231
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1281, Q2 Loss=1.1281, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9085

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.1%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.05s (20.1%)
Actor update: 0.10s (40.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.120400
Q1 loss: 1.587385
Q2 loss: 1.587385
Current threshold: -38.3470
Global Scale Offset: 0.0148
Reward stats: mean=-0.0066, std=0.1247, count=648
----------------------------------------------
SAC Update - Actor Loss: -0.1204, Q1 Loss: 1.5874, Q2 Loss: 1.5874, Entropy: 0.0000, Mean TD Error: 1.6179, Threshold: -38.3470
tensor([-0.0299,  0.2780,  0.6629,  0.8388, -0.0831,  0.8766,  0.9855,  1.0597,
         1.3702,  0.1225,  0.3596,  0.0678, -0.1585, -0.1468, -0.1848, -2.7519],
       device='cuda:0')
Original likelihood: -45.61219787597656
Adjusted likelihood: -45.61219787597656
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 45.859764099121094
Projection step: 1, Loss: 43.99542999267578
Projection step: 2, Loss: 45.343631744384766
Projection step: 3, Loss: 48.371620178222656
Projection step: 4, Loss: 45.07445526123047
Projection step: 5, Loss: 42.84571838378906
Projection step: 6, Loss: 43.656864166259766
Projection step: 7, Loss: 42.169857025146484
Projection step: 8, Loss: 42.383636474609375
Projection step: 9, Loss: 44.10755920410156
Projection step: 10, Loss: 43.81131362915039
Projection step: 11, Loss: 43.92340850830078
Projection step: 12, Loss: 42.29668426513672
Projection step: 13, Loss: 40.0471076965332
Projection step: 14, Loss: 41.91724395751953
Projection step: 15, Loss: 42.3984375
Projection step: 16, Loss: 41.90397644042969
Projection step: 17, Loss: 39.804203033447266
Projection step: 18, Loss: 42.052101135253906
Projection step: 19, Loss: 39.13566970825195
Projection step: 20, Loss: 42.53473663330078
Projection step: 21, Loss: 39.34743118286133
Projection step: 22, Loss: 41.323368072509766
Projection step: 23, Loss: 39.075172424316406
Projection step: 24, Loss: 39.999168395996094
Final likelihood: tensor([-39.4910, -38.5515, -40.0879, -41.1516, -39.0869, -42.5576, -42.4291,
        -36.2212, -38.7527, -35.9041, -37.1225, -36.0561, -35.7517, -38.4923,
        -39.7416, -36.1885])
Final projection likelihood: -38.5991
1 mode projection failed, trying anyway
New goal: tensor([-0.0191,  0.2870,  0.6752,  0.8539, -0.0909,  0.8134,  0.8514,  1.1207,
         1.3907,  0.1312,  0.3461,  0.0613, -0.1576, -0.1426,  0.1010],
       device='cuda:0')
tensor([[0.0025]], device='cuda:0') tensor([[0.0030]], device='cuda:0') tensor([[0.0340]], device='cuda:0')
Original likelihood: -46.1082763671875
Adjusted likelihood: -46.1082763671875
Likelihood residual: 0.0
{'index': 46.1082763671875, 'thumb_middle': inf}
Current yaw: tensor([-0.1585, -0.1468, -0.1848], device='cuda:0')
3 index
tensor([-0.0299,  0.2780,  0.6629,  0.8388, -0.0831,  0.8766,  0.9855,  1.0597,
         1.3702,  0.1225,  0.3596,  0.0678, -0.1585, -0.1468, -0.1848, -2.7519],
       device='cuda:0')
Solve time for step 1 10.66679099603789
Current ori: tensor([-0.1585, -0.1468, -0.1848], device='cuda:0')
Middle force: tensor([0.5621, 0.5056, 0.5006, 0.5548], device='cuda:0')
Thumb force: tensor([0.5049, 0.5033, 0.5003, 0.5452], device='cuda:0')
tensor([-0.0503,  0.3634,  0.6731,  0.8268, -0.1052,  0.9495,  0.9179,  1.1172,
         1.3609,  0.1285,  0.3190,  0.0810, -0.1992, -0.1656, -0.0924,  0.1575],
       device='cuda:0')
Solve time for step 2 4.088518442003988
Current ori: tensor([-0.1992, -0.1656, -0.0924], device='cuda:0')
Middle force: tensor([0.5038, 0.5002, 0.5516], device='cuda:0')
Thumb force: tensor([0.5028, 0.5002, 0.5442], device='cuda:0')
tensor([-0.0490,  0.3785,  0.6837,  0.8344, -0.1203,  1.0135,  0.9131,  1.1109,
         1.3652,  0.1294,  0.3525,  0.0616, -0.2222, -0.1420,  0.0164, -5.7676],
       device='cuda:0')
Solve time for step 3 4.148402808990795
Current ori: tensor([-0.2222, -0.1420,  0.0164], device='cuda:0')
Middle force: tensor([0.5005, 0.5488], device='cuda:0')
Thumb force: tensor([0.5003, 0.5411], device='cuda:0')
tensor([-0.0489,  0.4023,  0.6963,  0.8375, -0.1193,  1.0629,  0.9212,  1.0912,
         1.3857,  0.0940,  0.3763,  0.0147, -0.2904, -0.1248,  0.1476,  3.3509],
       device='cuda:0')
Solve time for step 4 3.8895861390046775
Current ori: tensor([-0.2904, -0.1248,  0.1476], device='cuda:0')
Middle force: tensor([0.5455], device='cuda:0')
Thumb force: tensor([0.5403], device='cuda:0')
Storing RECOVERY transition: reward=-0.5103 (scaled=-0.1021), steps=5
Reward stats updated: mean -0.0066 -> -0.0067, std: 0.1247
Collected 649 transitions for RL
SAC Update 1/5: Actor Loss=-0.2724, Q1 Loss=1.0758, Q2 Loss=1.0758, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0226
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3559, Q2 Loss=1.3559, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3372
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=2.4895, Q2 Loss=2.4895, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1430
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.7772, Q2 Loss=0.7772, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2492
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0112, Q2 Loss=1.0112, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1885

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (18.0%)
Q1 update: 0.04s (19.1%)
Q2 update: 0.04s (18.4%)
Actor update: 0.09s (40.9%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.146588
Q1 loss: 1.341932
Q2 loss: 1.341932
Current threshold: -38.3547
Global Scale Offset: 0.0148
Reward stats: mean=-0.0067, std=0.1247, count=649
----------------------------------------------
SAC Update - Actor Loss: -0.1466, Q1 Loss: 1.3419, Q2 Loss: 1.3419, Entropy: 0.0000, Mean TD Error: 0.9881, Threshold: -38.3547
Original likelihood: -341.88348388671875
Adjusted likelihood: -341.88348388671875
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 42
Loaded trajectory sampler
Current yaw: tensor([ 0.0007,  0.0140, -0.0469], device='cuda:0')
Current yaw: tensor([ 0.0007,  0.0140, -0.0469], device='cuda:0')
1 turn
Sampling time 3.6602718220092356
tensor([ 1.5680e-01,  6.0237e-01,  5.8620e-01,  6.0492e-01, -1.0271e-01,
         5.3322e-01,  8.8610e-01,  9.0551e-01,  1.1764e+00,  3.3627e-01,
         3.0596e-01,  1.1513e+00,  6.9266e-04,  1.4032e-02, -4.6945e-02,
         1.9440e-01], device='cuda:0')
Original likelihood: -20.069656372070312
Adjusted likelihood: -20.069656372070312
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.365857467986643
Current ori: tensor([ 0.0007,  0.0140, -0.0469], device='cuda:0')
Middle force: tensor([1.1109, 0.6274, 0.4985, 1.5797, 0.5200, 0.5006, 0.5572, 0.5073, 0.9391,
        1.5854, 0.5063, 0.5985], device='cuda:0')
Thumb force: tensor([1.1813, 1.7558, 0.6923, 0.9607, 0.5982, 0.5254, 0.8404, 1.0767, 0.7285,
        0.9253, 0.6525, 0.5706], device='cuda:0')
Index force: tensor([0.7207, 0.6693, 0.6994, 0.5107, 0.5198, 0.7958, 0.6116, 0.5483, 0.5790,
        0.5294, 0.7098, 0.5706], device='cuda:0')
Storing NORMAL transition: reward=0.0662 (scaled=0.0662), steps=1
Reward stats updated: mean -0.0067 -> -0.0066, std: 0.1246
Collected 650 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.7995, Q2 Loss=0.7995, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3094
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=0.8624, Q2 Loss=0.8624, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4917
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1894, Q2 Loss=1.1894, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7263
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8741, Q2 Loss=0.8741, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5451
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.9975, Q2 Loss=0.9975, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3118

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.4%)
Q1 update: 0.05s (20.1%)
Q2 update: 0.05s (18.6%)
Actor update: 0.11s (40.6%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 0.944583
Q2 loss: 0.944583
Current threshold: -38.3593
Global Scale Offset: 0.0148
Reward stats: mean=-0.0066, std=0.1246, count=650
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 0.9446, Q2 Loss: 0.9446, Entropy: 0.0000, Mean TD Error: 0.8769, Threshold: -38.3593
tensor([ 0.1690,  0.6309,  0.5559,  0.6015, -0.1988,  0.5595,  0.9210,  0.9363,
         1.3333,  0.2234,  0.2398,  1.0687, -0.0313,  0.0316, -0.1151,  0.6016],
       device='cuda:0')
Original likelihood: -27.465412139892578
Adjusted likelihood: -27.465412139892578
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.671622919966467
Current ori: tensor([-0.0313,  0.0316, -0.1151], device='cuda:0')
Middle force: tensor([0.6286, 0.5208, 1.1412, 0.5291, 0.5387, 0.5466, 0.5310, 0.5281, 0.5040,
        0.5502, 0.5480], device='cuda:0')
Thumb force: tensor([0.6599, 1.4897, 1.0754, 0.7025, 1.0164, 0.5751, 0.6043, 1.6811, 0.5516,
        0.6578, 0.6570], device='cuda:0')
Index force: tensor([0.5152, 0.5409, 0.5055, 0.5246, 0.5203, 0.5835, 0.6251, 0.7691, 0.7016,
        0.5682, 0.6069], device='cuda:0')
Storing NORMAL transition: reward=0.0642 (scaled=0.0642), steps=1
Reward stats updated: mean -0.0066 -> -0.0065, std: 0.1246
Collected 651 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.3121, Q2 Loss=1.3121, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5040
SAC Update 2/5: Actor Loss=-0.4350, Q1 Loss=3.4194, Q2 Loss=3.4194, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.4026
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8994, Q2 Loss=0.8994, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7134
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.8009, Q2 Loss=1.8009, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7950
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.1678, Q2 Loss=1.1678, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5729

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (17.8%)
Q1 update: 0.05s (19.3%)
Q2 update: 0.05s (18.2%)
Actor update: 0.11s (41.5%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.086997
Q1 loss: 1.719932
Q2 loss: 1.719932
Current threshold: -38.3620
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1246, count=651
----------------------------------------------
SAC Update - Actor Loss: -0.0870, Q1 Loss: 1.7199, Q2 Loss: 1.7199, Entropy: 0.0000, Mean TD Error: 1.3976, Threshold: -38.3620
tensor([ 0.1188,  0.6174,  0.5035,  0.6443, -0.1650,  0.4948,  0.9153,  1.0822,
         1.3380,  0.0582,  0.2210,  0.9169, -0.0459,  0.0384, -0.1817,  1.0038],
       device='cuda:0')
Original likelihood: -27.176485061645508
Adjusted likelihood: -27.176485061645508
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.205294895044062
Current ori: tensor([-0.0459,  0.0384, -0.1817], device='cuda:0')
Middle force: tensor([1.5580, 0.7416, 0.5238, 0.5007, 0.5944, 0.6308, 0.5002, 0.5168, 0.5791,
        0.5016], device='cuda:0')
Thumb force: tensor([0.6325, 1.2765, 1.0778, 0.6031, 0.5794, 0.5714, 0.6438, 0.6495, 0.5893,
        0.5465], device='cuda:0')
Index force: tensor([0.6313, 0.5473, 0.7371, 0.6200, 0.5289, 0.5430, 0.7403, 0.6090, 0.6190,
        0.7170], device='cuda:0')
Storing NORMAL transition: reward=0.0145 (scaled=0.0145), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1245
Collected 652 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0116, Q2 Loss=1.0116, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0677
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3744, Q2 Loss=1.3744, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3801
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.0713, Q2 Loss=1.0713, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6712
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8864, Q2 Loss=0.8864, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7767
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7679, Q2 Loss=0.7679, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3006

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (17.1%)
Q1 update: 0.04s (18.3%)
Q2 update: 0.04s (18.4%)
Actor update: 0.10s (41.6%)
Target update: 0.00s (2.1%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.022321
Q2 loss: 1.022321
Current threshold: -38.3636
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1245, count=652
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.0223, Q2 Loss: 1.0223, Entropy: 0.0000, Mean TD Error: 0.6392, Threshold: -38.3636
tensor([ 0.0784,  0.5468,  0.5593,  0.6482, -0.1017,  0.5116,  0.8921,  1.0950,
         1.2595,  0.2740,  0.3187,  0.9410, -0.0560,  0.0106, -0.1958,  1.7184],
       device='cuda:0')
Original likelihood: -22.02075958251953
Adjusted likelihood: -22.02075958251953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.151451704034116
Current ori: tensor([-0.0560,  0.0106, -0.1958], device='cuda:0')
Middle force: tensor([1.4865, 0.5314, 0.5012, 0.5619, 0.5145, 0.9123, 1.5205, 0.5111, 0.5916],
       device='cuda:0')
Thumb force: tensor([0.9047, 0.5526, 0.5140, 0.7921, 0.9460, 0.7043, 0.8720, 0.5907, 0.5597],
       device='cuda:0')
Index force: tensor([0.5126, 0.5159, 0.8335, 0.5917, 0.5348, 0.5656, 0.5239, 0.6939, 0.5605],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0042 (scaled=-0.0042), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1244
Collected 653 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.2216, Q2 Loss=1.2216, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8212
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=7.9857, Q2 Loss=7.9857, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=8.5183
SAC Update 3/5: Actor Loss=-0.2268, Q1 Loss=1.4505, Q2 Loss=1.4505, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5772
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.5086, Q2 Loss=1.5086, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4398
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8416, Q2 Loss=0.8416, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4040

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (14.7%)
Q1 update: 0.06s (20.4%)
Q2 update: 0.06s (20.4%)
Actor update: 0.11s (40.7%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.137457
Q1 loss: 2.601585
Q2 loss: 2.601585
Current threshold: -38.3645
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1244, count=653
----------------------------------------------
SAC Update - Actor Loss: -0.1375, Q1 Loss: 2.6016, Q2 Loss: 2.6016, Entropy: 0.0000, Mean TD Error: 2.3521, Threshold: -38.3645
tensor([ 0.0489,  0.5681,  0.4803,  0.6705,  0.0183,  0.4576,  0.9973,  1.0124,
         1.1315,  0.2477,  0.3551,  1.0613, -0.0492, -0.0413, -0.1928,  2.2852],
       device='cuda:0')
Original likelihood: -35.00279998779297
Adjusted likelihood: -35.00279998779297
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.895078039029613
Current ori: tensor([-0.0492, -0.0413, -0.1928], device='cuda:0')
Middle force: tensor([1.2840, 0.5214, 0.6000, 0.5430, 0.5006, 0.5401, 0.5744, 0.6233],
       device='cuda:0')
Thumb force: tensor([0.7613, 0.5185, 0.5154, 0.7977, 0.5014, 1.3115, 0.6091, 0.5709],
       device='cuda:0')
Index force: tensor([0.6027, 0.6510, 0.5623, 0.5049, 0.6929, 0.6374, 0.5444, 0.5462],
       device='cuda:0')
Storing NORMAL transition: reward=0.0224 (scaled=0.0224), steps=1
Reward stats updated: mean -0.0065 -> -0.0064, std: 0.1243
Collected 654 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.0650, Q2 Loss=1.0650, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7844
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.0213, Q2 Loss=1.0213, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1126
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.7614, Q2 Loss=0.7614, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5554
SAC Update 4/5: Actor Loss=-0.2090, Q1 Loss=1.4374, Q2 Loss=1.4374, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7309
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.8135, Q2 Loss=0.8135, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3870

------ SAC Update Summary (5 iterations) ------
Total time: 0.21s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.5%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (19.4%)
Actor update: 0.08s (39.3%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.133901
Q1 loss: 1.019720
Q2 loss: 1.019720
Current threshold: -38.3651
Global Scale Offset: 0.0148
Reward stats: mean=-0.0064, std=0.1243, count=654
----------------------------------------------
SAC Update - Actor Loss: -0.1339, Q1 Loss: 1.0197, Q2 Loss: 1.0197, Entropy: 0.0000, Mean TD Error: 0.5141, Threshold: -38.3651
tensor([-0.0602,  0.6212,  0.3472,  0.6498,  0.0844,  0.3802,  1.0753,  0.9623,
         1.2431,  0.2438,  0.3995,  0.8909, -0.1246,  0.0108, -0.2322,  1.4723],
       device='cuda:0')
Original likelihood: -33.102561950683594
Adjusted likelihood: -33.102561950683594
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.704226857982576
Current ori: tensor([-0.1246,  0.0108, -0.2322], device='cuda:0')
Middle force: tensor([0.5112, 0.6118, 0.5525, 0.5026, 0.5524, 0.5466, 0.6352],
       device='cuda:0')
Thumb force: tensor([0.5256, 0.5092, 0.7783, 0.5025, 1.2838, 1.1047, 0.5610],
       device='cuda:0')
Index force: tensor([0.6461, 0.5563, 0.5026, 0.6653, 0.6030, 0.5170, 0.5717],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0102 (scaled=-0.0102), steps=1
Reward stats updated: mean -0.0064 -> -0.0064, std: 0.1242
Collected 655 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.4110, Q2 Loss=1.4110, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.5868
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0653, Q2 Loss=1.0653, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8907
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.8433, Q2 Loss=0.8433, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2281
SAC Update 4/5: Actor Loss=-0.1282, Q1 Loss=1.9048, Q2 Loss=1.9048, Entropy=0.3080, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.5728
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2131, Q2 Loss=1.2131, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1245

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (20.6%)
Q1 update: 0.04s (18.3%)
Q2 update: 0.04s (18.5%)
Actor update: 0.09s (39.1%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.071682
Q1 loss: 1.287514
Q2 loss: 1.287514
Current threshold: -38.3717
Global Scale Offset: 0.0148
Reward stats: mean=-0.0064, std=0.1242, count=655
----------------------------------------------
SAC Update - Actor Loss: -0.0717, Q1 Loss: 1.2875, Q2 Loss: 1.2875, Entropy: 0.0616, Mean TD Error: 1.6806, Threshold: -38.3717
tensor([-0.1676,  0.6805,  0.4263,  0.5636, -0.0106,  0.4139,  0.9686,  1.0249,
         1.2327,  0.2516,  0.4521,  0.9534, -0.2198,  0.0391, -0.2743,  0.6472],
       device='cuda:0')
Original likelihood: -65.42538452148438
Adjusted likelihood: -65.42538452148438
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 59.27114486694336
Projection step: 1, Loss: 61.63142395019531
Projection step: 2, Loss: 64.26345825195312
Projection step: 3, Loss: 68.6188735961914
Projection step: 4, Loss: 70.78450012207031
Projection step: 5, Loss: 60.99797058105469
Projection step: 6, Loss: 62.310150146484375
Projection step: 7, Loss: 62.42021942138672
Projection step: 8, Loss: 62.90913391113281
Projection step: 9, Loss: 63.30233383178711
Projection step: 10, Loss: 59.406494140625
Projection step: 11, Loss: 64.78173828125
Projection step: 12, Loss: 59.10734558105469
Projection step: 13, Loss: 54.38031005859375
Projection step: 14, Loss: 63.98076248168945
Projection step: 15, Loss: 63.59364318847656
Projection step: 16, Loss: 63.424720764160156
Projection step: 17, Loss: 65.19051361083984
Projection step: 18, Loss: 63.01718521118164
Projection step: 19, Loss: 58.99375534057617
Projection step: 20, Loss: 58.2730712890625
Projection step: 21, Loss: 58.62776184082031
Projection step: 22, Loss: 60.6719970703125
Projection step: 23, Loss: 61.08702850341797
Projection step: 24, Loss: 63.400787353515625
Final likelihood: tensor([-61.0339, -58.5822, -66.4256, -69.6145, -59.4996, -60.2306, -44.6072,
        -81.8227, -40.4078, -72.8941, -81.1448, -49.0486, -60.9311, -52.6789,
        -59.4833, -46.4571])
Final projection likelihood: -60.3039
1 mode projection failed, trying anyway
New goal: tensor([-0.1437,  0.6416,  0.3961,  0.5483, -0.0040,  0.4211,  0.9479,  1.0386,
         1.2390,  0.2970,  0.4573,  0.8926, -0.2172,  0.0395, -0.1721],
       device='cuda:0')
tensor([[0.0117]], device='cuda:0') tensor([[0.0149]], device='cuda:0') tensor([[0.0026]], device='cuda:0')
Original likelihood: -60.77149963378906
Adjusted likelihood: -60.77149963378906
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 60.77149963378906}
Current yaw: tensor([-0.2198,  0.0391, -0.2743], device='cuda:0')
2 thumb_middle
tensor([-0.1676,  0.6805,  0.4263,  0.5636, -0.0106,  0.4139,  0.9686,  1.0249,
         1.2327,  0.2516,  0.4521,  0.9534, -0.2198,  0.0391, -0.2743,  0.6472],
       device='cuda:0')
Solve time for step 1 8.89510716201039
Current ori: tensor([-0.2198,  0.0391, -0.2743], device='cuda:0')
Index force: tensor([0.5834, 0.5866, 0.6067, 0.5045], device='cuda:0')
tensor([-0.1372,  0.7668,  0.4624,  0.5682, -0.0298,  0.4979,  0.9371,  0.9973,
         1.1995,  0.2486,  0.4503,  0.9184, -0.5468,  0.0933, -0.2743,  0.5656],
       device='cuda:0')
Solve time for step 2 3.635993812989909
Current ori: tensor([-0.5468,  0.0933, -0.2743], device='cuda:0')
Index force: tensor([0.5512, 0.5462, 0.4999], device='cuda:0')
tensor([-0.1945,  0.9055,  0.5300,  0.5635, -0.0456,  0.5591,  0.9446,  0.9908,
         1.2845,  0.3117,  0.4493,  0.8932, -1.2272,  0.1709, -0.2742,  2.5225],
       device='cuda:0')
Solve time for step 3 3.524230573035311
Current ori: tensor([-1.2272,  0.1709, -0.2742], device='cuda:0')
Index force: tensor([0.5000, 0.5281], device='cuda:0')
tensor([-0.2927,  1.0528,  0.4647,  0.7409, -0.1645,  0.6150,  1.0589,  1.0384,
         1.2861,  0.3263,  0.4459,  0.9061, -1.9346,  0.1631, -0.2742,  2.2137],
       device='cuda:0')
Solve time for step 4 3.398894048004877
Current ori: tensor([-1.9346,  0.1631, -0.2742], device='cuda:0')
Index force: tensor([0.5457], device='cuda:0')
Storing RECOVERY transition: reward=-1.7493 (scaled=-0.2915), steps=6
Reward stats updated: mean -0.0064 -> -0.0069, std: 0.1246
Collected 656 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8782, Q2 Loss=0.8782, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5298
SAC Update 2/5: Actor Loss=-0.0594, Q1 Loss=0.9356, Q2 Loss=0.9356, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7990
SAC Update 3/5: Actor Loss=-0.2350, Q1 Loss=1.1150, Q2 Loss=1.1150, Entropy=0.2856, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2877
SAC Update 4/5: Actor Loss=-0.1947, Q1 Loss=1.3350, Q2 Loss=1.3350, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4614
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.7951, Q2 Loss=1.7951, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8538

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (15.6%)
Q1 update: 0.05s (19.7%)
Q2 update: 0.05s (20.7%)
Actor update: 0.10s (40.8%)
Target update: 0.00s (1.3%)
Priority update: 0.00s (0.1%)
Actor loss: -0.097811
Q1 loss: 1.211786
Q2 loss: 1.211786
Current threshold: -38.3864
Global Scale Offset: 0.0148
Reward stats: mean=-0.0069, std=0.1246, count=656
----------------------------------------------
SAC Update - Actor Loss: -0.0978, Q1 Loss: 1.2118, Q2 Loss: 1.2118, Entropy: 0.0571, Mean TD Error: 0.9863, Threshold: -38.3864
Original likelihood: -1296.13134765625
Adjusted likelihood: -1296.13134765625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 43
Loaded trajectory sampler
Current yaw: tensor([-0.0017,  0.0144, -0.0273], device='cuda:0')
Current yaw: tensor([-0.0017,  0.0144, -0.0273], device='cuda:0')
1 turn
Sampling time 3.762229070009198
tensor([ 0.1436,  0.6266,  0.5199,  0.6427, -0.1499,  0.5715,  0.9129,  0.8750,
         1.2733,  0.2447,  0.2082,  1.2162, -0.0017,  0.0144, -0.0273,  0.0421],
       device='cuda:0')
Original likelihood: -25.236740112304688
Adjusted likelihood: -25.236740112304688
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.456509827985428
Current ori: tensor([-0.0017,  0.0144, -0.0273], device='cuda:0')
Middle force: tensor([0.5871, 0.5742, 1.1857, 0.5656, 1.1510, 0.6526, 0.5383, 0.5205, 0.5145,
        0.8725, 0.5009, 0.5905], device='cuda:0')
Thumb force: tensor([0.8787, 0.8180, 0.7540, 1.0852, 0.9840, 0.6504, 0.5226, 0.9176, 0.5387,
        0.5289, 0.6321, 0.6149], device='cuda:0')
Index force: tensor([0.5921, 0.6039, 0.5559, 0.5713, 0.8081, 0.5232, 1.0295, 0.9401, 0.5820,
        0.5973, 0.5514, 0.6052], device='cuda:0')
Storing NORMAL transition: reward=0.0227 (scaled=0.0227), steps=1
Reward stats updated: mean -0.0069 -> -0.0068, std: 0.1245
Collected 657 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.8641, Q2 Loss=1.8641, Entropy=0.0000, Time=0.08sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8711
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.2044, Q2 Loss=1.2044, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7810
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3677, Q2 Loss=1.3677, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1575
SAC Update 4/5: Actor Loss=-0.2220, Q1 Loss=1.4071, Q2 Loss=1.4071, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3808
SAC Update 5/5: Actor Loss=-0.1008, Q1 Loss=1.1895, Q2 Loss=1.1895, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2383

------ SAC Update Summary (5 iterations) ------
Total time: 0.27s, Avg iteration: 0.05s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (18.1%)
Q1 update: 0.05s (18.7%)
Q2 update: 0.05s (18.6%)
Actor update: 0.11s (41.0%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.064576
Q1 loss: 1.406549
Q2 loss: 1.406549
Current threshold: -38.3969
Global Scale Offset: 0.0148
Reward stats: mean=-0.0068, std=0.1245, count=657
----------------------------------------------
SAC Update - Actor Loss: -0.0646, Q1 Loss: 1.4065, Q2 Loss: 1.4065, Entropy: 0.0000, Mean TD Error: 1.2857, Threshold: -38.3969
tensor([ 1.5866e-01,  6.3965e-01,  5.2208e-01,  6.2721e-01, -2.4258e-01,
         5.7003e-01,  1.0191e+00,  8.9034e-01,  1.2774e+00,  2.5736e-01,
         2.3215e-01,  1.1967e+00,  6.0180e-04,  2.2944e-02, -5.0322e-02,
        -3.7013e-01], device='cuda:0')
Original likelihood: -34.54027557373047
Adjusted likelihood: -34.54027557373047
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.8830545310047455
Current ori: tensor([ 0.0006,  0.0229, -0.0503], device='cuda:0')
Middle force: tensor([0.5686, 1.1772, 0.5612, 1.1306, 0.6495, 0.5361, 0.5129, 0.5124, 0.8588,
        0.5007, 0.5896], device='cuda:0')
Thumb force: tensor([0.7988, 0.7364, 1.0650, 0.9740, 0.6520, 0.5200, 0.9469, 0.5392, 0.5270,
        0.6292, 0.6077], device='cuda:0')
Index force: tensor([0.5942, 0.5478, 0.5646, 0.7803, 0.5192, 1.0065, 0.9175, 0.5715, 0.5884,
        0.5457, 0.5977], device='cuda:0')
Storing NORMAL transition: reward=0.0593 (scaled=0.0593), steps=1
Reward stats updated: mean -0.0068 -> -0.0067, std: 0.1244
Collected 658 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=4.3997, Q2 Loss=4.3997, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.0834
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.1377, Q2 Loss=1.1377, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1499
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.6870, Q2 Loss=1.6870, Entropy=0.0000, Time=0.11sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2302
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3037, Q2 Loss=1.3037, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3663
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.7787, Q2 Loss=0.7787, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2422

------ SAC Update Summary (5 iterations) ------
Total time: 0.34s, Avg iteration: 0.07s
Sampling: 0.00s (0.5%)
Target Q: 0.05s (14.7%)
Q1 update: 0.06s (17.5%)
Q2 update: 0.10s (30.2%)
Actor update: 0.12s (34.5%)
Target update: 0.00s (1.2%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.861374
Q2 loss: 1.861374
Current threshold: -38.4031
Global Scale Offset: 0.0148
Reward stats: mean=-0.0067, std=0.1244, count=658
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.8614, Q2 Loss: 1.8614, Entropy: 0.0000, Mean TD Error: 1.4144, Threshold: -38.4031
tensor([ 0.2140,  0.5786,  0.4691,  0.6801, -0.1870,  0.6582,  0.9050,  0.9238,
         1.2842,  0.2696,  0.1528,  1.1716, -0.0380, -0.0109, -0.1109,  0.3032],
       device='cuda:0')
Original likelihood: -34.7801399230957
Adjusted likelihood: -34.7801399230957
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.299777177977376
Current ori: tensor([-0.0380, -0.0109, -0.1109], device='cuda:0')
Middle force: tensor([1.4056, 0.5799, 0.5650, 0.5639, 0.5086, 0.5120, 0.5405, 1.8320, 0.5943,
        0.5045], device='cuda:0')
Thumb force: tensor([1.7594, 0.7378, 1.0500, 0.5761, 0.6508, 0.8501, 1.9370, 0.5561, 0.6052,
        0.5803], device='cuda:0')
Index force: tensor([0.5531, 0.5542, 0.5940, 0.5791, 0.6903, 0.9114, 0.6118, 0.6076, 0.6148,
        0.5618], device='cuda:0')
Storing NORMAL transition: reward=0.1520 (scaled=0.1520), steps=1
Reward stats updated: mean -0.0067 -> -0.0065, std: 0.1245
Collected 659 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.5774, Q2 Loss=1.5774, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9271
SAC Update 2/5: Actor Loss=-0.0348, Q1 Loss=1.3301, Q2 Loss=1.3301, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=3.1998
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.3240, Q2 Loss=1.3240, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5724
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=0.9616, Q2 Loss=0.9616, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6780
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0812, Q2 Loss=1.0812, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6868

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (18.3%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (18.7%)
Actor update: 0.09s (40.0%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.053012
Q1 loss: 1.254866
Q2 loss: 1.254866
Current threshold: -38.4068
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1245, count=659
----------------------------------------------
SAC Update - Actor Loss: -0.0530, Q1 Loss: 1.2549, Q2 Loss: 1.2549, Entropy: 0.0000, Mean TD Error: 1.4129, Threshold: -38.4068
tensor([ 0.1893,  0.6053,  0.5722,  0.6908, -0.1656,  0.6445,  0.9289,  0.9856,
         1.3433,  0.2718,  0.1233,  1.0070, -0.0537, -0.0229, -0.2671,  0.7452],
       device='cuda:0')
Original likelihood: -31.2523250579834
Adjusted likelihood: -31.2523250579834
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 5.076392661023419
Current ori: tensor([-0.0537, -0.0229, -0.2671], device='cuda:0')
Middle force: tensor([1.0470, 0.5415, 0.5191, 0.5753, 0.5538, 0.7475, 0.5672, 1.1053, 0.5553],
       device='cuda:0')
Thumb force: tensor([1.2490, 0.8806, 0.6336, 0.5778, 0.5382, 0.7541, 0.5425, 0.5603, 0.6115],
       device='cuda:0')
Index force: tensor([0.5219, 0.5424, 0.6058, 0.5064, 0.6353, 0.7792, 0.5330, 0.6086, 0.5119],
       device='cuda:0')
Storing NORMAL transition: reward=0.0884 (scaled=0.0884), steps=1
Reward stats updated: mean -0.0065 -> -0.0064, std: 0.1245
Collected 660 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8551, Q2 Loss=0.8551, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4712
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.0217, Q2 Loss=1.0217, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7146
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.7419, Q2 Loss=0.7419, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2255
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.3555, Q2 Loss=1.3555, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1287
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=3.9421, Q2 Loss=3.9421, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=7.5782

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (21.1%)
Q1 update: 0.05s (20.2%)
Q2 update: 0.05s (18.6%)
Actor update: 0.09s (36.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.2%)
Actor loss: -0.184207
Q1 loss: 1.583296
Q2 loss: 1.583296
Current threshold: -38.4090
Global Scale Offset: 0.0148
Reward stats: mean=-0.0064, std=0.1245, count=660
----------------------------------------------
SAC Update - Actor Loss: -0.1842, Q1 Loss: 1.5833, Q2 Loss: 1.5833, Entropy: 0.0000, Mean TD Error: 1.8236, Threshold: -38.4090
tensor([ 0.1791,  0.5828,  0.5333,  0.8014, -0.1749,  0.6400,  0.8949,  1.0735,
         1.4263,  0.2305,  0.1063,  0.9186, -0.0495, -0.0138, -0.3562,  0.9717],
       device='cuda:0')
Original likelihood: -29.01529884338379
Adjusted likelihood: -29.01529884338379
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 5.072722084005363
Current ori: tensor([-0.0495, -0.0138, -0.3562], device='cuda:0')
Middle force: tensor([0.5367, 0.5218, 0.5762, 0.5516, 0.7379, 0.5648, 1.0842, 0.5525],
       device='cuda:0')
Thumb force: tensor([0.8680, 0.6193, 0.5726, 0.5348, 0.7488, 0.5392, 0.5566, 0.6137],
       device='cuda:0')
Index force: tensor([0.5386, 0.5952, 0.5054, 0.6311, 0.7655, 0.5306, 0.6035, 0.5097],
       device='cuda:0')
Storing NORMAL transition: reward=0.0151 (scaled=0.0151), steps=1
Reward stats updated: mean -0.0064 -> -0.0063, std: 0.1244
Collected 661 transitions for RL
SAC Update 1/5: Actor Loss=-0.1129, Q1 Loss=3.1657, Q2 Loss=3.1657, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.1154
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=2.2201, Q2 Loss=2.2201, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9189
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.9702, Q2 Loss=0.9702, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9986
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.3391, Q2 Loss=1.3391, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6585
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.4708, Q2 Loss=1.4708, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5730

------ SAC Update Summary (5 iterations) ------
Total time: 0.25s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.05s (19.0%)
Q1 update: 0.05s (19.2%)
Q2 update: 0.05s (18.7%)
Actor update: 0.10s (39.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.114685
Q1 loss: 1.833178
Q2 loss: 1.833178
Current threshold: -38.4102
Global Scale Offset: 0.0148
Reward stats: mean=-0.0063, std=0.1244, count=661
----------------------------------------------
SAC Update - Actor Loss: -0.1147, Q1 Loss: 1.8332, Q2 Loss: 1.8332, Entropy: 0.0000, Mean TD Error: 1.6529, Threshold: -38.4102
tensor([ 0.0973,  0.5171,  0.5606,  0.7359, -0.1081,  0.7289,  0.6650,  1.0874,
         1.4313,  0.2851,  0.0976,  0.8289, -0.0786, -0.0252, -0.3839,  1.4266],
       device='cuda:0')
Original likelihood: -23.077835083007812
Adjusted likelihood: -23.077835083007812
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.596536627970636
Current ori: tensor([-0.0786, -0.0252, -0.3839], device='cuda:0')
Middle force: tensor([0.5035, 1.0631, 0.7487, 0.5090, 0.5388, 0.5841, 0.7099],
       device='cuda:0')
Thumb force: tensor([0.6117, 0.9168, 0.5554, 0.5335, 1.1828, 0.5618, 0.5551],
       device='cuda:0')
Index force: tensor([0.6546, 1.1654, 0.5033, 0.5531, 0.5219, 0.5950, 0.5412],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0171 (scaled=-0.0171), steps=1
Reward stats updated: mean -0.0063 -> -0.0063, std: 0.1243
Collected 662 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=2.1489, Q2 Loss=2.1489, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0962
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8412, Q2 Loss=0.8412, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9800
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=0.8472, Q2 Loss=0.8472, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4418
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.8827, Q2 Loss=0.8827, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3202
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2835, Q2 Loss=1.2835, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4470

------ SAC Update Summary (5 iterations) ------
Total time: 0.26s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.7%)
Q1 update: 0.05s (18.1%)
Q2 update: 0.05s (19.4%)
Actor update: 0.11s (42.5%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.200704
Q2 loss: 1.200704
Current threshold: -38.4110
Global Scale Offset: 0.0148
Reward stats: mean=-0.0063, std=0.1243, count=662
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.2007, Q2 Loss: 1.2007, Entropy: 0.0000, Mean TD Error: 1.0570, Threshold: -38.4110
tensor([ 0.2076,  0.5280,  0.5974,  0.8920, -0.1598,  0.7713,  0.6963,  0.8891,
         1.4555,  0.2565, -0.1044,  0.7988, -0.0935, -0.0688, -0.3839,  1.0341],
       device='cuda:0')
Original likelihood: -40.69287872314453
Adjusted likelihood: -40.69287872314453
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 38.63865280151367
Projection step: 1, Loss: 37.3992919921875
Projection step: 2, Loss: 36.387725830078125
Projection step: 3, Loss: 35.54844284057617
Projection step: 4, Loss: 37.83533477783203
Projection step: 5, Loss: 38.39387512207031
Projection step: 6, Loss: 33.301177978515625
Projection step: 7, Loss: 35.64839172363281
Projection step: 8, Loss: 35.076637268066406
Projection step: 9, Loss: 36.99758529663086
Projection step: 10, Loss: 34.52360153198242
Projection step: 11, Loss: 34.77864456176758
Projection step: 12, Loss: 35.097023010253906
Projection step: 13, Loss: 36.053035736083984
Projection step: 14, Loss: 35.23585510253906
Projection step: 15, Loss: 35.10334014892578
Projection step: 16, Loss: 34.56243133544922
Projection step: 17, Loss: 36.32105255126953
Projection step: 18, Loss: 31.783836364746094
Projection step: 19, Loss: 36.68084716796875
Projection step: 20, Loss: 34.335487365722656
Projection step: 21, Loss: 34.35883331298828
Projection step: 22, Loss: 36.50733184814453
Projection step: 23, Loss: 36.92195510864258
Projection step: 24, Loss: 32.91504669189453
Final likelihood: tensor([-39.3390, -44.2539, -23.8676, -39.0325, -45.1306, -40.3716, -31.1347,
        -40.9595, -39.3579, -28.3776, -29.3076, -25.7981, -42.7167, -35.7169,
        -34.9113, -39.8715])
Final projection likelihood: -36.2592
1 mode projection succeeded
New goal: tensor([ 0.2148,  0.5048,  0.5959,  0.9388, -0.1064,  0.7940,  0.8073,  0.9543,
         1.4443,  0.1944, -0.0604,  0.8485, -0.0925, -0.0640, -0.2994],
       device='cuda:0')
tensor([[0.0128]], device='cuda:0') tensor([[0.0117]], device='cuda:0') tensor([[0.0020]], device='cuda:0')
Original likelihood: -35.60801315307617
Adjusted likelihood: -35.60801315307617
Likelihood residual: 0.0
{'index': inf, 'thumb_middle': 35.60801315307617}
Current yaw: tensor([-0.0935, -0.0688, -0.3839], device='cuda:0')
2 thumb_middle
tensor([ 0.2076,  0.5280,  0.5974,  0.8920, -0.1598,  0.7713,  0.6963,  0.8891,
         1.4555,  0.2565, -0.1044,  0.7988, -0.0935, -0.0688, -0.3839,  1.0341],
       device='cuda:0')
Solve time for step 1 9.224920261010993
Current ori: tensor([-0.0935, -0.0688, -0.3839], device='cuda:0')
Index force: tensor([0.5542, 0.6019, 0.6069, 0.5591], device='cuda:0')
tensor([ 0.2158,  0.5432,  0.6134,  0.9357, -0.2024,  0.7658,  0.7573,  0.9251,
         1.4370,  0.2005, -0.1109,  0.8276, -0.1362, -0.1136, -0.3671,  1.2911],
       device='cuda:0')
Solve time for step 2 3.740802281012293
Current ori: tensor([-0.1362, -0.1136, -0.3671], device='cuda:0')
Index force: tensor([0.5934, 0.5958, 0.5531], device='cuda:0')
tensor([ 0.1679,  0.5673,  0.6501,  0.9518, -0.1916,  0.7818,  0.7748,  0.9386,
         1.4173,  0.1906, -0.1624,  0.8080, -0.2352, -0.1900, -0.3073,  2.8415],
       device='cuda:0')
Solve time for step 3 3.5173486029962078
Current ori: tensor([-0.2352, -0.1900, -0.3073], device='cuda:0')
Index force: tensor([0.5762, 0.5435], device='cuda:0')
tensor([ 0.1140,  0.6540,  0.6598,  0.9438, -0.1514,  0.8253,  0.8011,  0.9455,
         1.3859,  0.1843, -0.1891,  0.7830, -0.3176, -0.2263, -0.1745,  3.6100],
       device='cuda:0')
Solve time for step 4 3.3537778640165925
Current ori: tensor([-0.3176, -0.2263, -0.1745], device='cuda:0')
Index force: tensor([0.5403], device='cuda:0')
Storing RECOVERY transition: reward=-0.5470 (scaled=-0.0912), steps=6
Reward stats updated: mean -0.0063 -> -0.0065, std: 0.1242
Collected 663 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.9648, Q2 Loss=0.9648, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4323
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.4206, Q2 Loss=1.4206, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.1199
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=1.1786, Q2 Loss=1.1786, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0900
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.3049, Q2 Loss=1.3049, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8166
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.0271, Q2 Loss=1.0271, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.5310

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (18.2%)
Q1 update: 0.04s (18.5%)
Q2 update: 0.04s (18.0%)
Actor update: 0.09s (41.3%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: 0.000000
Q1 loss: 1.179178
Q2 loss: 1.179178
Current threshold: -38.4115
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1242, count=663
----------------------------------------------
SAC Update - Actor Loss: 0.0000, Q1 Loss: 1.1792, Q2 Loss: 1.1792, Entropy: 0.0000, Mean TD Error: 0.9980, Threshold: -38.4115
Original likelihood: -320.71929931640625
Adjusted likelihood: -320.71929931640625
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Probably dropped the object
Marked last transition as done (episode completed)
Marked last transition as done (final step)
{}

Trial 44
Loaded trajectory sampler
Current yaw: tensor([-0.0029,  0.0146, -0.0296], device='cuda:0')
Current yaw: tensor([-0.0029,  0.0146, -0.0296], device='cuda:0')
1 turn
Sampling time 3.7026622950215824
tensor([ 0.1789,  0.6043,  0.5930,  0.6307, -0.1003,  0.5356,  0.9182,  0.8284,
         1.2412,  0.3263,  0.2053,  1.2032, -0.0029,  0.0146, -0.0296,  0.2485],
       device='cuda:0')
Original likelihood: -19.6787109375
Adjusted likelihood: -19.6787109375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 1 14.358439943986014
Current ori: tensor([-0.0029,  0.0146, -0.0296], device='cuda:0')
Middle force: tensor([1.7995, 0.5939, 0.5152, 1.0257, 0.5284, 0.5162, 0.5540, 0.5353, 0.5490,
        0.4729, 0.8588, 0.4771], device='cuda:0')
Thumb force: tensor([1.0608, 0.7263, 1.5351, 1.1658, 0.7466, 1.1150, 0.5769, 0.5682, 1.6269,
        0.6361, 0.7261, 0.4969], device='cuda:0')
Index force: tensor([0.6607, 0.5240, 0.5666, 0.5005, 0.5431, 0.5132, 0.5900, 0.6872, 0.8430,
        0.8663, 0.5949, 0.5717], device='cuda:0')
Storing NORMAL transition: reward=-0.0478 (scaled=-0.0478), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1241
Collected 664 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=0.9733, Q2 Loss=0.9733, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.7943
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.0497, Q2 Loss=1.0497, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4163
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.8618, Q2 Loss=0.8618, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2341
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.5108, Q2 Loss=1.5108, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7878
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=0.9683, Q2 Loss=0.9683, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.4927

------ SAC Update Summary (5 iterations) ------
Total time: 0.30s, Avg iteration: 0.06s
Sampling: 0.00s (0.7%)
Target Q: 0.05s (16.2%)
Q1 update: 0.05s (16.4%)
Q2 update: 0.05s (15.3%)
Actor update: 0.10s (32.6%)
Target update: 0.00s (1.4%)
Priority update: 0.00s (0.1%)
Actor loss: -0.046052
Q1 loss: 1.072756
Q2 loss: 1.072756
Current threshold: -38.4117
Global Scale Offset: 0.0148
Reward stats: mean=-0.0065, std=0.1241, count=664
----------------------------------------------
SAC Update - Actor Loss: -0.0461, Q1 Loss: 1.0728, Q2 Loss: 1.0728, Entropy: 0.0000, Mean TD Error: 0.9450, Threshold: -38.4117
tensor([ 0.1337,  0.5220,  0.6138,  0.7245, -0.0637,  0.5861,  0.8547,  0.8709,
         1.1794,  0.3673,  0.2536,  1.1325, -0.0050, -0.0201,  0.0181,  1.0703],
       device='cuda:0')
Original likelihood: -19.72304916381836
Adjusted likelihood: -19.72304916381836
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 2 5.60184197302442
Current ori: tensor([-0.0050, -0.0201,  0.0181], device='cuda:0')
Middle force: tensor([0.5717, 1.1700, 0.5727, 1.1778, 0.6483, 0.5439, 0.5499, 0.5158, 0.5722,
        0.5054, 0.5639], device='cuda:0')
Thumb force: tensor([0.8748, 0.7597, 1.0454, 0.9396, 0.6322, 0.5265, 0.7824, 0.5296, 0.6098,
        0.5146, 0.6049], device='cuda:0')
Index force: tensor([0.6000, 0.5641, 0.5734, 0.8240, 0.5319, 1.0003, 0.9032, 0.5904, 0.5748,
        0.8399, 0.6345], device='cuda:0')
Storing NORMAL transition: reward=-0.0374 (scaled=-0.0374), steps=1
Reward stats updated: mean -0.0065 -> -0.0066, std: 0.1240
Collected 665 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2058, Q2 Loss=1.2058, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3317
SAC Update 2/5: Actor Loss=-0.2303, Q1 Loss=1.2098, Q2 Loss=1.2098, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6338
SAC Update 3/5: Actor Loss=-0.1645, Q1 Loss=1.4301, Q2 Loss=1.4301, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.1875
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=1.0798, Q2 Loss=1.0798, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4299
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=2.6438, Q2 Loss=2.6438, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=4.1581

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (1.3%)
Target Q: 0.05s (19.5%)
Q1 update: 0.05s (19.5%)
Q2 update: 0.04s (18.3%)
Actor update: 0.09s (38.1%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.078955
Q1 loss: 1.513869
Q2 loss: 1.513869
Current threshold: -38.4119
Global Scale Offset: 0.0148
Reward stats: mean=-0.0066, std=0.1240, count=665
----------------------------------------------
SAC Update - Actor Loss: -0.0790, Q1 Loss: 1.5139, Q2 Loss: 1.5139, Entropy: 0.0000, Mean TD Error: 1.3482, Threshold: -38.4119
tensor([ 0.1907,  0.5478,  0.6799,  0.6412, -0.1079,  0.6033,  0.9662,  0.9200,
         1.2002,  0.2800,  0.1789,  1.2017, -0.0097, -0.0462,  0.0540,  1.1332],
       device='cuda:0')
Original likelihood: -30.367576599121094
Adjusted likelihood: -30.367576599121094
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 3 5.256994404015131
Current ori: tensor([-0.0097, -0.0462,  0.0540], device='cuda:0')
Middle force: tensor([1.1546, 0.5715, 1.1901, 0.6418, 0.5473, 0.5467, 0.5145, 0.5708, 0.5041,
        0.5617], device='cuda:0')
Thumb force: tensor([0.7392, 1.0296, 0.9043, 0.6276, 0.5222, 0.7783, 0.5281, 0.6061, 0.5130,
        0.6029], device='cuda:0')
Index force: tensor([0.5610, 0.5692, 0.8062, 0.5296, 0.9839, 0.8850, 0.5853, 0.5703, 0.8447,
        0.6270], device='cuda:0')
Storing NORMAL transition: reward=0.0177 (scaled=0.0177), steps=1
Reward stats updated: mean -0.0066 -> -0.0065, std: 0.1240
Collected 666 transitions for RL
SAC Update 1/5: Actor Loss=-0.1056, Q1 Loss=1.1459, Q2 Loss=1.1459, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9956
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8909, Q2 Loss=0.8909, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5952
SAC Update 3/5: Actor Loss=-0.2446, Q1 Loss=2.0021, Q2 Loss=2.0021, Entropy=0.2035, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8121
SAC Update 4/5: Actor Loss=-0.0279, Q1 Loss=0.9754, Q2 Loss=0.9754, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.1061
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.8010, Q2 Loss=0.8010, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.4159

------ SAC Update Summary (5 iterations) ------
Total time: 0.22s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.7%)
Q1 update: 0.04s (18.9%)
Q2 update: 0.04s (19.0%)
Actor update: 0.09s (40.4%)
Target update: 0.00s (1.7%)
Priority update: 0.00s (0.2%)
Actor loss: -0.121663
Q1 loss: 1.163032
Q2 loss: 1.163032
Current threshold: -38.4247
Global Scale Offset: 0.0147
Reward stats: mean=-0.0065, std=0.1240, count=666
----------------------------------------------
SAC Update - Actor Loss: -0.1217, Q1 Loss: 1.1630, Q2 Loss: 1.1630, Entropy: 0.0407, Mean TD Error: 1.1850, Threshold: -38.4247
tensor([ 0.2195,  0.5405,  0.7096,  0.6532, -0.1337,  0.6704,  0.8492,  0.9730,
         1.3014,  0.2444,  0.0745,  1.2314, -0.0334, -0.0327,  0.0363,  1.2801],
       device='cuda:0')
Original likelihood: -31.627857208251953
Adjusted likelihood: -31.627857208251953
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 4 4.906281932024285
Current ori: tensor([-0.0334, -0.0327,  0.0363], device='cuda:0')
Middle force: tensor([1.0279, 0.5351, 0.5216, 0.5509, 0.5345, 0.5458, 0.5060, 0.8386, 0.5006],
       device='cuda:0')
Thumb force: tensor([1.0734, 0.6903, 1.0106, 0.5664, 0.5498, 1.5436, 0.6791, 0.6966, 0.5019],
       device='cuda:0')
Index force: tensor([0.5004, 0.5351, 0.5104, 0.5797, 0.6788, 0.8116, 0.8706, 0.5877, 0.6624],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0005 (scaled=-0.0005), steps=1
Reward stats updated: mean -0.0065 -> -0.0065, std: 0.1239
Collected 667 transitions for RL
SAC Update 1/5: Actor Loss=-0.2303, Q1 Loss=1.4740, Q2 Loss=1.4740, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6828
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=1.3582, Q2 Loss=1.3582, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8486
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.0645, Q2 Loss=1.0645, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.8296
SAC Update 4/5: Actor Loss=0.0000, Q1 Loss=0.9114, Q2 Loss=0.9114, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9785
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.6846, Q2 Loss=1.6846, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.0259

------ SAC Update Summary (5 iterations) ------
Total time: 0.20s, Avg iteration: 0.04s
Sampling: 0.00s (0.7%)
Target Q: 0.04s (17.9%)
Q1 update: 0.04s (19.0%)
Q2 update: 0.04s (18.6%)
Actor update: 0.08s (40.6%)
Target update: 0.00s (1.6%)
Priority update: 0.00s (0.2%)
Actor loss: -0.092103
Q1 loss: 1.298541
Q2 loss: 1.298541
Current threshold: -38.4388
Global Scale Offset: 0.0147
Reward stats: mean=-0.0065, std=0.1239, count=667
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.2985, Q2 Loss: 1.2985, Entropy: 0.0000, Mean TD Error: 1.2731, Threshold: -38.4388
tensor([ 0.3083,  0.5353,  0.7835,  0.7204, -0.0658,  0.5920,  0.8009,  0.9202,
         1.3349,  0.1904,  0.0070,  1.1532, -0.0194, -0.0855,  0.0321,  1.5047],
       device='cuda:0')
Original likelihood: -33.12445068359375
Adjusted likelihood: -33.12445068359375
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 5 4.730154666991439
Current ori: tensor([-0.0194, -0.0855,  0.0321], device='cuda:0')
Middle force: tensor([0.5030, 0.8398, 0.5005, 1.0601, 0.5437, 0.5210, 0.5188, 0.5866],
       device='cuda:0')
Thumb force: tensor([0.7998, 0.8793, 1.1122, 0.7857, 0.5141, 0.5930, 0.5939, 0.5575],
       device='cuda:0')
Index force: tensor([0.6636, 0.6396, 0.6878, 0.5099, 0.5329, 0.5937, 0.5129, 0.5497],
       device='cuda:0')
Storing NORMAL transition: reward=0.0491 (scaled=0.0491), steps=1
Reward stats updated: mean -0.0065 -> -0.0064, std: 0.1238
Collected 668 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=1.2061, Q2 Loss=1.2061, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.9273
SAC Update 2/5: Actor Loss=-0.0178, Q1 Loss=0.8571, Q2 Loss=0.8571, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.8989
SAC Update 3/5: Actor Loss=-0.2303, Q1 Loss=1.2835, Q2 Loss=1.2835, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.7872
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=2.2434, Q2 Loss=2.2434, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.9384
SAC Update 5/5: Actor Loss=0.0000, Q1 Loss=1.2500, Q2 Loss=1.2500, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.3778

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (16.9%)
Q1 update: 0.05s (19.8%)
Q2 update: 0.04s (18.0%)
Actor update: 0.10s (41.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.095658
Q1 loss: 1.368053
Q2 loss: 1.368053
Current threshold: -38.4471
Global Scale Offset: 0.0147
Reward stats: mean=-0.0064, std=0.1238, count=668
----------------------------------------------
SAC Update - Actor Loss: -0.0957, Q1 Loss: 1.3681, Q2 Loss: 1.3681, Entropy: 0.0000, Mean TD Error: 1.3859, Threshold: -38.4471
tensor([ 0.2591,  0.5104,  0.7644,  0.7094, -0.0457,  0.6332,  0.8410,  0.9378,
         1.3349,  0.2616,  0.0893,  1.0502, -0.0216, -0.0535, -0.0136,  1.4730],
       device='cuda:0')
Original likelihood: -27.156085968017578
Adjusted likelihood: -27.156085968017578
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 6 4.587933254952077
Current ori: tensor([-0.0216, -0.0535, -0.0136], device='cuda:0')
Middle force: tensor([0.5033, 0.5222, 1.1917, 0.8752, 0.7932, 0.5034, 0.5208],
       device='cuda:0')
Thumb force: tensor([0.6017, 0.8433, 0.7704, 1.1325, 0.7310, 0.5050, 0.6168],
       device='cuda:0')
Index force: tensor([0.5090, 0.6030, 0.8504, 1.1955, 0.5624, 0.6490, 0.6044],
       device='cuda:0')
Storing NORMAL transition: reward=-0.0026 (scaled=-0.0026), steps=1
Reward stats updated: mean -0.0064 -> -0.0064, std: 0.1237
Collected 669 transitions for RL
SAC Update 1/5: Actor Loss=-0.3760, Q1 Loss=1.1949, Q2 Loss=1.1949, Entropy=0.0000, Time=0.06sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5723
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=0.8790, Q2 Loss=0.8790, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3671
SAC Update 3/5: Actor Loss=-0.2233, Q1 Loss=1.5998, Q2 Loss=1.5998, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.0791
SAC Update 4/5: Actor Loss=-0.1810, Q1 Loss=1.5182, Q2 Loss=1.5182, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2582
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.9215, Q2 Loss=0.9215, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.5781

------ SAC Update Summary (5 iterations) ------
Total time: 0.23s, Avg iteration: 0.05s
Sampling: 0.00s (0.8%)
Target Q: 0.04s (19.5%)
Q1 update: 0.04s (19.3%)
Q2 update: 0.04s (18.9%)
Actor update: 0.09s (38.0%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.202124
Q1 loss: 1.222667
Q2 loss: 1.222667
Current threshold: -38.4521
Global Scale Offset: 0.0147
Reward stats: mean=-0.0064, std=0.1237, count=669
----------------------------------------------
SAC Update - Actor Loss: -0.2021, Q1 Loss: 1.2227, Q2 Loss: 1.2227, Entropy: 0.0000, Mean TD Error: 0.7709, Threshold: -38.4521
tensor([ 0.2852,  0.6550,  0.6531,  0.6262, -0.0394,  0.6859,  0.9357,  0.8805,
         1.3227,  0.2719, -0.0040,  1.0787, -0.0547, -0.0994, -0.0198,  1.6564],
       device='cuda:0')
Original likelihood: -37.260284423828125
Adjusted likelihood: -37.260284423828125
Likelihood residual: 0.0
Stochastic decision: ID (sampled from p=1.0000)
Solve time for step 7 4.634273633011617
Current ori: tensor([-0.0547, -0.0994, -0.0198], device='cuda:0')
Middle force: tensor([0.5216, 1.1988, 0.9258, 0.7870, 0.5035, 0.5280], device='cuda:0')
Thumb force: tensor([0.8292, 0.7701, 1.0482, 0.7258, 0.5042, 0.5886], device='cuda:0')
Index force: tensor([0.6150, 0.8537, 1.1670, 0.5594, 0.6454, 0.6001], device='cuda:0')
Storing NORMAL transition: reward=-0.0044 (scaled=-0.0044), steps=1
Reward stats updated: mean -0.0064 -> -0.0064, std: 0.1236
Collected 670 transitions for RL
SAC Update 1/5: Actor Loss=0.0000, Q1 Loss=0.8825, Q2 Loss=0.8825, Entropy=0.0000, Time=0.07sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.6972
SAC Update 2/5: Actor Loss=0.0000, Q1 Loss=2.2607, Q2 Loss=2.2607, Entropy=0.0000, Time=0.05sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=2.2215
SAC Update 3/5: Actor Loss=0.0000, Q1 Loss=0.7205, Q2 Loss=0.7205, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.2157
SAC Update 4/5: Actor Loss=-0.2303, Q1 Loss=1.4446, Q2 Loss=1.4446, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=1.2056
SAC Update 5/5: Actor Loss=-0.2303, Q1 Loss=0.7989, Q2 Loss=0.7989, Entropy=0.0000, Time=0.04sActor IP Change=0.00000000, Critic IP Change 1=0.00000000, Critic IP Change 2=0.00000000, Mean TD Error=0.3034

------ SAC Update Summary (5 iterations) ------
Total time: 0.24s, Avg iteration: 0.05s
Sampling: 0.00s (0.6%)
Target Q: 0.04s (17.6%)
Q1 update: 0.05s (20.9%)
Q2 update: 0.05s (19.4%)
Actor update: 0.09s (38.7%)
Target update: 0.00s (1.5%)
Priority update: 0.00s (0.1%)
Actor loss: -0.092103
Q1 loss: 1.221441
Q2 loss: 1.221441
Current threshold: -38.4550
Global Scale Offset: 0.0147
Reward stats: mean=-0.0064, std=0.1236, count=670
----------------------------------------------
SAC Update - Actor Loss: -0.0921, Q1 Loss: 1.2214, Q2 Loss: 1.2214, Entropy: 0.0000, Mean TD Error: 0.9287, Threshold: -38.4550
tensor([ 0.3654,  0.7809,  0.5681,  0.5406, -0.1345,  0.7744,  0.9879,  0.8742,
         1.3064,  0.3326, -0.0121,  0.9665, -0.0996, -0.1365, -0.0301,  1.6426],
       device='cuda:0')
Original likelihood: -40.44794464111328
Adjusted likelihood: -40.44794464111328
Likelihood residual: 0.0
Stochastic decision: OOD (sampled from p=0.0000)
State is out of distribution
Projection step: 0, Loss: 40.225486755371094
Projection step: 1, Loss: 39.34791946411133
Projection step: 2, Loss: 39.922767639160156
Projection step: 3, Loss: 39.674617767333984
Projection step: 4, Loss: 42.23462677001953
[1;34mwandb[0m: 🚀 View run [33mallegro_screwdriver_recovery_data_orig_lower_proj_lr_2025-03-17-17-42-48[0m at: [34mhttps://wandb.ai/abhinavk99/ccai-screwdriver/runs/36awdyr0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250317_174248-36awdyr0/logs[0m
